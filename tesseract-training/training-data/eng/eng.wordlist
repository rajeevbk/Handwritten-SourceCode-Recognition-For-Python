=
def
if
return
import
from
not
==
in
class
is
None:
%
for
None
name
assert
else:
value
elif
options=None,
{
params
_descriptor.EnumValueDescriptor(
type=None),
name,
}
pass
raise
)
+
__init__(self,
response
{}
containing_type=None,
with
and
as
try:
connection):
or
],
True
_descriptor.FieldDescriptor(
has_default_value=False,
message_type=None,
enum_type=None,
is_extension=False,
extension_scope=None,
options=None),
+=
[]
except
label=1,
False
None)
*
params,
value,
default_value=0,
},
1
'
0
s
\
to
attrs,
key
1)
body
self.set_http_response(status_code=200)
b
rv
#
result
request,
value)
app
startElement(self,
body=json.dumps(params))
"
[
**kwargs):
endElement(self,
a
unittest
the
yield
response,
'Action':
d
flask.Flask(__name__)
i
path='/',
setUp(self):
self.assert_request_parameters({
'true'
default_body(self):
**kw):
})
value):
dry_run=False):
__repr__(self):
!=
params=params)
connection_class
>
setattr(self,
None,
-
headers
params)
%s'
%s
region
verb='POST',
request
k
self.build_list_params(params,
type=5,
cpp_type=1,
while
boto
self._make_request(
self.name
item
x
*args,
of
marker
else
be
self.spider)
v
data
url
**kwargs)
response)
self.id
r
boto.compat
key,
True)
response.status
spider):
body)
e:
c
boto.exception
]
])
{'S':
dry_run:
params['DryRun']
__name__
connection
verb='POST')
0)
1,
args
req
message
(
bucket
self.assertEqual(
rs
filename=None,
rv.data
tests.unit
os
scrapy.exceptions
response):
%s"
file=DESCRIPTOR,
index=0,
self.connection
response.reason,
time
@property
val
headers=None,
status
2)
''
False)
'SignatureVersion',
path
2
self.name,
'Timestamp',
number=1,
kw,
>=
))
@classmethod
'SignatureMethod',
0:
results
dict(
'username':
ignore_params_values=['AWSAccessKeyId',
uri
headers=None):
index=1,
0,
del
'Version'])
headers,
DESCRIPTOR
isinstance(value,
warnings
next_token
self.status
max_records
number=2,
_descriptor.Descriptor(
fields=[
extensions=[
enum_types=[
is_extendable=False,
extension_ranges=[],
oneofs=[
(_message.Message,),
__module__
unittest.main()
1:
h
connection=None):
@defer.inlineCallbacks
l
headers=headers)
<
f
headers=headers,
json
index():
app.test_client()
api_response
e
int(value)
nested_types=[],
self._post_request(request,
six
lambda
t
expected
datetime
'__main__':
msg
spider
@app.route('/')
{'N':
n
response.read()
scrapy.http
obj
break
200:
type=11,
cpp_type=10,
p
uri,
boto.regioninfo
/
settings
path,
domain_name,
regions():
must
spider)
:
params['Marker']
except:
<=
1024
description
ScrapyDeprecationWarning
stacklevel=2)
crawler
index=2,
found
headers)
search
'RpcSub_pb2'
**kw)
[])
port,
deprecated,
aws_secret_access_key=None,
resp
f:
sys
request):
fp
file
2,
self).setUp()
tests.compat
'Hello
k,
name)
action,
response.status,
number=3,
cpp_type=9,
host,
policy
name):
10,
limit
i,
ImportError:
ScrapyDeprecationWarning,
ignore_params_values=['Version'])
logging
this
'',
name=None,
cls
res
conn
connection=None,
default_value=None,
on
error
query
'date_joined':
tearDown(self):
AWSMockServiceTestCase
instead",
domain,
o
'')
params['MaxRecords']
'first_name':
x:
port
size
{'name':
marker=None):
retval
warnings.warn("Module
default
NotConfigured
class:
max_records=None,
self.description
__future__
twisted.internet
VPCConnection
5,
self
{})
body=body)
aws_secret_access_key,
key_name
prefix
r1
index=3,
"use
continue
200
lx
self.bucket
JSONResponseError
finally:
_
host
_required_auth_capability(self):
boto.connection
re
logger
extra={'spider':
True,
domain
&
fp,
validate=False,
self.region
'last_name':
logging.getLogger(__name__)
handler
six.string_types):
scrapy.utils.python
3
scheme
self.size
type=14,
cpp_type=8,
'Version':
now
Request
by
filename
attr
connection)
region.name
str(
mock.Mock()
use
self)
Field()
number=4,
region_name:
'AttributeName':
john
spider=spider)
getattr(self,
server
'true':
h)
scrapy
'status':
{},
config
self.state
**kw_params):
{'DomainName':
scrapy.spiders
crawler):
spider,
False,
kwargs
bytes):
test
3)
dry_run=dry_run
SearchConnection(endpoint=HOSTNAME)
Response
tests
5
app.test_request_context():
type=3,
cpp_type=2,
mock.patch.object(
Spider
log
values
items
an
"__main__":
_buildresponse(
provider
RegionInfo
boto.utils
['hmac-v4']
boto.log.debug(body)
key_name,
instance_id,
id
index=4,
region.connect(**kw_params)
text
self:
key):
start
self.type
command
instance
4
::
APIVersion
connect_to_region(region_name,
time.sleep(5)
table_name,
'friend_count':
args,
StringIO
mock
self.config
value:
type=1,
cpp_type=5,
region:
self.connection.provider.storage_response_error(
method
time.time()
[('item',
output
endpoint
type
type=9,
acl
application_name,
match
item,
protocol
method,
int(time.time())
self.message
kwargs['host']
fs
port=None,
rv.status_code
list):
BotoServerError
self.region.endpoint,
to_xml(self):
True:
does
dfd
obj,
ValueError:
link
rule
m
S3Connection
tag
warnings.catch_warnings(record=True)
10)
bp
index=5,
default_value=_b("").decode('utf-8'),
get_regions
DefaultRegionName
max_items
description=None,
params['NextToken']
'false'
filters=None,
query_args
print_function
reason
3,
w:
self.mw.process_response(req,
exceptions
self.make_request('GET',
i]
prop
filters:
part
group
print
cb,
attrs
s1
was
il
"foo",',
number=5,
DefaultRegionEndpoint
AWSQueryConnection
next_token=None):
ResultSet([('member',
'AttributeType':
attributes
regions
__init__(self):
reactor,
debug
will
six.moves.urllib.parse
None):
hostname
are
self.value
self.assertEqual(1,
l:
rsp
'foo',
flask
RegionInfo,
xml
document
self.base_domain,
got
KeyError:
10
'version':
),
boto.config.get('Boto',
self.DefaultRegionName,
Request,
no
self.spider
url,
from_crawler(cls,
data):
db
running
r2
c.get('/')
app.test_client().get('/')
app.testing
version_id
region_name
boto.resultset
ResultSet
len(rs)
self.vpc_id
k.generation
self.users.connection,
deferred
_,
requests
Exception
boto.s3.connection
aws_access_key_id=None,
params):
number=6,
index=6,
number=7,
label=2,
IAMConnection
conn):
action
self.instance_id
RegionInfo(self,
identity_pool_id,
instance_id
self.build_filter_params(params,
headers=DEVPAY_HEADERS)
version
settings):
timeout
self).__init__(*args,
q
'/',
base64
doc
parent
'Content-Type':
rsp,
u'value':
'This
'false',
42
version_id=None,
marker:
json.loads(response_body)
max_results
'Value':
rs:
k.name
"urllib.request"),
module
loc
reason):
self).__init__()
encoding
code
out
u'name':
20)
debug=0,
ResponseError
cb=None,
self.DefaultRegionEndpoint)
marker=None,
value.lower()
'true',
'write':
self).__init__(connection)
dry_run=dry_run)
fp.close()
headers=NOT_IMPL,
DocumentServiceConnection(
self.assert_request_parameters(
'rb')
__iter__(self):
it
headers:
'domain':
tags
info):
c:
req2
self.assertEqual(len(w),
mw.process_request(req,
,
decorator
__name__)
%s',
response.read().decode('utf-8')
'read':
filters)
self.build_complex_list_params(
endpoint="doc-demo-userdomain.us-east-1.cloudsearch.amazonaws.com")
line
*a,
arg
set()
template
'name':
AttributeError:
only
path)
auth
'cached'
end
'POST')
'Content-Length':
20,
err
version="1.0"
self.request_class.from_response(response,
<input
self.response_class("http://www.example.com",
il.add_value('name',
type=12,
EC2Connection
proxy=None,
proxy_port=None,
xml.sax.parseString(body,
self).startElement(name,
num_cb,
attr):
'KeyType':
part_size
sample_service_call(self,
all
spider})
exception
reason,
(200,
at
data)
called
None),
delete(self):
bucket_name
is_secure=True,
self).__init__(response)
next_token=None,
self.id,
ec2
{'Action':
tests.integration
ServiceCertVerificationTest
ServiceCertVerificationTest):
-=
attribute
_:
isinstance(response,
mw
timeout=None):
body):
scrapy.utils.test
using
__str__(self):
key)
bucket,
tempfile
hashlib
path):
self._state
data,
5)
_descriptor.EnumDescriptor(
values=[
number=0,
index=7,
index=8,
index=9,
default_value=_b(""),
label=3,
default_value=[],
config,
security_token=None,
proxy_port,
self.get_bucket(validate,
limit=None,
boto.ec2.ec2object
'Name':
Element(ComplexMoney)
"some
request)
entry
',
result,
action='store_true',
content
valid
status,
filename,
tags,
30,
(key,
name.lower()
state
'GET',
text=u'sample
md5
500
'id':
Layer1
is_secure,
proxy,
num_cb=10,
s3
'VpcId':
@api_action()
self.get_object(action,
item_type
"urllib2",
self.assertEqual(len(response),
self.bucket.new_key("k")
"title":
"category":
self.assertEquals(api_response,
scrapy.utils.misc
opts):
six.PY2:
fields
self.code
object()
""
links
threading
%s,
html
"%s"
set([
but
description,
self.assertEqual(lx.extract_links(response),
g
debug,
time.sleep(60)
number=10,
six,
profile_name=None):
make_request(self,
self.request_id
self.timestamp
next_token:
datetime.strptime(value,
'ComparisonOperator':
Fields
key_id,
self._value
stack_id,
'aws_secret_access_key':
'aws_access_key_id':
{'SS':
'jane',
self._MakeBucket()
k.set_contents_from_string(s1)
{"id":
node
urlparse
defer
signals
self.crawler
s:
%d
new
Thread
metadata
cls,
60
100
header
tmpdir
method='POST',
instances
lip,
f):
rv.close()
'true')
6
xml.sax
member
[('member',
id,
'WriteCapacityUnits':
availability_zone
0.5)
'johndoe'},
"Title
self.get_args(HTTPretty.last_request.raw_requestline)
Python
hasattr(self,
already
queue
count
methods
one
items,
'w')
endpoint,
func
self.port
default=None):
'%s'
s2
'host'
record
self.download_request(request,
req,
'foo')
login
'%s.%s'
loader
get_new_coords(loc,
index=10,
number=9,
cpp_type=4,
'RpcEnvelope_pb2'
https_connection_factory=None,
'Message':
"us-east-1"
region.endpoint
self).__init__(**kwargs)
region=None,
profile_name=profile_name)
'Exists':
group_name,
vpc_id,
cluster
user_name,
'DBInstanceIdentifier':
4)
"urllib.parse"),
1",
'johndoe',
b.get_key("foo")
'0.0.0.0/0',
factory
2:
can
that
field
(name,
body=body,
chunk
first
self.name)
Request)
ctx
url)
date
'wb')
copy
should
'default')
have
keys
twisted.trial
str(w[0].message))
'bar')
text=u''),
self.assertEqual(l.get_output_value('name'),
{u'id':
80,
name=None):
%s>'
table
app.register_blueprint(bp,
RDSConnection
proxy_user=None,
proxy_pass=None,
proxy_user,
self.bucket_name
_faults
self.make_request('POST',
self.ResponseError)
self.domain
condition
label
(self.name,
id=None,
'PolicyName':
'Description':
log_group_name,
HTTPRequest(
"1234",
["cat_a",
"cat_b",
"AttributeName":
self.assertEqual(p.access_key,
self.assertEqual(p.secret_key,
pytest
collections
.
scrapy.item
available
when
dict)
Exception:
row
enabled
Response,
default=None,
than
HTTP
url):
location
'.'
Item
self.parent
self.prefix
'name',
LogCapture()
_qs(req)
StringIO()
World!'
flask.Blueprint('bp',
self._auth_provider
number=8,
number=11,
aws_access_key_id=aws_access_key_id,
aws_secret_access_key=aws_secret_access_key,
bucket_name,
validate_certs=True,
validate_certs=validate_certs,
self.version_id
'StackName':
domain_name):
dynamodb_type
name}
zone
'AutoScalingGroupName':
policy_arn,
'RouteTableId':
{'username':
'friends':
mock,
reactor
scrapy.settings
field_name,
six.text_type):
set
headers):
response:
run(self,
result)
supported
URL
delay
version,
start_time
self.conn
v)
self.host
self.response
b''
isinstance(body,
you
deprecated
proc
buf
global
resource,
data:
get_crawler
TestItem(Item):
self.assertEqual(il.get_output_value('name'),
self,
step
sender=None,
http_request
proxy_pass,
parent=None):
token
duration
query_args=query_args,
self.key
expected_status=200,
max_items=None):
params['Description']
'deletes':
self.availability_zone
'>'
{'LoadBalancerName':
upload_id,
queue,
stream_name,
cluster_identifier,
"urllib",
self.my_cb_cnt
self.my_cb_last
self.assertTrue(isinstance(response,
document.commit()
search.search(q='Test',
'arn:aws:iam::123456789012:policy/S3-read-only-example-bucket',
twisted_version
stats
has
kwargs:
[],
body,
defer.Deferred()
factory,
absolute_import
spider=None):
shutil
functools
BytesIO
type,
'https',
username,
URI
settings)
'foo'
mw:
}),
'foo':
self.assertRaises(TypeError):
nofollow=False),
443,
@setupmethod
url_prefix='/py')
thread
latitude
google.protobuf
index=11,
index=12,
**kwargs
lname
https_connection_factory,
self.uri)
version_id=None):
permission,
res_upload_handler=res_upload_handler)
description=None):
params['MaxItems']
Key
kwargs.pop('region',
json_body
fault_name
body=json_body)
doc_path
self.get_response(doc_path,
identity_id,
'S':
'adds':
availability_zone=None,
self.owner_id
self.zone
ResultSet([('item',
self.connection.make_request('GET',
stack_id
db_instance_identifier,
consistent_read=True)
throughput={
'bob',
self.assertRaisesRegexp(GSResponseError,
VERSION_MISMATCH):
self.assertEqual(self.my_cb_last,
'ContentType':
'JSON',
document.add("1234",
"cat_c"]})
self.assertEqual(next(results),
os.path
app,
self.settings
option
io
dict):
list
kw
exc_value,
cb
None))
'value':
self.uri
"%s
policy=None,
self[key]
's3':
body:
msg)
')
urllib
required
'Host':
self._process_requestresponse(mw,
'baz':
self.assertRaises(ValueError,
user
'The
(%s)'
%d'
uuid
rv.mimetype
bool)
1):
default=False)
provider)
AutoScaleConnection
dst_key
end_time
max_records:
self.domain_name
response_body
boto.log.debug(response_body)
exception_class
self._faults.get(fault_name,
exception_class(response.status,
{'TableName':
'ReadCapacityUnits':
self.instance_type
source_type
res_upload_handler
annotation
'Doe'},
cb=harness.call,
dst_key.set_contents_from_file(
version_id=NOT_IMPL):
self.set_http_response(status_code=200,
'username',
"KeyType":
return_value={})
provider.Provider('aws')
parse(self,
parser
Settings
{'request':
item):
Failure
message,
failure
TestCase
task
slot
ValueError
cc
socket
[x
ret
attr,
results,
'/'
out,
self.mw
new_item
self.spider))
404
isinstance(e,
(%s)
index
app)
Pokemon
swLng,
neLat,
lat
lon
longitude
.select()
.dicts())
3.0
(i
index=13,
index=14,
index=15,
number=12,
default_value=False,
type=2,
cpp_type=6,
self._mexe(http_request,
%s)'
boto.log.debug("Using
fp.tell()
'Value'))
1)]
self.enabled
domain_name
self.etag
'DomainName':
attributes_to_get=None,
data_type
'IndexName':
'PUT',
'state':
self.ec2
attribute,
'InstanceId':
TaggedEC2Object
{'UserName':
@requires(['MarketplaceId',
g1
route_table_id,
simple
print('---
HashKey('username'),
k.metageneration
key_name="foo",
self._MakeKey(set_contents=False)
'stack_name',
mock_response.read.return_value
'date_joined',
self.set_http_response(200)
text,
8
Request(url,
%s\n"
more
self.debug
get
fragment='',
exception,
subject,
*args):
(lambda
spider))
longer
after
find
filename)
[1,
d:
Headers()
message):
'?'
('http',
u
uri)
2.0
v:
*processors,
update(self,
delete(self,
r:
self.scheme
description:
Attribute(
other
access
str(len(body)),
'bar'
'bar',
b,
self.assertEqual([link
body=html)
were
data={
attr_name
attr_name,
app.debug
//
s[::-1]
next
time.sleep(10)
MTurkConnection
Route53Connection
http_request):
retry
self.owner
email_address,
key_name='',
'Status':
ServiceName
action),
params['nextToken']
schema,
'Key':
'Item':
filters
'AttributeValueList':
attributes,
'results':
instance_id=None,
{'InstanceId':
instance_type
policy_name,
preferred_maintenance_window
pipeline_id
tracker
cb_count
Model
question
get_as_xml(self):
type_name
prop,
tt
workflow_id,
{'AttributeName':
---')
b.new_key("foo")
self.assertEqual(SMALL_KEY_SIZE,
harness
'2010-05-15',
self.assertEqual(next(self.results),
@attr(sqs=True)
Scrapy
run
sys.exit(1)
scrapy.commands
",
cls()
case
__getitem__(self,
%r
twisted.python.failure
ssl
action="store_true",
argument
into
len(args)
string
specify
close(self):
self.slot
exc
context
where
self.headers
BytesIO()
bytes
aws_access_key_id,
False):
ttl
get(self,
__call__(self,
results:
provided
prefix,
six.text_type)
lines
parts
endpoints
protocol,
This
self.assertEqual(0,
{'foo':
res0)
newresponse
r3
self.assertEqual(expected,
'utf-8')
self.assertEqual(fs,
qs
u'marta')
[u'Marta'])
30)
'1',
top
_request_ctx_stack.top
self.record_once(lambda
direction
DoubleField()
pi)
RPC
index=16,
index=17,
index=18,
index=19,
index=20,
index=21,
index=22,
type=8,
cpp_type=7,
EmrConnection
BotoClientError
self.instances
response_body:
json_body.get('__type',
'fields':
boto.jsonresponse.XmlHandler(e,
h.parse(body)
domain_name}
hash_key,
'KeySchema':
return_consumed_capacity
attributes=None,
propget.get(prop)
instance_ids,
volume_id,
volume
vpc_id
engine_version
'description',
upload
Element()
self.provider)
verbose_name=None,
'DestinationCidrBlock':
"skipping
self.assertEqual(None,
parts=[
'Doe',
'johndoe')
['alice',
dry_run=False
time()
root
requires
ScrapyCommand
load_object
your
NotImplementedError
values):
self.protocol
to_bytes
__getattr__(self,
message)
cfg
d.addCallback(lambda
local
obj):
short_desc(self):
process
runner
"%s"'
isinstance(x,
names
self.__class__.__name__
scheme,
[value]
b'application/x-www-form-urlencoded')
element
encoded
base_url
info)
last_modified
checksum
args:
limit=None):
object
1024)
second
range(0,
'test',
6)
27
res,
req0,
encoding='utf-8')
i2
'text/html'
{'default':
2'),
sel
self.assertEquals(None,
60)
v1
'a
"#000000",
"bold
comment
page
403
400
y
boto.ec2.connection
uri_str
param
verb,
self.comment
AWS_HEADER_PREFIX
replace=True,
query_params
params=query_params)
self.make_request('DELETE',
self).__init__(aws_access_key_id,
self.tags
(self.Version,
max_items:
self.caller_reference
TargetPrefix
'X-Amz-Target':
(self.TargetPrefix,
params={},
override_num_retries=10)
field_type
'ProvisionedThroughput':
'S'
'RANGE'
params['Limit']
self._limit
filter
{'GroupName':
self.groups
topic,
lb
'ClusterId':
vault_name,
part_size,
self.bucket.connection.provider
ResumableUploadException
ml_model_id,
qualification_type_id,
vars()
filters,
p_name
choices=None,
self).__init__(verbose_name,
exponent
"urlparse",
'test-%d'
test")
Amazon
self._MakeTempDir()
@attr(route53=True)
ResourceRecordSets(self.conn,
self.zone.id)
hc
self.assertEqual(response,
validate=NOT_IMPL,
k.set_contents_from_file(sfp,
'title':
'Document
search.search(q='Test')
"username",
return_value=expected)
env
info
settings=None):
category=ScrapyDeprecationWarning,
self.encoding
file,
support
'item':
defer,
Request):
default_settings
args[0]
load
open(filename,
client
uri):
download
request.method
engine
priority
meta
number
())
environment
(prefix,
func(*args,
mod
'%s'"
password):
server,
pairs
current
type=float)
self.assertEqual(2,
resource
200)
Spider('foo')
password
(False,
ie
self.assertRaises(TypeError,
rows,
Link(url='http://example.com/sample2.html',
files
'test
Language
Flask
@pytest.fixture
methods=['POST'])
decorator(f):
}}
fake
app.app_context():
Server
b'Hello
6,
map
basestring):
False:
index=23,
index=24,
index=25,
index=26,
index=27,
index=28,
index=29,
number=14,
number=16,
name='latitude',
name='longitude',
Layer2
capability
auth_path
body.encode('utf-8')
signature
security_token,
80)
resp.status
security_token=security_token,
(label,
self.ResponseError(response.status,
prefix='',
version_id)
acl_str,
val)
spos
AWSAuthConnection
json.loads(body)
self._build_list_params(params,
"Description":
self.origin
self.id)
'application/x-amz-json-1.1',
self.build_base_http_request(
auth_path='/',
ip):
default,
bool):
pipeline_id,
json_input
json.dumps(data)
boto.sdb.db.property
attribute:
ValueError('%s
params['AvailabilityZone']
group_name
self.image_id
{'AutoScalingGroupName':
'Tags':
lb_name,
self.storage_class
ResumableTransferDisposition.ABORT)
{'PolicyArn':
{'KeyId':
use_method=True)
self.template
@requires(['NextToken'])
@api_action('Reports',
subscription_name,
('Key',
db_name
db_instance_class
S3ResponseError
transition
required=False,
self.assertIsInstance(result,
Linux
'John',
'alice',
"test1"
self._MakeVersionedBucket()
rrs
response[0]
self.assertTrue(self.my_cb_cnt
self.region.connect(aws_access_key_id='access_key',
Mock()
self.assertEqual(api_response,
\n'
'Result
'true'},
'2009-03-31'
'PolicyArn':
Bucket(self.service_connection,
'mybucket')
suppress_consec_slashes=False)
0):
**options)
3:
NOQA
ScrapyDeprecationWarning)
csv
to_bytes,
TypeError:
to_unicode
to_native_str
section,
request.callback
default)
priority='cmdline')
Command(ScrapyCommand):
importlib
import_module
created
self.args
wrapper
self.args:
logged
processing
random
Headers
parsed
delimiter
waiting
self.body
self.path
'no
HtmlResponse
cookies
expires
f,
sep
|
el
used
key_name)
'default':
self[name]
isinstance(val,
@staticmethod
text)
s,
groups
os.environ:
fp:
self.data
[None]
interface
before
'-c',
Response),
self.mw.process_request(req,
self.yesterday}),
{'Cache-Control':
self.yesterday,
os.environ
headers={'Location':
isinstance(key,
{'Content-Type':
headers={"Content-type":
Link(url='http://example.com/sample1.html',
default:
TestItemLoader()
self.assertEqual(l.get_output_value('url'),
(0,
4,
DeprecatedName)
self.assertRaises(StopIteration,
i)
1'
timedelta
funcs
between
blueprint
self.options
types
tb):
@app.teardown_request
@app.route('/',
'POST',
content_type='application/json')
id=None):
(i,
7
boto.vpc
pos
next_sleep
self.endpoint
self.append(value)
expected_status=200)
{'ApplicationName':
'%Y-%m-%dT%H:%M:%SZ')
data=body)
arn
attributes_to_get
range_key
read_units,
'N':
throughput
self.table_name,
index,
status=None,
iops
pre]
image_id,
params['Name']
ls]
snaps
from_port
to_port
duration=None,
bucket=None,
policy_name):
add_attrs_from(func,
vault_name
completed
bucket_name)
key.delete()
user_name}
print('\t%s'
self._instance:
15,
25,
@api_action('OffAmazonPayments',
db_subnet_group_name
option_group_name
'Tags.member',
identifier=None,
g2
validator=None,
validate(self,
required,
validator,
choices,
unique)
self.item_type
'1366056668'},
dst_fp
res_download_handler
res_download_handler=res_download_handler)
rrs.commit()
content)
mock.Mock(),
'success',
'rank':
Item(self.users,
self.assertEqual(len(results._results),
next(results)
ignore_params_values=[
'Filter.1.Name':
'Filter.1.Value.1':
ignore_params_values=['Version',
self.assertEqual(p.security_token,
boto.storage_uri(uri_str,
Selector
Item,
Field
six.moves
signal
spidercls
self.stats
lambda:
self.reason
options,
'utf-8'
scrapy.utils.conf
bases,
isinstance(v,
(self.__class__.__name__,
str):
self.url
other):
instead',
open_spider(self,
timeout)
IndexError:
w3lib.url
response.request
connect(self,
scrapy.utils.spider
requires_project
syntax(self):
subprocess
exc_type,
False}
without
getattr(spider,
{'url':
source
self.start_time
%r"
spider=self.spider)
()
urlparse_cached
respcls
-1
b'')
self._body
content_type
unique=False):
a:
function
too
300,
arguments
following
300:
length
order
e)
query,
address
did
os.environ['http_proxy']
j
self.call('startproject',
req1
Request('http://scrapytest.org/')
self._middleware()
(True,
idx,
Request('http://scrapytest.org')
exported
self.assertEqual(exported,
{'egg':
''},
self.assertRaises(KeyError,
str)
self.assertEqual(r.url,
self.request_class("http://www.example.com",
_qs(r1)
<option
TestItem()
text=u'Item
[u'marta'])
self.assertEqual(il.get_collected_values('name'),
{'key':
TestItemLoader(response=self.response)
str(w[-1].message))
[(True,
retcls
datetime.now()
'b',
w
u'1',
45)
follow_redirects=True)
b'the
[]).append(f))
external
':
decode
datetime,
app.secret_key
my_reverse(s):
World!',
get_args()
swLat,
'false')
'latitude':
'longitude':
pokemons
query:
p['latitude'],
itteration
math
response_dict
'code':
'google':
number=13,
number=15,
number=19,
number=21,
number=23,
_REQUEST
type=6,
type=4,
RpcEnum_pb2._POKEMONID
os.path.join(tmpdir,
SQSConnection
config.get('Credentials',
headers_to_sign
scope
'us-east-1'
data='',
boto.handler.XmlHandler(rs,
'instanceId':
%s",
delimiter='',
policy,
acl_or_str,
k]
data=json.dumps(params),
get_regions(
'%Y-%m-%dT%H:%M:%S.%fZ')
etag,
204:
self.dns_name
ip)
statement
{'Name':
max_results=None,
boto.sqs.message
table,
exclusive_start_key
schema
'Projection':
'ProjectionType':
expression_attribute_names
'last_key':
private_ip_address=None,
iops=None,
'Attribute':
attribute.lower()
params['InstanceId']
snapshot
group_id
cidr_ip=None,
cidr_ip
self.location
rs[0]
validate:
'availabilityZone':
**params):
auto_minor_version_upgrade
subnet_ids,
input
result_queue,
more_results
self.bucket:
role_name,
grant_tokens
kn
@api_action('Products',
{'StackId':
boto.config.get("Trac",
tags=None):
self.connection.make_request('PUT',
control
'workflowId':
o['decisionType']
self._data.append(o)
'NetworkAclId':
SimpleModel.all()
assert(query.count()
S3Connection()
k.set_contents_from_string(s1,
'0'},
'HASH',
'John'},
'jane'},
small_src_key
dst_fp,
self.fail('Did
small_src_file_as_string,
small_src_file
self.make_small_file()
dst_key.size)
b.get_acl("foo",
tries
inst
cb=NOT_IMPL,
num_cb=NOT_IMPL,
headers=NOT_IMPL):
Mock
self.assertEqual(len(result.deleted),
self.assertEqual(len(result.errors),
self.conn.list_closed_workflow_executions(self._domain,
aws_secret_access_key='secret',
{'par1':
'par2':
'baz'},
self.set_http_response(status_code=201)
self.service_connection.create_domain('demo')
"HASH"
self.connection_class(
aws_access_key_id='less',
'j-123',
'rtb-g8ff4ea2',
language
match:
start_requests(self):
__all__
print("
scrapy.utils.log
start(self,
named
request:
field,
isinstance(item,
item)
warnings.warn(
__setitem__(self,
__len__(self):
str
%(request)s
close_spider(self,
mimetype
try
add_options(self,
parser):
instead
module_name
range(1,
file:
shell
dfd.addBoth(lambda
scrapy.utils.httpobj
port):
80
proxy
handlers
failed
m:
process_response(self,
process_request(self,
redirected
Deferred()
self.secret_key
session
3600
1)[0]
'':
size=None):
v,
update
keys,
setattr(obj,
IOError
isinstance(o,
self.persistent
'_'
port)
headers.copy()
getarg(request,
0),
os.path.join(os.path.dirname(__file__),
0.1
log)
self.results)
Spider('foo'))
skip
headers={
username
mock.MagicMock()
resp,
self.mw.process_response(request,
sig
hdrs
self.assertRaisesRegexp(ValueError,
["text/html;
property
self.extractor_cls()
lx.extract_links(self.response)],
Link(url='http://example.com/sample3.html',
1',
TakeFirst(),
'hello',
pipeline
pipe_attr,
install
attr_value
self.pipe.process_item(item,
get_testdata('feeds',
keywords
"#4e9a06",
_app_ctx_stack.top
@app.before_request
'You
'the
application
reqctx
returned
object_hook=object_hook)
secret
target
%}
'hello
app.add_url_rule('/',
app.test_client().get('/',
flask.request.endpoint
app.config['SEND_FILE_MAX_AGE_DEFAULT']
b'<p>Hello
[]):
Queue
boto.mturk.connection
reg
generation
provider):
add_auth(self,
keys:
self.errors
GOOG_HEADER_PREFIX
'aws':
self.is_truncated
self.next_token
version_id,
'key',
torrent=False,
response_headers=None,
self.generation
self.is_latest
replace,
query_args=query_args)
role
environment_name:
params['EnvironmentName']
environment_name
start_time=None,
end_time=None,
self.api
'us-east-1')
self._current_value
response_headers
layer1
'ReturnEnabled':
returnable,
source_field:
CloudSearchConnection
self.layer2
consistent_read=False,
dynamodb
conditional_operator=None,
op,
delete
'vpcId':
self.current_value
EC2ResponseError
instance_ids=None,
'description':
snap
offering_type
self.key_name
self.subnet_id
details
'Protocol':
params['cluster']
preferred_maintenance_window=None,
params['AutoMinorVersionUpgrade']
cluster_id
'Version',
Bucket
decorator(func):
wrapper.__doc__
worker_queue,
response_headers=response_headers)
self.get_acl(headers=headers)
'</'
'VersionId':
resumable
os.SEEK_END)
tracker_file_name
ResumableUploadException(
alg
'RoleName':
data_source_id,
lt
self.server
boto.sdb.db.model
server')
calculated_type=str,
instances[0]
StringProperty()
Question
'SellerId',
'0')
{'DBInstanceIdentifier':
self.date
'Description',
exception.DNSServerError(response.status,
weight
self.days
xml)
Domain(self,
_objects
str(int(time.time()))
'Version'
params[p_name]
'Credentials':
task_token,
'workflowType':
task_token
---'
Tomcat
'Amazon
'User
'S3
schema=[
'alice'},
nose.plugins.attrib
CloudWatchConnection()
self.assertEqual(s,
small_src_key_as_string,
get_cur_file_size(dst_fp))
self.assertEqual(e.disposition,
dst_key.get_contents_as_string())
small_src_file.seek(0)
small_src_file,
generation=g1)
self.base_domain)
self.conn.create_health_check(hc)
create_hit_rs
aws_access_key_id='aws_access_key_id',
aws_secret_access_key='aws_secret_access_key')
'glacier.us-east-1.amazonaws.com',
mock_response
mock_response.status
"KeySchema":
Schema.create(hash_key=('foo',
self.johndoe.mark_clean()
greeting='Hello',
self.results.fetch_more()
'AWSAccessKeyId',
aws_secret_access_key='more',
self.assertEqual(retval,
'master',
uri.scheme)
uri.uri)
self.assertFalse(hasattr(uri,
scrapy.selector
UsageError
opts)
self.engine
pickle
options:
(list,
tuple)):
'request':
hasattr(mw,
filename):
scrapy.utils.response
response=None,
signal,
setting
total
ScrapyCommand.add_options(self,
parser)
exists
follow
rules
sys.path
False})
response=response,
"A
status=status,
warn
aws_access_key_id
signal=signals.spider_opened)
cachedresponse
creds
method='GET',
threads
dirname
self.access_key
validate=False)
_get_exporter(self,
24
response.url,
expired
type:
encoding='utf-8',
'POST':
n,
unicode
parent=None,
change
headers={'Content-Type':
st
priority)
p,
100,
defaults
OrderedDict
isinstance(obj,
Settings()
obj:
responses
open(path,
value.strip()
**
mode
passed
-1)
d.addCallback(self.assertEquals,
True})
1})
get_crawler(Spider)
i1
'age':
self.request_class.from_response,
save
'''
HtmlResponse("http://example.org/somepage/index.html",
self.assertEqual(il.get_output_value('url'),
Request("http://www.example.com",
Response("http://www.example.com",
custom_value)
fail
BaseSpider)
chunksize
self.queue()
['a',
Deprecated
create_deprecated_class('Deprecated',
TextResponse(url="http://example.com/",
obj)
'lastmod':
MySpider))
'file://'),
'http://'),
b'You
select
.globals
text_type
._compat
host=None,
'HEAD',
tuple):
contains
app):
contents
attempt
{%-
f.close()
405
test():
err:
23
is_boolean(value):
recorded
swLat
swLng
neLat
neLng
neLng)
database
neLng):
swLat)
swLng)
neLat)
neLng))
gyms
step,
120
Queue()
str(e))
ydist,
xdist/2,
step))
adjust_lon
latitude,
pi))
self._auth_token
subresponse_return
number=102,
number=104,
number=17,
number=18,
number=20,
number=22,
number=25,
number=27,
number=29,
102
104
9
name='unknown1',
name='unknown2',
PY3:
implicit_setuptools
implicit_wheel
turn_steps_so_far
boto.ec2
version_id=version_id,
headers['Content-Type']
label):
label,
boto.log.error('%s
(response.status,
boto.log.error('%s'
self.error_code
self).endElement(name,
security
'Owner':
self.check_response(key,
cors
user_id,
size=None,
user_data
setReadOnly,
"ResourceNotFoundException":
exceptions.ResourceNotFoundException,
parameters
self.application_name
str(response['ApplicationName'])
self.namespace
bucket:
self._sdf
self.layer1
arn,
sort_order
max_results=None):
params['limit']
connection_id,
to_dict(self):
exclusive_start_key=None,
{'HashKeyElement':
decode(self,
'HASH'
conditional_operator
expression_attribute_values
"IndexName":
EC2Object
self.iops
self.encrypted
'InstanceId')
key_name:
sg
product_description
{'VolumeId':
'SourceRegion':
group_name}
params[prefix
self.block_device_mapping
self._name
'AvailabilityZone':
params['Port']
params['SourceType']
step_args
OR
hash
self.closed
if_metageneration
policy:
'<'
fp.seek(0,
e.message)
path_prefix
role_arn,
file')
elastic_ip
boto.log.info('Task[%s]
Volume()
boto.mturk.question
'GetServiceStatus')
@api_action('Subscriptions',
params['StackId']
main(self):
boto.connect_s3()
time.sleep(30)
notify=True,
service_name):
params['DBInstanceClass']
backup_retention_period
ec2_security_group_name
ec2_security_group_owner_id
snapshot_identifier,
ln
sn
comment=""):
version_id:
encoding_type=None):
self.bucket.name,
implemented
KeyFile')
item_name,
num_results
self).validate(value)
t.put()
ExceptionToRaise
exc_reason
platform_application_arn
workflow_name,
{'VpcId':
time.sleep(3)
"A")
21,
'---
self.assertTrue(response)
'DynamoDB
'LastPostedBy':
'Views':
'Replies':
'Answered':
'12/9/2011
11:36:03
users.put_item(data={
batch:
sfp
"test2"
s2)
self.make_small_key()
self.assertEquals(result[u'CreateHealthCheckResponse'][
self.bucket.delete()
size=5)
cb=callback,
self.assertEqual(response.id,
max_assignments=2,
approval_delay=60*60,
self.service_connection
aws_secret_access_key='secret')
HTTPretty.register_uri(HTTPretty.POST,
'us-west-2')
{'x-amz-glacier-version':
'use-sigv4':
self.assertRaises(ValueError):
'john',
'ami-id',
self.ec2.modify_network_interface_attribute('id',
mock.Mock(return_value=mock_response)
self.volume_one.connection
'db.m1.large',
bucket='examplebucket',
uri.bucket_name)
uri.object_name)
self.assertEqual(uri.names_provider(),
self.assertEqual(uri.names_bucket(),
self.assertEqual(uri.is_stream(),
<item>
</item>
project
release
please
delta
same
1))
inspect
scrapy.utils.request
export_item(self,
zope.interface
scrapy.utils.trackref
attrs):
attrs:
value))
copy(self):
__eq__(self,
failure_to_exc_info
level
'referer':
defaultdict
'foo
*args)
self.timeout
body=None):
responsetypes
twisted.python
shells
request.url
(k,
20
'-':
callback
depth
for:
paths
'Unknown
Response):
closed
IgnoreRequest
downloading
hostname=None,
hostname,
which
connection,
protocol):
http_request,
aws_secret_access_key
spider_opened(self,
TextResponse
urljoin
1000
ts
'url':
invalid
self.request
query):
allowed
unique
key=lambda
selector
'Cache-Control':
'last_modified':
'Content-Encoding':
'Expires':
90
'File
image
size:
eb
errors
namespace
%s."
write(self,
errors)
None}
ext
d)
request(self,
'No
backlog=50,
domain=None,
header,
200,
mkdtemp()
'2',
self.mktemp()
request.callback(response)
download_handler_cls
Mar
'Size':
HtmlResponse),
Response('http://scrapytest.org/',
self.assertIsInstance(
main()
storage_class
tempfile.mkdtemp()
self.assertEqualResponse(res1,
res0b)
{'Date':
os.environ['no_proxy']
middleware
middleware),
i3
'John')
'egg':
encoding='latin1')
h1
headers=hdrs)
self.assertRaises(AttributeError,
self.assertEqual(req.method,
self.assertEqual(req.url,
fs)
123
'X')
text'),
str(e)
['one',
custom_value
@inlineCallbacks
charset=UTF-8',
Selector))
msg')
'loc':
dt
convert=lambda
arg1,
v2)
NewName)
[{u'id':
u'2',
self.assertEqual([row
12)
Sitemap(b)
self.assertEqual(list(s),
30
cm:
#004461",
revised
jinja2
session,
methods=['GET',
'POST'])
'Invalid
login(client,
werkzeug.exceptions
redirect
Config
Session
PY2
d):
json,
secret_key
options
**options):
endpoint=None,
tb
values)
'r')
__get__(self,
__set__(self,
wrapper(*args,
val:
byte
main():
test'
World'
['GET',
rv.headers['set-cookie'].lower()
'42'
str(e.value)
flask.render_template('template_filter.html',
value='abcd')
b'dcba'
flask.render_template('template_test.html',
value=False)
b'Success!'
common_object_test(app)
2'
val):
buffer
magic
retrieve
101
50
(2
boto.rds
self.region_name
'='
service
long_type
self.provider
profile_name)
self._connection
key_file
num_retries
i)]
response.reason))
ResultSet()
'Type':
dict.__init__(self)
'NextToken':
float(value)
'RequestId':
torrent,
generation=None,
self.md5
recursive=False,
if_generation=None,
md5=None,
md5,
src_bucket_name,
query_args=None,
self.head
environment_id=None,
environment_id:
params['EnvironmentId']
environment_id
application_name
params['StartTime']
params['EndTime']
self.date_created
datetime.fromtimestamp(response['DateCreated'])
self.date_updated
datetime.fromtimestamp(response['DateUpdated'])
str(response['Description'])
self.environment_name
str(response['EnvironmentName'])
self.solution_stack_name
str(response['SolutionStackName'])
self.events
self.stack_id
self.stack_name
"Value":
self.resource_type
handler.XmlHandler(rs,
distribution_id,
'Enabled':
'Id':
replace:
boto.s3.key
client_token
subnet_id
'CreationDate':
'FacetEnabled':
facet,
'SearchEnabled':
"LimitExceededException":
exceptions.LimitExceededException,
params['MaxResults']
self.table
object_hook=None):
limit:
json_input,
return_values
encode(self,
key_data
expects
self.table_name
table_name
public_ip
self.private_ip_address
network_interface_id=None,
self.delete_on_termination
boto.ec2.networkinterface
dry_run=False,
BlockDeviceMapping()
reservation
SecurityGroup):
l,
security_groups:
attr)
ip_protocol
cidr_ip:
self.id:
ID'
self.group_name
params[full_prefix
grant
prefix=None,
'InstanceType':
policy_name}
self.target
engine_version=None,
params['PreferredMaintenanceWindow']
cache_parameter_group_name,
cluster_id,
'Name',
get_file(self,
to=wrapper)
upload_id
boto.jsonresponse.Element()
self.provider.storage_response_error(
self.last_modified
self.current_tag
InvalidLifecycleConfigError(
hash_algs
-y
self.device)
self.instance:
qualifications
page_size=10,
worker_id,
comparator,
required_to_preview=False):
@strip_namespace
stack_id=None,
option,
S3'
exit_on_error=True)
license_model
'DBInstanceClass':
'Engine':
'MasterUsername':
'Port':
DBInstance)
{'DBParameterGroupName':
engine_name
major_engine_version
self.is_modifiable
db_parameter_group_name,
parameter_group_name,
self.route53connection._make_qualified(name)
'A',
reduced_redundancy=False,
allowed_origin
self.content_type
data_len
ResumableDownloadException
file=f)
identity,
queue.id)
partition
run_id,
{'runId':
task_list
'error',
vpc_id}
'CidrBlock':
egress
num_iter
num_keys
'LastPostDateTime':
[{'S':
users.get_item(
consistent=True
'Doe'
'3'},
'jane']},
list))
expected_params
self.assertEqual(params,
k.set_contents_from_string(s2)
small_src_key.get_contents_to_file(
self.assertEqual(small_src_file_as_string,
@unittest.skipUnless(simple
isolator,
wait_timeout
policy=NOT_IMPL,
sfp.seek(0)
k.BufferSize
queue_name
frame_height=800)
'doctest']
lifetime=60*65,
External
keywords=keywords,
duration=60*6,
aws_access_key_id='access_key',
'2012-06-01'},
aws_secret_access_key='aws_secret_access_key',
self.assertDictEqual(
9,
mock_response,
self.assertEqual('request_id'
['response_metadata'],
self.set_http_response(
status_code=200,
self.shared_config
self.assertEqual(uri_str,
self.assertEqual(uri.is_version_specific,
'ContentType'])
fake_response
VPCConnection,
self.assertIsInstance(api_response,
'vpc-1a2b3c4d')
'acl-2cb85d45',
six.PY3:
run(self):
1]
render(self,
factory)
pkgutil
(2,
sys.version_info[0]
command')
sys.exc_info()
force
from_settings(cls,
self.file
request},
show
pprint
BaseItem
x)
field_name
self.encoding)
values:
base
dict
scrapy.linkextractors
cStringIO
email.utils
subject
url:
''))
found:
scrapy.utils.deprecate
usage
start=0,
__enter__(self):
__exit__(self,
help="print
self.exitcode
format
'_')
parsing
cb:
given
t.start()
(1,
directory
results):
exc_info=failure_to_exc_info(f),
spider}))
logkws
slot)
self.method
scrapy.responsetypes
self._url
download_request(self,
extra
max
boto.auth
'https'
{'Set-Cookie':
password,
response.status)
up
rp,
cached
self._open(os.path.join(rpath,
XmlResponse
kwargs)
TypeError(
t:
isinstance(result,
cache
self.__class__.__name__,
HtmlResponse,
type=None,
__contains__(self,
16
'%s
allowed_domains
cls)
r,
read(self,
errno
offset
e.errno
reactor.listenTCP(0,
getattr(obj,
b"\r\n"
twisted.web
decimal
cls.__name__
domains
self._reactor
description):
name:
giving
provide
"The
os.environ['https_proxy']
type=int)
exists(join(self.proj_mod_path,
testfixtures
LogCapture
req0
'bar'})
httpreq
self.download_request(req,
(TypeError,
2007
'Date':
'text/html',
resp2
self.assertEqual(req2.url,
first,
Request('http://scrapytest.org/',
res2
XmlResponse),
mw.spider_opened(spider)
res2.flags
{}),
{'Expires':
'Last-Modified':
url2
status=302)
'GET')
isinstance(req,
_check_output(self):
22,
age='22')
self.assertExportResult(
b'<?xml
S3
'bar2',
url="http://www.example.com/this/list.html")
"http://www.example.com/this/post.php")
self.assertEqual(r1.method,
</form>''')
type="radio"
type="checkbox"
'iso-8859-1',
Link("http://www.example.com",
Link(url='http://www.google.com/something',
name_out
il.add_value('url',
x})
self.tempdir}))
return_value=True)
warnings.simplefilter('always')
deprecated'
100)
self.assertEqual(new_item['results'],
source,
self.assertTrue(hasattr(spider,
{'loc':
'value')
7)
self._mywarnings(w)
u'3',
u'4',
'c'])
Foo()
o2
12345,
'Programming
client.get('/')
get_db()
so
key'
username)
register(client,
name=name)
configuration
file'
install(self):
modname
'-'
app.config['SERVER_NAME']
attempts
new_contents
parser.parse_args()
{{
'prefix',
}}')
flask.request.method
c.get('/').data
parse_cache_control_header(rv.headers['Cache-Control'])
cc.max_age
app.jinja_env.filters.keys()
my_reverse
'dcba'
app.jinja_env.tests.keys()
'boolean'
ext_id
rv.direct_passthrough
12
43
get(self):
Pokestop,
position
args.china:
CharField(primary_key=True,
max_length=50)
IntegerField()
DateTimeField()
(Pokemon
(Pokemon.latitude
(Pokemon.longitude
<<
active_pokemon_id
pokestops
datetime.utcfromtimestamp(
scan
API
loop
WEST)
i):
(20.0
sin(y
sin(x
web
shared
args.password
'type':
'message':
self.log
self._login
'execution':
ServerBusyOrOfflineException
self._api_endpoint
player_position
AttributeError
http_response
entry_id
proto_name
response_proto_dict
longitude,
number=101,
number=103,
number=106,
number=111,
number=113,
number=114,
number=115,
number=116,
number=118,
number=120,
number=122,
number=123,
number=124,
number=125,
number=126,
index=30,
number=127,
index=31,
number=128,
index=32,
number=129,
index=33,
number=133,
index=34,
index=35,
index=36,
index=37,
number=137,
index=38,
index=39,
index=40,
index=41,
index=42,
index=43,
index=44,
index=45,
index=46,
index=47,
index=48,
index=49,
index=50,
index=51,
index=52,
index=53,
index=54,
index=55,
index=56,
index=57,
index=58,
index=59,
index=60,
index=61,
index=62,
index=63,
index=64,
index=65,
index=66,
index=67,
index=68,
index=69,
index=70,
number=24,
number=26,
number=28,
number=30,
number=31,
number=100,
103
106
111
113
114
115
116
118
122
124
125
126
127
128
129
133
137
21
88
serialized_end=637,
_RESPONSE
_AUTHTICKET
name='item',
name='pokemon_id',
_SPAWNPOINT
RpcEnum_pb2._POKEMONMOVE
RpcEnum_pb2._ITEMTYPE
acc
calculated
total_workers
brng
InvalidUriError
UserAgent
boto.ec2.elb
STSConnection
generation=generation,
hmac
auth_path,
service_name
datetime.datetime.utcnow()
region,
boto.provider
self.num_retries
profile_name
self.proxy
boto.utils.parse_host(host)
override_num_retries=None,
parent:
self.current_text
ACL
self.connect()
get_key(self,
subresource,
if_metageneration=None):
if_generation=if_generation,
size=size,
src_key_name,
read
fp.seek(spos)
content_length
self.make_request('PUT',
boto.jsonresponse
self.template_name
str(response['TemplateName'])
self.resources
event
parent):
CloudFrontServerError(response.status,
str(uuid.uuid4())
'<?xml
id='',
{1}
handle_bool(value)
Domain
condition_name
query)
num_pages_needed
deployed
_make_request(self,
params['marker']
hash_key
boto.dynamodb.exceptions
expected=None,
query(self,
'N'
self._dynamizer.encode(value)
params['ReturnConsumedCapacity']
return_item_collection_metrics
op
'Keys':
'PutRequest':
self.snapshot_id
snapshot_id
verbose_name='EC2
Instance
Zone
boto.ec2.reservedinstance
boto.ec2.blockdevicemapping
groups:
instance_ids:
security_groups=None,
instance_type='m1.small',
isinstance(group,
availability_zone:
params['Iops']
device
snapshot_id,
from_port=None,
to_port=None,
tags):
self.platform
details=None):
'id',
self.instance_count
self.status_message
ResultSet([
self.vpc_id:
self.security_groups
dimensions=None,
ListElement()
cluster=None,
self.action
params['EngineVersion']
cache_parameter_group_name
auto_minor_version_upgrade).lower()
'SubnetIds.member')
source_type=None,
source_identifier
ascending
page_token
specified
'.join(['+'.join(g)
groups])
@needs_caller_reference
boto.glacier.utils
tree_hash,
upload_id)
linear_hash,
self.vault
response_headers:
'<%s>'
permission
self.type)
boto.s3.bucket
more_results:
self.parse_level
self.validateParseLevel(name,
collection
isinstance(fp,
'invalid
print('Caught
'PolicyDocument':
user_name=None):
{'RoleName':
bool:
base64.b64decode(
encryption_context
'logGroupName':
filter_variable
eq
gt
ge
le
ne
sftp_client
self.run('apt-get
self.get_ec2_connection()
self.mount_point)
time.sleep(15)
hit_id,
{'HITId':
comparator=comparator,
required_to_preview=required_to_preview)
@api_action('Inbound',
@api_action('Outbound',
elastic_ip,
layer_id,
custom_json
enable_ssl
install_updates_on_boot
boto.config.get('EBS',
'AllocatedStorage':
'DBSubnetGroupName':
multi_az
preferred_backup_window
params['MultiAZ']
cidrip
'Filters.member',
('FilterName',
'FilterValue'))
snapshot_cluster_identifier
cluster_version
hsm_client_certificate_identifier
hsm_configuration_identifier
{'ClusterIdentifier':
identifier=identifier,
self.identifier
rr
lifecycle
key_marker
bucket):
url_base
bucket)
"value":
expose_header
self.get_domain_and_name(domain_or_name)
self.model_class
t.name
mantissa
self.s3
{'TopicArn':
'taskList':
oldest_date,
run_id
'RuleNumber':
self.cidr_block
len(policy.acl.grants)
cloudsearch
iam
'stringValue':
self.dynamodb
'RangeKeyElement':
item1_range,
batch.put_item(data={
post
'1'},
'Alice'},
dry_run=True
autoscale
expected_params)
self.assertEqual([lb.name
self.assertEqual(k.get_contents_as_string(),
'<Scope
type="AllUsers"></Scope><Permission>READ</Permission>'
self.assertEqual(acl.to_xml(),
g1)
self.make_dst_fp()
self.assertEqual(small_src_key_as_string,
small_src_key.get_contents_as_string())
CallbackTestHarness(
generation=g2)
'1')
self.assertIsInstance(connection,
bad
'available')
self.base_domain
**base_record)
port=443,
boto.s3.website
(unittest.TestCase):
self.conn.create_bucket(self.bucket_name)
boto.config.set('Credentials',
ks
kn.get_contents_as_string().decode('utf-8')
self.assertEqual(ks,
k.get_contents_as_string(cb=callback,
self.bucket.initiate_multipart_upload(key_name)
mpu.upload_part_from_file(sfp,
br
'String',
Qualifications()
qualifications=qualifications)
json.dumps({'test':
"/",
"POST")
'Bad
7',
self.set_http_response(status_code=400,
api_response)
json.loads(HTTPretty.last_request.body.decode('utf-8'))[0]
Smith','Mark
str(cm.exception))
self.actual_request.headers['X-Amz-Target']
target)
"string",
"TableName":
self.johndoe['last_name']
self.assertEqual(self.results._results,
results.next)
results['results']]
'the_callable',
'instance_id',
'active')
'NetworkInterfaceId':
'DryRun':
taggedEC2Object
TaggedEC2Object(self.service_connection)
taggedEC2Object.id
"i-abcd1234"
'ResourceId.1':
'i-abcd1234',
'Tag.1.Key':
self.assertEqual(taggedEC2Object.tags,
'autoscale',
'inst1',
self._setup_mock()
self.get_instance_metadata.return_value
self.assertIsNone(p.security_token)
'SimCoProd01',
'Password01',
key='test.txt',
prov
uri.version_id)
uri.generation)
self.assertEqual(uri.names_container(),
self.assertEqual(uri.names_object(),
self.assertEqual(uri.names_directory(),
self.assertEqual(uri.names_file(),
boto.swf.layer2
110,
twisted
file_path
rawtext,
NOT_DONE_YET
self.tail
self.start
urls
__version__
scrapy.crawler
cmd
inproject)
warnings.catch_warnings():
stop(self):
self.spider_loader
again
call
name.startswith('_'):
'use
its
src,
cc:
cc,
failure,
msg):
msg,
scrapy.utils.defer
extra={'crawler':
request.errback
Use
become
urlencode
stdout=subprocess.PIPE,
10000
{'LOG_ENABLED':
contracts
-a
'%s',
UsageError()
supported")
sys,
sys.stdout
join,
items:
platform
errors='replace')
open(self,
'reason':
self.queue
result:
fname
iterable
delay,
SSL
certificate
urlparse,
connectionLost(self,
self.host,
filepath
dataReceived(self,
(11,
UNKNOWN_LENGTH
self._contextFactory
timeout=30,
contextFactory
protocolFactory):
bodyProducer,
pool
HtmlResponse)
self._timeout
crawler.signals.connect(o.spider_opened,
signal=signals.spider_closed)
spider_closed(self,
'%s:%s'
b'Basic
twisted.internet.defer
file):
storage
x,
weakref
isinstance(r,
currentage
self.db
f.read()
memory
hosts
int):
%s>"
v):
(None,
xpath
body=b'',
'utf-8',
regex
links):
link.url
scrapy.link
Link
Please
self.close()
self.current_link
values,
partial
info.spider}
Error
status):
_warn()
create_deprecated_class(
priority='project'):
25
response')
list)
either
highlight
struct
TextResponse,
record):
mods
b"Host:
twisted.trial.unittest
msg=None):
ValueError(
Content-Length
reason=None):
self._parser
self._port
(host,
'SSL':
11,
getHost():
(including
The
os,
b'{}'
such
contextlib
'spiders',
self.call('genspider',
'example.com')
self.results
11)
self.runner.create_crawler(SimpleSpider)
None})
_test(response):
self.assertEquals(response.body,
Spider('foo')).addCallback(_test)
self.assertFailure(d,
r.body)
payload
+0000'
self._mocked_date(date):
b'AWS
'Tue,
'User-Agent':
pre
__doctests__
'C1=value1;
mw.process_response(req,
'value1',
time.sleep(2)
res1
idx)
req.meta
Response(url,
rsp1
middleware)
spider_class
self.output
1),
self.ie.start_exporting()
self.ie.finish_exporting()
self.assertEqual(name,
test_nonstring_types_item(self):
ie.export_item(item)
expected_value)
expected_value
self.exported_data(items,
'bar1',
123)
r4
bytes)
1},
['two',
'six':
_qs(req,
fs,
{b'i1':
'''<form>
<select
self.request_class.from_response(res)
Request("http://www.example.com")
'test'
2',
3',
name_in
[u'mar',
u'ta'])
'world']),
[None,
body=b)
l.add_xpath('name',
subject)
self.tempdir
random.randint(1,
prefix:
2000)
self.info)
mappings
mappings:
self.assertEqual(url,
str(w[1].message))
{'one':
self.assertEqual(stats.get_value('test2'),
steps
NewName,
warn_category=MyWarning)
Headers({"Content-Type":
self.xmliter(response,
u'alpha',
u'foobar'},
u'unicode',
u'\xfan\xedc\xf3d\xe9\u203d'},
u'multi',
u'empty',
u''}])
csv],
self.handler
o1
HTTP/1.0\r\n"
six.assertCountEqual(self,
setup
render_template,
Flask,
db.commit()
@app.teardown_appcontext
flash('You
enter
BuildError
make
import_name,
package_path
chain(funcs,
need
code_or_exception,
e):
'<%s
changes
param,
help='Enable
disabled.')
reload
trying
path=None):
fullname):
test_string
attribute):
flashes
werkzeug.http
'/')
app.config['LOGGER_HANDLER_POLICY']
path=path,
doc)
force=False,
flask._compat
sorted(rv.allow)
'POST']
bar
b'42'
app.config.update(
@app.after_request
u'Hello
evts
11
b'a'
boolean(value):
'import
view_func=Index.as_view('index'))
CORS
time.sleep(1)
them
os.path.join(
class_
13
15
32
tmpdir:
packages
boto.pyami.config
SESConnection
access_key
-1:
'gs':
self._hmac_256
self._provider.security_token:
self._provider.security_token
'&'.join(pairs)
'application/x-www-form-urlencoded;
encodebytes
filter,
map,
Provider
BotoClientError(
port:
self.proxy_user
host)
cert
status:
params=None,
auth_path=None,
self.make_request(action,
self.box_usage
self.error_message
content):
self.parser
element_name
list.__init__(self)
boto.gs.acl
boto.s3.acl
config.has_option('Credentials',
region_info
connection_cls=None):
self.marker
'Marker':
self.max_items
self.object_name:
self.connection:
delete_key(self,
mfa_token=None):
hash_algs=None):
get_acl(self,
recursive:
add_email_grant(self,
email_address)
add_user_grant(self,
set_acl(self,
if_metageneration=if_metageneration)
super(LazyLoadMetadata,
hashfunc
ResourceNotFoundException(BotoServerError):
"true",
params['ContentType']
'JSON'
environment_name=None):
self.default_value
listener
parameters,
{'ContentType':
self._current_key
debug=debug,
id)
comment=comment)
[{
self).__init__(connection,
self.create_time
marker='',
max_items=None,
subnet_id,
Unicode
StopIteration
facet=None,
boto.cloudsearch2.layer1
LimitExceededException(BotoServerError):
v2
['CreateDomainResult']
['DomainStatus'])
consistent_read=False):
consistent_read
binascii
data.get('__type'):
create_table(self,
{dynamodb_type:
self._data
return_consumed_capacity=None,
query_filter=None,
"ProvisionedThroughput":
"ReadCapacityUnits":
"WriteCapacityUnits":
raw_key
consistent=False,
limit,
'EQ',
public_ip=None,
allow_reassociation=False,
self.progress
SecurityGroup
Tag
j)]
attribute}
groups,
l.append(group)
instance_id}
'ModifyNetworkInterfaceAttribute',
network_interface_id
datetime(now.year,
now.month,
'SourceImageId':
[self.id],
BlockDeviceMapping
_update(self,
updated):
i.id
self.price
description)
src_group_name,
src_group_owner_id,
cidr_ip,
src_group_group_id,
'key':
'LaunchConfigurationName':
as_group,
ListElement
self.metric
self.created_time
self.availability_zones
unit
self.dimensions
load_balancer_name,
load_balancer_name}
lb_port,
cache_security_group_name,
cache_cluster_id,
params['Duration']
auto_minor_version_upgrade=None,
data=json.dumps(params))
output_bucket
'Id',
response_headers=None):
{1}".format(func.__doc__,
"{0}
KeyError(message)
{0}
part_size)
vault
resource)
boto.glacier.layer1
_MEGABYTE
self.part_size
file")
email_address):
'</%s>'
generation=None):
"if_generation
cb=cb,
torrent=torrent,
self.append(rule)
file_length)
num_cb
during
conn.port,
ResumableTransferDisposition.ABORT_CUR_PROCESS)
user_name):
user_name:
params['UserName']
user_name
six.binary_type):
choices
private
ami
group:
ami_id
DNS
deleted
boto.connect_ec2()
ssh_client
assignment_id,
integer_value,
integer_value=integer_value,
self.display_name
members=True)
SimpleList()
params['Attributes']
iam_user_arn,
%s\n'
stat
backup_retention_period=None,
'AutoMinorVersionUpgrade':
'DBParameterGroupName':
'MasterUserPassword':
'VpcSecurityGroupIds.member')
{'DBSecurityGroupName':
DBSnapshot)
multi_az=None,
params['DBSubnetGroupName']
params['OptionGroupName']
self.vpc_security_groups
db_security_group_name,
db_instance_class=None,
cluster_security_group_name,
cluster_identifier):
"",
identifier
self.alias_evaluate_target_health
records
boto.s3
('CommonPrefixes',
'text/xml'
self.connection.make_request('DELETE',
Lifecycle()
encoding_type
"%s"}'
bucket.name
self._storage_class
next(self):
self.bucket.connection.debug
cb_size
bytes_togo
self.expiration
SDBResponseError
Password
self.data_type):
empty(self,
cv
boto.config.get('DB',
order_by
self.get_text_value(value)
type="string",
('member',
'RoleArn':
'taskToken':
maximum_page_size=None,
next_page_token=None,
reverse_order=None):
'maximumPageSize':
maximum_page_size,
'nextPageToken':
next_page_token,
'reverseOrder':
reverse_order,
workflow_id
self.last_tasktoken
"itertools",
association_id,
'AssociationId':
destination_cidr_block,
vpc_peering_connection_id
network_acl_id,
key_counter
query.filter("name
boto.sdb.db.sequence
assert(s.val
assert(s.next()
mdval1
mdval2
advanced
application_name=self.app_name,
pipeline_id)
self.hash_key_name,
self.range_key_name,
{hash_key_type:
{range_key_type:
users
username='jane',
friend_count=3,
users.query_2(
'jane')
'user_id':
bucket.get_all_keys()
g2)
tracker_file_name=tracker_file_name,
self.assertTrue(
tmp_dir
ResumableUploadHandler(
ResumableUploadException')
HealthCheck(ip_addr="54.217.7.118",
port=80,
self.conn.delete_health_check(result['CreateHealthCheckResponse']['HealthCheck']['Id'])
test')
replace=NOT_IMPL,
self.bucket.new_key(key_name)
connection.host)
S3Connection)
1000)
mpu
SQSConnection()
statements
'string_value':
ExternalQuestion
https_connection_factory=self.https_connection_factory,
self.assertRaises(BotoServerError):
bse
Request',
HmacAuthV4Handler('glacier.us-east-1.amazonaws.com',
canonical_uri
self.assertEqual(canonical_uri,
self.assertEqual(fake._required_auth_capability(),
self.assertTrue(isinstance(response[0],
encoded_sig
self.dist._url_base64_encode(sig)
encoded_sig)
'errors':
[{'message':
document.commit)
'12345',
'N'))
RangeKey('date_joined')
'gender',
self.assertFalse(self.johndoe.needs_save())
"Projection":
"ProjectionType":
'users':
'bob'},
mock_query:
usernames
[res['username']
self.assertEqual(usernames,
self.assertEqual(len(results['results']),
self.assertEqual(results['last_key'],
self.assertTrue(isinstance(results,
public_ip="192.168.1.1",
'us-west-2',
'Tag.1.Value':
"value2"})
name='foo',
elb,
self.assertTrue(result)
'MASTER',
1024))
sentinel.upload_id,
body=MOCK_SERVER_RESPONSE)
ExternalQuestion(
external_url="http://samplesite",
['boto',
title
"Boto
Test"
'An
self.service_connection.create_hit(
question=q,
title=title,
reward=0.05,
annotation=annotation,
'QualificationRequirement.1.Comparator':
'QualificationRequirement.1.QualificationTypeId':
'Title',
'Question',
'AssignmentDurationInSeconds',
'RequesterAnnotation',
'LifetimeInSeconds',
'AutoApprovalDelayInSeconds',
'Reward.1.Amount',
'MaxAssignments',
'Reward.1.CurrencyCode',
'Keywords',
'Operation'])
self.assertEquals(create_hit_rs.status,
self.assertSequenceEqual(
provider.Provider('aws',
'shared_access_key',
'shared_secret_key',
'cfg_access_key',
self.assertIn(
self.assertEqual('bucket',
config.to_xml()
self.assertEquals(result,
self.assertEquals(api_response.id,
'rtb-e4ad488d',
dirname,
setuptools
extensions
'index'
%s;
lineno,
inliner,
options={},
content=[]):
ref
[node],
IOError:
line)
"w")
latency
request.finish()
slots
cmds
active
Settings(settings)
crawler,
been
cPickle
self.fields_to_export
item:
7,
may
_get_mwlist_from_settings(cls,
object_ref
__delitem__(self,
fragment
"the
create
response.flags
Failure):
email
to,
send
settings,
Response)
v))
b)
pydispatch
create_deprecated_class
stats,
long_desc(self):
action="append",
self.crawler_process.start()
'-m',
"[options]
default=False,
contract
print('
without_none_values
"")
"to
help="use
body")
custom
template):
template)
display
each
scrapy.utils.url
dst):
wraps
AssertionError:
self.running
StopIteration:
d.addErrback(lambda
(Response,
f)
self.active
self.active_size
returns
'__iter__')
'Middleware
logger.warning(
443
urlparse(url)
self.port,
dh
(scheme,
scrapy.utils.decorators
404,
factory):
agent
bindAddress=None):
self._bindAddress
http_request.headers
cls(crawler.settings)
cookie
gzip
archive
TimeoutError,
DNSLookupError,
ConnectionRefusedError,
ConnectionDone,
ConnectError,
ResponseFailed
'HEAD'
interval
cls(crawler)
self._parsers[netloc]
Request(
stack
posixpath
file)
self.is_botocore:
'-')
self.expiration_secs
batch
key:
update(self):
For
items(self):
body=None,
':'
formdata
convert
removed
self.unique
self.base_url
unicode):
future
warnings.simplefilter('ignore',
'path':
hashlib.sha1(to_bytes(url)).hexdigest()
size)
class_name
(),
self.set(name,
'default'),
instead'),
custom_settings
Spider)
iterator
int)
os.path.join(path,
banner=''):
"and
doesn't
wrapped
work
input,
old
chunked_body
weakref.WeakKeyDictionary()
encoding=None,
name))
receive
encoder
self._producer
cannot
'CONNECTION_LOST'
self._response
location,
message=None):
0;
tests.mockserver
MockServer()
crawler.spider.meta['close_reason']
lookup
0.2
self.project_name,
@contextmanager
to_native_str(p.stderr.read())
'''\
foo
req3
str(log))
'RETRY_ENABLED')
credentials
self.assertEquals(
http_proxy
self.assertEqual(httpreq.headers['Authorization'],
date,
self.assertEqual(r.status,
self.assertEqual(r.headers,
Request),
crawler._create_spider('foo')
second,
path=/'}
self.assertRaises(
main
fmt
**new_settings):
mw.process_response(self.request,
self.response,
mw.process_request(self.request,
mw.storage.retrieve_response(self.spider,
res0
{'Last-Modified':
self.tomorrow}),
'Age':
'ETag':
self.assertEqual(response.headers['Content-Encoding'],
HttpProxyMiddleware()
Request(url)
url2},
isinstance(req2,
self.assertNotIgnored(Request('http://site.local/allowed'),
found')
TestItem
self._get_exporter()
'boolean':
self.assertRaisesRegexp(TypeError,
self._get_nonstring_types_item()
ie.start_exporting()
ie.finish_exporting()
expected)
json.loads(to_unicode(self.output.getvalue()))
rows
['foo',
test_copy(self):
self.assertEqual(h1.getlist('header1'),
[b'1'])
'GET'
test_init(self):
formdata={'one':
'three'],
self.assertEqual(fs[b'test2'],
self.assertEqual(fs[b'one'],
self.request_class.from_response(response)
'2'},
b'i2':
</select>
name="i2">
name="i1"
checked>
{"foo":
{'load':
nofollow=True),
self.assertEqual(lx.extract_links(self.response),
lx.extract_links(response)],
HtmlResponse("http://www.example.com",
MapCompose(lambda
il.load_item()
self.assertEqual(item['name'],
ChildItemLoader(TestItemLoader):
url_in
['val'])
'hello')
self.assertRaises(RuntimeError,
self.assert_(l.selector)
[u'http://www.scrapy.org'])
{"bar":
rmtree(self.tempdir)
1000),
"_"
settings_attr,
self.file_cls_attr_settings_map:
getattr(pipeline,
user_pipeline
self.assertEqual(getattr(user_pipeline,
settings_attr
self.img_cls_attribute_names:
super(MockedMediaPipeline,
==>
(source,
retcls,
head
self.assertIn('scrapy.Selector',
XPathSelector))
self.spider_class('example.com')
msg'))
self.assertSitemapBody(r,
test_process_spider_output(self):
Response('http://scrapytest.org')
self.assertEqual(q.pop(),
q.pop()
'two':
'three':
8,
cb3],
'v1',
'v2')
subclass
req1a
next(iter)
csviter(response)
FOOBAR_NL},
itertools
@unittest.skipUnless(six.PY2,
d2)
place
self.assertEqual(request_fingerprint(r1),
Decimal
self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html',
timeout=timeout)
'/foo')),
client.ScrapyHTTPClientFactory(Request(
self._test(factory,
/bar
update_wrapper
Flask(__name__)
r.status_code
__name__,
silent=True)
username'
password'
_app_ctx_stack
g.user:
user.user_id
foo'
bar'
hasattr(sys,
werkzeug.routing
sure
self._logger
fn
root_path
funcs:
occurred
f.__name__]
reraise(exc_type,
status_or_headers
endpoint:
self.app
.setdefault(self.name,
.setdefault(None,
__traceback_hide__
'is
debugger
sys.argv[1:]
open(filename)
template,
sys.modules[fullname]
error:
doc=None):
both
record)
permanent
the_key
blinker
'get',
argparse
parse
tags:
monkeypatch):
package
cleanup_stuff
'OPTIONS',
b'GET'
is_permanent
messages
'never'
'text/plain'
flask.make_response(
'application/json'
t.join()
[True,
64
foo():
[42]
says
is_boolean
app.jinja_env.tests['boolean']
app.jinja_env.tests['boolean'](False)
'bar
stuff
app.config['DEBUG']
c.get(url)
self.val
flask.url_for('index',
('/a',
app.instance_path
flask.request
message',
app.config['TEMPLATES_AUTO_RELOAD']
app.jinja_env.auto_reload
pogom.utils
Pokemon,
log.info('Parsing
config['ORIGINAL_LATITUDE']
config['ORIGINAL_LONGITUDE']
.models
location:
lat,
min
'disappear_time':
Time:
datetime.utcnow())
p['longitude']
transform_from_wgs_to_gcj(p['latitude'],
p['longitude'])
p['longitude'],
'enabled':
log.debug("Upserting
Create
api
ring
NORTH)
RIGHT
EAST)
LEFT
login(args,
log.debug("{}:
failed_consecutive
failed.
pi
longitude):
adjust_lat
type=int,
help='Time
type=float,
default=5)
Starts
help='Disables
db)',
database')
config["PASSWORD"]
num_pokemon
login(self,
self.log.info('Google
Login
seems
data=data,
access_token
self._position_lat
self._position_lng
self._position_alt
self._req_method_list:
self.log.info('Login
subrequests,
59
definition
altitude
_b=sys.version_info[0]<3
x:x)
x:x.encode('latin1'))
descriptor
_descriptor
symbol_database
_symbol_database
_sym_db
_symbol_database.Default()
_descriptor.FileDescriptor(
_sym_db.RegisterFileDescriptor(DESCRIPTOR)
number=105,
number=110,
number=112,
number=117,
number=119,
number=121,
number=134,
number=135,
number=136,
number=138,
number=140,
number=142,
number=143,
number=144,
number=145,
number=146,
number=147,
number=150,
number=151,
number=32,
number=35,
number=37,
number=39,
number=41,
number=43,
number=46,
number=48,
number=50,
number=52,
number=54,
number=56,
number=58,
number=60,
number=63,
number=66,
number=69,
index=71,
index=72,
number=72,
index=73,
index=74,
number=74,
index=75,
index=76,
index=77,
number=77,
index=78,
index=79,
number=79,
number=81,
number=83,
number=84,
number=86,
number=88,
number=90,
number=92,
number=95,
number=96,
number=98,
number=107,
number=108,
number=109,
number=131,
number=132,
105
110
112
117
119
121
134
135
136
138
140
142
143
144
145
146
147
150
151
19
29
35
37
39
41
45
46
48
52
54
56
58
63
66
69
72
74
77
79
81
83
84
85
86
92
95
96
98
107
108
109
131
132
name='type',
_REQUEST_AUTHINFO
_REQUEST_UNKNOWN6
_RESPONSE_UNKNOWN6
\x01(\x05\x12
\x01(\x01\x12\x11\n\tlongitude\x18\x04
_PROFILE
_DAILYBONUS
_CURRENCY
_AVATARDETAILS
_INVENTORYDELTA
_INVENTORYITEM
_INVENTORYITEMDATA
_POKEMON
name='id',
_ITEM
_POKEDEXENTRY
_PLAYERSTATS
_PLAYERCURRENCY
_PLAYERCAMERA
_INVENTORYUPGRADES
_INVENTORYUPGRADE
_APPLIEDITEMS
_APPLIEDITEM
_EGGINCUBATORS
_EGGINCUBATOR
_POKEMONFAMILY
_MAPCELL
_FORTDATA
name='last_modified_timestamp_ms',
_FORTLUREINFO
_FORTSUMMARY
_WILDPOKEMON
name='encounter_id',
_POKEMONDATA
_MAPPOKEMON
_NEARBYPOKEMON
_GLOBALSETTINGS
_FORTSETTINGS
_MAPSETTINGS
_LEVELSETTINGS
_INVENTORYSETTINGS
_b85dec[c]
character
implicit_pip
req.name
delete_tmpdir
[0]
turn_steps
jump
lon1
Version
"Programming
boto.sqs.connection
port=port,
is_secure=is_secure,
kwargs['region']
boto.dynamodb.layer2
CloudSearchDomainConnection
object_name
sha256
HmacKeys):
AuthHandler.__init__(self,
b64_hmac
host):
['hmac-v4-s3']
len(value)
simplejson
'Boto',
self.host_header
params:
boto.log.debug(msg)
'max_retry_delay',
seconds'
verb='GET'):
verb)
parent)
CannedACLStrings
'aws_secret_access_key',
self.security_token
profile_name,
file.")
keyring
metadata:
self.delete_marker
User
boto.s3.deletemarker
DeleteMarker
function_name,
**args):
get_bucket(self,
get_contents_to_file(self,
hash_algs=hash_algs)
self.get_key(validate,
delim
self.object_name
self.object_name)
reduced_redundancy:
set_contents_from_file(self,
metadata=None,
encrypt_key=False,
enable_logging(self,
lifecycle_config
locale
metadata,
timeout=None,
command,
self.command
parameters=None):
parameter
params=None):
'VersionLabel':
option_settings,
template_name=None,
template_name:
params['TemplateName']
template_name
application_name=None,
severity
self.version_label
str(response['VersionLabel'])
response['OptionSettings']:
get_regions,
load_regions
RegionData
"JSON",
template_body
tags)
stack_name_or_id,
self.creation_time
self.capabilities
'StackId':
(self.key,
self.logical_resource_id
self.physical_resource_id
self.resource_status
self.resource_status_reason
config):
caller_reference='',
cnames=None,
comment='',
self.trusted_signers
self.logging
encoding="UTF-8"?>\n'
signer
new_config
objs
replace)
ip_address
'Path':
prefix=''):
external_id
subnet_id=None,
_id,
sdf
default='',
lookup(self,
self.update_date
self.save()
'IpAddress':
ip
q=None,
return_fields=None,
size=10,
facet
self(query)
'SortEnabled':
sortable
source_field
self.build_complex_param(params,
deployed=None):
params['Deployed']
deployed).lower()
self.make_request(action=action,
s3_key_prefix
{'applicationName':
deployment_config_name
'applicationName':
service_role_arn
deployment_id,
instance_id):
'IdentityPoolId':
max_results,
dataset_name,
resource_type,
resource_id,
Message
body='',
pipeline_id):
bandwidth,
owner_account,
attributes_to_get,
expected_value=None,
return_values=None):
400:
(msg,
expected_crc32
get_item(self,
request_items,
return_values=None,
delete_item(self,
exclusive_start_key:
esk
d[attr_name]
{'ReadCapacityUnits':
object_hook=self.dynamizer.decode)
'limit':
self.hash_key_name
self.range_key_name
schema(self):
write_units)
isinstance(n,
dct:
projection_type
'KEYS_ONLY'
schema_data
self._data[key]
boto.dynamodb2
condition_expression
params['ExpressionAttributeNames']
projection_expression
segment=None,
self._results_left
self._max_page_size
ResultSet,
self.schema
indexes
describe(self):
data=data)
**filter_kwargs):
attributes=None):
'BETWEEN',
regions(**kw_params):
instance_id=instance_id,
self.attribute_values
self.volume_id
volume_id
self.attach_time
'volumeId':
StringProperty,
'm1.large',
Snapshot
boto.ec2.securitygroup
user_ids=None,
groups=None,
user_ids,
security_group_ids=None,
private_ip_address
product_description=None,
params['ProductDescription']
hour
vpc_id=None,
ip_protocol=None,
src_security_group_name:
ip_protocol:
param_name
params[param_name]
ip_protocol,
offering_type=None,
value=None,
'groupName':
self.owner_alias
self.__dict__.update(updated.__dict__)
self.kernel
self.ramdisk
self.attrs
self.attrs:
'ownerId':
self._in_monitoring_element
self.instance_profile
'instanceType':
'subnetId':
self._update(rs[0])
self[self._name]
self.currency_code
'start':
self.modification_id
from_port,
to_port,
src_group_group_id
self.cidr_ip
self.fault
self.actions
boto.ec2.autoscale.policy
zones
topic}
'DesiredCapacity':
self.end_time
'member':
dimensions
instances:
self.policies
self.health_check
success
self.policy_name
task_definition,
params['maxResults']
cache_node_type
notification_topic_arn
source=None,
('ParameterName',
input_bucket
notifications
'Ec2KeyName',
self.steps
self.instancegroups
ClusterStatus()
boto.s3.bucketlistresultset
'.join(fields))
AssertionError(message)
total_parts,
part_number,
response['TreeHash']
vault_name):
(vault_name,
item_name
power
uploader
archive_id
self._buffer_size
self.closed:
operation
'<%s>%s</%s>'
self.email_address
self.name:
GSResponseError
e.status
details,
if_generation
xml_str
1;
boto.s3.keyfile
KeyFile
num_cb=num_cb,
md5[1]
size=size)
RULE
self.server_has_bytes
file_length
key.bucket.connection.debug
params['PathPrefix']
'UserName':
role_name):
type(allow_users_to_change_password)
'StreamName':
grant_tokens=None):
params['GrantTokens']
key_id):
number_of_bytes
log_stream_name,
batch_prediction_id,
data_source_name
compute_statistics
evaluation_id,
over
iobject.get_filename('Path
Name",
option)
region.connect()
instance.id
self._instance
vtimeout)
self.put()
print('-->
okay
self._config
self.get_ssh_client()
response_groups=None):
response_groups:
response_groups,
'ResponseGroup')
sort_by,
'PageSize':
page_size,
'PageNumber':
hit_id}
{'QualificationTypeId':
integer_value
identifier,
@api_action('Feeds',
@api_action('Orders',
Element(
default_os
hostname_theme
default_availability_zone
default_subnet_id
configuration_manager
chef_configuration
use_custom_cookbooks
use_opsworks_security_groups
custom_cookbooks_source
default_ssh_key_name
default_root_device_type
ssh_public_key
stack_id):
layer_ids
db_user,
db_user
boto.pyami.scriptbase
ScriptBase
(NoOptionError,
NoSectionError):
boto.config.get(self.name,
'available':
'mysql')
master_username,
db_subnet_group_name,
engine,
option_group_name,
ec2_security_group_name=None,
params['EC2SecurityGroupName']
params['EC2SecurityGroupOwnerId']
params['CIDRIP']
engine_name,
'DBSnapshotIdentifier':
self.allowed_values
option_group_name=None,
publicly_accessible=None,
db_parameter_group_name
multi_az).lower()
publicly_accessible
params['PubliclyAccessible']
publicly_accessible).lower()
'ClusterIdentifier':
cluster_subnet_group_name
cluster_parameter_group_name
automated_snapshot_retention_period
p.name
ptype
boto.route53.record
10:
self.alias_hosted_zone_id
self.alias_dns_name
ttl=None,
all=False):
self.find_records(name,
all=all)
idn_lang_code
boto.s3.lifecycle
self.connection.provider
404:
encoding_type=encoding_type)
key_marker='',
key.name
'&versionId=%s'
'REDUCED_REDUNDANCY'
fields.append({"name":
allowed_method
self.mode
cb(data_len,
cb_size)
cur_file_size
tag_set
RoutingRules()
self.converter:
domain_or_name,
expected_value=None):
query='',
currently
obj.id
cls:
isinstance(prop,
prop_name
prop.name,
get_value_for_datastore(self,
model_instance):
item_type,
443)
boto.config.get(db_section,
getattr(prop,
"item_type")
self._connect()
self.decode_value(prop,
prop.make_value_from_datastore(value)
self.encode_value(property,
action="store",
endpoint_arn
queue)
messages):
(base,
'RoleSessionName':
duration_seconds
self.credentials
attachment_set_id
task_list=None,
workflow_version},
registration_status,
latest_date,
{'oldestDate':
'latestDate':
"builtins",
"__builtin__",
attr.name,
'RuleAction':
vpn_gateway_id,
insert_option(params,
'GatewayId':
self.associations
self.destination_cidr_block
endpoint_prefix
partition_name):
service_name,
download'
phony_mimetype
k.md5
value'
newly
EC2
instance",
test_create_domain(self):
RegionInfo(
DynamoDB'
PM'}
key=key1,
'multipart
text',
A',
retrieved
'N'),
self.addCleanup(users.delete)
'Jane',
10},
self.assertEqual(len(batch._unprocessed),
thread__eq='Favorite
chiptune
band?',
'RANGE',
'tcp',
'V',
elb
'HTTP')]
balancers
balancers],
[self.name])
self.conn.get_all_load_balancers(
NetworkInterfaceSpecification(
tests.integration.gs.testcase
GSTestCase
self.assertEqual(s1,
k.set_contents_from_file(sfp)
mg1
self.assertEqual(str(mg1),
"1")
mg2
self.assertEqual(g2,
self.assertGreater(mg2,
mg1)
("Received
"argument
argument")):
if_metageneration=123)
if_generation=int(g2)
if_generation=g2,
if_metageneration=int(mg2)
if_generation=g2)
g3
mg3
self.assertEqual(g3,
self.assertGreater(mg3,
mg2)
if_generation=g3,
if_metageneration=mg3)
b.set_xml_acl(acl,
ResumableDownloadHandler(
num_retries=1)
ResumableUploadHandler(num_retries=1)
'my
len(entries1g2))
entries2g1
entries2g2
[e
e.permission
"READ"
e.scope.type
acl.ALL_USERS]
_HAS_GOOGLE_CREDENTIALS
expected")
description')
time.sleep(self.wait_time)
'creating')
resource_path="/testing")
self.assertEquals(record.ttl,
'Enabled',
"Enabled",
self.assertEqual(rule.prefix,
S3ResponseError:
write_failed
self.assertEqual(self.my_cb_cnt,
pn
match')
close_latest_date=latest_date,
boto.mturk.qualification
Qualifications,
maxDiff
section_name,
httpretty
'secret_key')
'Two',
AWSAuthConnection(
'mockservice.cc-zone-1.amazonaws.com',
'https://%s/'
conn.make_request('myCmd1',
.html',
'application1',
'version1',
'Tags.member.1.Key':
'MyStack')
boto.resultset.ResultSet([
self.assertEqual(response.status,
private_key_string=self.pk_str)
json.loads(policy)
len(policy.keys()))
policy["Statement"]
len(statements))
statements[0]
statement["Resource"]
statement["Condition"]
len(condition.keys()))
'demo',
self.assertEqual(args['type'],
self.assertEqual(doc.status,
'success')
self.assertEqual(doc.adds,
self.assertEqual(doc.deletes,
'[]')
boto.log.error
self.assertRaisesRegexp(SearchServiceException,
response_status
search.build_query(q='Test')
dynamizer
self.assertEqual([part.attr_type
'S'},
'S'}
self.johndoe['first_name']
mock.patch.object(self.table,
"date_joined",
"RANGE"
self.assertEqual(self.users.throughput['read'],
self.assertEqual(self.users.throughput['write'],
'update_table',
mock_update:
mock_get_item:
'batch_write_item',
mock_batch:
self.users.batch_write()
12342547
self.assertEqual(mock_query.call_count,
check_that_attribute_has_been_set(self,
attribute),
copied_ami
self.ec2.copy_image('us-west-2',
self.assertEqual(copied_ami.image_id,
'ami-copied-id')
'CopyImage',
self.ec2.delete_snapshot
"value1",
"key2":
'key1',
taggedEC2Object.tags["key1"]
"value1"
taggedEC2Object.tags["key2"]
"value2"
'DeleteTags',
self.assertEqual(len(api_response),
dry_run=False)
value",
AutoScalingGroup(
'vault_name',
'LastInventoryDate':
'VaultARN':
'VaultName':
'NumberOfArchives':
self.vault,
upload_part_calls,
LocaleRequirement(
locale=[('US',
'US',
'CA',
'env_access_key'
'env_secret_key'
'cfg_secret_key',
'db.m1.large')
self.service_connection.create_bucket('mybucket')
qa
host='s3.amazonaws.com'
conn.generate_url_sigv4(86400,
self.assertEqual(self.keyfile.tell(),
self.assertEqual('',
'Publish',
'Subject':
'Number',
'creationDate':
boto.utils.host_is_ipv6(hostname)
self.assertFalse(result)
self.assertIsInstance(api_response[0],
'vpc-1a2b3c4d',
self.vpc_id,
'vgw-8db04f81',
sys.path.insert(0,
nodes
operator
info,
rolename
set_classes(options)
nodes.reference(rawtext,
match.group(1)
seconds
reactor.run()
self).__init__(*a,
dont_filter=True)
sys.version_info
print("Scrapy
optparse
inproject):
print()
see
sys.modules:
isinstance(settings,
exc_info
down
close(self,
debug)
self.debug:
xml.sax.saxutils
self._configure(kwargs)
dict(self._get_serialized_fields(item))
'\n'
scrapy.middleware
MiddlewareManager
build_component_list
component_name
creating
common
isinstance(url,
could
python
'level':
'attachment;
d,
_add_middleware(self,
scrapy.utils.datatypes
mimetypes
'scrapy.http.XmlResponse',
'scrapy.http.TextResponse',
d.callback,
kwargs.setdefault('sender',
self.sender)
implementer
start)
arglist_to_dict
metavar="FILE",
stderr
(default:
%s)"
default=[],
LinkExtractor
time.sleep(0.2)
start,
write
stop
(%s)"
DefaultSpider
downloader
You
want
disable
six.PY2
exists,
module,
(BaseItem,
items)
metavar="SETTING",
BaseSettings):
names:
dst)
%r,
lxml.etree
iterate_spider_output
_create_testcase(method,
sys.exc_info())
objects
output:
self.closing
crawler.settings
logger.error('Error
exc_info=True,
slot.closing
request=request,
logger.error(
close
request_to_dict,
len(self)
priority):
self.crawler.engine.close_spider(spider,
output,
ex
request.meta:
slot.delay
defer.returnValue(response)
twisted.web.http
encoding='ascii')
netloc
self.length
self.transport.loseConnection()
self.deferred
responsetypes.from_args(headers=headers,
urlparse_cached(request)
ex:
status=200,
HTTP10DownloadHandler
HTTPDownloadHandler
stacklevel=1)
twisted.internet.error
proxyPort,
{'status':
self._port,
tunnel_req
self._connectTimeout
txresponse,
self._finished
scrapy.utils.boto
is_botocore
_mexe(self,
kw:
self.anon:
v}
w3lib.http
ConnectionLost,
NotConfigured,
stats):
crawler.signals.connect(o.spider_closed,
'Date'
response.headers:
isinstance(exception,
type)
'proxy'
Deferred,
maybeDeferred
rp
True}
last
traceback
os.path.isdir(path):
's3',
{'scheme':
[int(x)
301,
'headers':
mtime
self.interval
60.0
pages
listening
self.port.stopListening()
diff
1000,
'size':
new_delay
values(self):
self._encoding
self.priority
get_base_url
'POST'
form
seq
(n,
%r'
request"
self._cached_ubody
_extract_links(self,
response_url,
link.url)
urljoin(base_url,
extract_links(self,
unique=False,
response.body
default_item_class
Identity()
**context):
field_name):
buf,
'checksum':
mapping
'file':
settings.get(
referer},
info=info)
isinstance(request,
image,
`_root`
set(self,
self.attributes
self._assert_mutability()
modify
400,
'localhost'
verbose
Rule
compatibility
s.type
nl
botocore
dict.__setitem__(self,
"Returns
empty
key.lower()
f(*args,
cname
"deprecated
there
new_path
**kwargs))
checks
tests:
chunk:
self._ptr
encoding)
six.binary_type)
emit(self,
logkws['args']
fullpath
isinstance(text,
ImportError
inside
priority='project')
errors='strict'):
encoding,
getattr(func,
spec
func):
(status,
default(self,
sender=sender,
caught
interface="127.0.0.1")
__new__(cls,
oldest:
urllib.parse
host:
If
twisted.internet.interfaces
decoder
self._responseDeferred
many
transport,
code,
self.version
defaultPort
endpoint):
bodyProducer=None):
self.type,
privateKey
ValueError("Unknown
C{str}
like
tests_datadir
render_GET(self,
'error'
MockServer
self.mockserver
self.mockserver.__enter__()
self.mockserver.__exit__(None,
crawler.crawl()
out)
self.project_name)
exists(join(join(abspath(project_dir),
self.project_name),
'a')
self.should_succeed()
t[0]
self._assert_retried(l)
l.records[0]
ZeroDivisionError)
self.assertIn('scheme',
self.download_request
self.port.getHost().port
(self.scheme,
Request(self.getURL('file'))
b"0123456789")
(self.host,
'gzip',
self.portNum,
meta={"ftp_user":
self.username,
"ftp_password":
self.download_handler.download_request(request,
_test(r):
self._add_test_callbacks(d,
test_basic(self):
req.headers
Request('http://scrapytest.org/sub1/')
self.assertEquals(req.headers.get('Cookie'),
request.headers)
storage.retrieve_response(self.spider,
self.assertRaises(IgnoreRequest,
304,
Response(req0.url,
res304
(_,
open(join(SAMPLEDIR,
self._getresponse('gzip')
Request("http://www.example.com/")
'application/octet-stream'
self.assertEquals(req.meta,
status=301)
'text/plain',
'Content-Type'
present
HtmlResponse(req.url,
body=self._body())
''')
self.run.signals_catched
self.ie.export_item(item)
age=i1)
TestItem(name=u'Jesus',
age=i2)
{'age':
dict:
os.path.dirname(os.path.abspath(__file__))
scrapy.Field()
settings.update({'FEED_FORMAT':
'spam2',
hello
'spam',
'bar'},
[b'ip1',
b'value3'])
'Unsupported
type',
self.assertEqual(r1.url,
self.assertEqual(r2.encoding,
r2.url)
setattr,
100'}
'seven'})
self.assertEqual(req.headers['Content-type'],
1</option>
2</option>
type="hidden"
response_class
self.response_class,
body=b"Some
self.assertEqual(response.text,
self.assertEqual(response.encoding,
absolute
body')
body2
i['name']
u'john
TestItem,
'test')
text="test",
nofollow=False)
get_testdata('link_extractor',
Link(url='http://example.com/innertag.html',
text=u'inner
text=u'About
self.assertEqual(lx.matches(url1),
self.assertEqual(lx.matches(url2),
text='Item
BaseSgmlLinkExtractor()
['text/html;
il.replace_value('name',
NameFirstItemLoader()
'//div/text()')
NestedItemLoader(response=self.response)
2],
Response("http://www.example.com")
mailsender
MailSender(debug=True)
mailsender.send(to=['test@scrapy.org'],
subject='subject',
_callback=self._catch_mail_sent)
self.catched_msg
'subject')
self.catched_msg['msg']
attach
m2,
[M1,
k:
self.assertNotEqual(custom_value,
pipeline_cls
name),
random.randint(1000,
pipe_attr.lower()),
Request('http://url1',
cert_path
get_crawler(SimpleSpider)
l)
TextResponse),
self.assertTrue(issubclass(cls,
self.assertTrue(isinstance(sel,
self.assertTrue(isinstance(usel,
'_follow_links'))
self.assertEquals(out,
self.assertRaises(HttpError,
self.mw.process_spider_input,
reqs
reqs,
self.assertEqual(stats.get_stats(),
's3')
'four':
'four',
2},
CaselessDict()
self.assertEqual(d['A'],
self.assertEqual(d['key'],
self.assertEqual(d.get('key'),
2])
another
arg2):
(value,
'res',
5:
UserClass(Deprecated):
self.assertTrue(is_gzipped(r1))
iter
self.failIf(equal_attributes(a,
shouldn't
self.failIf(any(isinstance(x,
rs)
'2009-08-16',
'changefreq':
self.assertTrue(url_is_from_any_domain(url,
`%s`
start=1):
str))
foo\r\n"
self.settings.attributes)
self.settings.attributes
{'key1':
options):
Error,
Name:
entries
successfully
one=True)
whom_id
?''',
do
register_and_login(client,
add_message(client,
1')
.helpers
_request_ctx_stack,
.signals
PY2:
integer_types
(int,
text_type,
Lock()
view
static_url_path
name(self):
rule,
provide_automatic_options
[]).append(f)
tb)
dispatch_request(self):
_sentinel:
environ,
start_response)
subdomain
url_prefix
app_id
self._app
self._bg_loading_exc_info
app_import_path
kwargs.setdefault('cls',
namespace,
%r)'
self.session
app_ctx
_request_ctx_stack
'file
system
int,
info.append('
fullname
outside
gen
blueprint_name
headers['Content-Length']
cache_timeout
because
configured
'loads',
{'
datetime.utcnow()
app.config['SESSION_REFRESH_EACH_REQUEST']
trv
self.application
shortname
pattern,
flask.ext
font-size:
endfor
changed
put
inner
flask.g
method='OPTIONS')
]))
SECRET_KEY='foo',
flask.session['testing']
rv.headers
response.data
2]
called.append(True)
Error'
flask.abort(403)
found'
u'Hällo
_external=True)
dict()
b'foo'
c.get('/',
super_reverse(s):
Module:
StringIO,
flask.request.get_json()
data=None,
render('{{
flask.send_file('static/index.html')
flask.send_file(f)
post(self):
flask.request.args['name']
Index(flask.views.MethodView):
dist
4:
error')
check
action='store_true')
required'
enabled=True,
provider,
provider:
stdin=subprocess.PIPE,
res:
origin
300
31
202
j,
"wb")
shutil.rmtree(tmpdir,
ignore_errors=True)
boto.ec2.autoscale
boto.sns
SNSConnection
CloudFormationConnection
boto.ec2.regioninfo
host=host,
boto.cloudsearchdomain.layer1
CloudTrailConnection
validate=True,
suppress_consec_slashes=True,
sha
HmacKeys.__init__(self,
string_to_sign
string_to_sign)
sts
req.path
AuthHandler):
'X-Amz-Date':
(qs,
'sqs',
_wrapper
0.0
is_secure)
is_secure
self.https_connection_factory
config.getint('Boto',
self.proxy_port
self.use_proxy
boto.log.debug(
sock
resp.status,
sslSock
request.port,
60))
msg:
build_list_params(self,
isinstance(items,
[items]
self).__init__(status,
self.body)
'Code':
self).__init__(message)
self.connection)
http_client
(%s):
security_token
self._credential_expiry_time
self._secret_key
'ItemName':
function_name):
self.scheme)
delimiter=delimiter,
get_all_keys(self,
new_key(self,
version_id=version_id)
res_download_handler=None,
is_latest
self.generation:
self.get_provider().name
bool(not
create_bucket(self,
set_xml_acl(self,
set_canned_acl(self,
set_contents_from_string(self,
set_contents_from_stream(self,
copy_key(self,
dst_bucket
preserve_acl=preserve_acl,
disable_logging(self,
enabled,
exists(self,
qsa
self.exit_code
self._dict
previous
value.encode('utf-8')
boto.config.get_value('Notification',
size,
data_size
kwargs.get('region')
super(Layer1,
application_name}
'TemplateName':
'OptionSettings.member',
('Namespace',
'OptionName',
describe_events(self,
'{
str(member)
str(response['Name'])
load_balancer
self.listeners
'DisableRollback':
disable_rollback
boto.cloudfront.distribution
boto.cloudfront.identity
'text/xml'},
201:
cnames
'Comment':
condition["DateLessThan"]
msg_base64
add(self,
client_arn,
{'type':
self.content
DocumentServiceConnection
facet=facet,
create_domain(self,
save(self):
'Statement'
rank=None,
self.q
self.sign_request
api_version)
api_version
boto.cloudsearch2.optionstatus
searchable,
','.join(source_field)
boto.cloudsearch2.domain
expr
self.partial
**params)
sort_order=None,
identity_pool_id):
'MaxResults':
'pipelineId':
'RangeKeyElement'
{'AttributeValueList':
v1,
range_key=None,
attr_value)
save(self,
expected_value,
reg.name
json_input)
put_item(self,
expected:
scan(self,
Table
callable
self.remaining
expected_value:
{'Value':
write_units):
'attributes_to_get':
isinstance(attr,
attr))
attr]
boto.dynamodb2.layer1
DynamoDBConnection
'ALL'
self._orig_data
final_data,
expression_attribute_names=None,
expression_attribute_values=None):
params['ConditionalOperator']
params['ExpressionAttributeValues']
select=None,
total_segments=None,
data=body,
response.read())
self._offset
self._last_key_seen
connection=connection)
self.schema:
gsi_data
self._build_filters(
delete_on_termination=False,
encrypted
(value
boto.ec2.snapshot
kernel_id=None,
ramdisk_id=None,
architecture
'ImageId':
{'ImageId':
next_token=next_token)
key_name=None,
instance_type:
params['EbsOptimized']
force:
attr.lower()
enumerate(value):
instance_type=None,
params['Prefix']
params['NetworkInterfaceId']
volume_id}
new_value
group.name
group_id,
params['OfferingType']
self.get_object(
attribute_names
value},
'blockDeviceMapping':
'imageId':
'groupSet':
Group)])
self.state_reason
self.client_token
self.ebs_optimized
r.instances:
section
code=None,
'details':
self.attachment
add_rule(self,
src_group_name
r.instances]
self.grants.append(grant)
self.volume_size
self.attach_data
Volume
'eventType':
'MinSize':
'MaxSize':
'ScalingAdjustment':
autoscale_group
params['AutoScalingGroupName']
topic):
boto.ec2.elb.listelement
self.process_name
self.min_size
self.max_size
default_cooldown
self.launch_config_name
self.desired_capacity
ebs_optimized
'Iops':
self.adjustment_type
'AdjustmentType':
'ResourceId':
statistics,
dimensions:
statistics
key)]
period,
'Namespace':
value=value,
end_date:
self.actions_enabled
subnets):
self.instance_port
self.instance_protocol
SecurityGroup()
force=False):
'GroupName':
self.curItem
engine=None,
params['Engine']
'CacheParameterGroupName':
cache_subnet_group_name,
replication_group_id,
params['Source']
params['ApplyImmediately']
params['ResetAllParameters']
instance_group
self.properties
'Marker'
market
input=None,
Key(self.name,
fp)
self.key_type
self.fp
wrapper(self,
func(self,
'Result',
boto.glacier.exceptions
worker_queue
result_queue
self._shutdown_threads()
work):
start_byte
byte_range
job
tree
ok_responses=(204,))
boto.glacier.vault
chunks
part_index
{'Type':
sns_topic
self.entries
self.permission
self.type.lower()
query_args_l
generation:
new_key_name,
isinstance(acl_or_str,
get_utf8_value(self.name),
acl_str
self.connection.provider.storage_permissions_error(
Permission:
permission)
self.bucket_class(self,
self.collection
self.content_encoding
response_headers=response_headers,
headers[provider.acl_header]
os.SEEK_SET)
KeyFile):
self.set_contents_from_file(fp,
self.bucket.connection.make_request(
encoding="UTF-8"?>'
Restarting
'ID':
to_xml(self,
'<ID>%s</ID>'
instance_profile_name,
{'InstanceProfileName':
assume_role_policy_document
{'StreamName':
params['Policy']
{'logGroupName':
{'MLModelId':
self.uname
self.ssh_key_file
print('%s
print('...complete!')
IP
StringProperty(required=True,
week_boundary
prompt='Choose
hit_type,
questions
final_keywords
HIT)])
sort_direction='Ascending',
'SortDirection':
sort_direction,
page_number}
{'AssignmentId':
get_as_params(self):
get_as_params(self,
self.style_suggestion
members=members)
suffix,
@requires(['MarketplaceId'])
'Destination'])
@structured_objects('Destination',
ElementList()
{'ElasticIp':
rds_db_instance_arn,
sdb
=class
BY
allocated_storage,
db_name=None,
preferred_backup_window=None,
db_name,
'OptionGroupName':
'SourceDBInstanceIdentifier':
params['BackupRetentionPeriod']
params['PreferredBackupWindow']
params['DBParameterGroupName']
ec2_security_group_owner_id=None):
'LogFileName':
{'DBSubnetGroupName':
engine_name=None,
major_engine_version=None,
self.master_username
'Endpoint':
self.apply_type
resource_name,
vpc_security_group_ids=None,
vpc_security_group_ids
vpc_security_group_ids,
db_snapshot_identifier,
event_categories=None,
event_categories
event_categories,
'EventCategories.member')
cluster_subnet_group_name,
self.required
l):
'%d'
boto.route53.connection
ResourceRecordSets
default_ttl
comment)
name=name,
Prefix
expires_in,
headers[provider.storage_class_header]
self.set_acl(policy,
CORSConfiguration()
key=''):
query_part
allowed_header
'<CORSRule>'
'</CORSRule>'
self.resp
self.transition
how_long
greater
self.etag_value_for_current_download
cls):
expected_value[1]
doc):
self.decode_value(value)
**kwds):
manager
object.__setattr__(self,
objtype):
expecting
type(value)))
default_value(self):
self).__set__(obj,
six.integer_types:
lv
t.id
tt.name
item_type.mro():
self.db_name
self._domain
obj.id:
self.sd.get_obj('output_bucket')
elapsed_time
boto.lookup('s3',
bucket.new_key(key_name)
empty_reads
super(ServiceDef,
message_attributes=None):
queue):
queue.arn
params['PlatformApplicationArn']
self.s3_url
'VisibilityTimeout'
queue.id,
(i+1))
sep='\n'):
params['DurationSeconds']
case_id
params['language']
task_list},
workflow_id},
task_list}
attrs['control']
child_policy
task_token=None,
(3,
old_attr
rule_number,
internet_gateway_id,
'InternetGatewayId':
'VpnGatewayId':
vpn_connection_id,
'VpnConnectionId':
'VpcPeeringConnectionId':
original
"A",
like",
Object")
55,
k.generate_url(3600,
mdval1)
mdval2)
@unittest.skipUnless(simple,
self.random_id
'32bit
cls.random_id
environment_name=self.environment)
"Fn::GetAtt":
"Ec2Instance",
Layer2()
boto.connect_iam()
read_units
write_units
item1_key
item1_range
self.schema,
set(['largeobject',
results.count
'S'))
'jane'
username__eq='johndoe',
global_indexes=[
[post['posted_on']
results],
'2013-12-24T12:30:54',
'2013-12-24T12:35:40',
'2013-12-24T13:45:30',
'2013-12-24T14:15:14',
'2013-12-24T14:25:33',
'2013-12-24T15:22:22',
users.describe()
self.assertEqual(len(users.global_indexes),
'date_joined'
'Jane'},
'Expert'},
test_1_basic(self):
reservation.instances[0]
self.fail("Should
time_string
min_size=1,
unittest,
c.build_put_params(params,
'MetricData.member.1.MetricName':
'N',
'D',
more_listeners)
'HTTP',
self.balancer.get_attributes()
self.assertEqual(False,
self.assertEqual(True,
self.assertEqual(5,
cluster_id)
k.get_contents_as_string()
bucket.get_acl()
self.assertEqual(len(acl.entries.entry_list),
bucket.get_def_acl()
uri.get_def_acl()
k.set_xml_acl(acl,
KB.
self.assertTrue(os.path.exists(tracker_file_name))
self.assertNotEqual(
versions
"foo")
time_suffix
MarketplaceId=self.marketplace_id,
rds
Route53Connection()
self.zone.delete()
hc_type="HTTP",
self.assertEquals(result[u'CreateHealthCheckResponse'][u'HealthCheck'][u'HealthCheckConfig'][u'Type'],
self.assertEquals(result[u'CreateHealthCheckResponse'][u'HealthCheck'][u'HealthCheckConfig'][u'Port'],
u'HealthCheck'][u'HealthCheckConfig'][u'ResourcePath'],
"test"
version_id=NOT_IMPL,
md5=NOT_IMPL,
key_name=NOT_IMPL,
mock_connection
rule.id
connect_args
urlopen(url)
file.read().decode('utf-8'),
self.assertEqual(v1,
self.assertEqual(k.size,
content="01234567890123456789"
self.bucket.get_versioning_status()
next(i)
'test%d'
mock_request:
self.addCleanup(c.delete_queue,
msgs
'data_type':
self.assertEqual(err.status,
'REGISTERED',
close_oldest_date=oldest_date,
sel.wait_for_page_to_load("30000")
HTTPretty
HTTPretty.enable()
HTTPretty.disable()
'secure'}),
parse_qs(HTTPretty.last_request.body)
b'{"test":
HTTPretty.register_uri(HTTPretty.GET,
body=xml)
'us-east-1':
'x/./././x
template_body=SAMPLE_TEMPLATE,
parameters=[('KeyName',
'Tags.member.1.Value':
'arn:aws:cfn:us-east-1:1:stack')
'CREATE_COMPLETE')
'next_token',
datetime.datetime(2011,
"InProgress")
max_items)
("Nql641NHEUkUaXQHZINK1FZ~SYeUSoBJMxjdgqrzIdzV2gyEXPDN"
"v0pYdWJkflDKJ3xIu7lbwRpSkG98NBlgPi4ZJpRRnVX4kXAJK6td"
"Nx6FucDB7OVqzcxkxHsGFd8VCG1BkC-Afh9~lOCMIYHIaiOB6~5j"
"t9w2EOwi6sIIqrg_")
aws_epoch_time
aws_epoch_time)
self.dist._custom_policy(url,
SearchServiceException
'-text_relevance',
'match-expr':
"Test",
'hits':
'found':
'hit':
'rid':
'b7c167f6c2da6d93531b9a7b314ad030b3a74803b4b7797edb905ba5a6a08',
'time-ms':
'cpu-time-ms':
api_response['CreateDomainResponse']
"AttributeType":
self.assertNotEqual(s1,
[{'N':
'friend_count'
'first_name',
'12345'}
mock_put_item:
self.result_function.assert_called_with('john',
self.result_function.reset_mock()
#0')
#1')
count['value']
'Update':
'get_item',
'query',
items_1
self.assertEqual(results.the_callable,
return_value=items_1)
res_1
self.assertEqual(res_1['username'],
res_2
self.assertEqual(res_2['username'],
test_endElement_sets_correct_attributes_with_values(self):
name",
"/dev/null",
'Encrypted':
'2567o137-8a55-48d6-82fb-7258506bb497'
self.ec2.register_image('name',
'RegisterImage',
self.eni_one.connection
@mock.patch("boto.ec2.volume.TaggedEC2Object.startElement")
startElement):
mock.ANY,
ELBConnection(aws_access_key_id='aws_access_key_id',
elb.make_request
'One',
InstanceGroup(1,
'm1.small',
'master')
'upload_id',
'tree_hash'
1024),
"CreationDate":
"VaultARN":
"SHA256TreeHash":
compute_hashes_from_fileobj(f,
chunk_size=512)
create=True):
self.part_size,
self.chunk_size)
MOCK_SERVER_RESPONSE
test_requirement
qualifications.add(test_requirement)
str(err.exception))
self.environ['AWS_ACCESS_KEY_ID']
self.environ['AWS_SECRET_ACCESS_KEY']
'env_access_key')
'env_secret_key')
'simcoprod01')
@mock.patch('time.sleep')
sleep_mock):
ip_ranges)
rrsets.add_change_record('CREATE',
tests.integration.s3.mock_storage_service
'versionless_uri'))
uri.versionless_uri)
pretty_print_xml
self.service_connection\
'TopicArn':
'subject',
Queue(
domain='test',
'1.0'},
'REGISTERED'},
{'activityType':
'ProcessFile',
self.assertEqual(_build_instance_metadata_url(
self.assertEquals(len(api_response),
'pending')
'rtb-f9ad4890')
glob
refid
app.add_crossref_type(
directivename
indextemplate
"pair:
refuri=ref,
'r'
deque
twisted.web.server
twisted.web.resource
self.concurrent
getChild(self,
request.args:
FormRequest
[options]
func,
sys.argv
conf
cmd,
ISpiderLoader
self.spidercls
self.signals
signals.engine_stopped)
self.crawling
instantiate
started
signum,
signame
request_fingerprint
log(self,
path=None,
request_fingerprint(request)
duplicate
marshal
to_unicode,
serialize_field(self,
serializer
field_iter
default_value
self.encoder
4):
six.PY3
strings
value]
pformat
attrs)
n)
key))
__slots__
encoding='utf8')
library
Failure()
(referer:
flags
src
mail
logger.error('Unable
defaultdict(list)
crawler)
mw):
self.crawler.settings
b.append("
dispatcher
scrapy.spiderloader
protocol=2)
self._stats
get_value(self,
ID
metavar="NAME=VALUE",
(may
repeated)")
process_options(self,
UsageError("Invalid
NAME=VALUE",
print_help=False)
"Run
get_testenv
link_extractor
failed,
spider_loader
self.crawler_process.spider_loader
self.settings.set('FEED_URI',
opts.output_format
EDITOR
'.py')
parser.add_option("--spider",
dest="spider",
prefix):
callback=cb,
spidercls_for_request(spider_loader,
abspath,
template_file
"."
module)
CrawlSpider
STATUS
spider:
e))
value")
interpreted
isinstance(s,
ignore
project_dir
'project')
OpenSSL
bug
seen
contracts:
results)
args):
self.min_bound
scrapy.utils.iterators
download(self,
slot,
received
spider=spider,
reason},
spider}
failure'))
dupefilter
being
%(request)s',
spiders
'cancelled')
object,
cls_name
DownloadHandlers(crawler)
age
None)))
getContext(self,
port=None):
against
netloc,
handler:
'%s':
'close'):
unquote
len(data)
self.filename
HTTP11DownloadHandler
scrapy.xlib.tx
self._pool
contextFactory,
proxyHost,
open
contextFactory=None,
pool)
parsedURI,
bindAddress
maxsize
self._warnsize
warnsize
larger
respcls(url=url,
finished
self._bytes_received
_S3Connection
settings['AWS_ACCESS_KEY_ID']
aws_secret_access_key:
settings['AWS_SECRET_ACCESS_KEY']
anon
is_botocore():
keyword
'http'
%(request)s",
jar
to:
cookie_str
BytesIO(response.body)
response.replace(body=body,
cls=respcls)
fmt,
usr
IOError)
flag
cache:
gunzip,
encoding):
urlunparse
self.priority_adjust
HtmlResponse):
retries
cls(crawler.stats)
request.method,
item_scraped(self,
scrapy.utils.engine
log_args,
self.crawler})
urlparse(uri)
remove
botocore.session
botocore.session.get_session()
storage,
y:
feed
(300,
304
self._request_key(request)
'body':
gc.collect()
wdict
self.limit
sys.platform
tsk
s)
"This
scrapy.http.response.text
matches
super(Headers,
def_val=None):
getlist(self,
self).__getitem__(key)
__copy__(self):
self._meta
"<%s
self.url)
__repr__
__str__
'body',
x))
response.encoding)
dict((k,
AttributeError(
any
TypeError('%s
kwargs.pop('encoding',
self._cached_benc
self._cached_selector
type(self).__name__)
arg_to_iter
0))
self.restrict_xpaths
url_is_from_any_domain(url,
links:
tag,
self.scan_tag(tag):
self.current_link.text
etree
arg_to_iter,
response_encoding,
response.encoding,
xpath,
subloader
css,
context):
self.default_loader_context
loader_context:
basedir
1)[1]
'private'
is_secure=False)
buf.seek(0)
kwarg
EXPIRES
settings.getint(
'file'
logger.debug(
self.file_path(request,
referred
status)
get_media_requests(self,
[])]
item_completed(self,
info=None)
'_base'):
deprecation
file_key(self,
settings=settings,
width,
height
thumb_id,
file_key(url)
pipe
_)
eb)
wad
rt
select(self,
default=0):
compbs
BaseSettings()
self.attributes[name]
'https':
-m
'xml':
report
[500,
50,
'127.0.0.1'
needed
variable
process_spider_output(self,
allow
result_item
define
csviter(response,
response=None):
associated
self.setlistdefault(key,
%d"
self.normkey(key),
pop(self,
dict_
self.dicts:
KeyError
callable,
elem
[x[1]
super(DeprecatedClass,
test,
"\n"
header_start
header_end
'DEBUG',
filename:
full
%s:
isinstance(data,
@deprecated
interface=host)
self._call
safe
password)
*arguments,
'}'
'default'
CrawlerRunner
os.environ.copy()
static,
r.putChild(b"redirect",
single
some
something
just
directly
len(parts)
transferDecoder
contentLength
persistent
transport):
RuntimeError(
RuntimeError("Cannot
self._finishedRequest
'Error
._newclient
connection:
endpoint)
('https',
'%s:%d'
self._agent.request(method,
self.message,
@implementer(interfaces.IStreamServerEndpoint)
backlog,
backlog
listen(self,
interface=''):
defer.fail()
certKey
plugin
example,
kwargs['port']
child
write(data):
type=None):
self.deferRequest(request,
_delayedRender(self,
abort
failure):
'simple'
tests.spiders
close_on
close_on})
self.assertEqual(reason,
close_on)
scrapy.utils.testproc
ProcessTest
err)
contextmanager
tempfile.TemporaryFile()
self.project_name))
'__init__.py'))
test_list(self):
content,
open(fname,
self.assertIn("INFO:
'-s',
TestSpider(Spider):
response.url}
TestSpider()
self.should_fail()
(1
self.assertTrue(crawler.spider.t2
self.assertTrue(crawler.spider.t2_err
self.assertEqual(len(l.records),
spider_settings
{'TEST1':
'spider')
self.assertOptionIsDefault(runner.settings,
self.assertNotIn('scheme',
self.assertEquals(response.url,
request.url)
self.assertEquals(response.status,
timeout=None)
self.portno
Spider('foo',
get_crawler(SingleRequestSpider)
meta={'proxy':
self.assertEqual(r.body,
_test)
settings_dict
get_crawler(Spider,
request=req)
{'body':
'Cookie'
self.mw.process_request(req2,
crawler.spider)
b'C1=value1')
get_testdata
body=b'')
self.assertEquals(req.meta.get('download_timeout'),
request2)
self.assertEqualResponse(self.response,
wait
req0.replace(headers={'Cache-Control':
self.assertEqualResponse(res2,
'foo'}),
Request('http://example-%d.com'
self.today,
res1.flags
Request('http://scrapytest.org',
b'gzip')
newresponse.body.startswith(b'<!DOCTYPE')
response.headers['Content-Type']
self.assertIs(newresponse,
self._oldenv
{'proxy':
'https://proxy:3128'})
self.assertEquals(req.headers.get('Proxy-Authorization'),
'http://www.example.com/301'
'http://www.example.com/redirected'
self.assertEqual(req2.method,
req2.headers,
self.mw.max_redirect_times
self.mw.process_response,
Request('http://scrapytest.org/first')
rsp2
meta={'handle_httpstatus_list':
self.mw.process_exception(req,
self.assertEqual(req.meta['retry_times'],
b'''
DeferredList([
fireOnOneErrback=True)
self.req
self.get_spider_and_mw('default_useragent')
spider.user_agent
hit
urls_expected
'age',
test_nested_item(self):
TestItem(name=u'Joseph',
[{'age':
i2)
include_headers_line=False,
storage.store(file)
egg
settings=None,
[{k:
row.items()
rows]
self.assertExportedCsv(items,
self.assertExportedJsonLines(items,
self.MyItem({'foo':
'spam1',
self.assertExported(items,
ordered=False)
{'FEED_EXPORT_FIELDS':
'hello':
'baz',
h['Content-Type']
hlist
['ip1',
h.getlist('X-Forwarded-For')
olist
Headers({'header1':
Headers)
[b'2'])
request_class
r1.copy()
shallow
copy,
identical"
body",
b"New
'url',
{b'one':
formdata=data)
self.assertEqual(r2.method,
self.assertQueryEqual(r2.body,
self.assertEqual(r2.headers[b'Content-Type'],
self.assertEqual(req.headers[b'Content-type'],
to_unicode=True)
self.assertEqual(fs[b'two'],
[b'clicked2'])
clickdata={u'name':
[b'3']})
value="i1v1">
value="i3v1">
type="text"
<textarea
self.assertEqual(fs[b'test1'],
[b'val1'])
[b'val2'])
self.assertEqual(fs[b'button1'],
'http://example.com')
req.method
XmlResponse,
self.response_class))
body"
joined
self.assertEqual(joined,
absolute)
self.assertEqual(resp.url,
'http://www.example.com',
self.response_class('http://www.example.com',
body=b"\xc2\xa3")
self._assert_response_values(r1,
u"\xa3")
self.response_class(url,
u'WORD')
r.text,
repr(r.text)
Selector)
'name')
doe')
'other':
test_repr(self):
Field(default='A')
'A'},
'C'},
{'save':
'load':
l2
fragment='something',
Test
Link(url='http://google.com/something',
body=html,
12')])
test_link_wrong_href(self):
HtmlResponse("http://example.org/index.html",
Link(url='http://example.org/item1.html',
Link(url='http://example.org/item3.html',
text=''),
TestItemLoader(NameItemLoader):
[u'Marta',
[u'marta',
ChildItemLoader()
[u'Mar',
u'Ta'])
u'text')
il.replace_value('url',
u'text2')
l.add_css('name',
l.add_css('url',
l.add_xpath('url',
[u'Paragraph'])
'a::attr(href)',
[u'/images/logo.png'])
self.assertEqual(l.get_output_value('name_div'),
self.assertEqual(l.get_output_value('name_value'),
"baz"}},
logline
logkws['msg']
self.assertEqual(self.catched_msg['subject'],
self.assertEqual(self.catched_msg['body'],
self.assertEqual(msg['subject'],
self.assertEqual(msg.get('Content-Type'),
mwman
self.pipeline
fullpath)
mock.patch.object(FilesPipeline,
'abc',
patchers:
cls({'name':
'item1',
[url]})
list(pipeline.get_media_requests(item,
self.assertEqual(requests[0].url,
url})]
pipeline.item_completed(results,
[results[0][1]])
prefix=None):
random_string(),
self._generate_fake_settings()
pipe_ins_attr
pipe_attr))
pipeline_cls.from_settings(Settings(settings))
self._generate_fake_settings(prefix=prefix)
settings.get(prefix
settings_attr)
50)
UserDefinedImagePipeline(ImagesPipeline):
self.default_pipeline_settings[pipe_attr])
Request('http://url')
Failure(Exception())
Response('http://url1')
dict(requests=req)
hook
rsp1)])
'http://scrapy:scrapy@localhost:8888'
crawler.crawl("https://localhost:8999/status?n=200")
self.assertSubstring('Use
get_crawler()
self.assertEqual(len(output),
self.assertEqual(output,
body=self.BODY)
self.BODY)
self.res404,
test_logging(self):
list(self.mw.process_spider_output(res,
queue(self):
'dict'})
{'a':
'value',
build_component_list,
custom,
3}
x.lower())
priority=20)
'two'])
'val1',
test_delete(self):
self.assertEqual(d['B'],
'A')
MyDict()
"(cb3
cb2,
self.assertEqual(x,
TypeError
XmlResponse(url="http://example.com",
next,
'feed-sample3.csv')
u"'name'":
u"'value'":
a,
'Got
logger.warning('test
'WARNING',
two
['x',
'y']))
self.assertTrue(equal_attributes(a,
123,
c):
self._assert_serializes_ok(r)
request_fingerprint(r2))
Not
signal=test_signal)
arg,
[{'priority':
'http://www.example.com/',
'daily'},
o3
0s
self.assertFalse(url_is_from_any_domain(url,
self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html',
'example.com'
'/?c=v&c2=v2')),
'/')),
int))
url='http://foo/bar',
b"GET
b"\r\n")
b"X-Meta-Multivalued:
self.assertEquals,
'--get',
self.assertEqual(attr.value,
self.assertEqual(attr.priority,
BaseSettings({1:
self.assertEqual(self.settings.get('BAR'),
'fez')
@mock.patch('scrapy.settings.default_settings',
self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'),
Settings({'SPIDER_MODULES':
SpiderLoader.from_settings(settings)
self.assertEquals(set([m.__name__
mods]),
set(expected))
ast
'and
TemplateNotFound
abort(404)
sqlite3
url_for,
database.')
'sqlite_db'):
cur
abort(401)
(?,
in')
here
b'Invalid
follower
desc
[session['user_id'],
redirect(url_for('timeline'))
email,
registered
'password':
'email':
'user1',
string_types
BROKEN_PYPY_CTXMGR_EXIT
'exc_clear'):
sys.exc_clear()
BadRequest,
'http',
static_folder
self.static_folder
self.import_name
self._got_first_request
blueprint,
"%s".
string_types):
()))
exc_class,
done
request.path,
isinstance(rv,
exc=_sentinel):
sys.exc_info()[1]
start_response):
ctx.push()
WSGI
means
find_default_import_path()
self._loaded_app
e.strerror
'Unable
scopes
self._refcnt
context.
float,
'%s:
total_found
srcobj,
'application
self.module_choices
self.wrapper_module
wrapper_module
find_module(self,
extension
important_module,
os.path.sep
information
appctx
'.':
old_scheme
specifying
os.path.getsize(filename)
hasattr(loader,
happen
self.__doc__
self._static_url_path
datetime):
_fail
NotImplementedError()
urlsplit
'localhost')
backend
'delete',
meth
getter
JSON
importer
contents)
lineiter:
new_contents)
contents,
margin:
15px
background:
padding:
iptr
result.name
<td>{{
args))
'setup.py',
interpreters,
failed')
Template
codename
os.path.dirname(__file__),
flask._request_ctx_stack.top
False))
flask._app_ctx_stack.top
Exception('dummy')
flask.current_app
app.test_client().open('/',
c.head('/')
'index',
b'index'
'testkey'
'httponly'
list(messages)
len(called)
b'Internal
@app.errorhandler(500)
@app.route('/fail')
Wörld'.encode('utf-8')
"msg":
rv.data.strip()
flask.Flask(__name__,
response'
b'Foo'
False])
app.got_first_request
index'
rv['result']
frontend
max_age_default
expected_max_age
AssertionError,
'my_reverse'
app.jinja_env.filters['my_reverse']
app.jinja_env.filters['my_reverse']('abcd')
'strrev'
app.jinja_env.filters['strrev']
app.jinja_env.filters['strrev']('abcd')
'boolean')
"testapp"
click.echo(current_app.name)
result.exit_code
result.output
@app.route('/json',
c.post('/json',
object'
render
app.use_x_sendfile
app.root_path
app.logger_name
trigger
__next__
under
range(10):
app.test_request_context()
record(sender,
len(recorded)
c.session_transaction()
flask.g.value
(AttributeError,
common_test(app)
get_args,
Gym
args.debug:
config['parse_pokestops']
config['parse_gyms']
init_database()
Pokestops
Gyms
args.only_server:
log.debug('Starting
search_thread
s2sphere
get_args
Gym,
args.fixed_location:
d['pokemons']
jsonify(d)
request.args.get('lat',
request.args.get('lon',
%s,%s'
(lat,
lon))
'bad
pokemon_list
config['ORIGINAL_LONGITUDE'],
origin_point
pokemon_point
diff_lat
diff_lng
'S')
1e-4
'%d
obj.utcoffset()
millis
int(
.utils
pokemon_id
OperationalError
log.info('Connecting
result['latitude'],
disappear_time
p['pokemon_name']
get_pokemon_name(p['pokemon_id'])
pokemons.append(p)
.where((Pokemon.pokemon_id
ids)
(Pokemon.disappear_time
BooleanField()
lure_expiration
(Pokestop
(Pokestop.longitude
(Gym
(Gym.longitude
(ScannedLocation.latitude
(ScannedLocation.longitude
scans
config['parse_pokemon']:
d_t
1000.0)
'encounter_id':
b64encode(str(p['encounter_id'])),
'spawnpoint_id':
p['spawnpoint_id'],
'pokemon_id':
p['pokemon_data']['pokemon_id'],
f.get('type')
lure_expiration,
f['id'],
f['enabled'],
f['latitude'],
f['longitude'],
f['last_modified_timestamp_ms']
1000.0),
pokemons_upserted
pokestops_upserted
gyms_upserted
pokemon,
pokestops,
{}".format(i,
locations
complete
pgoapi
PGoApi()
send_map_request(api,
position):
R
radius
math.asin(
centers
xdist,
DOWN
SOUTH)
UP
Trying
{:g}
Go
successful.')
step_location,
lock
'NEXT_LOCATION'
config:
q.task_done()
response_dict:
step_location)
log.debug('{}:
servers
time.sleep(config['REQ_SLEEP'])
log.info("Search
search(args,
args.step_limit
remaining_time
position)
ee
is_location_out_of_china(latitude,
adjust_lat,
105,
35.0)
sqrt_magic
180.0)
y):
sqrt(abs(x))
sin(6.0
20.0
sin(2.0
40.0
12.0
300.0
decoded_string
threads',
help='Display
help='Set
Mode.
args.location
parser.print_usage()
sys.argv[0]
150,
last_modified=datetime.now(),
open(file_path,
self._ticket_expire
self._ticket_start
self._ticket_end
self._ticket_start,
Auth
Auth.__init__(self)
Token:
self._session
requests.session()
self._session.verify
self.log.info('PTC
headers=head)
'lt':
ticket
self.log.error('Could
token:
data1
utilities
protos.RpcEnum_pb2
RpcEnum
self._req_method_list
api_endpoint
RPC')
player_position)
lng,
request...')
func.upper()
self._req_method_list.append(
self.log.info("Adding
sequence
(app
simulation)')
response!')
DecodeError
protobuf_to_dict
stderr=subprocess.PIPE)
subrequests)
self.log.debug('Generating
sub
isinstance(entry,
entry_name
RpcEnum.RequestMethod.Name(entry_id)
to_camel_case(entry_name.lower())
proto_classname
'pogom.pgoapi.protos.RpcSub_pb2.'
get_class(proto_classname)()
subrequest
mainrequest.requests.add()
subrequest.type
self.log.debug('Parsing
request_entry
subresponse_extension
self.log.debug(error)
google.protobuf.internal
struct.pack('<d',
long):
prev
_RPCDIRECTION
_TEAMCOLOR
_REQUESTMETHOD
number=139,
number=141,
number=148,
number=149,
number=301,
number=401,
number=402,
number=403,
number=404,
number=701,
number=801,
_POKEMONMOVE
number=33,
number=34,
number=36,
number=38,
number=40,
number=42,
number=44,
number=45,
number=47,
number=49,
number=51,
number=53,
number=55,
number=57,
number=59,
number=61,
number=62,
number=64,
number=65,
number=67,
number=68,
number=70,
number=71,
number=73,
number=75,
number=76,
number=78,
index=80,
number=80,
index=81,
index=82,
number=82,
index=83,
index=84,
index=85,
number=85,
index=86,
index=87,
number=87,
index=88,
index=89,
number=89,
index=90,
index=91,
number=91,
index=92,
index=93,
number=93,
index=94,
number=94,
index=95,
index=96,
index=97,
number=97,
index=98,
index=99,
number=99,
index=100,
index=101,
index=102,
index=103,
index=104,
index=105,
index=106,
index=107,
index=108,
index=109,
index=110,
index=111,
index=112,
index=113,
index=114,
index=115,
index=116,
index=117,
index=118,
index=119,
index=120,
index=121,
index=122,
index=123,
index=124,
index=125,
index=126,
index=127,
index=128,
index=129,
index=130,
number=130,
index=131,
index=132,
index=133,
index=134,
index=135,
index=136,
index=137,
index=138,
index=139,
number=201,
index=140,
number=202,
index=141,
index=142,
index=143,
index=144,
index=145,
index=146,
index=147,
index=148,
index=149,
index=150,
index=151,
_ITEMTYPE
_INVENTORYUPGRADETYPE
_ITEMTYPECATEGORY
_EGGINCUBATORTYPE
_POKEMONFAMILYID
_MAPOBJECTSSTATUS
_FORTTYPE
_POKEMONID
_FORTSPONSOR
_FORTRENDERINGTYPE
139
141
148
149
301
401
402
701
801
14
17
18
22
26
28
33
34
36
38
40
44
47
49
51
53
55
57
61
62
65
67
68
70
71
73
75
76
78
82
87
89
91
93
94
97
99
130
201
DEFAULT
_message
reflection
_reflection
RpcEnum_pb2
dependencies=[RpcEnum_pb2.DESCRIPTOR,])
_REQUEST_REQUESTS
_REQUEST_AUTHINFO_JWT
name='unknown13',
serialized_end=501,
_REQUEST_UNKNOWN6_UNKNOWN2
name='Unknown2',
serialized_start=609,
name='Unknown6',
name='direction',
name='unknown6',
name='auth_ticket',
name='unknown12',
_REQUEST_AUTHINFO,
_REQUEST_UNKNOWN3,
_REQUEST_UNKNOWN6,
_RESPONSE_UNKNOWN6_UNKNOWN2
serialized_end=994,
_RESPONSE_UNKNOWN7,
RpcEnum_pb2._RPCDIRECTION
Unknown6
_reflection.GeneratedProtocolMessageType('Unknown6',
Unknown2
_reflection.GeneratedProtocolMessageType('Unknown2',
\x01(\x0e\x32\x12.RpcEnum.PokemonId\x12\n\n\x02\x63p\x18\x03
\x01(\x05\x12\x0f\n\x07stamina\x18\x04
\x01(\x05\x12\x13\n\x0bstamina_max\x18\x05
\x01(\x05\x12$\n\x06move_1\x18\x06
\x01(\x0e\x32\x14.RpcEnum.PokemonMove\x12$\n\x06move_2\x18\x07
\x01(\x0e\x32\x14.RpcEnum.PokemonMove\x12\x18\n\x10\x64\x65ployed_fort_id\x18\x08
\x01(\x05\x12\x12\n\nowner_name\x18\t
\x01(\t\x12\x0e\n\x06is_egg\x18\n
\x01(\x08\x12\x1c\n\x14\x65gg_km_walked_target\x18\x0b
\x01(\x05\x12\x1b\n\x13\x65gg_km_walked_start\x18\x0c
\x01(\x05\x12\x0e\n\x06origin\x18\x0e
\x01(\x05\x12\x10\n\x08height_m\x18\x0f
\x01(\x02\x12\x11\n\tweight_kg\x18\x10
\x01(\x02\x12\x19\n\x11individual_attack\x18\x11
\x01(\x05\x12\x1a\n\x12individual_defense\x18\x12
\x01(\x05\x12\x1a\n\x12individual_stamina\x18\x13
\x01(\x05\x12\x15\n\rcp_multiplier\x18\x14
\x01(\x05\x12\x10\n\x08pokeball\x18\x15
\x01(\x05\x12\x18\n\x10\x63\x61ptured_cell_id\x18\x16
\x01(\x04\x12\x18\n\x10\x62\x61ttles_attacked\x18\x17
\x01(\x05\x12\x18\n\x10\x62\x61ttles_defended\x18\x18
\x01(\x05\x12\x18\n\x10\x65gg_incubator_id\x18\x19
\x01(\x05\x12\x18\n\x10\x63reation_time_ms\x18\x1a
\x01(\x04\x12\x14\n\x0cnum_upgrades\x18\x1b
\n\x18\x61\x64\x64itional_cp_multiplier\x18\x1c
\x01(\x05\x12\x10\n\x08\x66\x61vorite\x18\x1d
\x01(\x05\x12\x10\n\x08nickname\x18\x1e
\x01(\t\x12\x11\n\tfrom_fort\x18\x1f
\x01(\x03\x12\x10\n\x08latitude\x18\x03
_GETPLAYERRESPONSE
_DOWNLOADSETTINGSREQUEST
name='hash',
_GETINVENTORYRESPONSE
name='inventory_upgrades',
name='cp',
name='stamina',
name='stamina_max',
name='move_1',
name='move_2',
name='deployed_fort_id',
name='owner_name',
name='is_egg',
name='egg_km_walked_target',
name='egg_km_walked_start',
name='origin',
name='height_m',
name='weight_kg',
name='individual_attack',
name='individual_defense',
name='individual_stamina',
name='cp_multiplier',
name='pokeball',
name='captured_cell_id',
name='battles_attacked',
name='battles_defended',
name='egg_incubator_id',
name='creation_time_ms',
name='num_upgrades',
name='additional_cp_multiplier',
name='favorite',
name='nickname',
name='from_fort',
name='item_type',
_GETMAPOBJECTSREQUEST
_GETMAPOBJECTSRESPONSE
name='spawnpoint_id',
_DOWNLOADSETTINGSRESPONSE
RpcEnum_pb2._TEAMCOLOR
PY3
iterbytes
padding
5):
starting
implicit_setuptools:
noqa
implicit_wheel:
r_hex
w_worker
math.radians(0)
jump_points
lat1
lat2
lon2
2",
BotoConfigLocations
boto.gs.connection
GSConnection
ELBConnection
boto.ec2.cloudwatch
CloudWatchConnection
SDBConnection
boto.rds2.layer1
is_secure=False,
boto.glacier.layer2
Layer2(aws_access_key_id,
boto.ses
boto.swf.layer1
boto.beanstalk.layer1
DirectConnectConnection
KinesisConnection
CloudWatchLogsConnection
KMSConnection
AWSLambdaConnection
MachineLearningConnection
key.name)
urllib,
self._get_hmac()
pickled_dict
__setstate__(self,
formatdate(usegmt=True)
headers[key]
self._provider.access_key
{'Host':
req.headers:
self.service_name
http_request.body
qs_to_post
expires,
boto.utils.get_utf8_value(params[key])
pairs.append(key
expanduser
BytesIO,
StandardError
ConfigParser
boto.handler
all(key
authorize(self,
self.https_validate_certificates
HAVE_HTTPS_CONNECTION:
certs_file
'debug',
self.is_secure)
self.request_hook
self.provider.access_key
self.provider.secret_key
signature_host
self.proxy_pass
config.get_value('Boto',
self.proxy_pass:
"a
**http_connection_kwargs)
cert_file
i),
next_sleep)
boto.log.error('Null
self.reason,
'BoxUsage':
_cleanupParsedProperties(self):
super(SQSError,
super(StorageResponseError,
self._errorResultSet
disposition
key_file=None,
get_name(self,
Policy
'acl',
os.path.join(expanduser('~'),
self.HostKeyMap[self.name]
self._populate_keys_from_metadata_server()
access_key_name):
access_key_name)
credential
"profile
secret_key_name):
secret_key_name)
'default',
self.auth_header
self.connection_cls:
**kw_params)
boto.s3.user
self.key_marker
self.next_marker
self.to_boolean(value)
'Bucket':
'MaxItems':
'Prefix':
self.to_boolean(value,
self.bucket_name:
bucket.get_key(self.object_name,
get_contents_as_string(self,
getattr(key,
hasattr(key,
get_def_acl(self,
get_subresource(self,
add_group_email_grant(self,
list_grants(self,
is_stream(self):
storage_class)
delete_bucket(self,
get_all_buckets(self,
set_def_acl(self,
set_subresource(self,
src_version_id=None,
storage_class='STANDARD',
preserve_acl=False,
metadata=metadata,
storage_class=storage_class,
encrypt_key=encrypt_key,
target_bucket,
error_key=None,
reading
self._num_retries
last_exception
self._materialize()
_build_instance_metadata_url(url,
num_retries=5):
num_retries=num_retries,
sep:
'%Y-%m-%dT%H:%M:%SZ'
locale.setlocale(locale.LC_ALL,
tempfile.NamedTemporaryFile()
self.command)
shell=True,
item.next
isinstance(other,
s1)
'text':
role,
role=None,
memory_size
expected_status=201,
expected_status=None,
expected_status:
api_version=None,
s3_bucket
self._encode_bool(
option_settings:
version_label=None,
'EnvironmentName':
version_label:
params['VersionLabel']
version_label
options_to_remove,
'OptionsToRemove.member')
tier_name
tier_type
application_name:
params['ApplicationName']
environment_name=None,
options=None):
str(value)
self.cname
str(response['CNAME'])
self.endpoint_url
str(response['EndpointURL'])
self.environment_id
str(response['EnvironmentId'])
self.health
str(response['Health'])
response['Resources']:
EnvironmentResourcesDescription(response['Resources'])
str(response['Status'])
self.load_balancer
self.application_version
self.applications
self.messages
self.DefaultRegionEndpoint,
stack_name,
notification_arns,
template_url:
template_url
TemplateBody
stack_policy_body
stack_policy_url
call,
template_body=None,
tags=None,
stack_policy_body=None,
stack_policy_url=None):
stack_name_or_id:
params['StackName']
stack_name_or_id
self.parameters
self.stack_status
self.stack_status_reason
stack_name_or_id=self.stack_id,
self.stack_id)
self.value)
DefaultHost
https_connection_factory=https_connection_factory,
distribution_id):
etag):
comment=''):
distribution_id
params.items():
caller_reference
<CallerReference>%s</CallerReference>\n'
self.trusted_signers:
'Self':
self.last_modified_time
self._object_class
comment=None):
ACL()
replace=True):
condition["IpAddress"]
"Resource":
private_key_file
private_key_string
self.s3_user_id
next_marker
result_set
bucket='',
'DNSName':
self.http_port
self.https_port
'SubnetId':
eni_ip
params['ClientToken']
syslog_ip
hapg_arn,
boto.cloudsearch.layer1
SearchServiceException(Exception):
endpoint=None):
self.documents_batch
'add',
self.documents_batch.append(d)
adapter)
boto.log.error('Error
'error':
"Illegal
boto.cloudsearch.optionstatus
self.domain_id
self._created
self._deleted
self._processing
self._requires_index_documents
self._search_partition_count
self._search_instance_count
data]
field_type,
domain_names=None):
Layer2(object):
region=region,
list_domains(self,
self.refresh(data)
self.creation_date
data=None):
self.new_statement(arn,
add_statement
self['Statement']:
statement['Resource']
arn:
statement['Condition']:
statement['Condition'][condition_name]
condition['aws:SourceIp']:
self._allow_ip(arn,
need_update
self._disallow_ip(arn,
self.query
self.query.page
self.page
Query.RESULTS_PER_PAGE
return_fields=return_fields,
start=start,
403:
num_pages_needed:
results.num_pages_needed
query.start
query.real_size
self.domain.layer1:
self.sign_request:
fields):
sdf,
searchable
'MultiAZ':
json_body.get('Error',
{}).get('Code',
"Effect":
self.domain.service_arn
sort=None,
highlight=None,
partial=None,
sort
sns_topic_name
include_global_service_events
cloud_watch_logs_log_group_arn
cloud_watch_logs_role_arn
deployment_ids
deployment_group_name
params['description']
deployment_config_name,
deployment_group_name,
ec_2_tag_filters
auto_scaling_groups
identity_pool_name,
allow_unauthenticated_identities,
supported_login_providers
developer_provider_name
open_id_connect_provider_ar_ns
{'IdentityPoolId':
logins
logins,
identity_id
developer_provider_name,
dataset_name)
'Token':
client_context
configuration_recorder_names
delivery_channel_names
queue=None,
{'pipelineId':
unique_id,
object_ids,
instance_identity
pipeline_objects,
interconnect_id,
'connectionId':
virtual_interface_id,
connection_id
self.keys
batch_dict
self.keys:
range_key)
boto.dynamodb.types
self.__class__.__name__}
self._updates
crc32
boto.dynamodb
'region',
self.DefaultRegionName)
self.throughput_exceeded_events
self._validate_checksums
"%s,
actual_crc32
table_name):
consistent_read:
batch_get_item(self,
{'RequestItems':
count=False):
boto.dynamodb.table
Schema
boto.dynamodb.batch
self.last_evaluated_key
dynamodb_value
consistent_read,
create(cls,
self.hash_key_type
self.range_key_type
'ACTIVE':
'NULL'
'B'
encode
'{0}
boto.dynamodb2.types
self.throughput
self._loaded
self._data:
orig_value
final_data
final_data[key]
'DELETE',
"ResourceInUseException":
exceptions.ResourceInUseException,
params['ReturnItemCollectionMetrics']
provisioned_throughput,
'TableName':
params['ConsistentRead']
filter_expression
attribute_updates
STRING)
index_klass
field['Projection']['ProjectionType']
gsi_name,
self.connection.update_table(
global_secondary_index_updates=gsi_data,
method'
boto.log.error(msg)
Item(self)
self.describe()
item_data,
filter_kwargs,
index=None,
'conditional_operator':
conditional_operator,
using=QUERY_OPERATORS
count_buffer
raw_results
last_key
unprocessed
'LE',
'GE',
'privateIpAddress':
public_ip=self.public_ip,
network_interface_id=network_interface_id,
private_ip_address=private_ip_address,
allow_reassociation=allow_reassociation,
self._current_attr
snapshot_id=None,
volume_type=None,
self.no_device
self.volume_type
self.update_time
boto.manage
params.get('region',
choices=boto.ec2.regions)
params['region']
get_zone(self,
params.get('zone',
Availability
params['region'].connect()
boto.ec2.tag
'Owner')
block_device_map=None,
kernel_id
ramdisk_id
snapshot_id:
img
user_data=None,
placement_group=None,
instance_profile_name=None,
security_group_ids:
l.append(group.name)
params['InstanceType']
placement
Instance)],
product_code,
kv
zones,
private_ip_addresses
allocation_id=None,
allocation_id
params['PublicIp']
zone,
zone.name
device,
instance_id:
source_region,
source_snapshot_id,
snap_found_for_this_time_period
group_id=None,
params['GroupId']
src_security_group_name=None,
src_security_group_owner_id=None,
src_security_group_name
src_security_group_owner_id:
src_security_group_owner_id
params['IpPermissions.1.FromPort']
params['IpPermissions.1.ToPort']
src_group_id
instance_tenancy
'InstanceCount':
'ClientToken':
schedule
resource_ids,
TagSet()
'groupId':
backwards
self.architecture
self.kernel_id
self.ramdisk_id
self.product_codes
self.root_device_type
self.root_device_name
self.virtualization_type
self.hypervisor
'platform':
security_groups,
instance_type,
'launchPermission',
'groups'
Group
self.monitored
self.ip_address
stop(self,
ip_address,
'sourceDestCheck',
state=None):
Event(object):
self.not_before
self.not_after
Region')
kp
self.private_ip_addresses
attach(self,
self.instance_tenancy
availability_zone,
instance_count
'instanceCount':
self.create_date
owner_id=None,
owner_id
src_group_owner_id
self.from_port
self.to_port
self.grants
self.AttrName,
self.product_description
AttachmentSet()
self.device
'eventId':
LaunchConfiguration
boto.ec2.autoscale.group
boto.ec2.autoscale.tag
delete_policy(self,
as_group
scaling_processes=None):
'TopicARN':
'StartTime':
'EndTime':
self.granularity
self.default_cooldown
list:
'VPCZoneIdentifier':
self.block_device_mappings
security_groups
self.instance_monitoring
self.alarm_arn
'AlarmName':
self.time
key=None,
self.region.name
params['%s.%d.Name'
params[label
unit=None,
(index
metric_name,
unit=None):
'MetricName':
'AlarmNames.member.%s')
start_date=None,
HealthCheck
listeners
complex_listeners:
listener[2].upper()
params['Listeners.member.%d.LoadBalancerPort'
listener[0]
params['Listeners.member.%d.InstancePort'
listener[1]
params['Listeners.member.%d.Protocol'
listener[2]
'HTTPS'
params['Listeners.member.%d.SSLCertificateId'
listener[3]
subnets
instances):
InstanceInfo)])
'LoadBalancerName':
policies):
self.subnets
self._attributes
new_zones
get_attributes(self,
new_instances
new_subnets
{'taskDefinition':
container_instances,
container_instance
overrides
self.iter
ec2_security_group_name,
'CacheSecurityGroupName':
cache_parameter_group_name=None,
num_cache_nodes
replication_group_id
params['CacheParameterGroupName']
cache_subnet_group_name
cache_parameter_group_family,
source_identifier=None,
params['SourceIdentifier']
reserved_cache_node_id
reserved_cache_nodes_offering_id
notification_topic_status
apply_immediately
apply_immediately).lower()
subnet_ids
outputs
content_config
thumbnail_config
ascending=None,
page_token=None):
params['Ascending']
params['PageToken']
self).make_request(
args(self):
'%s.%s(name=%r,
self.__class__.__module__,
'JobFlowIds.member')
boto.utils.ISO8601)
jobflow_id,
action_on_failure='TERMINATE_JOB_FLOW',
'CreationDateTime',
'State',
self._jar
step_args.extend(self.BaseArgs)
step_args=step_args)
Connection
self.full_path
hasgroup
"Invalid
pair:
'Unrecognized
super(ComplexAmount,
AmountCollection(name=name))
chunk_hashes,
part_size):
range(total_parts):
api,
log.debug("An
uploading
total_parts):
result_queue)
self._retry_exceptions
byte_range,
threads:
TreeHashDoesNotMatchError(
response_name,
self.ResponseDataElements:
self.arn
expected_tree_hash
boto.glacier.response
GlacierResponse
self.Version
ok_responses=(201,),
status_code
('Location',
'vaults/%s/multipart-uploads/%s'
response_data
vaults
self.http_response
Job
boto.glacier.concurrent
file_size
part_tree_hash
start_date
part_index,
ValueError("I/O
(start,
self.entries.entry_list.append(entry)
User(self)
domain)
named_entity
STANDARD_ACL
'acl'
('<?xml
'delimiter',
mfa_token=None,
handler.XmlHandler(acl,
get_xml_acl(self,
query_args,
self.connection.make_request(
self.set_acl(acl,
e,
'Enabled')
more_results=
rs.is_truncated
InvalidCorsError('Tag
collection'
'LastModified':
'StorageClass':
override_num_retries=override_num_retries,
spos:
rewind
key.size
etag
self.base64md5
self.local_hashes
ACTION
CONDITION
conditions
self.validateStartTag(name,
InvalidLifecycleConfigError('Unsupported
rule.to_xml()
httplib
ResumableTransferDisposition
self.tracker_uri
%s.
scratch.'
(self.tracker_file_name,
print('Invalid
conn,
server_end
conn.debug
(server_start,
file_length,
total_bytes_uploaded,
total_bytes_uploaded
conn.is_secure)
(e.disposition
non-retryable
self._remove_tracker_file()
self.progress_less_iterations
{'ServerCertificateName':
serial_number,
auth_code_1,
'SerialNumber':
alias
role_name})
self.get_response(
path_prefix=None,
role_name}
'SAMLMetadataDocument':
'SAMLProviderArn':
policy_arn}
'Data':
'PartitionKey':
records,
encryption_context=None,
params['EncryptionContext']
response.get('CiphertextBlob')
response['CiphertextBlob']
response['CiphertextBlob'].encode('utf-8'))
key_spec
'logStreamName':
filter_pattern,
'MLModelId':
'DataSourceId':
data_spec,
ml_model_name
filter_variable=None,
eq=None,
gt=None,
lt=None,
ge=None,
le=None,
ne=None,
params['FilterVariable']
params['EQ']
params['GT']
params['LT']
params['GE']
params['LE']
params['NE']
params['SortOrder']
record,
paramiko
self._ssh_client
self.open_sftp()
put_file(self,
channel
(command,
t[1].read()
uname='root'):
prop.name
choices:
print('[%d]
SSHClient
/mnt
'/mnt/%s'
'-s
'-r
iobject
IObject()
RSA
BotoConfigPath)
t[0])
Security
self.cls
production
'aws_access_key_id',
'running':
s.name
s.instance_id
super(Server,
self._cmdshell
self._reservation
cmd:
int(val)
IntegerProperty()
ListProperty,
self.zone_name
self.server:
ValueError('server
still
-t
snap.date
l.append(snap)
self.get_snapshot_range(snaps,
current_day
snap.keep
current_month
size):
sys.stdout.flush()
choice
val))
self.quantity
self.ami
self.choose_from_list(l,
BotoConfigPath
self._ec2
ssh_client.open_sftp()
command)
self._set_notification(hit_type,
instance)
question_param
final_price
[('HIT',
'SortProperty':
assignment_id}
retry_delay
test_duration
answer_key
[('QualificationType',
QualificationType)])
value}
expiration
self.comparator
display_name=None):
{label:
self.get_as_xml()}
''.join(item.get_as_xml()
schema_url
vars(self)
self.default
selections
selections_xml
boto.mws.response
into,
"".format(func.action,
digest
'ASINList'])
@structured_lists('ASINList.ASIN')
@api_action('Recommendations',
@api_action('CustomerInfo',
@api_action('CartInfo',
@requires(['AmazonOrderReferenceId'])
_hint
end(self,
super(MemberList,
DeclarativeType):
MemberList()
layer_ids,
elastic_load_balancer_name,
layer_id):
service_role_arn,
default_instance_profile_arn
params['CustomJson']
data_sources
app_source
ssl_configuration
instance_ids
install_updates_on_boot=None,
auto_scaling_type
ssh_key_name
params['InstallUpdatesOnBoot']
custom_instance_profile_arn
custom_security_group_ids
volume_configurations
enable_auto_healing
auto_assign_elastic_ips
auto_assign_public_ips
custom_recipes
use_ebs_optimized_instances
lifecycle_event_configuration
ssh_username
allow_self_management
{'LayerId':
fp.write('%s
num_remaining_attempts
os.path.join(self.working_dir,
self.set(section,
self.get(section,
required:
")
boto.pyami.installers.ubuntu.installer
Installer
self.run('a2enmod
self.install()
self).__init__(config_file)
minute
mysql
mysql_path)
self.run("a2enmod
params['DBInstanceIdentifier']
instance_class,
multi_az=False,
ParameterGroup)
'DBSecurityGroups.member')
vpc_security_groups:
id}
groupname
DBSecurityGroup)
log_file_name,
{'DBSnapshotIdentifier':
'TargetDBInstanceIdentifier':
major_engine_version,
'EngineName':
'MajorEngineVersion':
params['MajorEngineVersion']
self.allocated_storage
self.engine_version
self.license_model
self.engine_name
self.major_engine_version
self.db_security_groups
self.permenant
self.depends_on
self._current_param
self.source
self.apply_method
ValueError('value
'SubscriptionName':
cidrip=None,
ec2_security_group_id
master_user_password,
db_subnet_group_name=None,
license_model=None,
params['DBName']
params['LicenseModel']
source_db_instance_identifier,
db_parameter_group_family,
sns_topic_arn,
params['Enabled']
enabled).lower()
{'SubscriptionName':
db_instance_identifier
snapshot_type
subscription_name
vpc
reserved_db_instance_id
reserved_db_instances_offering_id
master_user_password
'Parameters.member',
'ParameterValue',
'Source',
'DataType',
'AllowedValues',
'IsModifiable',
sns_topic_arn
'ClusterSecurityGroupName':
account_with_restore_access,
'SnapshotIdentifier':
params['SnapshotClusterIdentifier']
cluster_version=None,
hsm_client_certificate_identifier=None,
hsm_configuration_identifier=None,
cluster_type
params['ClusterVersion']
number_of_nodes
params['HsmClientCertificateIdentifier']
params['HsmConfigurationIdentifier']
parameter_group_family,
{'ParameterGroupName':
cluster_identifier
owner_account
node_type
self).__init__(s)
label)
rp[label]
'string',
self.aws_response
self.request_params,
python_name
fmt['type']
action='store',
param.ptype
hasattr(cls,
self.short_name
self.long_name
self.doc
self.choices
hosted_zone_id)
caller_ref
err.error_code
failure_threshold
'CNAME',
ttl,
self.weight
health_check
self.failover
identifier[1]
ns
duration_in_years,
admin_contact,
registrant_contact,
tech_contact,
auto_renew
privacy_protect_admin_contact
privacy_protect_registrant_contact
privacy_protect_tech_contact
kw_params['host']
grants
g.type
policy=None):
display_name=display_name)
self.grants:
'EmailAddress':
boto.s3.prefix
six.string_types
force_http=False,
mfa_token:
provider.storage_response_error(response.status,
encrypt_key:
headers[provider.server_side_encryption_header]
'AES256'
boto.utils.merge_meta(headers,
blogging
headers['Content-MD5']
k.version_id
get_bucket_server(self,
'<CORSConfiguration>'
self.content_language
'md5'
self.local_hashes['md5']
override_num_retries=override_num_retries)
preserve_acl=True,
save_debug
self.BufferSize
fp.read(self.BufferSize)
chunked_transfer:
digesters:
space
whence
expiration=None,
self.current_transition_property
bucket=None):
'Error':
self.download_start_point
key.get_file(fp,
progress_less_iterations
self.routing_rules
self.converter
domain):
boto.sdb.db.key
props
sc
Blob
obj._loaded
self.default_value():
TypeError('Validation
super(PasswordProperty,
self).get_value_for_datastore(model_instance)
super(S3KeyProperty,
super(DateTimeProperty,
self.auto_now
super(DateProperty,
self.collection_name
item_type):
self.manager
self._db
"foo"
test()
db_passwd
db_type
db_table
db_host
db_port
db_passwd,
db_host,
db_port,
db_table,
self.type_map:
hasattr(prop,
'item_type'):
'%03d'
'%f'
'1'
self._sdb
consistent
self.domain.get_attributes(obj.id,
consistent_read=self.consistent)
order_by:
order_by_method
ORDER
self.domain.put_attributes(obj.id,
{name:
xml.dom.minidom
find_class(class_name)
obj_node
prop_node
prop_node)
(self.db_name,
self.sd.get_obj('input_queue')
ib
self.sd.get_obj('input_bucket')
ob:
self.sd.get_obj('output_domain')
self.sd
MHMessage
mime_type
self.num_files
file_name
file_name))
get_file)
self.queue:
exceeded."
text_body
html_body
'Identity':
topic
self._build_dict_as_list_params(params,
'Attributes')
self.current_key
SQSDecodeError
visibility_timeout=None,
aws_account_id,
fp.readline()
self.get_messages(page_size,
self.read()
boto.sts.credentials
Credentials()
communication_body,
cc_email_addresses
after_time
before_time
task_list,
run_id=None):
child_policy=None,
'childPolicy':
activity_name,
tag=None,
workflow_id=None,
workflow_name=None,
'startTimeFilter':
'typeFilter':
'executionFilter':
{'workflowId':
'tagFilter':
{'tag':
tag},
task_list):
control=None,
start_to_close_timeout
attrs['input']
execution_start_to_close_timeout
tag_list
self.aws_secret_access_key,
self.version)
self.version,
self.workflowId,
MAXSIZE
self.mod
_moved_attributes
new_attr
"os",
"urllib.error"),
"urllib.response"),
"Return
dictionary.")
_locs_
want_unicode
PEP
cidr_block,
destination_cidr_block
gateway_id
interface_id
association
rule_action,
icmp_code
icmp_type
port_range_from
port_range_to
static_routes_only
gateway_id,
self.attachments
'cidrBlock':
region_name)
SimpleListModel()
t.strs
self.objs.append(t)
"Simple
self.objs[0].id)
query.filter("num
"B")
self.sequences.append(s)
bucket.new_key()
'foobar'
bucket.delete_key(k,
'application/x-boto-test'
phony_mimetype}
'foo/bar'
k.set_contents_from_string(s2,
bucket.get_all_keys(headers=DEVPAY_HEADERS)
k.get_metadata(mdkey1)
k.get_metadata(mdkey2)
test',
self.assertRaises(KeyError):
ResourceNotFoundException
version_label=self.app_version)
self.wait_for_env(self.environment)
('Parameter2',
stacks
True),
Layer1()
self.domain_name)
self.assertEqual('test',
self.provisioned_throughput
self.addCleanup(self.dynamodb.delete_table,
c.describe_table(table_name)
new_read_units
new_write_units
item1_key},
text'},
A'},
"primarykey",
{'Views':
table.refresh(wait_for_active=True)
table.new_item(item1_key,
item1_attrs)
table.get_item(item1_key,
upload']),
PM'
items.consumed_units
batch_list
batch_list.add_batch(table,
batch_list.submit()
self.create_sample_table()
table.new_item('foo',
data_type=NUMBER)
'J'
HashKey('user_id')
self.assertEqual(results,
['johndoe'])
posts.query_2(
posted_on__gte='2013-12-24T00:00:00',
self.assertEqual(users.global_indexes[0].throughput['read'],
self.assertEqual(users.global_indexes[0].throughput['write'],
self.create_table(
key={
['johndoe']},
key_conditions={
['jane']},
['johndoe',
{'M':
group2.name,
gotten
exception")
self.assertTrue(dry_run_msg
image_id='ami-a0cd60c9',
lc
'thing2',
'MetricData.member.1.Value':
'MetricData.member.1.Dimensions.member.1.Name':
'MetricData.member.1.Dimensions.member.1.Value':
'W',
'D1',
8000,
bucket.delete()
8001,
'HTTP'),
lb_policy_name
self.conn.set_lb_policies_of_listener(
self.create_pipeline()
topic_arn)
glacier
storage_uri
open(fpath,
mdval3
mdval3)
k.get_acl()
acl_xml
'<ACCESSControlList><EntrIes><Entry>'
'</Entry></EntrIes></ACCESSControlList>')
('<AccessControlList><Entries><Entry>'
'</Entry></Entries></AccessControlList>'))
'<AccessControlList></AccessControlList>')
storage_uri('gs://'
re.sub(r'\s',
self.assertEqual(cors,
self.assertEqual(xml,
StringIO.StringIO(s1)
k.set_contents_from_file(fp,
if_generation=999)
if_generation=0)
StringIO.StringIO(s2)
if_generation=int(g1)+1)
if_generation=g1)
b.set_acl("public-read",
b.set_acl("bucket-owner-full-control",
k.set_contents_from_stream(fp,
b.set_canned_acl("bucket-owner-full-control",
k.set_acl("bucket-owner-full-control",
k.set_canned_acl("bucket-owner-full-control",
tracker_file
CallbackTestHarness()
num_retries=0)
CallbackTestHarness(exception=exception)
ResumableDownloadHandler(num_retries=1)
self.assertFalse(os.path.exists(tracker_file_name))
self.assertEqual(LARGE_KEY_SIZE,
len(harness.transferred_seq_after_first_failure)
os.path.join(tmp_dir,
os.chmod(tmp_dir,
ResumableUploadHandler()
self.make_tracker_file()
larger_src_file.seek(0)
larger_src_file,
bucket_uri
self.assertTrue(uri.has_version())
b.name)
r"[0-9]+")
list(b.list_versions())
self.assertEqual(len(versions),
generations)
test_policy
self.assertEqual(data,
MWSConnection
description',
self.masterDB_name
self.renamedDB_name
(15
inst.status
self.assertTrue(inst.status
self.conn.get_all_dbinstances(self.replicaDB_name)
route53
base_record
dict(name="alias.%s."
type="A",
identifier="boto:TestRoute53AliasResourceRecordSets")
rrs.add_change(action="UPSERT",
rrs.add_change(action="DELETE",
"target.%s"
'HTTP')
u'HealthCheck'][u'HealthCheckConfig'][u'IPAddress'],
'54.217.7.118')
'/testing')
'HTTPS')
'443')
str(hostid)
self.assertEquals(record.name,
self.assertEquals(record.resource_records,
torrent=NOT_IMPL,
query_args=NOT_IMPL,
self.read_pos
self.assertEqual(config,
Rule('myid',
self.assertEqual(transition.storage_class,
self.assertEqual(rule.expiration.days,
**connect_args)
bucket.delete_key(k)
allowed_header='*',
'xyz')
ssl.SSLError)
INVALID_HOSTNAME_HOST)
StringIO(content)
'application/pdf')
raw_input('MFA
d['Versioning'])
u"テスト"
mpu.cancel_upload()
int(time.time()))
self.assertEqual(3,
'name2':
self.assertEqual(headers['Host'],
sqs
self.get_policy_statements(queue)
Message()
self.assertEqual(test.count(),
self.run_decider()
SetHostMTurkConnection
_init_environment
create_service_connection(self,
self.config[section_name][key]
suppress_consec_slashes=False,
self.assertEqual(resp.read(),
BotoServerError('400',
Request')
self.assertEqual(bse.error_message,
'ec2':
'us-west-2':
endpoints['ec2'])
west_2
auth.canonical_uri(request)
'ListAccountAliases',
'2010-05-08'},
'44',
'20130808T013210Z'
'Action=ListAccountAliases&Version=2010-05-08')
credential_scope
auth.credential_scope(request)
's3-us-west-2.amazonaws.com',
protocol='https',
host='awesome-bucket.s3-us-west-2.amazonaws.com',
body=''
cleaned
self.assertEqual(cleaned,
self.auth.determine_service_name(
['hmac-v4-s3'])
self.service_connection.upload_function(
function_name='my-function',
role='myrole',
handler='myhandler',
mode='event',
runtime='nodejs'
json.dumps(
u'32bit
{u'RequestId':
'2010-12-01',
'environment1',
'ENVVAR',
'TemplateBody':
self.assertRaises(self.service_connection.ResponseError):
'My
boto.resultset.ResultSet([('member',
boto.cloudformation.stack.Stack)])
rs[0].disable_rollback
self.assertEqual(response[0].id,
CustomOrigin))
"example.com")
cloudfront
accumulator
max_items=max_items)
self.dist._sign_string(self.canned_policy,
999999
date_less_than
"http://1234567.cloudfront.com/*"
domain.get_document_service()
domain.get_search_service()
self.service_connection.index_documents('demo')
test_cloudsearch_add_basics(self):
'add')
self.assertEqual(args['id'],
'1234')
self.objs.items():
document.add(key,
obj['fields'])
json.loads(HTTPretty.last_request.body.decode('utf-8'))
'10')
'delete')
self.assertEqual(document.get_sdf(),
MagicMock()
went
SearchConnection,
gave
me
mock.patch.object(json,
'non-json'):
search.search(q='test')
HOSTNAME
hits
CloudSearchSearchBaseTest.hits
self.assertEqual(args[b'q'],
[b"Test"])
self.assertEqual(args[b'start'],
self.assertEqual(args[b'size'],
search.search(
q='Test',
"'John
[b"'John
Smith'"])
"12342",
"12343",
"12344",
"12345",
"12346",
"12347"])
test_response(self):
FakeResponse()
fake.content
return_value=fake)
self.assertRaises(SearchServiceException)
wrong.
facet={'author':
self.assertEqual(args[b'facet.author'],
CloudSearchConnection(aws_access_key_id='aws_access_key_id',
sign_request=True)
Domain(layer1=layer1,
data=json.loads(self.domain_status))
aws_secret_access_key='aws_secret_access_key'
test_describe(self):
54,
[{'HashKeyElement':
'ConsistentRead':
"Table":
"ItemCount":
"TableStatus":
range_key=('bar',
types.Dynamizer()
'hoge':
{'sub':
"Python
only")
DynamoDBConnection(
'friend_count',
self.johndoe)
self.assertTrue(self.johndoe.needs_save())
self.johndoe['friends']
self.table.schema
self.johndoe['date_joined']
'delete_item',
mock_delete_item:
self.results.to_call(self.result_function,
limit=20)
#2')
#3')
#4')
"KEYS_ONLY"
RangeKey('date_joined',
mock_update.assert_called_once_with(
'users',
'johndoe'
'data',
self.assertEqual(str(e),
"ConsumedCapacity":
"CapacityUnits":
0.5,
"users"
'Bob'},
limit=4,
'bob'])
exclusive_start_key={
'adam',
ResultSet))
mock_query.call_args[1])
'1'}
("somethingRandom",
"somethingRandom",
"somethingRandom")]:
self.check_that_attribute_has_been_set(arguments[0],
arguments[1],
arguments[2])
allocation_id="aid1",
'BlockDeviceMapping.1.DeviceName':
'BlockDeviceMapping.1.Ebs.DeleteOnTermination':
max_results=10
'false'},
self.ec2.get_all_snapshots
'snap-1a2b3c4d',
'us-east-1e',
'Tag.2.Key':
PrivateIPAddress(
primary=False)
collection.build_list_params(params,
prefix='LaunchSpecification.')
'pending',
'Filter.2.Name':
'Filter.2.Value.1':
startElement.return_value
max_size=2,
self.service_connection.create_auto_scaling_group(autoscale)
'CreateAutoScalingGroup',
'OldestLaunchConfiguration'])
'DescribeLaunchConfigurations',
'Tags.member.2.Key':
['inst2',
'InstanceIds.member.1':
'inst2',
'InstanceIds.member.2':
'InstanceIds.member.3':
'inst4',
ATTRIBUTE_SET_RESPONSE
('crossZoneLoadBalancing',
'ListClusters',
'RUNNING',
cluster_id='j-123',
'SPOT',
self.job.archive_size
body=json.dumps(content).encode('utf-8'))
"RangeInBytes":
"01d34dabf7be316472c93b1ef80721f5d4"
}],
sha256(b'a'
self.getsize.return_value
mock.patch('boto.glacier.vault.open',
self.mock_open,
data_tree_hashes
self.writer.close()
'CreateRole',
'a_name'},
'arn:aws:iam::123456789012:policy/S3-read-only-example-bucket'},
'arn:aws:iam::123456789012:policy/S3-read-only-example-bucket')
'v1')
['list_entities_for_policy_result']
json.loads(self.actual_request.body.decode('utf-8'))
'AAECAwQF')
comparator='In',
'WA'),
'CA'])
'In',
'QualificationRequirement.1.LocaleValue.1.Country':
'QualificationRequirement.1.LocaleValue.1.Subdivision':
'WA',
'00000000000000000071'},
'one',
'profile
'aws_security_token':
'cfg_access_key')
provider.Provider('google')
self.assertEqual(db.id,
self.assertEqual(db.engine,
self.assertEqual(db.status,
self.assertEqual(db.allocated_storage,
self.assertEqual(db.instance_class,
self.assertEqual(db.master_username,
self.assertEqual(db.multi_az,
self.assertEqual(db.parameter_group.name,
self.assertEqual(db.parameter_group.description,
self.assertEqual(db.parameter_group.engine,
self.service_connection.create_dbinstance(
'CreateDBInstance',
'default.mysql5.1',
'dbSubnetgroup01',
'MySQL5.1',
'simcoprod01',
'restored-db',
VPCSecurityGroupMembership(self.service_connection,
self.assertIsInstance(response,
hc_xml)
self.assertEqual(hc_resp['ResourcePath'],
'/health_check')
60,
expected_xml
self.assertEqual(qa,
'initial=1')
mock_get_all.reset_mock()
mock_get_all.assert_called_with(
conn.generate_url(86400,
b.new_key('test_failure')
fail_file
StringIO('This
k.send_file(fail_file)
self.assertEqual(self.keyfile.read(4),
self.assertEqual(self.keyfile.read(1),
self._get_bucket_lifecycle_config()[0]
('gs',
's3'):
self.assertEqual(prov,
self.assertEqual('obj/a/b',
WebsiteConfiguration(suffix='index.html',
rules.add_rule(RoutingRule(condition,
redirect))
identity='user@example.com',
len(response))
len(result))
'message',
certificate',
'Attributes.entry.1.key':
'Attributes.entry.1.value':
self.service_connection.create_queue('my_queue')
'Message
self.assertEquals(bucket,
self.assertEquals(key,
boto.swf.layer2.Layer1
'inheritable
'1.0'}},
'executionStatus':
'OPEN',
'startTimestamp':
password.set('foo')
hmac.new(b'mysecretkey',
fake_response.read.return_value
'cgw-b4dc3961',
'state',
'Filter.1.Value.2':
'available',
self.assertEqual(api_response[0].id,
'ipsec.1',
self.assertEquals(api_response.state,
'vpc-1a2b3c4d'},
'igw-eaad4883',
'vpc-11ad4878'},
'udp',
'allow',
'Egress':
'deny',
'rtbassoc-faad4893',
self.service_connection.create_route(
'CreateRoute',
self.service_connection.replace_route(
'ReplaceRoute',
vpc_peering_connection
self.assertEqual(vpc_peering_connection.id,
self.assertEqual(vpc_peering_connection.status_code,
self.assertEqual(vpc_peering_connection.status_message,
self.assertEqual(vpc_peering_connection.requester_vpc_info.owner_id,
self.assertEqual(vpc_peering_connection.requester_vpc_info.vpc_id,
self.assertEqual(vpc_peering_connection.requester_vpc_info.cidr_block,
self.assertEqual(vpc_peering_connection.accepter_vpc_info.owner_id,
self.assertEqual(vpc_peering_connection.accepter_vpc_info.vpc_id,
<statusMessage/>
<acceptedRouteCount>0</acceptedRouteCount>
'11.12.0.0/16',
join
templates_path
['_templates']
source_suffix
'.rst'
master_doc
copyright
pygments_style
html_theme
html_theme_path
html_static_path
['_static']
htmlhelp_basename
latex_documents
itemgetter
'index':
''),
setup(app):
_contents
out:
newfilename
Resource
Root(Resource):
self.lastmark
self.lasttime
Root()
Site(root)
self.qps
self.download_delay
[url]
idx
7):
'bucket':
twisted.persisted.styles
cProfile
pkg_resources
walk_modules
scrapy.utils.project
cmdname
_get_commands_dict(settings,
_print_header(settings,
scrapy.__version__)
cmdname,
print('Use
"scrapy
about
command:
_run_print_help(parser,
sys.exit(0)
opts.profile:
zope.interface.verify
scrapy.interfaces
self.logformatter
self._spiders
crawl(self,
start_requests
self.settings)
signal_names[signum]
cache_size
self.stop()
deprecated.
fully
add
runtime
scrapy.utils.job
job_dir
debug=False):
self.logger
path:
os.linesep)
field.get('serializer',
serializer(value)
start_exporting(self):
finish_exporting(self):
include_empty
self._configure(kwargs,
dont_fail=True)
kwargs.setdefault('ensure_ascii',
itemdict
self.encoding:
serialized_value):
'items'):
contain
self.binary
binary
BaseItem):
(six.text_type,
six.iteritems(value):
class_name,
most
self._values[key]
field:
value"
__setattr__(self,
keys(self):
self.__class__(self)
['url',
self.text
^
documentation
DEBUG
referer_str
os.linesep
'msg':
'args':
formatdate
attachs:
mimetype,
msg.attach(part)
subject=subject,
sent
To=%(mailto)s
Cc=%(mailcc)s
'Subject="%(mailsubject)s"
{'mailto':
'mailcc':
'mailsubject':
'mailattachs':
middleware'
clspath
timeout):
timeout:
'scrapy.http.HtmlResponse',
'application/json':
url=None,
IgnoreRequest,
scrapy.utils.console
open_in_browser
BaseItem,
get_config()
self.crawler.spider
fetch(self,
request.meta['handle_httpstatus_all']
scrapy.utils
sender
receiver,
spider_closed
iter_spider_classes
SpiderLoader
scrapy.statscollectors
set_value(self,
self._stats[key]
count=1,
set_crawler(self,
pdb
self.settings.set('LOG_ENABLED',
[sys.executable,
traceback):
LinkExtractor()
qargs
{'total':
urlencode(qargs,
dest="verbose",
opts.list:
opts.verbose:
method)
parser.add_option("-a",
dest="spargs",
help="set
scraped
parser.add_option("-t",
ScrapyCommand.process_options(self,
opts.spargs
arglist_to_dict(opts.spargs)
**opts.spargs)
defined
"Fetch
spider")
b'
b':
opts.spider:
spider_loader.load(opts.spider)
scrapy.utils.template
standard
template_name,
spiders_module
'templates')
lst
Requests
dict)):
'please
{'spider':
'spider':
cb)
abspath
BaseSettings
raw
console
t.daemon
('${project_name}',
print('Error:
numbers
2):
abspath(project_dir))
args)
@wraps(cb)
self.args[0]
occurrences
scrapy.downloadermiddlewares.decompression
scrapy.exporters
scrapy.loader
scrapy.pipelines.files
scrapy.pipelines.media
Slot(object):
start_requests,
crawler.signals
self.paused
slot.start_requests
logger.info('Error
d.addBoth(lambda
removing
(Request,
"Spider
opened
_download(self,
logger.log(*logformatter_adapter(logkws),
{'reason':
exc_info=failure_to_exc_info(failure),
errback
self.signals.send_catch_log_deferred(
dfd.addErrback(log_failure('Error
sending
'spider',
dfds
dlist
self.dqs
prios
request_to_dict(request,
request_from_dict(d,
slot):
dfd.addErrback(
request_result,
_failure.value
isinstance(output,
%(request)s:
item=item,
mustbe_deferred
method(response=response,
(fname(method),
type(result))
self.delay,
hasattr(spider,
instead"
self.ip_concurrency
urlparse_cached(request).hostname
self._process_queue(spider,
slot.lastseen
twisted.internet.ssl
twisted.web.client
'method':
method(request=request,
Request)),
(six.get_method_self(method).__class__.__name__,
'Received
TLS
1.0
5),
ret):
connectionMade(self):
took
request.body
self.headers['Content-Length']
self.scheme,
self._handlers[scheme]
self._notconfigured[scheme]
urlparse_cached(request).scheme
twisted.protocols.ftp
twisted.internet.protocol
503,
callbackArgs=(request,
b'https':
twisted.web.http_headers
proxyConf,
proxyConf
timeout,
%s:%s
self._host,
host_value
b'\r\n'
bindAddress=None,
pool=None):
_getEndpoint(self,
self._maxsize
self._txresponse
txresponse.length
expected_size
error_message
"download
url=url,
consumer):
pauseProducing(self):
stopProducing(self):
aws_access_key_id:
self.anon
self._signer
w3lib
text:
'chunked':
cl
request.headers:
TCPTimedOutError
TCPTimedOutError,
self.policy
cachedresponse)
process_exception(self,
zlib
scrapy.utils.gz
is_gzipped
'HEAD':
content_encoding
is_gzipped(response):
TextResponse):
zlib.error:
urllib2
user,
scheme)
redirects
request.priority
(301,
302,
response.headers
self._redirect(redirected,
IOError,
logger.error("Error
UnicodeDecodeError:
exception):
log_args
th
dumps
frame
Interface,
@implementer(IFeedStorage)
store(self,
_store_in_thread(self,
file.close()
is_botocore()
"/"
file.seek(0)
key.close()
self.password)
ftp_makedirs_cwd(ftp,
exporter
storage.open(spider)
"Error
d[k]
scheme:
Headers,
self.ignore_schemes
cachedresponse,
freshnesslifetime
max(0,
ValueError):
maxage
date)
lastmodified
date:
self.cachedir
settings.getint('HTTPCACHE_EXPIRATION_SECS')
os.path.join(self.cachedir,
retrieve_response(self,
url=url)
store_response(self,
response.body,
rpath
self._get_request_path(spider,
request.url,
'timestamp':
self.multiplier
gc
six.iteritems(live_refs):
wdict:
cls.__name__,
scrapy.mail
MailSender
self.resource
*=
self.tasks.append(tsk)
tsk.start(self.check_interval,
now=True)
self.limit:
startup
%dM\r\n"
"Maximum
jobdir
'port':
telnet_vars
scrapy.http.headers
scrapy.http.request
self.processed
wreq
WrappedRequest(request)
wreq)
set_policy(self,
errors='replace'),
CaselessDict
encoding='utf-8'):
normkey(self,
normvalue(self,
'__iter__'):
self.getlist(key)
self.keys()]
safe_url_string
callback=None,
meta=None,
encoding=encoding)
_set_url(self,
_set_body(self,
(self.method,
replace(self,
cls(*args,
kwargs.setdefault('encoding',
parsel.csstranslator
HTMLTranslator
formxpath
doseq=1)
f[0]
encoded)
inputs
clickable
'select':
multiple
elements
"in
str,
"Response
'status',
w3lib.encoding
super(TextResponse,
benc
@memoizemethod_noargs
ubody
text):
UnicodeError:
self.allow_res
re.compile(x)
self.deny_res
self.allow_domains
self.deny_domains
matches(self,
_process_links(self,
LxmlLinkExtractor
tag="a",
attr="href",
releases.
scrapy.linkextractors.LinkExtractor",
stacklevel=2,
self.scan_tag
callable(tag)
self.scan_attr
callable(attr)
response_text,
link:
self.links
urljoin(response_url,
response_encoding)
reset(self):
tag):
get_base_url(response)
base_url)
unique=True,
w3lib.html
clean_url
self.__class__(
parent=self,
add_value(self,
arg_to_iter(value)
wrap_loader_context(proc,
self.context)
self._get_xpathvalues(xpath,
self._get_cssvalues(css,
loader_context=None):
next_values
MediaPipeline
'://'
checksum}
domain=None):
HEADERS
modified_stamp
threads.deferToThread(
meta:
'ContentType',
'Content-Disposition':
'Content-MD5':
FSFilesStore,
media_to_download(self,
returning
referer
referer_str(request)
'referred
<%(referer)s>',
media_failed(self,
isinstance(failure.value,
<%(referer)s>:
media_downloaded(self,
exc:
ok,
info=None):
_warn():
Image
background
self.image_key(url)
info))
_process_request(self,
XPathSelector
'__slots__':
'_default_type':
new_class_path='scrapy.Selector',
@deprecated(use_instead='.xpath()')
xpath):
_root
`root`
argument,
st)
response.url)
self.update(values,
opt_name):
opt_name
getbool(self,
default=False):
getint(self,
json.loads(value)
key.isupper():
getattr(module,
isinstance(values,
BaseSettings)
"be
self.o[k]
self.settings_module
CONCURRENT_REQUESTS_PER_DOMAIN
'en',
180
platform,
500,
EXTENSIONS
'json':
'jsonlines':
'csv':
enable
debugging
initial
'..',
supported'),
None]
"You
warnings.warn(msg,
prio
maxlength
name"
self.start_urls
make_requests_from_url(self,
SitemapSpider
Rule(object):
super(CrawlSpider,
process_results(self,
self.process_results(response,
selector):
self.iterator
quotechar
self.headers,
_getarg(request,
nlist
total)
'object,
sources
wrapper(namespace=namespace,
bpython
raised
releases.",
stacklevel=2
dict.__init__(self,
dict.__getitem__(self,
requested
setdefault(self,
('
self.normkey(key))
value=None):
@wraps(func)
func.__name__
instead."
count,
**named):
**named)
%s.%s
"\
cls.__name__)
lxml
nodename)
n=65535):
_body_or_str(obj,
os.path.exists(path):
'.')
formatter
LogFormatter
getattr(mod,
encoding),
function)
bytes))
to_unicode(text,
to_bytes(text,
bytes,
_BINARYCHARS
ch
hasattr(func,
'__call__'):
isinstance(k,
request.body,
of:
obj))
include_headers:
b"
referrer
fd,
o):
str(o)
dont_log
receiver
liveReceivers(getAllReceivers(sender,
signal)):
signal=signal,
%(receiver)s",
{'receiver':
len(snames)
skip_if_no_boto()
os.getcwd()
exit
util
self.site
self).tearDown()
url(self,
"text/plain"))
**kw))
class_name:
quote,
unquote_to_bytes
_safe_chars),
encoding=None):
qs,
name_value
followed
least
connect
HEADER
BODY
self._partialHeader
line:
major,
minor
rest):
log.err()
self.uri))
[None]:
state[0]
(self,
len(bytes)
self._allowNoMoreWrites()
Response:
phrase
self._bodyBuffer
deliverBody
self.transport
transport
'to
self._disconnectParser(reason)
division,
task,
buildProtocol(self,
addr):
240
connections
self._timeouts[connection]
parsedURI.host,
kwargs['timeout']
rest
agent,
self._agent
bodyProducer)
ResponseFailed([failure.Failure(err)],
error,
twisted.python.filepath
FilePath
self._backlog
protocolFactory,
backlog=self._backlog,
interface)
@implementer(interfaces.IStreamClientEndpoint)
wf
_WrappingFactory(protocolFactory)
wf,
timeout=self._timeout,
wf._onConnection
mode=0o666,
self._address
'backlog':
interface='',
sofar
(type,
'TCP':
'UNIX':
caCerts
maxPacketSize=8192,
getPeer():
['bar',
'baz'],
"headers
expires=None,
authentication
os.path.join(os.path.abspath(os.path.dirname(__file__)),
Popen,
PIPE
type(value)
b"n",
random.random()
self._delayedRender,
twisted.web.test.test_webclient
PayloadResource
PayloadResource())
self).parse(response):
errback=self.on_error)
scrapy.utils.testsite
SiteTest
SiteTest,
unittest.TestCase):
test_output(self):
self.assertEqual(out.strip(),
self.execute([self.url('/text'),
errcode,
self.assertEqual(errcode,
join(tests_datadir,
getattr(sys.stdout,
'encoding')
mkdtemp
join(self.temp_path,
call(self,
*new_args,
(sys.executable,
'scrapy.cmdline')
new_args
env=self.env,
waited
exists(join(self.proj_path,
'pipelines.py'))
'settings.py'))
project_dir))
to_native_str(retry_on_eintr(p.stdout.read))
spname,
It
Works!",
abspath(join(self.proj_mod_path,
self.spider_name,
'parse',
TestItem(url=response.url)
self.assertTrue(crawler.spider.t1
crawler.spider.t1)
1}
00:00:00
log):
log:
w,
HttpDownloadHandler
get_crawler(settings_dict={'DOWNLOAD_HANDLERS':
handlers})
dh._schemes)
handlers:
dh._get_handler(scheme)
dh._handlers)
dh._notconfigured)
test_download(self):
self.download_handler
getURL(self,
method='HEAD')
meta=meta)
error.TimeoutError)
b'example.com')
'HTTP1.1
11.1.0'
defer.CancelledError,
error.ConnectionAborted)
crawler.crawl(seed=request)
b'gzip,deflate')
(self.portno,
self.getURL('')
S3DownloadHandler(Settings(),
date):
Request('s3://johnsmith/photos/puppy.jpg',
headers={'Date':
'public-read',
(10,
missing
power!")
Request(url="ftp://127.0.0.1:%s/file.txt"
{'Local
Filename':
404)
self.crawler._create_spider('foo')
self.crawler.stats.open_spider(self.spider)
self.crawler.stats.close_spider(self.spider,
Response(req.url,
resp)
Request('http://example.com')
'Location':
{}))
self._req_resp('http://example.com/',
resp2)
self.mw.process_response(req2,
CookiesMiddleware
headers={'Set-Cookie':
path=/'})
<GET
'/foo',
'scrapytest.org'},
cookies={'galleta':
test_responses
formats:
self.mw.process_response(None,
defaults,
test_process_request(self):
self.assertEquals(req.headers,
self.get_request_spider_mw()
policy_class
_get_settings(self,
storage:
request2
self.request,
expire
self.request)
isinstance(cached,
self.assertEqualResponse(res,
cached)
cached.flags
req1,
res3
mw.process_request(req0,
res3.flags
self.assertEqualResponse(res4,
res4.flags
res5.flags
res0.replace(status=304)
shouldcache
res1)
sampledata
{'Age':
self.tomorrow,
enumerate(sampledata):
'86405'}),
Web
'Content-Encoding'
newresponse.headers
self.assertEqual(response.headers['Content-Type'],
AssertionError
self.assertRaises(NotConfigured,
'https://proxy.for.http:3128'
'*'
Request('http://noproxy.com')
test_priority_adjust(self):
req2.priority
req.priority
url2)
rsp.headers['Location']
'Content-Length'
'/redirected'},
Response('http://scrapytest.org/first',
wrong
Response('http://www.scrapytest.org/503',
status=503)
Request('http://www.scrapytest.org/503',
meta={'dont_retry':
crawler.settings.set('ROBOTSTXT_OBEY',
return_response(request,
reactor.callFromThread(deferred.callback,
crawler.engine.download.side_effect
return_response
RobotsTxtMiddleware(self._get_successful_crawler())
meta=meta),
self.crawler.settings.set('ROBOTSTXT_OBEY',
self.crawler.engine.download.side_effect
RobotsTxtMiddleware(self.crawler)
self.assertEquals(req.headers['User-Agent'],
Request('http://scrapytest.org/2')
dupefilter.request_seen(r1)
scrapy.utils.signal
price
item_cls
m.group(1)
start_urls
vars(signals).items():
self.assertEqual(item['url'],
self.assertEqual({'spider':
ExecutionEngine(get_crawler(TestSpider),
e.close()
self.assertEqual(len(e.open_spiders),
age=u'22')
'number':
3.14,
u'John\xa3')
isinstance(name,
dict(name=u'Maria',
ie.export_item(i3)
'22',
'Jesus'})
TestItem(name=u'Maria',
f.seek(0)
fp.seek(0)
expected,
[self.i,
dict(self.i)]:
expected_value):
encoding="utf-8"?>\n'
b'<items>'
b'<item>'
b'<age>'
b'</age>'
b'</item>'
b'</items>'
u'Jesus',
'Maria',
self.ie.export_item(i3)
os.path.abspath(self.mktemp())
verifyObject(IFeedStorage,
file.write(b"content")
self.assertEqual(fp.read(),
b"content")
test_store(self):
unittest.SkipTest("No
testing")
BlockingFeedStorage()
tmp
'file://'
self.assertEqual(rows,
BytesIO(data)
EOFError:
'spam1'}),
'quux2'}
'bar3',
rows_csv,
["foo",
ordered=True)
'bar'}
u'<?xml
[('Content-Type',
self.assertEqual(sorted(first),
sorted(second),
self.assertEqual(h.getlist('X-Forwarded-For'),
Headers({u'key':
dict(h).popitem()
val[0]
self.assertEqual(val[0],
test_update(self):
['value1',
h2
'value1'})
h1.appendlist('header1',
[b'value2',
h1['foo']
5})
3])
[b'5'])
"http://www.example.com")
"http://www.scrapy.org/price/%C2%A3")
encoding="latin1")
self.assertEqual(r2.url,
self.assertEqual(r3.url,
self.request_class(url="http://www.example.com/",
self.assertEqual(r4.body,
self.assertEqual(r1.headers,
r2.headers)
test_replace(self):
self.assertEqual(r1.body,
b'two',
self.assertEqual(set(fs[u'test
£']),
{u'val1',
u'val2'})
self.assertEqual(set(fs[u'one']),
{u'two',
u'three'})
self.assertEqual(fs[u'test2'],
[u'xxx
µ'])
self.assertEqual(fs[u'six'],
[u'seven'])
u,
formdata={'two':
FormRequest.from_response(response,
self.assertEqual(request.url,
[b'clicked1'])
self.assertFalse(b'clickable2'
dont_click=True)
'two'},
self.assertRaises(IndexError,
{b'four':
[b'4'],
b'three':
selected>option
name="i4"
name="i3"
value="i4v1">
req.body
uqs
self.assertTrue(isinstance(self.response_class('http://example.com/',
headers={},
isinstance(r2,
six.text_type))
r6
self._assert_response_values(r2,
'html')
b'<html><body><base
body2)
get_name(self):
self['name']
A(Item):
Field(default='A')}
B(A):
Field(default='C')}
Field(default='C')
D(B,
C):
self.assertEqual(D.fields,
'A'}})
E(C,
B):
self.assertEqual(E(save='X')['save'],
self.assertEqual(E.fields,
{'update':
self._assert_same_links(l1,
extractor_cls
'sgml_linkextractor.html')
HtmlResponse(url='http://example.com/index',
tag'),
self.extractor_cls(allow=('sample',
different
Link(url='http://example.org/about.html',
text=u'Follow
text=u'Dont
one',
text=u'Choose
text=u'External
follow',
lx.extract_links(response)
HtmlResponse("http://example.com/index.html",
us',
not',
Link(url='http://example.com/sample_%C3%B1.html',
text='sample
RegexLinkExtractor()
v[:-1])
Compose(lambda
MyLoader(item={})
self.assertEqual(il.load_item(),
0.0,
TakeFirst()
u'name:bar'],
u'jose'])
self.assertEqual(proc(['hello',
TakeFirstItemLoader(TestItemLoader):
TakeFirstItemLoader()
Compose(partial(join,
[u'rabbit',
u'hole'])
self.assertEqual(proc([None,
world')
'//a/@href')
'#name::text')
'div::text')
'//div/text()',
re='ma')
[u'Ma'])
l.replace_xpath('name',
nl1
l.nested_xpath('//footer')
nl2
'//footer/a/@href')
all(isinstance(x,
'body')
self.assertEqual(msg.get_payload(),
'text/plain')
Charset('utf-8'))
'text/plain;
M2(),
self.assertEqual([x.im_class
M3])
self.assertEqual([x.__self__.__class__
rmtree
_mocked_download_func(request,
request.meta.get('response')
response()
callable(response)
self.pipeline.open_spider(None)
info=object()),
'image',
}))
90,
1001
pipe_ins_attr),
self._generate_fake_pipeline()
self.default_cls_settings[pipe_attr])
self.assertEqual(value,
UserDefinedFilesPipeline(FilesPipeline):
store
'Missing
'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')
'RGB')
self.init_pipeline(ImagesPipeline)
self.assertTrue('image_key(url)
self._generate_fake_pipeline_subclass()
dict(name='name')
self.pipe.LOG_FAILED_RESULTS
self.assertEqual(self.pipe._mockcalled,
['get_media_requests',
'media_to_download',
'item_completed'])
meta=dict(response=rsp1))
dict(requests=[req1,
req2])
rsp1),
self.info.downloaded
self.assertTrue(new_item
Request(req1.url,
self.assertTrue(fp
self._assert_got_response_code(200,
echo
reload_module
({'Content-Type':
Selector(response)
self.assertEqual([x.extract()
self.assertIn('Ignoring
UserClass(cls):
str(cls))
usel
UserClass(text=self.text)
cls(text=self.text)
str((cls,
[x.message
w])))
Rule,
self.spider_class("example.com")
foo='bar')
self.spider_class.from_crawler(crawler,
'example.com',
'updated':
'custom':
body=self.test_body)
_CrawlSpider(self.spider_class):
name="test"
allowed_domains=['example.org']
Rule(LinkExtractor(),
_CrawlSpider()
list(spider._requests_to_follow(response))
self.assertTrue(all(map(lambda
output)))
self.assertEquals([r.url
output],
['http://example.org/somepage/item/12.html',
self.assertEqual(w[0].lineno,
self.res200,
_responses(self.req,
[200,
test_process_spider_input(self):
self.mw.process_spider_input(self.res200,
self.res404.copy()
[404]})
self.mw.process_spider_input(self.res404,
dict(name='foo',
Spider(name='default')
self.assertEqual(i,
stats.set_value('test',
40)
4}
x),
self.assertEqual(shell.__name__,
@unittest.skipIf(not
test_contains(self):
arg2)
v2)")
ZeroDivisionError
DeprecatedName
create_deprecated_class('DeprecatedName',
New
gunzip(f.read())
f.read())
self.assertFalse(is_gzipped(r1))
XmlResponse(url='http://mydummycompany.com',
my_iter
next(my_iter)
'product')
os.path.join(sample_feeds_dir,
body1
{u"'id'":
sample
self.ubody,
logging.getLogger('test')
l.check(('test',
[]))
function")
d2
self.assertEqual(d,
self.failIf(d
r)
self.assertNotEqual(request_fingerprint(r1),
self.assertEqual(request_httprepr(r1),
HTTP/1.1\r\nHost:
text/html\r\n\r\nSome
self.assertEqual(response_httprepr(r1),
b'HTTP/1.1
"rb")
Found')
datetime.datetime(2010,
decs
(t,
handlers_called
_get_result(self,
handlers_called):
'http://www.example.com/2',
{'lastmod':
'2013-07-15',
itself
Bar()
Live
References
ago
Foo
o1)
o2)
Spider(name='example.com',
"http://www.example.com/")
"http://www.example.com/do?a=1&q=a+space")
"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9")
encoding='latin1'),
"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9")
'http://www.example.com/caf%E9-con-leche.htm')
'http://www.example.com')
'http://www.example.com/some/page.html')
'http://www.example.com:80')
'http://www.example.com/some/page#frag')
'http://www.example.com/do?a=1&b=2&c=3')
'http://username:password@www.example.com')
'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')
lip+':100',
'/?param=value')),
b"Content-Length:
self.assertEqual(self._execute('settings',
99)
self.attribute
SettingsAttribute('value',
self.attribute.set('value2',
self.assertEqual(self.attribute.value,
self.assertEqual(self.attribute.priority,
attribute.value,
self.settings.set('TEST_OPTION',
self.assertIn('TEST_OPTION',
self.assertIsInstance(attr,
SettingsAttribute)
'set')
mock_set:
'x'
'value2'},
calls
0},
priority=50)
12,
self.assertEqual(settings['key_lowprio'],
3},
BaseSettings({'key':
'0',
'123',
'ke2':
'TEST':
30},
3000},
{1:
'TEST_LIST':
'bus')
default_settings)
15)
[module]})
len(self.spider_loader._spiders)
"scrapy3.org"]
load_object,
Status
submodule
"italic
#000000",
app.run()
Blueprint,
page)
TemplateNotFound:
g,
flash
key',
redirect(url_for('show_entries'))
error=error)
flaskr
teardown():
request.addfinalizer(teardown)
flaskr.app.config['USERNAME'],
flaskr.app.config['PASSWORD'])
logout(client)
jsonify,
SECRET_KEY
app.config.from_object(__name__)
query_db('select
user_id
?',
@
before_request():
session:
render_template('timeline.html',
messages=query_db('''
message.*,
user.*
message.author_id
?
message.pub_date
match'
'meh',
werkzeug.utils
current_app,
before_render_template
iteritems
reraise(tp,
(str,
Lock
werkzeug.datastructures
string_types,
self._got_first_request:
imported
decorators
functionality
fix
root_path=None):
instance_path
A
self._error_handlers
self.has_static_folder:
'new
@locked_cached_property
self.root_path
resource),
mode)
reqctx.request.blueprint
werkzeug.serving
bool(debug)
open_session(self,
save_session(self,
first_registration
'A
first_registration)
add_url_rule(self,
view_func=None,
methods:
view_func
old_func
queue:
e.code
handler(e)
InternalServerError()
.debughelpers
adapter
HTTPException
builder
self.url_prefix
url_defaults
'but
register(self,
register_template(state):
self.record_once(register_template)
reraise
'you
app_obj
__version__,
self._load_unlocked()
create_app
self.create_app
params=params,
self._loaded_plugin_commands
By
enabled.')
'Python
'prefix':
'python
self.__name__
silent=False):
Set
os.path.join(self.root_path,
config_file:
len(mapping)
self.__dict__
within
self.preserved
self._preserved_exc
To
'with
we
request.form:
%s.%s'
src_info
seems_fishy
__ne__(self,
self.__eq__(other)
fullname,
load_module(self,
fullname)
tb.tb_next
attempted
NotFound
url_adapter
endpoint[1:]
might
anchor
url_adapter.url_scheme
rv.last_modified
0xffffffff
exist,
etags
NotFound()
import_name
'get_filename'):
'not
needs
parent,
folder
base_dir
self._static_folder
get_send_file_max_age(self,
object')
_should_log_for(app,
bool(value)
session):
secure
environment,
context,
sess
self.preserve_context
nest
cls.__module__
{0}'.format(e))
_string_re_part)
make_diff(filename,
old,
1:]
new_content_lines
'{{
templates
bundled
_tempdir
font-family:
color:
5px
iptr,
outcome
self.logs
logfile
fn)
version)
file=sys.stderr)
Popen(['git',
textwrap
@pytest.fixture(autouse=True)
flask.url_for('index')
'https://localhost/'
pytest.raises(RuntimeError):
cleanup(exception):
cleanup_stuff.append(exception)
flask.g.foo
cake
flask.render_template_string('{{
index2():
b'POST'
random_uuid4
index)
c.get('/foo/').data
b'bar'
'domain=.example.com'
the_uuid
c.get('/bump').data
b'3'
run_test(expect_header=False)
flask.session.modified
World',
World'),
flask.get_flashed_messages(
'after'
resp.status_code
error',
error'
b'forbidden'
@app.errorhandler(403)
pytest.raises(NotFound):
c.get('/fail')
'Foo'
b'Awesome'
400)
compressed_msg
\n
flask.url_for,
pytest.raises(
flask.url_for('static',
filename='index.html')
recwarn.pop(DeprecationWarning)
subdomain='foo')
"server
c.get('/').status_code
monkeypatch.setattr(werkzeug.serving,
'run_simple',
run_simple_mock)
flask.g.lang_code
@app.route('/foo')
app.register_blueprint(bp)
data={},
pytest.raises(ZeroDivisionError):
'normal
...'
no',
app.register_blueprint(frontend)
no'
blueprintapp
7200
unexpected_max_age
@bp.app_template_filter()
@bp.app_template_test()
absolute_import,
find_best_app,
Flask('appname')
find_best_app(Module)
'FLASK_APP',
CliRunner()
obj=obj)
runner.invoke(cli,
'devkey'
pytest.raises(RuntimeError)
app.config.from_envvar('FOO_SETTINGS')
pytest.raises(IOError)
msg.startswith('[Errno
Unable
(No
directory):')
recwarn):
42}
'OK'
c.post('/',
imp
import_hooks
excinfo:
str(excinfo.value)
codecs
resp.data
23,
str(i),
flask.json.loads(rv.data)
flask.request.args['foo']
'x-sendfile'
open(os.path.join(app.root_path,
StringIO('Test')
b'Test'
parse_options_header(rv.headers['Content-Disposition'])
'attachment'
options['filename']
app.send_static_file('index.html')
logger1
app.logger
_scheme='https')
'http://localhost/'
view_func=myview)
flask.url_for('myview',
'!'
c.get('/?name=World')
__next__(self):
'a/b/c'),
purge_module):
flask\n'
'app
flask.Flask(__name__)\n'
init
app.join('__init__.py')
modules_tmpdir_prefix,
purge_module,
modules_tmpdir,
flask\napp
flask.Flask(__name__)')
threading.Lock()
flask.render_template('simple_template.html',
whiskey=42)
greenlet
end_of_request(exception):
buffer.append(exception)
ctx.pop()
app.test_request_context('/',
flask.request.url
index()
installed')
recorded[0]
context['whiskey']
@app.context_processor
context_processor():
b'&lt;p&gt;Hello
app.config['EXPLAIN_TEMPLATE_LOADING']
'testing'
sess:
len(sess)
Index(flask.views.View):
rv.headers['X-Method']
sys.exit()
ids
ID:
ids,
Waiting
again')
retrying')
set')
since
copying
default")
'--debug',
parser.add_argument('-t',
help='Name
including
basestring)
unknown
'Response'
5]
j)
hunk
cert:
argparse.ArgumentParser()
3",
Config,
boto.plugin
boto.sdb.connection
boto.fps.connection
FPSConnection
boto.cloudfront
CloudFrontConnection
boto.emr
boto.iam
boto.route53
boto.cloudformation
OrdinaryCallingFormat
secret_key,
Layer1(aws_access_key_id,
boto.elastictranscoder.layer1
ElasticTranscoderConnection
boto.opsworks.layer1
OpsWorksConnection
boto.redshift.layer1
RedshiftConnection
boto.support.layer1
SupportConnection
boto.cloudtrail.layer1
boto.directconnect.layer1
boto.kinesis.layer1
boto.logs.layer1
boto.kms.layer1
boto.awslambda.layer1
boto.ec2containerservice.layer1
EC2ContainerServiceConnection
boto.machinelearning.layer1
is_latest=False):
InvalidUriError('Unrecognized
is_stream
path_parts
(not
'%s://%s/%s'
update_provider(self,
hmac.new(self._provider.secret_key.encode('utf-8'),
__getstate__(self):
dct
self).__init__(host,
http_request.method
headers['Date']
self._provider.security_token_header
boto.log.debug('StringToSign:\n%s'
self._provider.access_key,
headers_to_sign(self,
http_request.headers.items():
headers_to_sign[name]
headers_to_sign):
string_to_sign,
req.headers['X-Amzn-Authorization']
req.headers['X-Amz-Date']
host_header_value
value.decode('utf-8')
safe='')
l.append('%s=%s'
canonical
cr
self.split_host_parts(host)
charset=UTF-8'
super(S3HmacAuthV4Handler,
self.region_name:
region_name):
offset,
req):
modified_req.params
value[0]
iso_date
'&Signature='
SignatureVersion
_calc_signature(self,
boto.log.debug('using
'ec2',
boto.config.get('s3',
http_client,
int
NoOptionError,
unquote_str
long
PleaseRetryException
{True:
size(self):
put(self,
self.last_clean_time
is_secure):
self.mutex:
self.protocol,
self._cached_response
amt=None):
provider='aws',
self.suppress_consec_slashes
suppress_consec_slashes
self.http_exceptions
socket.error,
https_connection.InvalidCertificateException)
self.provider,
connection(self):
path='/'):
path_elements
self.proxy:
'proxy',
'proxy_port',
self.skip_proxy(host):
self.host.split(':',
hostname):
cert,
host=None):
login_info
retry_handler=None):
'num_retries',
num_retries:
time.sleep(next_sleep)
request.path
exception:
BotoServerError(response.status,
profile_name=None,
len(items)
items[i
current_prefix
get_object(self,
self.body:
self.status,
base64.b64decode(value)
self)._cleanupParsedProperties()
disposition)
root_node,
characters(self,
cert_file=None,
pythonize_name
Element):
self.list_marker,
self.item_marker,
self.pythonize_name)
(element_name,
ListElement(list):
list_marker
_plugin_loaded
'restore',
'credentials')
self._credentials_need_refresh():
self._access_key
self._security_token
client.")
variable.")
config.has_option("profile
config.get("profile
security_token_name):
security_token_name)
expires_at
datetime.strptime(
metadata):
'an
additional_path
load_regions()
(now
duration,
self.markers
self.version_id_marker
to_boolean(self,
true_value='true'):
true_value:
'IsTruncated':
'return':
'StatusCode':
'IsValid':
'requestId':
connection_args
gsutil
BucketStorageUri)
(function_name,
self.provider_pool[self.scheme]
bucket.delete_key(self.object_name,
mfa_token)
DeleteMarker))
self._check_object_uri('get_key')
key.get_contents_to_file(fp,
self.connection_args
self._build_uri_strings()
self.versionless_uri
self.bucket_name,
self.version_specific_uri
self.version_id:
generation,
generation=self.generation)
clone_replace_name(self,
new_name):
get_cors(self,
set_cors(self,
recursive,
key.add_email_grant(permission,
key.add_user_grant(permission,
user_id)
is_file_uri(self):
is_cloud_uri(self):
names_container(self):
names_singleton(self):
names_directory(self):
names_provider(self):
names_bucket(self):
names_file(self):
names_object(self):
storage_class=None):
generation=self.generation,
xmlstring,
md5)
src_bucket_name=src_bucket_name,
get_logging_config(self,
configure_versioning(self,
metadata_minus,
preserve_acl,
get_lifecycle_config(self,
configure_lifecycle(self,
lifecycle_config,
self.get_bucket()
self.names_container()
JSONDecodeError
'versions',
'uploads',
provider=None):
boto.provider.get_default()
interesting_headers:
metadata[k]
six.string_types)
up')
num_retries,
LazyLoadMetadata(url,
timeout=self._timeout)
fields:
'corrupted
url='http://169.254.169.254',
retry_on_404=False,
user_data:
ISO8601
datetime.datetime.strptime(ts,
ISO8601)
c.get_bucket(bucket_name)
boto.log.exception('Problem
cwd=None):
self.log_fp
subprocess.Popen(self.command,
item.previous
from_string
rcontent
len(s)
(hex_digest,
'/2014-11-13/functions/{0}'.format(function_name)
AttributeError):
AWSAuthConnection.make_request(
error_type
response.getheader('x-amzn-ErrorType').split(':')[0]
error_class
self._faults.get(error_type,
error_class(response.status,
version_label,
version_label}
solution_stack_name=None,
template_name}
solution_stack_name:
params['SolutionStackName']
solution_stack_name
option_settings=None,
options_to_remove=None,
options_to_remove:
severity=None,
request_id
params['Severity']
start_time:
end_time:
_build_list_params(self,
counter
super(Response,
self.max_length
str(response['Namespace'])
self.deployment_status
str(response['DeploymentStatus'])
self.option_settings
option_setting
ConfigurationOptionSetting(member)
self.option_settings.append(option_setting)
str(response['Message'])
self.instances.append(instance)
self.load_balancers
super(Instance,
boto.cloudformation.connection
boto.cloudformation.stack
valid_states
converter=None,
encode_bool(self,
template_body,
template_url,
disable_rollback,
timeout_in_minutes,
capabilities,
stack_policy_body,
stack_policy_url,
params['TemplateBody']
params['TemplateURL']
template_url=None,
parameters=None,
stack_name_or_id):
stack_name_or_id}
stack_name_or_id=None,
'LogicalResourceId':
self.disable_rollback
self.notification_arns
self.outputs
"Parameters":
Tag()
str(value).lower()
self.deletion_time
"LogicalResourceId":
"PhysicalResourceId":
"ResourceStatus":
"ResourceStatusReason":
"ResourceType":
self.last_updated_time
self.template_parameters
OriginAccessIdentity
host=DefaultHost,
response.msg
'/%s/%s'
d.etag
handler.XmlHandler(d,
self.get_etag(response)
'distribution',
config)
enabled=enabled,
caller_reference=caller_reference,
cnames=cnames,
comment=comment,
self._create_object(config,
'streaming-distribution',
'origin-access-identity/cloudfront',
caller_reference=None):
max_items=max_items,
StreamingObject
enabled=False,
self.cnames
self.default_root_object
self.comment:
<Comment>%s</Comment>\n'
'CallerReference':
domain_name='',
last_modified_time=None,
get_distribution(self):
config=None,
status=''):
self.active_signers
self._bucket
enabled=None,
self.config.caller_reference,
new_config.comment
self.etag,
new_config)
self.etag)
self._get_bucket()
replace=False):
self.config.origin.origin_access_identity:
private_key_file=None,
private_key_string=None):
signed_url
expire_time
condition["DateGreaterThan"]
{"Statement":
"Condition":
both")
"/%s"
self.distribution_id
is_truncated
'%s://'
self.origin_protocol_policy
{'Label':
external_id=None,
client_token=None,
params['ExternalId']
certificate,
{'HapgArn':
hsm_arn,
get_config(self,
params['SubnetId']
CommitMismatchError(Exception):
fields}
commit(self):
requests.Session()
_body
json.loads(_body)
indexing
SearchServiceException("Error
self.deletes
type_,
boto.cloudsearch.document
boto.cloudsearch.search
Domain(object):
layer1,
self.created
self.deleted
data['search_service']
id(self):
OptionStatus(self,
field_names=None):
facet=False,
searchable=False,
default=default,
proxy=proxy,
proxy_port=proxy_port,
get_response(self,
'CreateDomain',
delete_domain(self,
'DeleteDomain',
'IndexFieldName':
'IndexDocuments',
session_token=None,
domain_data
self.update_version
_update_options(self,
refresh(self,
Query(object):
bq=None,
facet_constraints=None,
facet_sort=None,
facet_top_n=None,
t=None):
{'start':
self.start,
self.real_size}
self.q:
params['q']
self.return_fields:
','.join(self.return_fields)
self.facet:
rank=rank,
search(self,
self._data_function
{'http':
CloudSearchDomainConnection(
api_version):
'application/json')
boto.cloudsearch2.document
boto.cloudsearch2.search
'DefaultValue':
boto.cloudsearch2
k)]
region_info.name
"Allow",
"Action":
parser=None,
fq=None,
self.fq
self.parser:
AuthServiceName
json.loads(response.read().decode('utf-8'))
InternalErrorException(BotoServerError):
InvalidTimeRangeException(BotoServerError):
InvalidNextTokenException(BotoServerError):
"InvalidTimeRangeException":
exceptions.InvalidTimeRangeException,
"InvalidNextTokenException":
exceptions.InvalidNextTokenException,
'S3BucketName':
params['S3KeyPrefix']
application_name):
deployment_config_name=None,
params['deploymentConfigName']
{'deploymentConfigName':
'deploymentGroupName':
revision,
{'deploymentId':
InvalidParameterException(BotoServerError):
"InvalidParameterException":
exceptions.InvalidParameterException,
'IdentityPoolName':
'/identitypools/{0}/identities/{1}/datasets/{2}'.format(
identity_id)
query_params['nextToken']
query_params['maxResults']
"ValidationException":
exceptions.ValidationException,
self).__init__(queue,
boto.datapipeline
create_pipeline(self,
task_id,
connection_name,
'bandwidth':
'ownerAccount':
self.attributes_to_get
self.consistent_read
key_list
layer2
DynamoDBResponseError
self._hash_key_name
self.consumed_units
attr_value):
self._updates[attr_name]
return_values)
Layer1(AWSAuthConnection):
self).__init__(self.region.endpoint,
self.build_base_http_request('POST',
retry_handler=self._retry_handler)
_retry_handler(self,
next_sleep):
list_tables(self,
data['Limit']
start_table
describe_table(self,
table_name}
provisioned_throughput):
delete_table(self,
attributes_to_get:
data['AttributesToGet']
batch_write_item(self,
data['Expected']
return_values:
data['ReturnValues']
update_item(self,
scan_index_forward=True,
count:
scan_filter=None,
scan_filter:
scan_filter
boto.dynamodb.schema
item_class,
self._consumed_units
self._count
self._scanned_count
self._response:
lektuple
TypeError(msg)
this_round_limit
range_key_name
item_class=Item):
self.build_key_from_values(table.schema,
range_key,
'ConsumedCapacityUnits'
item.consumed_units
response['ConsumedCapacityUnits']
self.dynamize_expected_value(expected_value)
'count':
self._dict:
consistent_read=consistent_read)
new_item(self,
long_type,
'NS'
'SS'
'BS'
'B':
ResourceNotFoundException(JSONResponseError):
ValidationException(JSONResponseError):
STRING
attr_type
self.data_type
key_schema,
self._dynamizer
deepcopy(self._data)
self._data.get(key,
alterations
needs_save
fields.add(key)
overwrite=False):
return_item_collection_metrics=None,
condition_expression=None,
params['Expected']
params['ReturnValues']
params['ConditionExpression']
projection_expression=None,
params['AttributesToGet']
params['ProjectionExpression']
index_name=None,
provisioned_throughput
StopIteration()
[])):
boto.dynamodb2.fields
(HashKey,
RangeKey,
KeysOnlyIndex,
GlobalKeysOnlyIndex,
boto.dynamodb2.items
throughput=None,
self.global_indexes
global_indexes
raw_indexes
field['KeyType']
global_indexes:
gsi_data.append({
raw_key,
enumerate(args):
field_bits[-1]
'between':
lookup['AttributeValueList'].append(
reverse=False,
max_page_size=None,
query_filter,
using=FILTER_OPERATORS
last_evaluated_key
raw_item
item.load({
raw_item,
results.append(item)
self._dynamizer.decode(value)
self.table_name:
self._to_put
self.flush()
flush(self):
Item(self.table,
'GT',
'BEGINS_WITH',
instance_id=None):
self.public_ip
self.network_interface_id
'networkInterfaceId':
self.connection.associate_address(
delete_on_termination
volume_type
'encrypted':
dev_name
params['%s.Ebs.DeleteOnTermination'
IntegerProperty
propget
InstanceTypes
['m1.small',
'm1.xlarge',
'c1.medium',
get_region(self,
propget.get(prop,
params.get('instance_type',
params.get('quantity',
params['quantity']
StringProperty(name='zone',
Zone',
choices=self.ec2.get_all_zones)
params['zone']
self.get_region(params)
self.get_zone(params)
boto.ec2.instance
boto.ec2.address
Address
Volume,
boto.ec2.zone
boto.ec2.instanceinfo
InstanceInfo
NetworkInterface
aws_name
owners=None,
'ImageId')
owners:
owners,
self.get_list('DescribeImages',
Image)],
architecture=None,
params['Architecture']
kernel_id:
params['KernelId']
ramdisk_id:
params['RamdiskId']
block_device_map:
virtualization_type
image_id
create_image(self,
attribute='launchPermission',
max_count=1,
addressing_type=None,
placement=None,
monitoring_enabled=False,
additional_info=None,
instance_profile_arn=None,
ebs_optimized=False,
'MinCount':
'MaxCount':
l.append(group.id)
'SecurityGroupId')
placement_group
params['PrivateIpAddress']
additional_info
instance_profile_name
params['Force']
bool_reqs
bool_reqs:
{'NetworkInterfaceId':
(idx
request_ids,
SpotInstanceRequest)],
filters=None):
'SpotPrice':
ls)
private_ip_addresses=None,
secondary_private_ip_address_count
new_value,
isinstance(zone,
Zone):
params['Encrypted']
kms_key_id
owner=None,
volume_name
{'SnapshotId':
start_of_month
monthly_snapshots_added
snaps_for_volume
time_period_number
snaps:
check_this_snap
EC2ResponseError:
attribute='createVolumePermission',
{'KeyName':
params['VpcId']
group.description
params['GroupName']
from_port:
to_port:
group_name=None,
group_name:
params['IpPermissions.1.IpProtocol']
single_cidr_ip
params['IpPermissions.1.IpRanges.1.CidrIp']
regions:
instance_tenancy=None,
reserved_instances_id,
client_token,
'ModifyReservedInstances',
verb='POST'
delete_tags(self,
network_interface_id,
device_index,
attachment_id,
'AttributeName')
enable_dns_support
params['EnableDnsSupport.Value']
enable_dns_hostnames
params['EnableDnsHostnames.Value']
TagSet
self.is_public
'false':
'kernelId':
'ramdiskId':
img_attrs
self.connection.get_image_attribute(
'group':
'userId':
boto.ec2.group
self.code)
Instance)])
self.eventsSet
self.interfaces
self._previous_state
'keyName':
self._in_monitoring_element:
force,
Status()
self.region:
self.device_index
self.requester_managed
self.source_dest_check
Attachment()
detach(self,
'DeviceIndex']
params[key]
self.associate_public_ip_address
currency_code
self.recurring_charges
self.pricing_details
'productDescription':
self.amount
amount
instance_count=None,
status_message=None,
create_date
status_message
'statusMessage':
state=None,
platform=None,
instance_type=None):
modification_id
'reservedInstancesModificationId':
self.reserved_instances
self.modification_results
parse_ts(value)
self.rules:
grant.name
grant.group_id
self.grants[-1]
add_grant(self,
'snapshotId':
iops,
self.launch_specification
cancel(self,
Tag(object):
super(Volume,
Tag)])
self.attach_data:
boto.ec2.autoscale.launchconfig
ScalingPolicy
boto.ec2.autoscale.instance
self.use_block_device_types
tag.build_params(params,
'InstanceIds')
params['ShouldDecrementDesiredCapacity']
autoscale_group,
isinstance(autoscale_group,
AutoScalingGroup):
autoscale_group.name
{'PolicyName':
as_group=None,
as_group:
as_group}
'ScheduledActionName':
desired_capacity
min_size
max_size
'Granularity':
self.status_code
self.health_check_period
'Instances':
'AvailabilityZones':
'CreatedTime':
'SnapshotId':
self.ebs
delete_on_termination=True,
self.user_data
'UserData':
%s]'
self.metrics
self.granularities
self.propagate_at_launch
self.resource_id
dim_name
params['%s.%d.Value'
enumerate(items):
dimensions,
unit:
start_time,
end_time,
self.build_dimension_param(dimensions,
params['Unit']
alarm_names,
end_date=None,
start_date:
period=None,
statistic=None,
self.threshold
self.period
self.evaluation_periods
self.alarm_actions
self.insufficient_data_actions
self.ok_actions
action_arn=None):
action_arn:
'Timestamp':
self.namespace,
statistic,
'AvailabilityZones.member.%d')
subnets,
'Subnets.member.%d')
instances,
'Instances.member.%d.InstanceId')
'LoadBalancerPort':
cookie_expiration_period,
policy_type,
instance_port,
'InstancePort':
self.emit_interval
self.healthy_threshold
self.unhealthy_threshold
self.load_balancer_port
self.ssl_certificate_id
self.instance_port,
self.source_security_group
self.backends
instances)
policy_name)
self.app_cookie_stickiness_policies
self.lb_cookie_stickiness_policies
self.other_policies
cluster=None):
'type',
reason=None,
page:
self._curobj
'<%s:
len(self._nodepath)
self._curobj:
"Item":
ElastiCacheConnection
'EC2SecurityGroupName':
'EC2SecurityGroupOwnerId':
cache_node_type=None,
cache_security_group_names=None,
notification_topic_arn=None,
auto_minor_version_upgrade=None):
params['CacheNodeType']
cache_security_group_names
cache_security_group_names,
'CacheSecurityGroupNames.member')
security_group_ids
security_group_ids,
'SecurityGroupIds.member')
params['NotificationTopicArn']
subnet_ids):
{'CacheClusterId':
action='DescribeEvents',
apply_immediately=None,
parameter_name_values,
subnet_ids=None):
reset_all_parameters
reset_all_parameters).lower()
expected_status=202)
input_bucket=None,
params['InputBucket']
params['Role']
params['Notifications']
container
'/2012-09-25/pipelines/{0}'.format(id)
bootstrap_action_args
JobFlowStepList,
self.auth_region_name
cluster_id):
instance_groups
action_on_failure
master_instance_type,
slave_instance_type,
params['VisibleToAllUsers']
main_class
(i+1,
items):
(field,
'SPOT':
self.stepids
'Args':
Arg)])
'Value',
'EndDateTime',
'LastStateChangeReason',
'StartDateTime',
KeyValue)])
'ReadyDateTime',
self.bootstrapactions
self.timeline
self.ec2instanceattributes
self.clusters
jar(self):
NotImplemented()
main_class(self):
self).__init__(name,
Key(object):
fp=None,
key_type
self.KEY_STREAM_READABLE:
self.KEY_STREAM_WRITABLE:
fp.getvalue()
ResponseErrorFactory
boto.fps.response
ResponseFactory
decorated_attrs:
len(x)
argument(s)"
'Response')
@requires(['TransactionId'])
base,
@complex_amounts('TransactionAmount')
'TransactionAmount.Value',
'TransactionAmount.CurrencyCode'])
@requires(['TokenId'])
@requires(['SubscriptionId'])
self._name:
self._action
__float__(self):
Decimal(value)
ComplexAmount(name=name))
name)[-1]
minimum_part_size,
part_size=DEFAULT_PART_SIZE,
num_threads=10):
self._part_size
total_parts
threads.")
range(self._num_threads):
archive,
self._vault_name,
response['ArchiveId']
"an
archive:
_process_chunk(self,
num_retries=5,
retry_exceptions
part_number
linear_hash
(part_number,
vault,
range
num_chunks,
chunk_size,
verify_hashes,
retry_exceptions)
chunk_size)
boto.glacier
'vaults/%s'
delete_vault(self,
'vaults/%s/notification-configuration'
job_id)
u'Location')]
archive_id)
part_data):
'bytes
Vault
DEFAULT_PART_SIZE
hashes
chunk_size
len(hashes)
hashes.pop(0)
isinstance(chunk,
boto.glacier.job
writer
inside_end
part_hash_map
part_hash_map[part_index]
resume_file_upload(
self._buffer
chunk_size=_ONE_MEGABYTE):
self.chunk_size
self._uploaded_size
part_data)
data.encode('utf-8')
self.uploader
ACCESS_CONTROL_LIST
ENTRY
ENTRIES
SupportedPermissions
entries_repr
permission=permission)
self.entry_list:
self.scope
InvalidAclError('Invalid
NAME.lower()],
(self.type.lower()
get_utf8_value
generation_marker='',
delimiter,
self.validate_kwarg_names(params,
Policy):
details)
query_args)
'&generation=%s'
acl_or_str
headers['x-goog-if-generation-match']
str(if_generation)
self._set_acl_helper(acl_str,
canned
GSPermissions:
target_bucket
query_args='logging',
get_versioning_status(self,
query_args='versioning',
self.VersioningBody
LifecycleConfig()
self.delimiter
prefix=self.prefix,
delimiter=self.delimiter,
storage_class:
409:
CORS_CONFIG
self.element
(%s/%s)'
super(Key,
(self.bucket.name,
'IsLatest':
open_read(self,
query_args:
'&'
self.get_file(fp,
send_file(self,
chunked_transfer=False,
chunked_transfer=chunked_transfer,
self.get_acl()
self.set_acl(acl)
if_generation=None):
(etag,
self.bucket.lookup(self.name):
self.send_file(fp,
set_contents_from_filename(self,
key.bucket.name
(key.name,
299:
LIFECYCLE_CONFIG
LEGAL_ACTIONS:
LEGAL_ACTION_PARAMS:
LEGAL_CONDITIONS:
self.conditions:
validate(self):
'>')
self.upload_start_point
open(self.tracker_file_name,
print('Couldn\'t
'Couldn\'t
'if
put_headers
Completed
upload.
ResumableTransferDisposition.START_OVER)
server_end)
key.bucket.connection
print('Starting
503]:
cb(total_bytes_uploaded,
file_length:
range_header
self.digesters:
http_conn.set_debuglevel(conn.debug)
{'md5':
md5}
hash_algs[alg]())
complete.')
User(object):
boto.iam.connection
self.connection_cls(host=self.endpoint,
'Principal':
'Service':
'Effect':
'Allow',
path_prefix='/',
path}
group_name):
new_path=None):
new_path:
params['NewPath']
list_marker='PolicyNames')
access_key_id,
cert_id,
cert_body,
cert_name,
cert_name}
params['Path']
alias,
'AssumeRolePolicyDocument':
saml_metadata_document,
policy_document,
version_id):
version_id}
ResourceInUseException(BotoServerError):
shard_id,
alias_name,
constraints
ciphertext_blob,
"Value
"must
response.get('Plaintext')
response['Plaintext']
response['Plaintext'].encode('utf-8'))
params['NumberOfBytes']
log_group_name):
filter_name,
data_source_name=None,
compute_statistics=None):
params['DataSourceName']
params['ComputeStatistics']
ml_model_id):
predict_host
host_key_file='~/.ssh/known_hosts',
ssh_pwd=None):
uname
self._pkey
SSH
listdir(self,
command):
self.hostname
self.instance.id
boto.config.get('Instance',
(prompt,
raw_input('%s:
boto.mashups.iobject
IObject
IntegerProperty,
boto.manage.cmdshell
key_file,
bucket')
AMI
key_file:
public
image...')
(bucket,
params.get('name',
g.name
params['group']
keypair
params['keypair']
StringProperty(verbose_name="EC2
ID")
security_group
'db_type',
'db_name',
logical_volume
ec2.aws_secret_access_key)
instance.update()
s.ec2
str(i)
s.description
s.region_name
s.put()
description=''):
boto.ec2.regions()
self).__init__(id,
self._reservation:
self.instance_id:
co
gn
self.production:
ValueError("Can't
server")
self).delete()
reboot(self):
self.get_ssh_key_file()
queue_name):
self.now
nsecs
new_msg
poll(self,
Server')
print(status[1])
mount_point
vol
ec2.get_all_volumes([self.volume_id])
self.server.get_cmdshell()
closing(self.server.get_cmdshell())
lines:
(self.device,
self.mount_point))
previous_week:
remainder
self.name))
copy(self,
end='
print('No
okay:
self.items
self.items:
item.name,
ssh_client.exec_command(command)
t[2].read())
prefix)
Price
keywords=None,
event_types=None):
'SetHITTypeNotification',
event_types)
ValueError("Must
QuestionForm
isinstance(question,
total_records
page_size
page_number=1,
feedback=None):
feedback:
params['RequesterFeedback']
feedback
expiration_increment
'Reason':
reason}
auto_granted_value
qualification_type_id):
isinstance(keywords,
reward
layoutParameters
"Name":
verification_input
self.formatted_price
ord)
requirements
self.integer_value
get_inner_content(self,
OrderedContent()
self).get_as_xml()
%(attrs)s
/>'
self.num_lines
num_lines
style
count_xml
('2010-10-01',
field)
into[prefix]
filter(lambda
group)
accesskey,
@boolean_arguments('Acknowledged')
@structured_lists('ReportTypeList.Type')
@api_action('Inventory',
['LastUpdatedAfter'])
'SellerSKUList'])
@structured_lists('SellerSKUList.SellerSKU')
'Value'
str(getattr(self,
self._namespace
isinstance(attribute,
_ResultClass
MemberList(
self._amount))
Element(Price)
default_os=None,
hostname_theme=None,
default_availability_zone=None,
default_subnet_id=None,
custom_json=None,
configuration_manager=None,
chef_configuration=None,
use_custom_cookbooks=None,
custom_cookbooks_source=None,
default_ssh_key_name=None,
params['DefaultOs']
params['HostnameTheme']
params['DefaultAvailabilityZone']
params['DefaultSubnetId']
params['ConfigurationManager']
params['ChefConfiguration']
params['UseCustomCookbooks']
params['UseOpsworksSecurityGroups']
params['CustomCookbooksSource']
params['DefaultSshKeyName']
params['DefaultRootDeviceType']
params['Hostname']
{'IamUserArn':
params['SshPublicKey']
app_id,
params['LayerIds']
boto.config.get('Pyami',
open(path)
val.lower()
item_name):
boto.connect_sdb()
domain:
self.dst:
True):
new_key
usage()
get_ts
config_file=None):
get_ts()
find_class
script
script)
self.instance_id,
self.run('chmod
service_name)
cnf.write("<VirtualHost
On\n")
cnf.write("\t<Location
cnf.write("\t</Location>\n")
env)
boto.rds.dbsecuritygroup
DBSecurityGroup
boto.rds.parametergroup
ParameterGroup
boto.rds.vpcsecuritygroupmembership
VPCSecurityGroupMembership
master_password,
param_group=None,
character_set_name
vpc_security_groups=None,
):
'BackupRetentionPeriod':
'EngineVersion':
'LicenseModel':
instance_class=None,
allocated_storage=None,
params['MasterUserPassword']
allocated_storage
params['SkipFinalSnapshot']
id):
pg
'DBParameterGroupFamily':
dbinstance_id,
filename_contains
file_last_written
number_of_lines
'TargetDBSnapshotIdentifier':
'DBSubnetGroupDescription':
DBSubnetGroup)
'OptionGroupDescription':
{'OptionGroupName':
max_records=100,
params['EngineName']
self.parameter_groups
self.read_replica_dbinstance_identifiers
self.pending_modified_values
self._in_endpoint
self.status_infos
self.subnet_group
group_owner_id
'"%s"'
'SourceIdentifier':
self.log_filename
self.allow_both_vpc_and_nonvpc
db_security_groups=None,
db_security_groups
vpc_security_groups
self.is_collection
'AllowedValues':
'DataType':
'ApplyType':
'IsModifiable':
self.port_required
isinstance(self._value,
self.normal
self.vpc_group
{'ResourceName':
db_parameter_group_name=None,
source_ids=None,
source_ids
source_ids,
'SourceIds.member')
enabled=None):
snapshot_cluster_identifier=None):
cluster_security_groups=None,
cluster_subnet_group_name=None,
cluster_parameter_group_name=None,
allow_version_upgrade=None,
cluster_security_groups
cluster_security_groups,
'ClusterSecurityGroups.member')
params['ClusterSubnetGroupName']
params['ClusterParameterGroupName']
params['AutomatedSnapshotRetentionPeriod']
allow_version_upgrade
params['AllowVersionUpgrade']
allow_version_upgrade).lower()
'ClusterSubnetGroupName':
hsm_configuration_identifier,
self.filters
mthd
self.cli_args
param.long_name:
'array':
help='Override
provided')
value')
os.path.expandvars(path)
os.path.isfile(path):
fp.read()
fmt['name']
'aws'
len(l)
self.ptype
self.long_name:
self.short_name:
boto.route53.zone
'caller_ref':
caller_ref,
xml_body
xml_body)
[list,
tuple,
set]:
request_interval
'MX',
super(ResourceRecordSets,
resource_records
{"identifier":
self.identifier,
id=%s,
(self.identifier,
self.__setattr__(key.lower(),
ResourceRecordSets(self.route53connection,
Status(self.route53connection,
self._commit(changes))
changes.add_change_record("DELETE",
record:
self.route53connection._make_qualified(value)
ttl=ttl,
[r
old_record
old_record.ttl
self.update_record(old_record,
new_value=value,
new_ttl=ttl,
new_identifier=identifier,
self.delete_record(record)
params['IdnLangCode']
nameservers,
self.acl
grants.append("%s
g.permission))
'CanonicalUser':
'Group':
'Grant':
Grant
boto.s3.multipart
boto.s3.tagging
boto.s3.cors
CORSConfiguration
validate=True):
self.key_class(self)
'_')]
k.handle_version_headers(response)
version_id_marker='',
upload_id_marker='',
key_name=None):
generate_url(self,
boto.utils.compute_md5(fp)
'.join(mfa_token)
self.connection.make_request('POST',
src_bucket
subresource
ver
'Enabled'
mfa
d['Versioning']
d['MfaDelete']
query_args='lifecycle',
query_args='website',
query_args='policy',
query_args='cors',
'uploadId=%s'
"url":
self.encoding_type
headers=self.headers,
encoding_type=self.encoding_type)
boto.utils.get_utf8_value(key)
urllib.parse.quote(key)
path_base
's3.amazonaws.com')
'hmac-v4-s3'
['s3']
fields.append({'name':
self.calling_format.build_host(self.server_name(),
key='',
self.calling_format.build_path_base(bucket,
self.calling_format.build_auth_path(bucket,
self.provider.security_token
self.max_age_seconds
provider(self):
self.bucket.connection:
base64md5
self.resp.read()
self.open_read()
reduced_redundancy=True,
encrypt_key=False):
provider.storage_class_header:
chunked_transfer
chunk_len
merge_headers_by_name(
norm_metadata
'Undefined
KeyFile'
days=None,
days
self.temp_days
self.temp_date
self.temp_storage_class
self.part_number
self.initiator
self._parts
part.etag
(self.id,
position_to_eof:
ResumableDownloadException(
TRANSLATOR
self).__init__(self.TRANSLATOR)
self).to_xml())
self.redirect
item_name)
domain_name)
domain_or_name):
put_attributes(self,
item_name}
delete_attributes(self,
isinstance(attr_names,
self._metadata
id="%s">'
'ItemCount':
self.in_attribute
decode_value(self,
rs.next_token
read(self):
self.kind
NotImplementedError("Key
Query
Property):
NameError:
properties
xmlmanager
manager):
prop.name)
self.slot_name
self.slot_name)
fnc
model_class,
self.validator:
self.validator(value)
ValueError('Length
TypeError('Expecting
type(value))
self.default_value()
(self.data_type,
s3.get_bucket(match.group(1),
bucket.get_key(match.group(2))
super(IntegerProperty,
long_type(value)
super(ReferenceProperty,
six.integer_types
six.string_types:
count(self,
doc:
increment_string
init_val
_connect(self):
t.size
retrieving
t.save()
tt.put()
boto.config.getint(db_section,
enable_ssl)
(self.encode_reference,
self.decode_reference),
mantissa[0]
exponent[0]
value.id
value.id)
obj.properties(hidden=False):
order_by_filtered
("
type_query
order_by_query
self.domain.delete_attributes(obj.id,
replace=True)
item_node
values.append(value)
obj_node.getAttribute('class')
obj_node.getAttribute('id')
obj_node.getElementsByTagName('property'):
prop_node.getAttribute('name')
"/%s/%s"
obj.id)
buckets
AWS
help="the
command")
self.parser.error('No
self.options.path)
'm1.small')
get_ts(t)
Processing
boto.services.message
ServiceMessage
delete_message(self,
self).get(self.name,
self.output_bucket
self.input_bucket
self.output_bucket:
metadata)
boto.ses.connection
it's
verified."
"Domain
boto.sns.connection
add_permission(self,
label}
remove_permission(self,
message_attributes
'data_type'
attribute['data_type']
'string_value'
attribute['string_value']
'binary_value'
attribute['binary_value']
platform_application_arn=None,
params['EndpointArn']
boto.sqs.regioninfo
SQSRegionInfo
RawMessage
s3_url
boto.sqs.queue
SQSError
wait_time_seconds=None,
visibility_timeout
{'ReceiptHandle'
enumerate(messages):
BatchResults,
base]
visibility_timeout):
base64.b64decode(value.encode('utf-8')).decode('utf-8')
SQSDecodeError('Unable
message')
self.visibility_timeout
messages)
num_messages=1,
vtimeout=10):
file_name,
open(file_name,
boto.sts.connection
mfa_serial_number=None,
mfa_serial_number
mfa_token
role_session_name,
AssumedRole,
'WebIdentityToken':
'secret_key':
'session_token':
'expiration':
'request_id':
self.user
boto.support
attachment_set_id=None):
params['attachmentSetId']
language=None):
check_id,
SWFResponseError
boto.swf
result=None):
details=None,
'runId':
execution_start_to_close_timeout=None,
tag_list=None,
'executionStartToCloseTimeout':
'input':
'taskStartToCloseTimeout':
signal_name,
'activityType':
'registrationStatus':
activity_id
attrs['taskList']
attrs['details']
attrs['workflowId']
boto.swf.layer1_decisions
deprecate(self):
self.aws_access_key_id,
'region':
self.region,
executions
'3600',
'TERMINATE',
int((1
self).__init__(name)
new_mod
__path__
"collections"),
"http.server"),
unichr
_assertRaisesRegex
_assertRegex
_globs_
exec_()
sys.version_info[:2]
print_
newline
VPC
Subnet
ResultSet)
RouteTable)
isinstance(egress,
str(egress).lower()
params['Egress']
InternetGateway)
CustomerGateway)
customer_gateway_id,
VpnGateway)
{'VpnGatewayId':
Subnet)
DhcpOptions)
dhcp_options_id,
'CustomerGatewayId':
vpc_peering_connection_id,
self.network_acl_entries
self.rule_number
self.routes
self.classic_link_enabled
groups=groups,
self.outside_ip_address
self.tunnels
self.static_routes
make_node(rawtext,
type_
issue
append
resolver
endpoint_data
service_name_map
self._endpoint_prefix(service_name)
partition_name
self.is_service_in_partition(service_name,
static_endpoints
attribute_args
setup_class(cls):
teardown_class(cls):
["B",
"C",
!=",
obj.save()
obj.password)
Object"
Object").next().id
"%
=",
assert(s2.val
Sequence,
13,
urllib.urlopen(url)
force_http=True,
'foo/bas'
'foo/bat'
'fie/bar'
'fie/bas'
'fie/bat'
bucket.get_all_keys(prefix='',
bucket.key_class)
k.content_type
mdkey1
'meta1'
k.set_metadata(mdkey1,
mdkey2
'meta2'
k.set_metadata(mdkey2,
k.set_contents_from_string('This
self.fps.get_account_balance()
self.assertTrue(False)
self.assertRaises(ResourceNotFoundException):
self.create_application()
6')
6',
application_name=cls.app_name,
"Type":
'initial_value'),
stacks[0]
name='us-west-2',
identity_pool_id=self.identity_pool_id
{'fields':
'description')
self.read_units
self.write_units
self.dynamodb.create_table(table_name,
self.create_table(table_name,
result['Table']['TableStatus']
hash_key_name:
range_key_name:
["index",
"table"]},
c.put_item(table_name,
key1
item1_range}}
item2_key
item2_range
c.delete_item(table_name,
{'B':
self.hash_key_proto_value,
self.range_key_proto_value)
table_names
foobar_item
table.query('Amazon
DynamoDB',
float_value
integer_set
set([1,
float_set
mixed_set
str_set
item4
results.remaining
item.put()
table.get_item('foo',
self.assertEqual(retrieved['decimalvalue'],
Table.create('users',
]),
'Alice',
'Expert',
'Bob',
self.assertEqual(jane['first_name'],
'Jane')
jane['last_name']
reverse=True
['username'])
reverse=True,
self.assertEqual(next(all_users)['username'],
'username'
self.lsi
self.attributes,
result['TableDescription']['TableName'],
self.dynamodb.describe_table(self.table_name)
self.assertEqual(description['Table']['ItemCount'],
record_1_data
r1_result
self.dynamodb.put_item(self.table_name,
record_1_data)
self.dynamodb.get_item(self.table_name,
self.assertEqual(results['Count'],
'Johann'},
self.dynamodb.put_item(tiny_tablename,
@attr('notdefault')
EC2Connection()
instance.state
t.open(instance.dns_name,
'0.0.0.0/0')
c.get_all_groups()
name="N",
value=[1,
'MetricData.member.2.MetricName':
self.maxDiff
("D2",
'D2',
more_listeners
[(443,
self.conn.create_load_balancer_listeners(self.name,
balancers[0].listeners),
self.listeners[0][0],
load_balancer_names=[self.name])
self.change_and_verify_load_balancer_connection_draining(True,
self.vpc
subnet_id=self.subnet.id,
interfaces
NetworkInterfaceCollection(interface)
{'Progressing':
'Completed':
'Warning':
self.api.read_pipeline(pipeline_id)
topic_arn
['TopicArn']
1024,
self.vault_name
self.num_failures
'CreatedBefore':
'\s*<Scope
type="GroupById"><ID>[0-9a-fA-F]+</ID></Scope>'
self._GetConnection().get_bucket(bucket_name)
fpath
k.get_contents_to_file(fp)
fp.read())
bucket.new_key('foobar')
self.assertEqual(len(rs),
delimiter='/')
bucket.lookup('foo/bar')
logging_str)
self.assertEqual(bucket.get_subresource('logging'),
k2
key_name="foo")
sax.parseString(acl_xml,
k.set_contents_from_string("test1")
tmpdir=None):
'tracker')
self.make_tracker_file(tmpdir)
self.make_dst_fp(tmpdir)
13)
larger_src_key.get_contents_to_file(
fail_after_n_bytes=LARGE_KEY_SIZE/2)
larger_src_file_as_string,
larger_src_file
self.make_large_file()
-1,
storage_uri("gs://bucket/obj")
uri.generation
r"AllUsers")
self.assertRegexpMatches(str(key_uri.generation),
b.get_key("obj")
self.assertEqual(k.generation,
key_uri.generation)
self.assertEquals(k.get_contents_as_string(),
self.assertEqual(versions[0].name,
b.delete_key("foo",
acl1g1
acl1g2
owner1g1
acl1g1.owner.id
owner1g2
acl1g2.owner.id
self.assertEqual(owner1g1,
owner1g2)
entries1g1
acl1g1.entries.entry_list
entries1g2
acl1g2.entries.entry_list
self.assertEqual(len(entries1g1),
acl2g1
acl2g2
acl2g1.entries.entry_list
acl2g2.entries.entry_list
self.assertEqual(len(entries2g2),
public_read_entries1
public_read_entries2
self.assertEqual(len(public_read_entries1),
self.assertEqual(len(public_read_entries2),
iam.get_account_password_policy()
account
policy_doc
"Statement":
num_collected
boto.mws.connection
asin
self.assertIsInstance(response[0],
RDSConnection()
subnet
vpc.id,
values")
rds_api.get_all_db_subnet_groups(name=grp_name)
self.replicaDB_name
self.conn.get_all_dbinstances(self.masterDB_name)
self.replicaDB
self.assertTrue(found)
self.conn.describe_db_instances(self.db_name)
instances['DescribeDBInstancesResponse']\
['DescribeDBInstancesResult']['DBInstances'][0]
self.assertEqual(inst['DBInstanceStatus'],
self.assertEqual(inst['Engine'],
'postgres')
9)
'boto-test-%s.com'
'80')
self.conn.get_list_health_checks()
rset
'20
all=True)
identifier=('baz',
'us-east-1'))
identifier=('bam',
'us-west-1'))
res_upload_handler=NOT_IMPL):
self.set_etag()
self._handle_headers(headers)
reduced_redundancy=NOT_IMPL,
metadata=NOT_IMPL,
self.def_acl
self.acls[key_name]
mock_key
self.keys[key_name]
self.buckets:
mock_bucket
patch,
self.assertEqual(element.name,
self.bucket.get_tags()
self.bucket.configure_website('index.html')
self.bucket.get_website_configuration()
self.assertTrue(self.bucket.configure_lifecycle(lifecycle))
self.bucket.get_lifecycle_config()
storage_class='GLACIER')
self.assertEqual(rule.id,
self.assertEquals('s3.amazonaws.com',
rh
{'response-content-disposition':
k.storage_class
'DELETE'],
max_age_seconds=3000,
data'
k.get_contents_as_string().decode('utf-8')
boto.config.set('Boto',
self.assertConnectionThrows(S3Connection,
self.assertConnectionThrows(GSConnection,
self.enableProxy()
S3Connection,
sfp.seek(5)
num_cb=10)
sfp.close()
self.bucket.new_key('redirect-key')
self.assertEqual(key.get_redirect(),
'gzip')
'de')
'all')
Dec
self.bucket.list_versions():
self.bucket.get_key('testkey')
mfa_code
Code:
mfa_token=(mfa_sn,
mfa_code))
self.assertEqual('Enabled',
self.bucket.delete_key('foobar',
self.bucket.delete_keys(self.bucket.list())
key.set_contents_from_string('this
self.bucket.delete_keys(key_names)
self.assertEqual(len(result.deleted)
len(result.errors),
StringIO("small
lmpu
self.assertEqual(lmpu.id,
self.assertEqual(lmpu.key_name,
part_num=1,
mpu:
mock.patch('os.environ',
BIG_SIZE
actual
self.bucket.get_all_versions()
listed_kv1
self.assertEqual(listed_kv1.version_id,
kv1.version_id)
self.assertEqual(listed_kv1.is_latest,
{'name1':
sns
mpo
mock.patch.object
self.snsc.subscribe_sqs_queue(topic_arn,
c.create_queue(queue_name,
num_queues
queue_1.count_slow()
'ApproximateNumberOfMessages'
message_body
msgs:
self.put_queue_message(test)
role_arn=arn,
swf
time.sleep(PAUSE_SECONDS)
self._workflow_type_name,
self._workflow_type_version,
task_list=self._task_list,
self.assertNotEqual(found,
'list_domains;
self.assertEqual(found['description'],
self.assertEqual(found['status'],
'list_workflow_types;
'list_activity_types;
reverse_order=True)
event['eventType']
config_environment
hit:
config_environment()
SetHostMTurkConnection()
create_hit
keywords=['boto',
annotation='An
Test",
MTurkCommon
self.conn.create_hit(
**self.get_hit_params()
self.https_connection
self.initialize_service_connection()
self.service_connection._mexe
HTTPRequest
self.region.endpoint
conn.make_request('myCmd',
self.assertEqual(args[b'AWSAccessKeyId'],
[b'access_key'])
"secure"}')
conn.get_http_connection(conn.host,
conn.put_http_connection(conn.host,
conn.is_secure,
'https://%s/status'
conn.get_status('getStatus',
'status')
'amazon.com',
self.assertEqual(bse.status,
'400')
self.assertEqual(bse.reason,
BotoServerError('403',
'Forbidden',
MockServiceWithConfigTestCase
'access_key'
'secret_key'
'2012-06-01',
query_string
self.assertEqual(query_string,
credential_scope.split('/')[1]
self.assertEqual(region_name,
auth.headers_to_sign(request)
params={
'max-keys':
S3HmacAuthV4Handler(
config=mock.Mock(),
self.auth.canonical_uri(request)
self.auth.mangle_path_and_params(request)
FakeEC2Connection(region=self.standard_region)
['hmac-v4'])
STSAnonHandler('sts.amazonaws.com',
Mock(),
self.actual_request.path,
PHP
u'ResponseMetadata':
json.dumps({
'ApplicationName':
'mykey',
u'1'}}}).encode('utf-8')
self.service_connection.create_stack(
SAMPLE_TEMPLATE,
'TemplateURL':
'http://url',
reason='Bad
'myKeyName')])
self.set_http_response(status_code=400)
next_token='next_token')
'CREATE_IN_PROGRESS')
datetime(2011,
datetime.datetime(2013,
timestamp_1
timestamp_1,
27,
timestamp_2
timestamp_2,
sample_xml_upper
SAMPLE_XML.replace(b'false',
xml.sax.parseString(sample_xml_upper,
["static.example.com"])
header={"Etag":
DistributionConfig))
invals
ir
pk_file
pk_file.write(self.pk_str)
len(date_less_than.keys()))
date_less_than["AWS:EpochTime"]
self.assertEqual(expires,
ip_range
ip_address=ip_range)
len(ip_address.keys()))
source_ip
ip_address["AWS:SourceIp"]
source_ip)
'2011-02-01',
MagicMock
document)
self.assertEqual(arg['version'],
document.delete("5",
"10")
'5')
test_fake_failure(self):
'12341',
'cat':
list(map(lambda
FakeResponse(object):
DEMO_DOMAIN_DATA
'2013-01-01',
self.assertEqual(fields,
document.delete("5")
{'sort':
[b'{"sort":
}'''
self.assert_request_parameters({})
b.to_dict(),
{'AttributesToGet':
['foo'],
'N')))
s4)
['1',
{'L':
'foo'},
self.assertEqual(dynamizer.decode({'N':
@unittest.skipUnless(six.PY3,
include_index
includes=[
self.assertEqual(include_index.schema(),
'IncludeKeys',
'INCLUDE',
'NonKeyAttributes':
connection=FakeDynamoDBConnection())
self.assertEqual(self.johndoe['last_name'],
'alice'])
1366056668)
RangeKey('date_joined'),
self.assertEqual(self.johndoe.build_expects(),
'Johann'
'last_name',
'_put_item',
'J'},
'new_attr':
mock_update_item:
username='johndoe',
ResultSet(max_page_size=10)
limit=10)
limit=5)
#0',
#1',
#2',
#3',
#4',
self.results._last_key_seen
#5',
#6',
self.assertEqual(self.results._limit,
#5')
self.results.next)
results.to_call(fake_results,
simulate_unprocessed
provisioned_throughput={
"AttributeDefinitions":
"S"
"LocalSecondaryIndexes":
"UsernameIndex",
"Thread",
"ACTIVE"
global_secondary_index_updates=[
self.users,
date_joined=23456))
batch.delete_item(username='johndoe')
12342888
self.assertEqual(mock_batch.call_count,
self.assertRaises(exceptions.UnknownFilterTypeError,
self.users._build_filters,
'1366057777'},
'johndoe']},
'1366056680'},
'Smith'},
'1366056888'},
scan_index_forward=False,
conditional_operator=None
'jane'])
items_2
return_value=items_2)
res_3
self.assertEqual(res_3['username'],
'zoeydoe',
'Count':
'batch_get_item',
self.users._batch_get(keys=[
self.assertEqual(results['unprocessed_keys'],
self.address
Address()
self.address.connection
self.address.public_ip
"192.168.1.1"
self.address.endElement(name,
self.assertEqual(getattr(self.address,
[("publicIp",
"192.168.1.1",
"public_ip"),
("instanceId",
"instance_id"),
("domain",
domain",
"domain"),
("allocationId",
"allocation_id"),
("associationId",
"association_id"),
test_release_calls_connection_release_address_with_correct_args(self):
self.address.release()
self.address.connection.release_address.assert_called_with(
test_associate_calls_connection_associate_address_with_correct_args(self):
self.address.connection.associate_address.assert_called_with(
test_disassociate_calls_connection_disassociate_address_with_correct_args(self):
self.address.disassociate()
self.address.connection.disassociate_address.assert_called_with(
'something
else',
dev_sdf
encrypted=True)
bdm
image_id='123456',
instance_type='m1.large',
'BlockDeviceMapping.1.Ebs.SnapshotId':
'123456',
'group1',
self.assertEqual(instance.id,
'USD')
'id2'],
'id1',
'id2',
test_serialized_api_args(self):
'vpc-id',
self.assertEqual(len(result),
self.assertEqual(result[0].id,
self.assertEquals(1,
'deleteOnTermination',
attachment_id='bar')
'SecurityGroupId.1':
'sg-1',
'SecurityGroupId.2':
boto.ec2.connect_to_region(
datetime(2013,
'ImageLocation':
's3://foo',
'DescribeInstances'},
test_next_token(self):
self.service_connection._required_auth_capability(),
conn._required_auth_capability(),
public_ip='192.0.2.1')
'vol-1a2b3c4d')
dry_run=True,
'key2',
'Tag.2.Value':
{"key2":
taggedEC2Object.remove_tags({"key1":
EC2Connection(aws_access_key_id='aws_access_key_id',
ec2.make_request
{'arn':
INSTANCE_STATUS_RESPONSE
self.assertDictEqual(params,
'description2',
'10.0.1.54',
'subnet_id2',
'group_id1',
'group_id2',
'10.0.1.10',
'10.0.1.11',
self.assertRaises(BotoClientError):
self.service_connection.get_spot_price_history(
'DescribeSpotPriceHistory',
self.assertEqual(response[0].instance_type,
'us-west-2b')
launch_config='lauch_config',
termination_policies=['OldestInstance',
ignore_params_values=['MaxSize',
'MinSize',
'LaunchConfigurationName',
'MaxRecords':
connection=self.service_connection,
ScalingPolicy(
as_name='bar',
self.service_connection.create_scaling_policy(policy)
'PutScalingPolicy',
test_autoscaling_group_put_notification_configuration(self):
'arn:aws:sns:us-east-1:19890506:AutoScaling-Up',
'auto-scaling-group',
'Tags.member.2.Value':
attr),
self.service_connection.detach_instances(
'DetachInstances',
'ShouldDecrementDesiredCapacity':
LoadBalancer
self.assertTrue(isinstance(attributes,
LbAttributes))
ATTRIBUTE_GET_FALSE_CZL_RESPONSE),
(lb.is_cross_zone_load_balancing,
DESCRIBE_RESPONSE
self.assertEqual(len(items),
'TERMINATED')
'ListInstances',
'COMPLETED',
'2.4.2')
self.assertEqual(response.name,
'FirstKey':
'SecondKey':
'FirstKey',
'SecondKey',
self.service_connection.add_tags('j-123',
'm1.large')
'DescribeJobFlows',
'j-aaaaaa',
self.assertEquals(expected,
self.stat_mock.return_value.st_size
'vault_name')
8)
self.filename,
self.api.get_job_output.return_value
mock.patch('boto.glacier.job.tree_hash_from_str')
t.return_value
self.job.get_output(byte_range=(1,
self.assertEqual(self.job._calc_num_chunks(self.job.DefaultPartSize),
header=self.json_header,
self.assertDictEqual(content,
self.set_http_response(status_code=204)
fake_data
'linear_hash',
fake_data)
Mock,
FIXTURE_VAULT
"2012-03-20T17:03:43.221Z",
'vuXO7SHTw-luynJ0Zu31AYjR3TcCn7X25r7ykpuulxY2lv8',
'VaultList':
[{'SizeInBytes':
'2013-05-17T02:38:39.049Z'},
{'SizeInBytes':
'2013-05-17T02:31:18.659Z'}]}
"ArchiveDescription":
"Marker":
"MultipartUploadId":
"PartSizeInBytes":
"Parts":
GlacierLayer2Base.setUp(self)
dt):
0o1,
4),
self.assertEqual(minimum_part_size(8
self.assertEqual(len(chunks),
self.assertEqual(chunks[0],
1024).digest())
'filename',
self.vault.create_archive_writer
self.vault.create_archive_from_file('myfile')
self.vault.create_archive_writer.assert_called_with(
description=mock.ANY,
data_tree_hashes,
self.writer.current_tree_hash
self.writer.current_uploaded_size
'arn'
self.service_connection.get_signin_url()
policy_doc,
['policy']
'v1'},
self.assertEqual(len(response['list_entities_for_policy_response']
self.assertTrue('PutRecord'
ml_endpoint)
'QualificationRequirement.1.LocaleValue.2.Country':
comparator='NotIn',
'NotIn',
Requirement
"333333333333333333333333333333"
Requirement(
{'Prefix.1':
'text/xml',
self.assertSequenceEqual(obj._result.Item,
INSTANCE_CONFIG
u'AccessKeyId':
u'Expiration':
u'Token':
'access_key',
self.environ['AWS_SECURITY_TOKEN']
'env_security_token'
provider.NO_CREDENTIALS_PROVIDED)
'cfg_secret_key')
self.assertRaises(InvalidInstanceMetadataError):
instance_config
self.config.get('Credentials',
self.assertEqual(db.pending_modified_values,
'general-public-license')
param_group='default.mysql5.1',
db_subnet_group_name='dbSubnetgroup01',
'default.mysql5.1')
3306,
self.assertEqual(options.name,
self.assertEqual(options.description,
'Test
self.assertEqual(options.engine_name,
'oracle-se1')
self.assertEqual(options.major_engine_version,
'11.2')
'db1',
'foo.log',
'us-east-1a',
self.service_connection.get_all_hosted_zones()
orig_retry
self.service_connection._retry_handler
self.service_connection.create_hosted_zone("example.com.",
"example.com.")
self.assertEqual(r['CreateHostedZoneResponse']['HostedZone']
self.assertEqual(response['GetHostedZoneResponse']
self.assertEqual(bucket.name,
self.assertFalse(mock_head_bucket.called)
@patch.object(Bucket,
self.service_connection.get_bucket('mybucket')
Key),
Prefix)
encoding_type='url'
self.assertRaises(S3ResponseError)
cm.exception
self.assertEqual(err.error_code,
self.assertEqual(err.message,
'<AllowedHeader>*</AllowedHeader>'
'<MaxAgeSeconds>3000</MaxAgeSeconds>'
'</CORSConfiguration>')
self.assertEqual(cfg.to_xml(),
b.get_key('myglacierkey')
b.get_key('fookey')
k.bucket
'STANDARD')
weird_timeout_body
body=weird_timeout_body)
pretend
chunk-able.')
k.should_retry
counter(k.should_retry)
self.assertEqual(k.should_retry.count,
self.assertTrue(k.should_retry.count,
self.assertEqual(self.keyfile.read(5),
self.contents[:4])
'2012-12-31T00:00:000Z')
r.to_xml()
Tag('foo',
self.assertEqual('gs',
self.assertEqual('file',
routing_rules=rules)
self.assertEqual(x(expected_xml),
x(xml))
'topic',
subject='subject')
'TargetArn':
'target_arn',
'Attributes.entry.2.key':
'Attributes.entry.2.value':
'name1',
'name2',
self.actual_request.headers['Authorization'])
url='http://sqs.us-east-1.amazonaws.com/123456789012/testQueue/',
message_class=RawMessage)
'arn:role',
self.assertEqual(response.credentials.access_key,
'accesskey')
self.assertEqual(response.credentials.secret_key,
'secretkey')
self.assertEqual(response.credentials.session_token,
'session_token')
self.assertEqual(response.user.arn,
'arn:role')
self.assertEqual(response.user.assume_role_id,
'roleid:myrolesession')
'eventTimestamp':
'test_workflow_name',
self.assertEquals(self.domain.region,
{'cancelRequested':
version='1')
self.assertEquals(password.str,
'http://169.254.169.254',
'latest',
'meta-data/'
self.opener.return_value.open.return_value
retry_url('http://10.10.10.10/foo',
filters=OrderedDict([('state',
['pending',
'cgw-b4dc3961')
'dopt-7a8b9c2d',
'igw-eaad4883'},
self.create_response(status_code=200,
'10.0.0.0/22')
'local')
'CreateRoute')
'us-east-1a')
<state>available</state>
instance_id='my_instance_id',
'my_instance_id',
'777788889999')
self.assertEqual(vpc_peering_connection.expiration_time,
'123456789012')
<status>UP</status>
'vpn-83ad48ea')
glob.glob(folder
(15,
line.strip()
'#':
'.'.join(map(str,
exclude_trees
'sphinx'
%d,
docutils.parsers.rst.roles
set_classes
docutils
entry_type,
node['entries'][0][:3]
setting')
app.builder.env
setting_name,
fromdocname):
refnode
"setting",
"signal",
"command",
"reqmeta",
app.add_role('issue',
_filename
match.group(2)
print("Not
_filename:
_file:
Site,
Resource.__init__(self)
self._reset_stats()
qps
download_delay
(x
category=DeprecationWarning,
_monkeypatches
'__module__',
CrawlerProcess
get_project_settings
inspect.isclass(obj)
issubclass(obj,
obj.__module__
inproject
inproject:
_print_commands(settings,
<command>
print("Available
(cmdname,
_print_unknown_command(settings,
"scrapy"
str(e):
sys.exit(2)
argv
conf.settings
get_project_settings()
opts,
_run_command_profiled(cmd,
writing
globals(),
scrapy.core.engine
ExecutionEngine
scrapy.resolver
signal_names
LogCounterHandler,
spidercls,
logging.root.addHandler(handler)
self.extensions
self.settings.freeze()
spiders(self):
spidercls):
self).__init__(settings)
_):
{'signame':
signame})
d.addBoth(self._stop_reactor)
reactor.addSystemEventTrigger('before',
'shutdown',
RuntimeError:
''Please
scrapy.dupefilters
request_seen(self,
self.logdupes
open(os.path.join(path,
self.file)
self.file:
request_fingerprint(self,
%(request)s"
self.logger.debug(msg,
reason='cancelled'):
self.print_help
scrapy.utils.serialize
ScrapyJSONEncoder
to_native_str,
is_listlike
_configure(self,
dont_fail=False):
self.export_empty_fields
ScrapyJSONEncoder(**kwargs)
self.encoder.encode(itemdict)
self.file.write(to_bytes(data,
self.encoding))
self.first_item
'item')
encoding=self.encoding)
self._get_serialized_fields(item,
isinstance(serialized_value,
_xg_characters(self,
self.xg.characters(serialized_value)
self.stream
self._headers_not_written
self.protocol)
'\n'))
self.binary:
bytes)):
MutableMapping
bases
_class
super(ItemMeta,
mcs).__new__(mcs,
getattr(_class,
avoid
self[k]
self.fields:
AttributeError("Use
item[%r]
self).__setattr__(name,
to_bytes(url,
self.fragment
self.nofollow
__hash__(self):
`scrapy.linkextractors`
ERROR
CRITICAL
through
kw.pop('level',
logger.log(level,
*[kw]
logging.ERROR)
"%(item)s"
logging.DEBUG,
'exception':
MIMENonMultipart
Encoders
encoders
self.mailfrom
send(self,
msg['From']
msg['To']
msg['Date']
msg['Subject']
charset
OK:
Attachs=%(mailattachs)d',
msg.as_string())
nattachs):
to_addrs,
self.smtpport,
process_parallel,
process_chain,
middlewares
crawler=None):
hasattr(mwcls,
crawler})
methodname,
dnscache
super(CachingThreadedResolver,
binary_is_text,
self.classes[mimetype]
content_encoding:
self.from_mimetype(mimetype)
encoding='latin-1',
self.from_filename(filename)
chunk.lower():
DEFAULT_PYTHON_SHELLS
Spider,
Settings)
self.item_class
request=None,
signal.signal(signal.SIGINT,
self.populate_vars(response,
self.code:
'shell'
env:
option):
DEFAULT_PYTHON_SHELLS.keys()
IgnoreRequest:
self.inthread:
Shell
help
request_callback
request_errback
request.errback)
spider_opened
request_scheduled
item_scraped
`scrapy.spiders`
spcls
iter_spider_classes(module):
load(self,
scrapy.squeues
super(SerializableQueue,
_serializable_queue(queue.FifoDiskQueue,
_pickle_serialize,
pickle.loads)
PickleLifoDiskQueue
_serializable_queue(queue.LifoDiskQueue,
marshal.dumps,
marshal.loads)
instead")
set_stats(self,
inc_value(self,
max_value(self,
value),
min_value(self,
logger.info("Dumping
_persist_stats(self,
set"
self._crawler
"Global
help="log
file.
help="write
FILE")
-s
'INFO',
'LOGSTATS_INTERVAL':
'-u',
self.proc
env=get_testenv())
self.proc.stdout.readline()
self.proc.kill()
self.proc.wait()
'follow'
'show':
self.link_extractor.extract_links(response):
callback=self.parse)
TextTestResult
scrapy.contracts
ContractsManager
failed:
<spider>"
parser.add_option("-l",
"--list",
dest="list",
parser.add_option("-v",
"--verbose",
tested_methods:
spider"
parser.add_option("-o",
"--output",
help="dump
FILE
(use
stdout)")
"--output-format",
metavar="FORMAT",
help="format
dumping
-o")
opts.output:
opts.output
'stdout:',
opts.output,
feed_exporters
without_none_values(
valid_output_formats
feed_exporters.keys()
opts.output_format:
os.path.splitext(opts.output)[1].replace(".",
valid_output_formats:
UsageError("Unrecognized
one"
'-t'
switch
extension"
(opts.output_format,
tuple(valid_output_formats)))
self.settings.set('FEED_FORMAT',
opts.output_format,
spname
"Edit
editor
sfile
is_url
spidercls_for_request,
<url>"
headers.items():
_print_response(self,
sys.stdout.buffer
is_url(args[0]):
self.crawler_process.crawl(spidercls,
render_templatefile,
string_camelcase
parser.add_option("-d",
output")
overwrite
template_file:
module:
project")
spiders_dir
genspider
templates_dir(self):
_templates_base_dir
self.settings['TEMPLATES_DIR']
join(scrapy.__path__[0],
join(_templates_base_dir,
spiders"
print(s)
iterate_spider_output,
looking
one")
help="don't
pygments
colorize
callback")
parser.add_option("-c",
levels
list(self.requests.keys())
levels:
lvl,
old_items
self.items.get(lvl,
old_reqs
self.requests.get(lvl,
lvl=None,
colour=True):
lvl
print("#
colorize=colour)
print('\n>>>
DEPTH
<<<'
opts.noitems:
colour)
opts.nolinks:
rule.callback
logger.error('No
spider.name})
%(spider)s',
%(url)s',
url})
_start_requests
self.first_response:
self.first_response
'parse'
callable(cb):
cb_method
callback,
req.callback
fext
dirname:
ValueError)
"Interactive
scraping
move,
IGNORE
project_name):
Module
exists'
names)
dstname)
project_name
scrapy.cfg
join(project_dir,
".".join(map(str,
openssl
method):
results.addError(case,
failure.value,
'@%s
wrapper(response):
list(iterate_spider_output(cb(response)))
adjust_request_args(self,
"[%s]
(spider,
'url'
dict),
'items':
len(self.args)
3]
self.max_bound
post_process(self,
output):
assertion
self.max_bound)
expected))
scrapy.extensions.feedexport
scrapy.extensions.spiderstate
scrapy.extensions.throttle
scrapy.downloadermiddlewares.ajaxcrawl
scrapy.downloadermiddlewares.cookies
`scrapy.downloadermiddlewares.decompression`
scrapy.downloadermiddlewares.defaultheaders
scrapy.downloadermiddlewares.downloadtimeout
scrapy.downloadermiddlewares.httpauth
scrapy.downloadermiddlewares.httpcache
scrapy.downloadermiddlewares.httpcompression
scrapy.downloadermiddlewares.httpproxy
scrapy.downloadermiddlewares.redirect
scrapy.downloadermiddlewares.retry
scrapy.downloadermiddlewares.robotstxt
scrapy.downloadermiddlewares.stats
scrapy.downloadermiddlewares.useragent
scrapy.linkextractors.htmlparser
scrapy.linkextractors.lxmlhtml
scrapy.linkextractors.regex
scrapy.linkextractors.sgml
scrapy.loader.processors
scrapy.pipelines.images
scrapy.spidermiddlewares.depth
scrapy.spidermiddlewares.httperror
scrapy.spidermiddlewares.offsite
scrapy.spidermiddlewares.referer
scrapy.spidermiddlewares.urllength
scrapy.spiders.crawl
scrapy.spiders.feed
scrapy.spiders.init
scrapy.spiders.sitemap
xmliter_lxml
DecompressionMiddleware
scrapy.utils.reactor
logformatter_adapter,
close_if_idle,
nextcall,
nextcall
scheduler
self._maybe_fire_closing()
crawler.logformatter
start(self):
self.running,
"Engine
running"
self._closewait
self._close_all_spiders()
self.running:
self._needs_backout(spider):
self._download(request,
output',
schedule(self,
slot.add_request(request)
slot.nextcall.schedule()
dwld
free
spider.name
logger.info("Spider
opened",
Failure)
res):
(%(reason)s)",
slot.close()
reason=reason))
unassigning
scrapy.utils.reqser
request_from_dict
stats=None,
self.dqdir
pqclass
dqclass
mqclass
self.logunser
logunser
self.dqs:
request.dont_filter
len(self.mqs)
-request.priority)
self.spider})
json.load(f)
jobdir:
dqdir
iter_errback
self.max_active_size
deque()
deferred))
max(len(response.body),
self.MIN_RESPONSE_SIZE)
self.MIN_RESPONSE_SIZE
self.active.add(request)
self.active.remove(request)
is_idle(self):
needs_backout(self):
self._check_if_closing(spider,
self._scrape_next(spider,
Failure))
defer_result(result)
isinstance(exc,
spider=spider
self.slot.itemproc_size
spider_failure
%(item)s',
item},
self)._add_middleware(mw)
'raise
exception=exception,
_isiterable(result),
result=result,
DownloaderMiddlewareManager
DownloadHandlers
concurrency,
self.concurrency
concurrency
self.delay
self.randomize_delay
self.latercall
self.concurrency,
self.randomize_delay)
spider.download_delay
self.total_concurrency
self.domain_concurrency
_deactivate(response):
_get_slot(self,
conc
conc,
self.slots[key]
request.meta['download_slot']
self._get_slot(request,
slot.latercall
delay:
penalty
ClientContextFactory
twisted.web.iweb
scrapy.core.downloader.tls
@implementer(IPolicyForHTTPS)
'method',
creatorForNetloc(self,
ClientContextFactory.getContext(self)
response.__class__.__name__)
type(response))
'TLS'
openssl_methods
getattr(SSL,
1.1
SSL_CB_HANDSHAKE_START
self._hostnameASCII,
'Ignoring
"{}"
urlunparse,
urldefrag
urlunparse(('',
parsed.path
parsed.params,
parsed.query,
b'https'
url.strip()
_parsed_url_args(parsed)
lineReceived(self,
line):
line.rstrip())
reason)
seconds."
timeout=180):
urldefrag(request.url)[0]
self.response_headers
request.meta.get('download_timeout')
b'POST':
_build_response(self,
request.meta['download_latency']
request.meta.get('proxy')
proxy:
_parse(proxy)
stores
schemes
self._schemes[scheme]
scheme):
str(ex)
file_uri_to_path
respcls(url=request.url,
FTPClient,
Protocol,
self.__filename
"default":
parsed_url
client,
protocol.filename
load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
reactor.connectSSL(host,
reactor.connectTCP(host,
PotentialDataLoss
ResponseDone,
TCP4ClientEndpoint
HTTPConnectionPool(reactor,
super(TunnelingTCP4ClientEndpoint,
self).__init__(reactor,
self._tunnelReadyDeferred
self._protocolDataReceived
protocol.dataReceived
respm
sslOptions
CONNECT
b':'
HTTP/1.1\r\n'
connectTimeout=None,
super(TunnelingAgent,
self._proxyConf
TunnelingTCP4ClientEndpoint(
self._reactor,
self._proxyConf,
self._contextFactory,
_requestWithEndpoint(self,
requestPath):
Agent
ProxyAgent
connectTimeout
proxyParams
contextFactory=self._contextFactory,
connectTimeout=timeout,
bindAddress=bindaddress,
pool=self._pool)
timeout=timeout,
to_bytes(request.method)
bodyproducer
b'0')
(url,
b'',
"size
txresponse._transport._producer.loseConnection()
maxsize,
txresponse
@implementer(IBodyProducer)
startProducing(self,
finished,
self._reached_warnsize
reason.check(ResponseDone):
self._finished.callback((self._txresponse,
reason.check(PotentialDataLoss):
kw.get('anon')
botocore.auth
kw)
p.path
p.query
method=request.method,
data=request.body)
bucket=bucket,
ajax_crawl_request
'noscript'))
scrapy.utils.http
decode_chunked_transfer
scrapy.http.cookies
CookieJar
request.meta.get('dont_merge_cookies',
cookiejarkey
request.meta.get("cookiejar")
self.jars[cookiejarkey]
[to_native_str(c,
cl:
{}\n".format(c)
cl)
cookies)
logger.debug(msg,
from:
cookie):
';
jar,
cookie_list
[{'name':
Response(request.url,
responsetypes.from_args(body=body)
response.body:
new_response
format:
self._timeout)
basic_auth_header
pwd
self.auth
(defer.TimeoutError,
ResponseFailed,
self.storage
request.meta.get('dont_cache',
request.meta.pop('cached_response',
self._cache_response(spider,
cachedresponse):
url=response.url)
kwargs['encoding']
response.headers['Content-Encoding']
_parse_proxy
auth_encoding
self.proxies:
proxy_url
creds,
'https')
enabled_setting
redirected,
redirections
body='')
'handle_httpstatus_list',
307)
location)
self._redirect_request_using_get(request,
scrapy.core.downloader.handlers.http11
request.meta.get('dont_retry',
self._retry(request,
(failed
%(retries)d
times):
%(reason)s",
'retries':
retries,
retryreq
six.moves.urllib
robotparser
netloc)
Deferred):
failure.value},
netloc):
hasattr(response,
response.text
rp_dfd
request_httprepr
"%s.%s"
self.user_agent
self.user_agent)
'itemcount':
signal=signals.response_received)
signal=signals.item_scraped)
self.counter['errorcount']
'closespider_errorcount')
self.counter['pagecount']
'closespider_pagecount')
self.task
self.counter['itemcount']
'closespider_itemcount')
crawler.signals.connect(o.item_scraped,
datetime.datetime.utcnow(),
signal.signal(signal.SIGUSR2,
self.dump_stacktrace)
frame):
id_,
"#
ftplib
FTP
_stdout
self._stdout
os.path.exists(dirname):
os.makedirs(dirname)
u.hostname
u.username
u.password
self.is_botocore
self.s3_client
session.create_client(
self.username
self.password
u.path
ftp
self.port)
exporter,
self.urifmt
uripar
signals.spider_closed)
signals.item_scraped)
self._get_storage(uri)
slot.itemcount
in:
{'format':
NotConfigured:
logger.error("Unknown
urlparse(uri).scheme
%(scheme)s",
scheme})
params[k]
k)
headers_dict_to_raw
settings.getlist('HTTPCACHE_IGNORE_SCHEMES')
self.ignore_http_codes
should_cache_request(self,
should_cache_response(self,
is_cached_response_fresh(self,
is_cached_response_valid(self,
self._cc_parsed[r]
self._parse_cachecontrol(request)
b'no-store'
self._parse_cachecontrol(response)
304:
b'Expires'
308):
203,
b'Last-Modified'
b'ETag'
self._parse_cachecontrol(cachedresponse)
ccreq
b'no-cache'
now)
reqmaxage
b'must-revalidate'
staleage
cachedresponse.headers:
now):
rfc1123_to_epoch(response.headers.get(b'Date'))
data_path(settings['HTTPCACHE_DIR'],
createdir=True)
dbpath
spider.name)
'c')
self._read_data(spider,
data['url']
data['status']
Headers(data['headers'])
data['body']
dict(response.headers),
key]
pickle.dumps(data,
_read_data(self,
tkey
db:
float(ts):
_request_key(self,
self.use_gzip
'response_body'),
'response_headers'),
spider.name,
pickle.load(f)
leveldb
batch.Put(key
self.db.Get(key
directives
cls(crawler.stats,
self.itemsprev
pages,
(at
live_refs
len(wdict),
get_engine_status
self.warned
self.mail
MailSender.from_settings(crawler.settings)
'darwin':
self.get_virtual_size())
self.warning:
self.get_virtual_size()
mem
exceeded
Shutting
{'memusage':
mem},
self.notify_mails:
subj
%dM
(self.crawler.settings['BOT_NAME'],
mem,
socket.gethostname())
self._send_report(self.notify_mails,
subj)
open_spiders:
once
reached
subject):
"\r\n"
self.jobdir
open(self.statefn,
spider.state
recipients,
recipients
stats\n\n"
"\n".join("%-50s
"Scrapy
telnet
TWISTED_CONCH_AVAILABLE
hpy
'slot':
self.target_concurrency
_response_downloaded(self,
olddelay
latency,
ms
XmlRpcRequest
scrapy.http.response
c)
AttributeError(msg)
self.check_expired_frequency
wrsp
WrappedResponse(response)
req_host:
hosts:
get_full_url(self):
get_host(self):
get_type(self):
is_unverifiable(self):
has_header(self,
get_header(self,
add_unredirected_header(self,
info(self):
hasattr(value,
x):
setlist(self,
list_):
list_
setlistdefault(self,
default_list=()):
default_list)
appendlist(self,
iteritems(self):
__copy__
scrapy.http.common
obsolete_setter
errback=None):
self._set_url(url)
self._set_body(body)
isinstance(priority,
errback,
self.callback
Headers(headers
dont_filter
meta(self):
_get_url(self):
unicode,
%s:'
property(_get_url,
obsolete_setter(_set_url,
'url'))
_get_body(self):
property(_get_body,
obsolete_setter(_set_body,
'body'))
encoding(self):
self.replace()
'headers',
'encoding',
kwargs.setdefault(x,
kwargs.pop('cls',
self.__class__)
_get_form(response,
formname,
formid,
formnumber,
_get_inputs(form,
formdata,
dont_click,
clickdata,
_get_form_url(form,
method=method,
urljoin(form.base_url,
vs
ValueError("No
<form>
nodes:
el.tag
ValueError('No
response))
not(re:test(.,
"http://exslt.org/regular-expressions"})
[(k,
u''
_get_clickable(clickdata,
clickable[0]
_select_value(ele,
multiple:
(el.get('name'),
el.get('value')
nr
len(el)
matching
clickdata:
(el,
xmlrpc_client
xmlrpclib
get_func_args
DUMPS_ARGS
kwargs.get('encoding',
request=None):
(type(self).__name__,
bytes.
"If
"or
'request',
urljoin(self,
memoizemethod_noargs,
'ascii'
to_native_str(url,
b''))
enc
canonicalize_url,
IGNORED_EXTENSIONS
'doc',
link_extractor,
self.link_extractor
_re_type)
deny_extensions
self.deny_extensions
_matches(link.url,
url_is_from_any_domain(parsed_url,
self.allow_domains):
self.deny_domains):
[regex.search(url)
denied
process=None,
self.process_attr
callable(process)
self.reset()
self.feed(response_text)
self.base_url)
response_url
isinstance(link.url,
link.url.encode(response_encoding)
safe_url_string(link.url,
link.text
ret.append(link)
self._extract_links(response.body,
'base':
dict(attrs).get('href')
self.scan_attr(attr):
self.links.append(link)
handle_data(self,
self.current_link:
rel_has_nofollow
unique_list,
FilteringLinkExtractor
attr_val
skipping
u'',
self._deduplicate_if_needed(links)
unique_list(links,
allow=(),
deny=(),
allow_domains=(),
deny_domains=(),
restrict_xpaths=(),
tags=('a',
'area'),
attrs=('href',),
canonicalize=True,
process_value=None,
deny_extensions=None,
restrict_css=()):
set(arg_to_iter(tags)),
set(arg_to_iter(attrs))
tag_func
attr_func
attr=attr_func,
unique=unique,
self).__init__(lx,
allow=allow,
deny=deny,
allow_domains=allow_domains,
deny_domains=deny_domains,
restrict_xpaths=restrict_xpaths,
restrict_css=restrict_css,
canonicalize=canonicalize,
deny_extensions=deny_extensions)
self.restrict_xpaths:
docs
SgmlLinkExtractor
re.compile(
base_url=None):
"Please
process_value
self._process_links(links)
.common
wrap_loader_context
default_input_processor
default_output_processor
self.selector
response=response)
self._local_item
self._local_values
context.update(selector=selector)
item=self.item,
**context
self.get_value(value,
field_name:
self._add_value(field_name,
self._values[field_name]
proc(value)
ValueError("Error
proc:
self._get_item_field_attr(field_name,
Item):
self.__class__.__name__)
self.add_value(field_name,
self.replace_value(field_name,
self.get_value(values,
xpaths,
self._check_selector_method()
*functions,
**default_loader_context):
self.functions
functions
default_loader_context
MergeDict(loader_context,
self.default_loader_context)
wrapped_funcs
[wrap_loader_context(f,
context)
self.functions]
wrapped_funcs:
'):
process_item(self,
md5sum
persist_file(self,
absolute_path
self._get_filesystem_path(path)
open(absolute_path,
stat_file(self,
path.split('/')
seen:
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
modified_tuple
self.AWS_SECRET_ACCESS_KEY,
'%s%s'
(self.prefix,
Bucket=self.bucket,
self._get_boto_bucket()
**extra)
MEDIA_NAME
DEFAULT_FILES_URLS_FIELD
DEFAULT_FILES_RESULT_FIELD
store_uri,
download_func=None,
resolve
functools.partial(self._key_for_pipe,
self.expires
self.EXPIRES
self.FILES_URLS_FIELD
self.FILES_RESULT_FIELD
self.files_result_field
s3store
cls.STORE_SCHEMES['s3']
s3store.AWS_ACCESS_KEY_ID
s3store.AWS_SECRET_ACCESS_KEY
s3store.POLICY
store_uri
cls(store_uri,
settings=settings)
win32
age_seconds
age_days
Downloaded
%(medianame)s
{'medianame':
self.MEDIA_NAME,
self.inc_stats(info.spider,
info.spider})
(unknown-error):
'%(request)s
referer,
FileException
'in
[Request(x)
file_downloaded(self,
md5sum(buf)
item.fields:
ok]
file_path(self,
hasattr(self.file_key,
self.file_key(url)
media_guid
media_ext
os.path.splitext(url)[1]
(media_guid,
media_ext)
self.file_path(url)
file_key._base
PIL
MIN_WIDTH
MIN_HEIGHT
THUMBS
DEFAULT_IMAGES_URLS_FIELD
DEFAULT_IMAGES_RESULT_FIELD
self.IMAGES_RESULT_FIELD
self.IMAGES_URLS_FIELD
self.images_result_field
self.min_width
width
height,
thumb_path
thumb_image,
thumb_buf
image.mode
255,
image_guid
(image_guid)
thumb_id)
thumb_guid
(thumb_id,
thumb_guid)
image_key(self,
thumb_key(self,
thumb_id):
mustbe_deferred,
download_func
self.spiderinfo
consumeErrors=1)
info.spider)
dfd.addCallbacks(
callback=self.media_downloaded,
info),
errback=self.media_failed,
errbackArgs=(request,
scrapy.selector.lxmlsel
XPathExpr,
GenericTranslator,
ScrapyXPathExpr
Selector,
SelectorList
'XmlXPathSelector',
'XPathSelector',
'html',
XmlXPathSelector
XPathSelector,
clsdict={
HtmlXPathSelector
_st(response,
st):
'xml'
_response_from_text(text,
'utf-8'))
object_ref):
@deprecated(use_instead='.extract()')
extract_unquoted(self):
self.xpath(xpath)
isinstance(self.value,
"<SettingsAttribute
values=None,
self.frozen
default))
getfloat(self,
default=0.0):
self.get(name,
value.split(',')
list(value)
get_settings_priority(priority)
SettingsAttribute(value,
import_module(module)
six.iteritems(values):
immutable
object")
freeze(self):
self.copy()
{k:
won't
0.26,
"`Settings.set(name,
self._overrides
_DictProxy(self,
'cmdline')
self._defaults
k):
settings_module
opt_name)
CrawlerSettings,
iter_default_settings():
getattr(default_settings,
5.0
DOWNLOAD_DELAY
'http':
'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
'ftp':
700,
900,
'vi'
'scrapy.extensions.feedexport.FileFeedStorage',
'scrapy.exporters.JsonLinesItemExporter',
'scrapy.extensions.httpcache.FilesystemCacheStorage'
'scrapy.extensions.httpcache.DummyPolicy'
%(message)s'
SCHEDULER_DEBUG
METAREFRESH_MAXDELAY
uses
502,
STATS_CLASS
DEPRECATED_SETTINGS
now)'),
('AUTOTHROTTLE_MAX_CONCURRENCY',
"\n
self.maxdepth
maxdepth
self.prio
_filter(request):
response.meta['depth']
"Ignoring
request.url},
self.verbose_stats:
depth,
response.meta:
(r
_filter(r))
allowed_statuses
HttpError):
{'response':
response},
self.host_regex
'allowed_domains',
ValueError("%s
logger(self):
self})
level=logging.DEBUG,
from_crawler
self.start_urls:
self.make_requests_from_url(url)
handles_request(cls,
id(self))
'"from
works
CrawlSpider,
XMLFeedSpider,
CSVFeedSpider
self.follow
self._parse_response(response,
lnk
Request(url=link.url,
_parse_response(self,
callback:
cb_res
requests_or_item
request_or_item
xmliter,
NotSupported
itertag
'item'
namespaces
adapt_response(self,
parse_node(self,
NotConfigured('You
scrape
feed')
self.adapt_response(response)
Selector(response,
type='xml')
self._register_namespaces(selector)
selector.xpath('//%s'
self.itertag)
When
python's
module's
row))
scrapy.utils.sitemap
Sitemap,
sitemap_urls_from_robots
['']
callback=self._parse_sitemap)
Request(loc,
gunzip(response.body)
isLeaf
[random.randint(1,
range(show)]
request.args.copy()
nlist:
argstr
urlencode(args,
doseq=True)
httpPort
reactor.listenTCP(8998,
httpHost
httpPort.getHost()
NotConfigured('missing
library')
SafeConfigParser
update_classpath
ValueError('Some
{!r}
custom)
os.path.abspath(path)
cfgfile
project):
project)
closest_scrapy_cfg()
projdir
xdg_config_home
InteractiveShellEmbed
load_default_config
banner=banner)
readline
code.interact(banner=banner,
known_shells
dict.__repr__(self))
[value])
memo
exist"
[]:
[]).append(value)
self.itemnames:
super(CaselessDict,
dict.__delitem__(self,
has_key
__contains__
self.normvalue(def_val))
item_list
has_key(self,
super(LocalCache,
use_instead
deco
wrapped(*a,
reactor.callLater(0.1,
defer.Deferred):
failure.Failure):
d.callback(input)
eb,
d.addCallbacks(lambda
oldattr,
inherits
inherit
"instantiate
clsdict_):
clsdict_)
metacls.deprecated_class
__init__(cls,
old_class_path),
new=_clspath(new_class,
new_class_path))
(warning
subclass,
warn_category,
cls).__init__(name,
deprecated_cls
parent_module
detecting
forced
'scrapy.linkextractors.'),
'scrapy.pipelines.'),
'scrapy.spiders.'),
('scrapy.contrib.',
replacement
True))
[(test,
ftp.cwd(path)
GzipFile
read1(gzf,
size=-1):
re.I).search
cenc
h,
_urlparse_cache[request_or_response]
nodename_patt,
re.S)
_body_or_str(obj)
nodetext
Selector(text=nodetext,
namespace=None,
reader
nodename
'//'
namespace:
self._text,
obj.encoding
self._is_unicode
self._ptr,
TextResponse)
delimiter:
_getrow(csv_r)
six.text_type,
expected_types),
os.makedirs(path)
logging.config
filter(self,
'ERROR',
logging.StreamHandler()
scrapy.__version__,
d})
buf):
(level,
isinstance(arg,
dot
hasattr(mod,
keep=['lt',
'amp'])
strings]
m.hexdigest()
ENVVAR
inside_project():
scrapy_module
el_
hasattr(x,
seenkey
(bytes,
six.text_type)):
type(text).__name__)
_chunk_iter():
pattern
cache[self]
method(self,
|=
inspect.getargspec(func)
inspect.ismethod(func):
'__name__',
callable'
type(func))
attributes:
object(),
self._weakdict[key]
os.W_OK)
hasattr(obj,
len(portrange)
portrange:
_find_method(spider,
_get_method(spider,
func_self
include_headers
hashlib.sha1()
hdr
cache[include_headers]
fp.hexdigest()
webbrowser
http
response.text[0:4096]
_baseurl_cache[response]
_metaref_cache[response]
open_in_browser(response,
response.url
fname)
datetime.datetime):
datetime.date):
(type(o).__name__,
o.url)
DeferredList,
disconnect
sender=Anonymous,
named.pop('dont_log',
named.get('spider',
deferreds
receiver},
result))
d[name]
render_path
open(render_path,
SkipTest
Key=path)
key.get_contents_as_string()
(content,
temp
"\n>>>
<<<\n%s"
pp.err
self.out
self.err
super(SiteTest,
test_site(),
util.Redirect(b"/redirected"))
r.putChild(b"redirected",
static.Data(b"Redirected
here",
port.getHost().port)
oldest
d))
[])))
parts.netloc
path_encoding),
_safe_chars)
canonicalize_url(url,
_safe_ParseResult(
parse_url(url),
keyvals
keep_blank_values)
encoding))
keep_blank_values=False,
strict_parsing:
nv
len(nv)
keep_blank_values:
\.
)?
optional
scrapy.xlib.pydispatch
supported')
__metaclass__
IPushProducer
fail,
.iweb
IResponse,
DONE
Exception.__init__(self,
reasons)
reasons,
"Unexpected
self.connHeaders
self.bodyDecoder
line[0]
isConnectionControlHeader(self,
statusReceived(self,
allHeadersReceived(self):
self._everReceivedData
strversion)
self.transport)
self.request.method
'content-length':
'HEAD'):
self.response.length
self._finished(self.clearLineBuffer())
contentLengthHeaders
self.response._bodyDataFinished()
exceptionClass
self.bodyProducer
bodyProducer
required")
self._writeHeaders(transport,
self.bodyProducer.startProducing(encoder)
encoder._allowNoMoreWrites()
transport.unregisterProducer()
ultimate
ultimate.errback(err)
called"
log.err(err,
passthrough
succeed(None)
producer,
self._length
producer
_allowNoMoreWrites(self):
self._producer.stopProducing()
ExcessWrite()
self._length:
deliverBody(self,
protocol.makeConnection(self._transport)
self._bodyProtocol
self._bodyBuffer:
self._bodyProtocol.dataReceived(data)
'FINISHED'
disconnected")
Failure(ResponseDone("Response
received"))
finish
once")
streaming):
resumeProducing(self):
'QUIESCENT'
self._quiescentCallback
quiescentCallback
self._abortDeferreds
state(self):
self.transport.abortConnection()
self._currentRequest
self._transportProxy
'TRANSMITTING':
'WAITING'
self._responseDeferred.chainDeferred(self._finishedRequest)
self._finishedRequest.errback(
splithost,
splittype
twisted.python.components
proxyForInterface
TCP4ClientEndpoint,
int(port)
b'':
contextFactory)
self._port)
getattr(os,
tell
originalPosition
tell()
self._inputFile.close()
ignored:
newConnection
("GET",
self._connections
self._timeouts
connections.pop(0)
connection.state
"QUIESCENT":
self._newConnection(key,
added
cid
'host',
_AgentBase.__init__(self,
parsedURI.port)
self._requestWithEndpoint(key,
cookieHeader
self.original
rawData
self._response)
rawData:
self.original.dataReceived(rawData)
_handleResponse(self,
contentEncodingHeaders
'content-encoding',
contentEncodingHeaders:
deferred.addCallback(
self._handleResponse,
redirectCount):
redirectCount
response.code,
location=uri)
response.code
('GET',
self._handleRedirect(response,
redirectCount)
deferred):
'Response',
self._process.writeToChild(0,
self._interface
interface=self._interface)
_TCPServerEndpoint.__init__(self,
interface='::'):
self._host
bindAddress=self._bindAddress)
sslContextFactory,
self._sslContextFactory
sslContextFactory
address,
wantPID=0):
checkPID=0):
self._used
backlog=50):
factory),
{'interface':
interface,
kw['method']
cf
ssl.CertificateOptions(
@implementer(IPlugin,
IStreamServerEndpointStringParser)
_parseServer(self,
parseStreamServer(self,
_OP,
_tokenize(description):
ops
_STRING,
'\\':
add(sofar)
_parseServer(description,
_parse(description)
kw):
"Use
_serverFromStringLegacy(reactor,
str:
nameOrPlugin
_parseClientTCP(*args,
int(kwargs['timeout'])
caCertsDir
certx509
verify
kwargs['path']
Attribute
mode=0o666):
addressFamily,
args=(),
stopListening():
connectionLost(reason):
stopProducing():
resumeProducing():
loseConnection():
C{pid}
write(packet,
interface=""):
"For
"Backwards-compatibility
instead.
behaves
mostly
L{http_headers.Headers}
implementation
set.")
finish():
comment=None,
"of
response.
Attribute("The
how
os.path.join(_sourceroot,
os.path.join(tests_datadir,
maxlatency
total,
"desc"
show,
b:
n):
request.startedWriting
(12,
Popen([sys.executable,
stdout=PIPE,
closed(self,
"http://localhost:8998/follow?%s"
self.t1
self.t2
self.t2_err
callback=self.parse,
url="http://localhost:8998",
Item()
super(BrokenStartRequestsSpider,
range(100):
on_error(self,
self.visited
FollowAllSpider,
get_crawler(FollowAllSpider,
crawler.crawl(total=1000000)
total_seconds
b'Works')
test_headers(self):
self.execute(['-c',
'item'])
'type(response)'])
self.execute([self.url('/html'),
test_redirect(self):
self.execute([filepath,
'item'],
check_code=False)
self.assertEqual(headers,
self.temp_path
self.cwd
self.env
get_testenv()
cwd=self.cwd,
stderr=subprocess.PIPE,
'scrapy.cfg'))
'testproject'))
'items.py'))
'wrong---project---name'))
'sys'))
exists(join(abspath(project_dir),
self.project_name
'root_template'))
self.env['SCRAPY_SETTINGS_MODULE']
'test_name.py'))
'test_spider'
self.proc('genspider',
'test.com',
module"
self.proc('runspider',
self.assertIn("DEBUG:
(finished)",
self.execute(['--spider',
self.url('/html')])
to_native_str(stderr))
Error',
ReturnsContract,
self.assertFalse(self.results.errors)
self.conman.from_method(spider.returns_request,
ResponseMock()
SimpleSpider,
CrawlerRunner()
self._test_delay(0.2,
self._test_delay(1,
totaltime
0.6
crawler.crawl(n=0.5)
LogCapture('scrapy',
level=logging.ERROR)
self.runner.create_crawler(BrokenStartRequestsSpider)
self.assertIsNotNone(record.exc_info)
self.assertIs(record.exc_info[0],
{"CONCURRENT_REQUESTS":
self.assertEqual(crawler.spider.visited,
OK
no-cache
01
GMT
Cache-Control:
req0.replace(headers={'Referer':
'http://example.com'})
self.runner.create_crawler(SingleRequestSpider)
crawler.spider.meta)
json.loads(to_unicode(crawler.spider.meta['responses'][2].body))
self.assertNotIn('Referer',
"http://localhost:8998/status?n=200")
self.assertIn("Got
200",
self.runner.crawl(SimpleSpider,
Settings,
Crawler(DefaultSpider,
test_deprecated_attribute_spiders(self):
self.crawler.spiders
sl_cls
self.assertIsInstance(spiders,
sl_cls)
'TEST2':
'spider'}
project_settings
'project',
'TEST3':
'project'}
self.assertOptionIsDefault(crawler.settings,
self.assertEqual(runner.settings['foo'],
installed_version
[0,
twisted.protocols.policies
WrappingFactory
util,
ForeverTakingResource,
NoLengthResource,
HostHeaderResource,
PayloadResource,
BrokenDownloadResource
path_to_file_uri
S3DownloadHandler
SingleRequestSpider
keyfile
'keys/cert.pem'
certfile
os.mkdir(name)
FilePath(name).child("file").setContent(b"0123456789")
static.File(name)
util.Redirect(b"/file"))
r.putChild(b"wait",
ForeverTakingResource())
r.putChild(b"nolength",
NoLengthResource())
r.putChild(b"host",
HostHeaderResource())
r.putChild(b"payload",
r.putChild(b"broken",
BrokenDownloadResource())
server.Site(r,
self.wrapper
WrappingFactory(self.site)
self.wrapper,
interface=self.host)
self.download_handler_cls(Settings())
self.download_handler.download_request
hasattr(self.download_handler,
self.download_handler.close()
Request(self.getURL('file'),
r.status)
302)
{'download_timeout':
0.2}
defer.TimeoutError,
headers={'Host':
'example.com'})
b'1'*100
plain
text\ndata
tabs\t
null
meta={'download_maxsize':
crawler.spider.meta['failure']
self.assertIsInstance(failure.value,
defer.CancelledError)
crawler.spider.meta.get('failure')
self.assertTrue(failure
self.assertTrue(reason,
'finished')
unittest.SkipTest("xpayload
"http://127.0.0.1:%d/%s"
http_proxy})
date})
method='PUT',
19:42:41
unittest.SkipTest(
'content-type':
"Twisted
self.password})
'I
power!')
[''],
['17']})
self.mwman.download(download_func,
dfd.addBoth(results.append)
self._wait(dfd)
results[0]
Request('http://example.com/index.html')
status=200)
self.assertTrue(isinstance(ret,
b'<p>You
redirected</p>'
'http://example.com/login',
response=resp)
"Not
self.assertRaises(IOError,
request=req,
HtmlResponse(url,
'HEAD'})
self.assertEqual(resp,
self.assertIs(resp,
'http://example.com/',
'http://example.com/?_escaped_fragment_=')
HtmlResponse(req2.url,
resp3
get_crawler(settings_dict={'COOKIES_ENABLED':
get_crawler(settings_dict={'COOKIES_DEBUG':
CookiesMiddleware.from_crawler(crawler)
LogCapture('scrapy.downloadermiddlewares.cookies',
propagate=False,
level=logging.DEBUG)
mw.process_request(req2,
('scrapy.downloadermiddlewares.cookies',
<200
self.assertIn('Cookie',
meta={'dont_merge_cookies':
'value2',
b'galleta=salada')
self.assertCookieValEqual(req2.headers.get('Cookie'),
meta={'cookiejar':
res5_1
self.mw.process_request(request,
TestCase,
get_testdata('compressed',
format)
uncompressed_body,
assert_samelines(self,
new.body,
Response(url='http://test.com',
TestDefaultHeadersMiddleware(TestCase):
self.get_defaults_spider_mw()
defaults)
180)
spider.download_timeout
self.mw.spider_opened(self.spider)
self.assertEquals(req.headers['Authorization'],
'scrapy.extensions.httpcache.DbmCacheStorage'
'scrapy.extensions.httpcache.RFC2616Policy'
email.utils.formatdate(time.time()
86400)
self.tmpdir
headers={'User-Agent':
self._middleware(**new_settings)
request1,
request2):
self.assertEqual(request1.url,
request2.url)
self.assertEqual(request1.body,
request2.body)
request1.headers
self.request.meta['dont_cache']
self.request),
self._storage()
storage.store_response(self.spider,
self.response)
response2
self.dbm_module)
self)._get_settings(**new_settings)
type(cached)
req)
Response(self.request.url,
headers={'Expires':
res3)
res0b
res4
res5
self.assertEqualResponse(res5,
{'Etag':
'no-store'}),
enumerate(responses):
resc
req0)
shouldcache:
self.assertEqualResponse(resc,
res2.status
self.assertFalse(resc)
headers.get('Cache-Control',
'299',
str(86400
res2)
'no-cache'}),
'no-cache',
res0a)
res0d)
'must-revalidate'
test_process_exception(self):
mw.process_exception(req0,
resolve_encoding
SAMPLEDIR
'compressed')
('html-gzip.bin',
'gzip'),
'deflate'),
b'deflate')
plainbody
zf
GzipFile(fileobj=f,
mode='wb')
zf.write(plainbody)
zf.close()
body=f.getvalue())
isinstance(newresponse,
self.assertEqual(newresponse.body,
plainbody)
self.assertEqual(newresponse.encoding,
resolve_encoding('gb2312'))
'application/gzip'
self.assertEquals(req.url,
Request('http://a.com')
method=method)
meta={'dont_redirect':
'http://www.example.com/302'
'http://www.example.com/redirected2'
body='test',
'Content-length':
'4'})
"Content-Type
"Content-Length
req2.body,
"Redirected
empty,
req2.body
'HEAD')
test_max_redirect_times(self):
'redirect_times'
self.assertEqual(req.meta['redirect_times'],
test_ttl(self):
Request('http://scrapytest.org/302',
meta={'redirect_ttl':
test_redirect_urls(self):
self.mw.process_response(req1,
rsp1,
rsp2,
'http://scrapytest.org/redirected')
self.assertEqual(req2.meta['redirect_urls'],
['http://scrapytest.org/first'])
self.assertEqual(req3.url,
'http://scrapytest.org/redirected2')
self.assertEqual(req3.meta['redirect_urls'],
['http://scrapytest.org/first',
'http://scrapytest.org/redirected'])
[404,
self.assertIs(r,
rsp)
_test_passthrough(Request(url,
True}))
supports
req_result
perc_encoded_utf8_url
self.assertEquals(perc_encoded_utf8_url,
req_result.url)
Request(url='http://example.org')
'http://example.org/newpage')
Request('http://www.scrapytest.org/503')
Disallow:
self.assertNotIgnored(Request('http://site.local'),
self.assertNotIgnored(Request('http://site.local/admin/main'),
self.assertNotIgnored(Request('http://site.local/static/'),
error.DNSLookupError('Robotstxt
reactor.callFromThread(deferred.errback,
middleware):
actually
MyException(Exception):
status=400)
'spider_useragent'
RFPDupeFilter()
dupefilter.open()
Request('http://scrapytest.org/1')
dupefilter.request_seen(r2)
dupefilter.close('finished')
RFPDupeFilter(path)
df.request_seen(r1)
df2.request_seen(r2)
shutil.rmtree(path)
test_request_fingerprint(self):
disconnect_all
re.M)
parse_item(self,
item['name']
disconnect_all(signal)
signalargs
self.run
self.run.run()
set([self.run.geturl(p)
urls_requested
dropped_requests_count
self.run.getpath(response.url)
item['url']:
self.assertEqual('Item
name',
item['name'])
item['price'])
self.run.spider},
e.open_spider(TestSpider(),
TestItem(name=u'John\xa3',
self.ie
NotImplementedError:
self._check_output()
self.assertEqual(res,
encoding='latin-1')
str(int(value)
self.assertEqual(ie.serialize_field(i.fields['name'],
i['name']),
self.assertEqual(ie.serialize_field(i.fields['age'],
i['age']),
age=[i1])
age=[i2])
u'Joseph'}],
u'Maria'}],
self.assertEqual(type(exported['age'][0]),
self.assertEqual(type(exported['age'][0]['age'][0]),
age='world')
self.assertEqual(pickle.load(f),
line))
assertExportResult(self,
expected=b'age,name\r\n22,John\xc2\xa3\r\n',
dict(name=u'bar',
TestItem(name=u'buz',
self.assertExportResult(i3,
b'<name>buz</name>'
_expected_nested
json.loads(to_unicode(self.output.getvalue().strip()))
item['time']
str(item['time'])
[expected])
'23')
self.assertEqual(ie.serialize_field({},
verifyObject
assert_aws_environ,
path_to_file_uri(path)
self._assert_stores(FileFeedStorage(uri),
'more',
self._assert_stores(FileFeedStorage(path),
test_interface(self):
_assert_stores(self,
scrapy.Spider("default")
self.assertTrue(os.path.exists(path))
TestSpider(scrapy.Spider):
tmp_path
os.path.dirname(tmp.name)
self.assertEqual(tmp_path,
tests_path
self.get_test_spider({'FEED_TEMPDIR':
assert_aws_environ()
os.environ.get('S3_TEST_FILE_URI')
uri:
storage.open(scrapy.Spider("default"))
\xe2\x98\x83"
self.assertEqual(content,
ordered=True):
got_rows
got_rows)
self._load_until_eof(data,
'quux2'}),
self.MyItem.fields.keys()
'spam3',
rows_csv
rows_jl
rows_jl)
['egg',
item_cls({'foo':
['foo']
formats
u'[\n{"foo":
u'{"foo":
{'FEED_FORMAT':
self.assertEqual(formats[format],
"text/html"})
self.wrapped
self.request.url)
urlparse(self.request.url).netloc)
urlparse(self.request.url).scheme)
'www.example.com')
'text/html')
['text/html'])
assertSortedEqual(self,
Headers({'Content-Type':
'*/*'),
self.assertEqual(h.getlist('Accept',
b'text/html')
self.assertEqual(h.getlist('Content-Type'),
[b'text/html'])
'ip2']
b'ip2')
b'ip2'])
u'\xa3'},
bytes),
b'\xc2\xa3')
test_setdefault(self):
h.setdefault('X-Forwarded-For',
'X-Forwarded-For':
{b'Content-Type':
[(b'X-Forwarded-For',
b'ip2']),
(b'Content-Type',
[b'text/html'])])
copy.copy(h1)
self.assertEqual(h1,
h2)
isinstance(h2,
'value3')
[b'value1',
[b'value1'])
['value2',
'value3'])
self.assertEqual(h1.getlist('foo'),
object())
cgi
parse_qs,
default_method
default_headers
self.assertRaises(Exception,
self.request_class,
isinstance(r.url,
self.assertEqual(r.method,
isinstance(r.headers,
self.assertEqual(r.meta,
meta=meta,
r.headers
'http://www.scrapy.org'
self.request_class(url=url,
'val2'})
self.request_class(url=url)
"http://www.scrapy.org/blank%20space")
self.request_class(url=u"http://www.scrapy.org/price/£",
encoding="utf-8")
"http://www.scrapy.org/price/%C2%A3?unit=%C2%B5")
"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3")
self.assertEqual(r4.url,
r1.body
body=u"Price:
\xa3100",
self.assertEqual(r3.body,
b"Price:
"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue")
r1.callback
somecallback
r2.errback
r1.meta
self.assertEqual(r1.meta,
r2.meta)
r1.headers
r2.headers,
self.assertEqual(r1.dont_filter,
r2.dont_filter)
test_copy_inherited_classes(self):
type(r2)
b'value'
self.assertEqual((r1.body,
r2.body),
(b'',
body"))
self.assertEqual((r1.headers,
r2.headers),
hdrs))
meta={'a':
test_immutable_attributes(self):
'http://example2.com')
'xxx')
b'price':
b'\xc2\xa3
one':
u'£
b'\xa3
formdata=data,
'latin1')
{'price':
self.assertEqual(set(fs[b'test']),
self.assertEqual(set(fs[b'one']),
[b'xxx'])
self.assertEqual(fs[b'six'],
[b'seven'])
to_unicode=True,
'2'})
'''<html><body>
<form
action="/app"></form>
</body></html>''')
FormRequest.from_response(response)
self.assertEqual(request.method,
method='POST')
self.assertEqual(fs[b'clickable1'],
clickdata={'name':
self.assertFalse(b'clickable1'
[b'i1v'],
self.assertEqual(fs[b'clickable'],
u'price
self.assertTrue(fs[u'price
clickdata={'nr':
[b'1']})
formname="form3",
formnumber=2)
formid="form2")
name="i1">
multiple>
value="i4v2"
value="iv2"
name="i2"
name="i3">
[b'iv2'],
[b'on']})
[b'i1v1'],
[b''],
[b'']})
self.assertEqual(fs[b'three'],
[b'3'])
[b'submit1'])
[b''])
self._test_request(params=(u'pas£',),
self.response_class)
self.response_class("http://www.example.com")
301)
r2.body)
flags=['cached'])
body_unicode
body_bytes
body_unicode)
'http://www.example.com/test'
r1.replace(url="http://www.example.com/other")
"http://www.example.com/other")
self._assert_response_encoding(r2,
self.assertEqual(r3._declared_encoding(),
self.response_class(url=u"http://www.example.com/price/\xa3",
to_native_str(b'http://www.example.com/price/\xc2\xa3'))
'http://www.example.com/price/\xa3')
self.response_class(u"http://www.example.com/price/\xa3",
unicode_string)
charset=utf-8"]},
body=u"\xa3")
charset=iso-8859-1"]},
body=b"\xa3")
r5
body=b"\xa8D")
r7
"utf-8")
"cp1252")
r4._body_inferred_encoding()
self._assert_response_values(r3,
self._assert_response_values(r6,
'gb18030',
u"\u2015")
self._assert_response_values(r7,
self._assert_response_values(r,
test_selector(self):
b"<html><head><title>Some
page</title><body></body></html>"
self.assertIsInstance(response.selector,
self.assertEqual(response.selector.type,
self.assertIs(response.selector,
response.selector)
self.assertIs(response.selector.response,
response.selector.xpath("//title/text()").extract(),
[u'Some
page']
response.selector.css("title::text").extract(),
test_selector_shortcuts(self):
body=body).urljoin('test')
self._assert_response_values(r4,
encoding="utf-8"?><xml><elem>value</elem></xml>'
'xml')
response.selector.xpath("//elem/text()").extract(),
i.__getitem__,
self.assertEqual(i2['name'],
u'John
self.assertEqual(itemrepr,
"{'name':
Doe',
123}")
'John
u'lala'
self.assertEqual(i.get_name(),
u'lala')
u'John'
self.assertEqual(list(i.keys()),
self.assertEqual(list(i.values()),
['John'])
i['keys']
'save':
self.assertEqual(E(load='X')['load'],
'C'}})
self.assertEqual(D(save='X')['save'],
'update':
not_allowed='value')
test_to_dict(self):
link1,
link2):
link2)
hash(link2))
l1
Link("http://www.example.com")
l2)
text="test")
self._assert_same_links(l4,
nofollow=True)
self._assert_different_links(l7,
repetition'),
'''Test
extractor's
behaviour
us'),
test_matches(self):
url1
'http://lotsofstuff.com/stuff1/index'
'http://evenmorestuff.com/uglystuff/index'
self.extractor_cls(restrict_xpaths=('//div[@id="subwrapper"]',
encoding='windows-1252')
test_restrict_xpaths_with_html_entities(self):
HtmlResponse("http://example.org",
self.extractor_cls(restrict_xpaths="//div")
[Link(url='http://example.org/foo',
nofollow=False)])
text=u'',
HtmlResponse("http://known.fm/AC%2FDC/",
Link(url='http://known.fm/AC%2FDC/?page=2',
text=u'BinB',
HtmlResponse("http://example.org/",
Link(url='http://example.org/page.html',
text=u'asd'),
[Link(url='http://otherdomain.com/base/item/12.html',
deny_extensions=())
Link(url='http://example.com/sample2.jpg',
Link(url='http://example.com/get?id=1',
Link(url='http://example.com/get?id=2',
body=xhtml)
[Link(url='http://example.com/about.html',
Link(url='http://example.com/follow.html',
link',
Link(url='http://example.com/nofollow.html',
Link(url='http://example.com/nofollow2.html',
nofollow=True)]
tag=a,
attr=href
Link(url='http://example.org/',
HtmlResponse("https://example.org/somepage/index.html",
Link(url='http://example.com/sample_%E2%82%AC.html',
\xe2\x82\xac
text'.decode('utf-8')),
test_extraction(self):
HtmlParserLinkExtractor()
[Link(url='http://example.com/sample2.html',
tag'),])
ItemLoader
self.assertEqual(item['summary'],
MyLoader(ItemLoader):
take
Compose(TakeFirst(),
float)
u'SKU:
re=sku_re)
'price':
url_out
'http://example.com/')
il.add_value('img_url',
'1234.png')
'img_url':
u'pepe')
u'Pepe'])
il.add_value('summary',
[{'key':
1}])
u'Jim',
NameItemLoader()
[u'Pepe'])
u'bar'],
il.get_value([u'name:foo',
re=u'name:(.*)$'))
re=u'name:(.*)$')
il.get_collected_values('name'))
'world'
MapCompose(filter_world,
str.upper)
v.title(),
[u'Mart'])
[u'mart'])
IdentityDefaultedItemLoader(DefaultedItemLoader):
IdentityDefaultedItemLoader()
six.text_type.swapcase)
u'Mar
Ta')
expected_exc_str
'name'
Join()
MapCompose(processor_with_args)
['marta'])
v[0],
u'Mart')
sep=None,
'world'])
proc,
HtmlResponse(url="",
test_constructor(self):
Selector(text=u"<html><body><div>marta</div></body></html>")
TestItemLoader(selector=sel)
self.assert_(l.selector
sel)
'a::attr(href)')
[u'http://www.scrapy.org',
u'/images/logo.png'])
[u'Paragraph',
'Marta'])
[u'paragraph'])
self.assertEqual(l.get_xpath('//p/text()',
TakeFirst()),
u'paragraph')
re='pa'),
u'pa')
[u'paragraph',
'marta'])
'div::text',
re='http://(.+)')
[u'www.scrapy.org'])
l.replace_css('name',
l.replace_css('url',
self.assertEqual(l.get_css('p::text',
l.add_css(None,
l.replace_css(None,
nl.add_xpath('name',
'div/text()')
nl.add_css('name_div',
'#id')
nl.add_value('name_value',
nl.selector.xpath('div[@id
"id"]/text()').extract())
[u'<div
id="id">marta</div>'])
nl.get_output_value('name'))
nl.get_output_value('name_div'))
nl.get_output_value('name_value'))
nl1.nested_xpath('a')
'img/@src')
nl1.add_xpath('url',
u'http://www.scrapy.org',
('foo.bar',
"baz"),
'dict':
{"name":
self.formatter.crawled(req,
self.assertEqual(logline,
"Crawled
(200)
http://www.example.com>
logline.splitlines()
lines)
self.assertEqual(lines,
body='body',
self.assertEqual(self.catched_msg['to'],
['test@scrapy.org'])
self.assertEqual(msg['to'],
'test@scrapy.org')
attach.seek(0)
attachs
[('attachment',
attach)]
attachs=attachs,
msg.get_payload()
isinstance(payload,
self.assertEqual(len(payload),
self.assertEqual(text.get_charset(),
u'sübjèçt'
u'bödÿ-àéïöñß'
charset='utf-8',
self.assertEqual(msg.get_charset(),
charset="utf-8"')
process(self,
m1,
m3
M1(),
M3()
m3)
mwman.methods['open_spider']],
M2])
mwman.methods['close_spider']],
[M2,
M1])
mwman.methods['process']],
classes
FilesPipeline.from_settings(Settings({'FILES_STORE':
self.pipeline.download_func
_mocked_download_func
test_file_path(self):
self.pipeline.file_path
'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')
self.assertEqual(file_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')
self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')
self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532"),
response=Response("http://www.dorma.co.uk/images/product_details/2532"),
item_url
_create_item_with_files(item_url)
patchers
'inc_stats',
mock.patch.object(FSFilesStore,
'stat_file',
return_value={
'get_media_requests',
p.start()
self.pipeline.process_item(item,
'abc')
p.stop()
init_pipeline(self,
pipeline_class):
test_default_file_key_method(self):
self.assertTrue('file_key(url)
test_overridden_file_key_method(self):
test_item_fields_default(self):
file_urls
'http://www.example.com/files/1.txt'
test_item_fields_override_settings(self):
"FILES_URLS_FIELD":
"FILES_RESULT_FIELD":
("EXPIRES",
_generate_fake_settings(self,
random_string():
"".join([chr(random.randint(97,
123))
range(10)])
{prefix.upper()
settings.items()}
test_different_settings_for_different_instances(self):
getattr(pipe,
pipe_ins_attr)
test_subclass_attrs_preserved_custom_settings(self):
self.assertNotEqual(value,
test_no_custom_settings_for_subclasses(self):
UserDefinedFilesPipeline.from_settings(Settings({"FILES_STORE":
test_custom_settings_for_subclasses(self):
pipe_inst_attr
pipe_inst_attr),
test_custom_settings_and_class_attrs_for_subclasses(self):
pipeline_cls.__name__.upper()
test_cls_attrs_with_DEFAULT_prefix(self):
S3FilesStore.HEADERS['Cache-Control'])
'image/png')
self.assertEqual(key.content_type,
TemporaryFile
ImagesPipeline(self.tempdir,
download_func=_mocked_download_func)
'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')
'thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')
self.assertEqual(thumb_path(Request("file:///tmp/some.name/foo"),
'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')
COLOUR
127,
im
SIZE,
COLOUR)
converted,
self.pipeline.convert_image(im)
self.assertEquals(converted.mode,
self.assertEquals(converted.getcolors(),
[(10000,
self.init_pipeline(DeprecatedImagesPipeline)
50),
self.assertTrue('thumb_key(url)
images
'http://www.example.com/images/1.jpg'
'small':
(random.randint(1,
'big':
(random.randint(1000,
2000),
pipe_attr.lower())
UserDefinedImagePipeline.from_settings(Settings({"IMAGES_STORE":
TemporaryFile()
pipeline_class
self.pipe.item_completed(results,
len(l.records)
self.pipe._mockcalled.append('request_callback')
self.pipe._mockcalled.append('request_errback')
errback=eb)
fail)])
request_fingerprint(req1)
meta=dict(response=Response('http://donot.download.me')))
dfd.callback,
Request('http://url',
authenticator
server)
self._assert_got_tunnel_error(l)
(b'attachment;
['attachment;
({'headers':
etree.fromstring(u'<html/>')
self.assertIs(root,
sel.root)
src="a.jpg"><p>Hello</div>'
body=text,
encoding='utf-8'))
self.assertEqual(sel.type,
self.assertEqual(sel.xpath("//div").extract(),
[u'<div><img
body_content
Selector(TextResponse(url="http://example.com",
body=b'<p>some
text</p>'))
.xpath()
.extract()
ScrapyGenericTranslator,
InitSpider
'crawler'))
self.assertIs(spider.crawler,
'settings'))
self.assertIs(spider.settings,
crawler.settings)
('a',
iterator)
[u'2009-08-16'],
process_links="dummy_process_links"),
dummy_process_links(self,
'http://example.org/about.html',
'http://example.org/nofollow.html'])
self.assertTrue(spider._follow_links)
{'CRAWLSPIDER_FOLLOW_LINKS':
self.assertFalse(spider._follow_links)
"application/gzip"})
robots
['http://example.com/sitemap.xml',
'http://example.com/sitemap-product-index.xml'])
Foo(object):
Foo2(object_ref):
issubclass(CrawlSpider,
isinstance(CrawlSpider(name='foo'),
list(self.mw.process_spider_output(resp,
self.mw.process_spider_exception(self.res404,
self.spider.handle_httpstatus_list
[404]
self.res402
402])
test_meta_overrides_settings(self):
res404
res404.request
res402
self.res402.copy()
res402.request
self.mw.process_spider_input(res404,
res402,
get_crawler(_HttpErrorSpider)
self.assertEqual(crawler.spider.parsed,
self.assertEqual(crawler.spider.failed,
{'404',
'500'})
self.assertNotIn('Ignoring
_get_spiderargs(self):
allowed_domains=['scrapytest.org',
onsite_reqs
offsite_reqs
reqs)
_get_spider(self):
ss
SpiderState(jobdir)
ss.spider_opened(spider)
ss.spider_closed(spider)
self.assertEqual(spider.state,
100000
chunksize=self.chunksize)
test_serialize(self):
q.push('a')
q.push(123)
q.push({'a':
test_nonserializable_object(self):
q.push,
test_serialize_item(self):
TestItem(name='foo')
q.push(i)
isinstance(i2,
TestItem)
test_serialize_loader(self):
TestLoader()
q.push(l)
isinstance(l2,
TestLoader)
l2.default_item_class
self.assertEqual(l2.name_out('x'),
'xx')
test_serialize_request_recursive(self):
Request('http://www.example.com')
r.meta['request']
q.push(r)
r2.meta['request']
self.assertEqual(stats.get_value('anything'),
self.assertEqual(stats.get_value('anything',
{'test':
'value'})
23)
23})
24)
stats.max_value('test2',
stats.min_value('test2',
35)
tuple)
'three'])
'five',
'c']
self.assertEqual(build_component_list(None,
x.upper()),
['A',
'B',
priority=0)
duplicate_bs.set('ONE',
self.assertEqual(build_component_list(duplicate_bs,
x.lower()),
bpy
IPython
ipy
self.assertTrue(callable(shell))
testenv')
CaselessDict(seq)
self.assertEqual(d['red'],
self.assertEqual(d['black'],
d.__getitem__,
self.assertEqual(d.get('c',
'b':
self.assertEqual(r,
'b')
d['a']
'a'
MyDict(CaselessDict):
_append(v):
steps.append(v)
mustbe_deferred(_append,
dfd.addCallback(self.assertEqual,
[1]
steps.append(2)
catched
assertEqual
reactor.callLater(0,
process_chain([cb1,
(cb2
gotexc
cb_fail,
None],
(eb1
process_parallel([cb1,
v2)',
xrange(10):
errors.append))
self.assertEqual(out,
1/0
4])
str(w[0].message),
self.assertIn('foo.NewClass',
self.assertIn('bar.OldClass',
self.assertIn('UserClass',
don't
UnrelatedClass(object):
OldStyleClass:
issubclass(UpdatedUserClass1,
issubclass(UpdatedUserClass1a,
issubclass(OutdatedUserClass1,
issubclass(OldStyleClass,
isinstance(UpdatedUserClass2(),
isinstance(UpdatedUserClass2a(),
isinstance(OutdatedUserClass2(),
isinstance(OutdatedUserClass2a(),
self.assertIn('AlsoDeprecated',
self.assertIn('foo.Bar',
urlparse_cached(request1)
req1b
urlp
xmliter
self.assertEqual(attrs,
['Name
['Type
self.xmliter(body,
'http://base.google.com/ns/1.0')
1'])
['http://www.mydummycompany.com/images/item1.jpg'])
iter)
test_xmliter_objtype_exception(self):
self.xmliter(42,
Turkish
Characters
next(namespace_iter)
self.assertEqual(node.xpath('text()').extract(),
'table',
'feed-sample4.csv')
'feed-sample5.csv')
self.assertEqual(result,
'feed-sample3.csv').replace(b',',
b'\t')
delimiter='\t')
'feed-sample6.csv')
quotechar="'")
u'test'},
u'something',
bbody
TextResponse(url='http://example.org/',
self.assertTrue(type(r1)
type(b),
ZeroDivisionError:
self.logger.propagate
self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))
self.stdout
u'lel\xf1e')
'latin-1'),
test_converting_a_strange_object_should_raise_TypeError(self):
test_errors_argument(self):
self.assertEqual(to_bytes(u'\xa3
49')
A(object):
a.cached()
three
Obj()
a.y
a.meta['z']
[compare_z,
'x']))
self.assertEqual(v,
wk[k])
u'b':
u'd':
u'e',
object():
u'e'}
keys_only=False)
d2.keys()))
d2.values()))
'c',
functools.partial(f1,
4})
Request("http://www.example.com/members/offers.html")
b"somehash"
b'en'
request_fingerprint(r3))
include_headers=['Accept-Language']))
fp2
b'GET
www.example.com\r\n\r\n')
open_in_browser,
"text/html"},
debug=True)
Unknown
[('foo',
self.encoder.encode(r)
self.assertIn(r.url,
test_signal
ok_handler(self,
handlers_called.add(self.ok_handler)
send_catch_log_deferred(signal,
test_signal)
'urlset'
[{'loc':
abstract
u'from
os.path.join(self.tmp_path,
'__main__'
Bar
@mock.patch('sys.stdout',
new_callable=six.StringIO)
stdout):
trackref.print_live_refs()
self.assertEqual(stdout.getvalue(),
['wheele-bin-art.co.uk']))
['art.co.uk']))
['testdomain.com']))
self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html',
self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html',
MySpider(Spider):
self.assertTrue(url_is_from_spider('http://example.com/some/page.html',
self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html',
self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html',
self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html',
keep_blank_values=False),
"http://www.example.com/do?a=2")
"http://www.example.com/r%C3%A9sum%C3%A9")
self.assertEqual(canonicalize_url(u"http://www.example.com/résumé?country=Россия",
self.assertEqual(canonicalize_url("http://www.example.com/a
"http://www.example.com/a%20do%C2%A3.html?a=1")
canonicalized)
test_query(self):
do_expected(self):
guess_scheme(args[0])
do_expected
enumerate
([
('localhost',
t_method
t_method.__name__
setattr
(GuessSchemeTest,
t_method.__name__,
t_method)
client.ScrapyHTTPClientFactory(Request(url))
'/foo?c=v&c2=v2')),
lip+':12345',
'spam.test.org',
'scrapytest.org',
test)
badInput
b"Connection:
close\r\n"
b"POST
'X-Meta-Single':
'single',
'X-Meta-Multivalued':
'value2'],
value1\r\n"
value2\r\n"
b"X-Meta-Single:
single\r\n"
client.ScrapyHTTPPageGetter()
protocol.factory
defer.gatherResults([
to_bytes("127.0.0.1:%d"
getPage(self.getURL("host"),
pageData):
No
'TEST1'),
'override')
stats)
settingsstr
'value2')
new_dict)
attribute.set(new_settings,
assertItemsEqual
unittest.TestCase.assertCountEqual
Note
{'TEST_OPTION':
attr}
'othervalue',
mock.patch.object(attr,
settings['key']
'b'
self.assertEqual(settings['key'],
self.assertEqual(settings.getpriority('key'),
'c'
'x')
mock.patch.object(self.settings,
any_order=True)
'value'
self.settings.setmodule(default_settings,
BaseSettings({'key_lowprio':
settings.set('key_highprio',
'key_highprio':
self.assertEqual(settings.getpriority('key_lowprio'),
self.assertEqual(settings['key_highprio'],
test_get(self):
123.45,
'two'],
45),
123.45)
0.0)
3})
'value'},
'project'),
'HASNOBASE':
BaseSettings({3:
'default')})
self.assertEqual(self.settings.maxpriority(),
self.settings.set('TEST_BOOL',
'TEST_BOOLEAN':
'TEST_BASE':
self.settings.set('BAR',
'fuz',
self.assertEqual(self.settings.overrides.get('BAR'),
self.assertIn('BAR',
@mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES',
10})
self.assertEqual(len(settings.attributes),
settings.attributes)
mydict
self.assertIsInstance(mydict,
self.assertIn('key',
mydict)
self.assertEqual(mydict['key'],
'val')
_get_settings({'DOWNLOAD_TIMEOUT':
'3'})
'val'}
set(['spider1',
test_load(self):
test_load_spider_module(self):
'tests.test_spiderloader.test_spiders.spider1'
found',
["scrapy1.org",
self.assertRaises(ImportError,
'tests.test_utils_misc.test_walk_modules.mod',
'tests.test_utils_misc.test_walk_modules.mod.mod0',
'tests.test_utils_misc.test_walk_modules.mod1',
3])),
setup(
Ronacher',
include_package_data=True,
install_requires=[
Audience
OSI
Approved
System
OS
'Topic
sys.path.append(os.path.join(os.path.dirname(__file__),
'sphinx.ext.intersphinx',
installed
build
'sourcelink.html',
'searchbox.html'
Documentation',
'manual'),
intersphinx_mapping
'python':
print('-'
74)
print('Warning:
inspect.getargspec
functools.update_wrapper
update_wrapper(wrapper,
wrapped,
String,
'w'
Error:
#8f5902",
"#888",
'gs'
simple_page
template_folder='templates')
defaults={'page':
dbapi2
redirect,
abort,
sqlite3.connect(app.config['DATABASE'])
sqlite3.Row
init_db():
app.open_resource('schema.sql',
mode='r')
db.cursor().executescript(f.read())
@app.cli.command('initdb')
initdb_command():
init_db()
print('Initialized
get_db():
hasattr(g,
g.sqlite_db
title,
cur.fetchall()
@app.route('/add',
db.execute('insert
?)',
@app.route('/login',
login():
request.form['password']
render_template('login.html',
@app.route('/logout')
logout():
out')
client(request):
db_fd,
tempfile.mkstemp()
os.close(db_fd)
client.post('/login',
data=dict(
logout(client):
client.get('/logout',
b'No
far'
test_login_logout(client):
in'
out'
'x',
here'
flash,
'development
hasattr(top,
top.sqlite_db
[username],
g.user
'user_id'
redirect(url_for('public_timeline'))
profile_user
query_db('''select
get_user_id(username)
whom_id])
redirect(url_for('user_timeline',
username=username))
db.execute('''insert
?,
?)''',
address'
passwords
taken'
password2
b'The
'wrongpassword')
b'test
&#34;foo&#34;'
Markup,
.app
.config
get_flashed_messages,
.ctx
.blueprints
Blueprint
.templating
template_rendered,
request_started,
request_finished,
got_request_exception,
request_tearing_down,
appcontext_pushed,
.sessions
SecureCookieSession
main(as_module=True)
_identity
iterkeys
itervalues
tb=None):
value.__traceback__
tb:
value.with_traceback(tb)
unicode)
long)
cls.__str__
with_metaclass(meta,
*bases):
this_bases,
meta(name,
type.__new__(metaclass,
'temporary_class',
_PackageBoundObject,
get_debug_flag
cli
_AppCtxGlobals
appcontext_tearing_down
_sentinel
indicates
everything
jinja_environment
Environment
self.app_ctx_globals_class
config_class
testing
get_converter=_make_timedelta)
'SECRET_KEY':
static_url_path=None,
template_folder='templates',
_PackageBoundObject.__init__(self,
root_path=root_path)
static_path
self.static_url_path
relative
self.instance_path
'/<path:filename>',
instead.'),
'__file__',
self._logger.name
self.logger_name:
'instance')
mode='rb'):
self.config['TEMPLATES_AUTO_RELOAD']
options['auto_reload']
create_global_jinja_loader(self):
run_simple
server_name
self.config['SERVER_NAME']
self.debug)
run_simple(host,
flask.testing
self.blueprints[blueprint.name]
_endpoint_from_view_func(view_func)
getattr(view_func,
'for
required_methods
'provide_automatic_options',
self.view_functions[endpoint]
route(self,
self.add_url_rule(rule,
endpoint(self,
exc_class
issubclass(exc_class,
HTTPException):
errorhandler(self,
code_or_exception):
self._register_error_handler(None,
register_error_handler(self,
behavior
before_request(self,
after_request(self,
teardown_request(self,
context_processor(self,
url_value_preprocessor(self,
url_defaults(self,
done:
self._find_error_handler(e)
exc_value
log_exception(self,
exc_info):
[%s]'
request.routing_exception
func()
error):
rv,
status_or_headers,
isinstance(status_or_headers,
list)):
handler(error,
_request_ctx_stack.top.request.blueprint
self.after_request_funcs:
func(exc)
exc=exc)
%r>'
self.blueprint
self.subdomain
self.url_defaults
view_func,
template_folder=None,
self._got_registered_once
modified
first_registration=False):
s.app.before_request_funcs
s.app.after_request_funcs
s.app.teardown_request_funcs
s.app.template_context_processors
s.app.url_value_preprocessors
s.app.url_default_functions
s.app._register_error_handler(
f))
click
iteritems,
Flask):
Maybe
'__init__.py':
filename.endswith('.py'):
exist
application.
os.path.realpath(filename)
dirpath,
'Flask
sys.version,
loader,
self.loader
self._lock:
self.app_import_path
get_debug_flag()
update_wrapper(decorator,
ctx,
self._load_plugin_commands()
ctx.ensure_object(ScriptInfo)
main(self,
short_help='Runs
help='The
bind
to.')
reloader
eager
eager_loading
@with_appcontext
'exec'),
os.name
'posix'
this_module
__package__
self.get_converter
obj.config[self.__name__]
silent:
RuntimeError('The
silent
(errno.ENOENT,
errno.EISDIR):
'expected
appcontext_popped
reqctx:
self.url_adapter
push(self):
'Popped
(rv,
self.push()
exc_type
_app_ctx_stack.top.g
url_rule
top.preserved:
clear_request
request_close
_request_ctx_stack.pop()
browser
submitted:
self.msg
''.join(buf)
automatically
"%s".'
buf.append('
trailing
slash
request.method)
request.files.__class__
_dump_loader_info(loader):
isinstance(srcobj,
detail
See
ExtensionImporter(object):
module_choices,
wrapper_module):
module_choices
self.prefix_cutoff
wrapper_module.count('.')
self.__class__.__module__
other.__class__.__module__
other.__class__.__name__
other.wrapper_module
other.module_choices
sys.meta_path[:]
sys.meta_path
x]
[self]
fullname.split('.',
self.prefix_cutoff)[self.prefix_cutoff]
.format(x=modname),
self.module_choices:
realname
__import__(realname)
sys.modules.pop(fullname,
self.is_important_traceback(realname,
sys.modules[realname]
modname:
setattr(sys.modules[self.wrapper_module],
modname,
ExtDeprecationWarning
ImportError('No
is_important_traceback(self,
self.is_important_frame(important_module,
is_important_frame(self,
tb.tb_frame.f_globals
'__name__'
g:
g['__name__']
important_module:
os.path.abspath(tb.tb_frame.f_code.co_filename)
important_module.replace('.',
os.path.sep)
'.py'
'__init__.py'
werkzeug.local
LocalProxy
Working
typically
request.
getattr(top,
RuntimeError(_app_ctx_err_msg)
LocalStack()
current_app
LocalProxy(partial(_lookup_req_object,
werkzeug.urls
url_quote
wrap_file
message_flashed
_missing
RuntimeError('Attempted
wrapped_g
generate
request.blueprint
endpoint.startswith('.'):
values.pop('_external',
able
attribute)
_request_ctx_stack.top.flashes
list(filter(lambda
filename_or_fp
os.path.isabs(filename):
os.path.join(current_app.root_path,
attachment_filename
'attachment',
OSError:
safe_join(directory,
BadRequest()
__import__(import_name)
RuntimeError('No
'module
302
'lib':
func.__doc__
_missing)
_missing:
self.template_folder
static
td.days
http_date
itsdangerous
_wrap_reader_for_text(fp,
_wrap_writer_for_text(fp,
uuid.UUID):
o)
current_app:
kwargs.setdefault('sort_keys',
dumps(obj,
_dump_arg_defaults(kwargs)
_load_arg_defaults(kwargs)
indent
separators
StreamHandler.emit(self,
b64decode
parse_date
t':
[_tag(x)
u':
None)):
m':
d':
flask.debughelpers
len(obj)
session_json_serializer
self.modified
_fail(self,
unavailable
app.config['SESSION_COOKIE_DOMAIN']
self.get_cookie_path(app)
'/':
app.config['APPLICATION_ROOT']
app.permanent_session_lifetime
session.modified:
save_each
self.session_class()
domain=domain,
val,
expires=expires,
signals_available
signal(self,
self._iter_loaders(template):
loader.get_source(environment,
TemplateNotFound(template)
self.app.jinja_loader
self.app.iter_blueprints():
blueprint.jinja_loader
template=template,
context=context)
ctx.app.update_template_context(context)
ctx.app)
url_parse
http_host
'http://%s/'
RuntimeError('Session
transactions
kwargs.setdefault('environ_overrides',
'Check
'options',
View(object):
cls.decorators:
view.__name__
view.__module__
view.view_class
self.url_rule
mt
self.mimetype
request_charset
ExtensionImporter(['flask_%s',
'flaskext.%s'],
importer.install()
OptionParser
filename)),
ast.Name)
resp_name.id:
fix_url_for(contents,
skip_module_test
last_index
handle_match(match):
match.group(0)
fix_single(match,
lines[lineno
block_lines
response_param_name
content_lines
os.path.split(filename)
fromlist[-1]
import_block.append(line)
lineiter
name_param
new_contents,
upgrade_python_file(filename,
modules
upgrade_template_file(filename,
walk_path(path):
os.walk(path):
'{%
2.6
ext_module
tdir
'..'))
'Georgia',
serif;
h3
font-weight:
normal;
Sans
Mono',
15px;
#eee;
line-height:
border:
1px
solid
black;
<th>{{
</tr>
'success'
result.version
result.logs|dictsort
}}-{{
logs
interpreters):
os.path.join(folder,
statuscode
self.logs[interpreter]
'sdist',
fetch_extensions_list():
checkout_path
root)
os.path.join(root,
create_tox_ini(checkout_path,
flask_dep):
tox_path
flask_dep
test_extension(name,
log('Running
tox
flask_dep)
run_tests(extensions,
tests')
log('Extension
render_results(results,
'\n')
only_approved
[x.strip()
match.groups()
map(int,
fail('Could
git_is_clean():
'commit',
%s)',
(%s
leaks
sys.path'''
monkeypatch.syspath_prepend(str(rv))
modules_tmpdir
flask.current_app._get_current_object()
flask.g.setdefault('bar',
flask.g.bar
lie'
flask.g.pop('bar')
'more
'eggs'
teardown_req(error=None):
teardown_app(error=None):
more():
c.post('/')
'OPTIONS']
truncates
c.post('/more').data
c.get('/more').data
c.delete('/more')
pytest.raises(TypeError):
Submount,
app.url_map.add(Submount('/foo',
Rule('/bar',
endpoint='bar'),
Rule('/',
endpoint='index')
bar():
c.get('/foo/bar').data
flask.session['value']
set'
'http://example.com:8080/')
pytest.raises(RuntimeError,
flask.session['test']
flask.session.permanent
@app.route('/test')
'set-cookie'
rv.headers['set-cookie'])
'development-key'
modify_session(response):
flask.session['foo']
dump_session_contents():
uuid.uuid4()
flask.Markup('Hello!')
b'\xff'
b'1'
b'2'
flask.flash(u'Hello
flask.Markup(u'<em>Testing</em>')
len(messages)
('message',
('warning',
flask.Markup(u'<em>Testing</em>'))
with_categories=True)
category_filter=['message',
after_request(response):
app.test_client().get('/').data
header'
'Test'
teardown_request(exc):
"Ignored"
root():
"Response"
b'Response'
type(exc)
TypeError()
@app.errorhandler(404)
internal_server_error(e):
flask.abort(404)
@app.route('/error')
@app.route('/forbidden')
handle_forbidden_subclass(e):
ForbiddenSubclass)
Forbidden)
Forbidden()
b'apple'
fail():
'X-Foo':
charset=utf-8'
b'Meh'
rv.headers['X-Foo']
b'Hello'
world'
b'{\n
flask.Response('',
app.config.update({"JSONIFY_PRETTYPRINT_REGULAR":
"submsg":
"msg2":
pretty_response
flask.url_for('hello',
name='test
pytest.raises(BuildError,
flask.url_for('spam')
'/test_handler/'
app.test_client().get('/foo/index.html')
'/foo/index.html'
SERVER_NAME='localhost.localdomain:5000'
SubDomain'
app.config.update(SERVER_NAME='localhost.localdomain')
'https://localhost.localdomain')
config_key
run_simple_mock(*args,
rv['passthrough_errors']
flask.request.form['myfile']
add_language_code(endpoint,
values.setdefault('lang_code',
flask.g.lang_code)
pull_lang_code(endpoint,
about():
c.get('/de/').data
b'/de/about'
c.get('/de/about').data
@bp.url_defaults
'Awesome'
pytest.raises(AssertionError)
'Meh'
@app.before_first_request
got.append(42)
c.post('/foo',
URL')
b'success'
test_route_decorator_custom_endpoint():
endpoint='bar')
endpoint='123')
fail_func():
c.get('/success')
len(errors)
flask.g.get('x')
flask.g.x
subdomain='<user>')
index(user):
'index
b'index
mitsuhiko'
app.test_client().open('/')
app.test_client().open('/b/')
b'b'
self.index)
'running...'
'running
(hostname,
'localhost',
flask.Blueprint('frontend',
app.register_blueprint(backend)
flask.Blueprint('test',
url_defaults={'bar':
Admin'
c.get('/admin/static/css/test.css')
pytest.raises(TemplateNotFound)
static_folder='static')
b'/fe'
@bp.route('/foo')
@bp.route('/bar',
foo_bar():
@bp.route('/bar/123',
foo_bar_foo():
c.get('/py/foo').data
b'bp.foo'
AssertionError('expected
raised')
test_template_filter():
test_add_template_filter():
test_template_filter_with_name():
test_add_template_filter_with_name():
bp.add_app_template_filter(my_reverse,
'strrev')
test_template_filter_with_template():
test_add_template_filter_with_template():
test_template_filter_with_name_and_template():
test_add_template_filter_with_name_and_template():
'super_reverse')
test_template_test():
'is_boolean'
app.jinja_env.tests['is_boolean']
app.jinja_env.tests['is_boolean'](False)
test_add_template_test():
test_template_test_with_name():
@bp.app_template_test('boolean')
test_add_template_test_with_name():
bp.add_app_template_test(is_boolean,
test_template_test_with_template():
test_add_template_test_with_template():
test_template_test_with_name_and_template():
test_add_template_test_with_name_and_template():
locate_app,
testapp
pytest.raises(NoAppException,
Module)
realpath
monkeypatch.setitem(os.environ,
expect_rv
obj.load_app().name
create_app(info):
obj.load_app()
ScriptInfo(create_app=lambda
info:
'testappgroup\n'
TEST_KEY
app.config.from_mapping(
app.config.from_envvar('FOO_SETTINGS',
{'FOO_SETTINGS':
msg.endswith("missing.cfg'")
app.send_file_max_age_default.seconds
foo_options
len(foo_options)
bar_options
app.config.get_namespace('BAR_',
len(bar_options)
trim_namespace=False)
sys.meta_path:
flaskext
modules_tmpdir.mkdir('flaskext')
flaskext.join('__init__.py').write('\n')
'newext_simple'
flask.ext.newext_package
'newext_package'
test_function
test_function()
submodule.__name__
submodule.test_function()
'oldext_simple'
flask.ext.oldext_package
'oldext_package'
range(2):
flask.ext.broken
StreamHandler
post_json():
b'Failed
return_json():
app.config['JSON_AS_ASCII']
flask.json.dumps(u'\N{SNOWMAN}')
app.add_url_rule(url,
list',
False],
"bar",
X(object):
'euc-kr'
sorted_by_str
'{',
'"values":
{',
'"0":
'"1":
'"10":
'"11":
'"12":
'"13":
'"14":
'"15":
'"16":
'"17":
'"18":
'"19":
'"2":
'"3":
'"4":
'"5":
'"6":
'"7":
'"8":
'"9":
"foo"',
'}',
sorted_by_int
app.open_resource('static/index.html')
rv.headers['x-sendfile']
os.path.join(app.root_path,
'static/index.html')
'static/index.html'))
flask.send_file(f,
as_attachment=True)
'index.html'
'test_apps',
'subdomaintestmodule')
flask.send_from_directory('static',
'this
app.logger.level
app.logger.addHandler(StreamHandler(out))
out.getvalue()
'1
'after':
_external=True,
flask.views
'List'
methods=['GET'],
_method='GET')
modules_tmpdir):
generate():
(('a/b/c',
'/a/b/c'),
(('/a/b/c',
app.write('import
flask\n\napp
os\n'
'here
os.path.abspath(os.path.dirname(__file__))\n'
str(modules_tmpdir.join('instance'))
init.write('import
len(gc.get_objects())
new_objects
fire()
app.test_request_context('/'):
'localhost'}):
environ_overrides={'SERVER_NAME':
%s!'
flask.has_request_context()
@pytest.mark.skipif(greenlet
reason='greenlet
greenlets
flask._request_ctx_stack.top.copy()
g():
flask.request.path
greenlets.append(greenlet(g))
app.test_client().get('/?foo=bar')
greenlets[0].run()
require
recorded.append((template,
context))
template.name
'simple_template.html'
b'stuff'
category
message'
value=23)
}}',
'<p>Hello
text=text,
html=flask.Markup(text))
app.test_client().get('/').data.splitlines()
foo='<test>')
@app.template_filter()
app.add_template_filter(my_reverse,
@app.template_test()
boolean
app.add_template_test(boolean)
@app.template_test('boolean')
app.add_template_test(is_boolean,
Custom
old_load_setting
old_handlers
app.logger.handlers[:]
ctx.request.url
c.get('/getsession')
flask.session.get('data')
sess['foo']
'Session
flask.request._get_current_object()
kind
view(company_id):
company_id
flask.url_for('view',
company_id='xxx')
response.status_code
b'xxx'
CustomException(Exception):
unregistered_test():
registered_test():
b'parent'
meths
parse_set_header(c.open('/',
method='OPTIONS').headers['Allow'])
sorted(meths)
'awesome'
admin
render_template
flask_cors
pogom
pogom.app
Pogom
insert_mock_data
pogom.search
search_loop,
create_search_threads,
fake_search_loop
pogom.models
init_database,
create_tables,
pogom.pgoapi.utilities
get_pos_by_name
logging.basicConfig(format='%(asctime)s
[%(module)14s]
[%(levelname)7s]
%(message)s')
logging.getLogger()
log.setLevel(logging.DEBUG);
log.setLevel(logging.INFO);
logging.getLogger("peewee").setLevel(logging.INFO)
logging.getLogger("requests").setLevel(logging.WARNING)
logging.getLogger("pogom.pgoapi.pgoapi").setLevel(logging.WARNING)
logging.getLogger("pogom.pgoapi.rpc_api").setLevel(logging.INFO)
logging.getLogger('werkzeug').setLevel(logging.ERROR)
config['parse_pokemon']
args.no_pokemon
args.no_pokestops
args.no_gyms
logging.getLogger("requests").setLevel(logging.DEBUG)
logging.getLogger("pgoapi").setLevel(logging.DEBUG)
logging.getLogger("rpc_api").setLevel(logging.DEBUG)
create_tables(db)
get_pos_by_name(args.location)
any(position):
log.error('Could
aborting.')
log.info('Parsed
is:
{:.4f}/{:.4f}/{:.4f}
(lat/lng/alt)'.
format(*position))
args.no_pokemon:
args.no_pokestops:
args.no_gyms:
position[0]
position[1]
config['LOCALE']
args.locale
config['CHINA']
args.china
args.mock:
real
thread(s)'.format(args.num_threads))
create_search_threads(args.num_threads)
Thread(target=search_loop,
args=(args,))
thread')
insert_mock_data()
Thread(target=fake_search_loop)
search_thread.daemon
search_thread.name
'search_thread'
search_thread.start()
Pogom(__name__)
args.cors:
CORS(app);
config['ROOT_PATH']
config['GMAPS_KEY']
args.gmaps_key
config['REQ_SLEEP']
args.scan_delay
args.no_server:
search_thread.is_alive():
app.run(threaded=True,
use_reloader=False,
debug=args.debug,
host=args.host,
port=args.port)
'LOCALE':
'LOCALES_DIR':
'static/locales',
'ROOT_PATH':
'ORIGINAL_LATITUDE':
'ORIGINAL_LONGITUDE':
'GMAPS_KEY':
'REQ_SLEEP':
'REQ_HEAVY_SLEEP':
'REQ_MAX_FAILED':
'PASSWORD':
calendar
flask.json
JSONEncoder
flask_compress
Compress
ScannedLocation
compress
Compress()
Pogom(Flask):
super(Pogom,
self).__init__(import_name,
compress.init_app(self)
self.json_encoder
CustomJSONEncoder
self.route("/",
methods=['GET'])(self.fullmap)
self.route("/raw_data",
methods=['GET'])(self.raw_data)
self.route("/loc",
methods=['GET'])(self.loc)
self.route("/next_loc",
methods=['POST'])(self.next_loc)
self.route("/mobile",
methods=['GET'])(self.list_pokemon)
fullmap(self):
"inline"
"none"
render_template('map.html',lat=config['ORIGINAL_LATITUDE'],lng=config['ORIGINAL_LONGITUDE'],gmaps_key=config['GMAPS_KEY'],lang=config['LOCALE'],is_fixed=display
raw_data(self):
request.args.get('swLat')
request.args.get('swLng')
request.args.get('neLat')
request.args.get('neLng')
request.args.get('pokemon',
request.args.get('ids'):
request.args.get('ids').split(',')]
Pokemon.get_active_by_id(ids,
swLng,neLat,
Pokemon.get_active(swLat,
request.args.get('pokestops',
d['pokestops']
Pokestop.get_stops(swLat,
request.args.get('gyms',
d['gyms']
Gym.get_gyms(swLat,
request.args.get('scanned',
d['scanned']
ScannedLocation.get_recent(swLat,
loc(self):
d['lat']
d['lng']
next_loc(self):
'Location
searching
turned
off',
request.form.get('lat',
request.form.get('lon',
(lat
lon):
log.warning('Invalid
parameters',
config['NEXT_LOCATION']
{'lat':
'lon':
lon}
log.info('Changing
'ok'
list_pokemon(self):
config['ORIGINAL_LATITUDE'],
LatLng.from_degrees(lat,
lon)
pokemon
Pokemon.get_active(None,
LatLng.from_degrees(pokemon['latitude'],
pokemon['longitude'])
diff.lat().degrees
diff.lng().degrees
(('N'
abs(diff_lat)
+\
(('E'
'W')
abs(diff_lng)
pokemon['pokemon_id'],
pokemon['pokemon_name'],
'card_dir':
direction,
'distance':
int(origin_point.get_distance(
pokemon_point).radians
6366468.241830914),
'time_to_disappear':
sec'
(divmod((
pokemon['disappear_time']-datetime.utcnow()).seconds,
60)),
pokemon['disappear_time'],
pokemon['latitude'],
pokemon['longitude']
pokemon_list.append((entry,
entry['distance']))
[y[0]
sorted(pokemon_list,
x[1])]
render_template('mobile_list.html',
pokemon_list=pokemon_list,
origin_lat=lat,
origin_lng=lon)
CustomJSONEncoder(JSONEncoder):
calendar.timegm(obj.timetuple())
obj.microsecond
iter(obj)
list(iterable)
JSONEncoder.default(self,
get_pokemon_name
printPokemon(id,lat,lng,itime):
args.display_in_console:
pokemon_name
get_pokemon_name(id).lower()
str(id)
doPrint
doPrint:
timeLeft
itime-datetime.utcnow()
"======================================\n
%s\n
Coord:
(%f,%f)\n
Remaining
%s\n======================================"
pokemon_name.encode('utf-8'),lat,lng,pokemon_id,str(timeLeft))
APIKeyException(Exception):
peewee
Model,
MySQLDatabase,
SqliteDatabase,
InsertQuery,\
IntegerField,
CharField,
DoubleField,
BooleanField,\
DateTimeField,
b64encode
get_pokemon_name,
send_to_webhook
.transform
transform_from_wgs_to_gcj
.customLog
printPokemon
init_database():
args.db_type
'mysql':
MySQLDatabase(
args.db_name,
user=args.db_user,
password=args.db_pass,
host=args.db_host)
MySQL
{}.'.format(args.db_host))
SqliteDatabase(args.db)
SQLLite
BaseModel(Model):
Meta:
get_all(cls):
[m
cls.select().dicts()]
result['longitude']
transform_from_wgs_to_gcj(
result['longitude'])
Pokemon(BaseModel):
encounter_id
spawnpoint_id
CharField()
get_active(cls,
.where(Pokemon.disappear_time
.where((Pokemon.disappear_time
get_active_by_id(cls,
datetime.utcnow()))
Pokestop(BaseModel):
pokestop_id
DateTimeField(null=True)
IntegerField(null=True)
get_stops(cls,
.where((Pokestop.latitude
(Pokestop.latitude
pokestops.append(p)
Gym(BaseModel):
UNCONTESTED
TEAM_MYSTIC
TEAM_VALOR
TEAM_INSTINCT
gym_id
team_id
guard_pokemon_id
gym_points
get_gyms(cls,
.where((Gym.latitude
(Gym.latitude
gyms.append(g)
ScannedLocation(BaseModel):
scanned_id
get_recent(cls,
(ScannedLocation
.where((ScannedLocation.last_modified
(datetime.utcnow()
timedelta(minutes=15)))
scans.append(s)
parse_map(map_dict,
iteration_num,
step_location):
scanned
cells
map_dict['responses']['GET_MAP_OBJECTS']['map_cells']
cell
cells:
cell.get('wild_pokemons',
(p['last_modified_timestamp_ms']
p['time_till_hidden_ms'])
printPokemon(p['pokemon_data']['pokemon_id'],
d_t)
pokemons[p['encounter_id']]
webhook_data
time.mktime(d_t.timetuple())
send_to_webhook('pokemon',
webhook_data)
iteration_num
50:
cell.get('forts',
'lure_info'
f['lure_info']['lure_expires_timestamp_ms']
f['lure_info']['active_pokemon_id']
pokestops[f['id']]
'pokestop_id':
'lure_expiration':
'active_pokemon_id':
Currently,
stops
gyms[f['id']]
'gym_id':
'team_id':
f.get('owned_by_team',
'guard_pokemon_id':
f.get('guard_pokemon_id',
'gym_points':
f.get('gym_points',
len(pokemons)
pokemon".format(len(pokemons)))
bulk_upsert(Pokemon,
pokemons)
config['parse_pokestops']:
len(pokestops)
pokestops".format(len(pokestops)))
bulk_upsert(Pokestop,
pokestops)
config['parse_gyms']:
len(gyms)
gyms".format(len(gyms)))
bulk_upsert(Gym,
gyms)
log.info("Upserted
gyms".format(
pokemons_upserted,
pokestops_upserted,
gyms_upserted))
scanned[0]
'scanned_id':
str(step_location[0])+','+str(step_location[1]),
step_location[0],
step_location[1],
datetime.utcnow(),
bulk_upsert(ScannedLocation,
scanned)
bulk_upsert(cls,
num_rows
len(data.values())
num_rows:
log.debug("Inserting
min(i+step,
num_rows)))
InsertQuery(cls,
rows=data.values()[i:min(i+step,
num_rows)]).upsert().execute()
log.warning("%s...
Retrying",
i+=step
create_tables(db):
db.connect()
db.create_tables([Pokemon,
ScannedLocation],
safe=True)
db.close()
Search
Architecture:
Holds
N
Each
responsible
hitting
"overseer"
Creates/updates
grid,
populates
Queue,
waits
Thread,
PGoApi
pgoapi.utilities
f2i,
get_cellid
parse_map
TIMESTAMP
'\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000'
search_queue
calculate_lng_degrees(lat):
float(lng_gap_meters)
(meters_per_degree
math.cos(math.radians(lat)))
api_copy
api.copy()
api_copy.set_position(*position)
api_copy.get_map_objects(latitude=f2i(position[0]),
longitude=f2i(position[1]),
since_timestamp_ms=TIMESTAMP,
cell_id=get_cellid(position[0],
position[1]))
api_copy.call()
log.warning("Uncaught
get_new_coords(init_loc,
distance,
bearing):
6378.1
#km
earth
bearing
math.radians(bearing)
init_coords
[math.radians(init_loc[0]),
math.radians(init_loc[1])]
lat/lng
radians
new_lat
math.sin(init_coords[0])*math.cos(distance/R)
math.cos(init_coords[0])*math.sin(distance/R)*math.cos(bearing))
new_lon
init_coords[1]
math.atan2(math.sin(bearing)*math.sin(distance/R)*math.cos(init_coords[0]),
math.cos(distance/R)-math.sin(init_coords[0])*math.sin(new_lat))
[math.degrees(new_lat),
math.degrees(new_lon)]
generate_location_steps(initial_loc,
step_count):
NORTH
EAST
SOUTH
WEST
270
pulse_radius
km
players
heartbeat
100m
xdist
math.sqrt(3)*pulse_radius
column
ydist
3*(pulse_radius/2)
(initial_loc[0],
initial_loc[1],
#insert
initial_loc
step_count:
range(6):
range(ring):
(loc[0],
loc[1],
log.info('Attempting
Go.')
api.set_position(*position)
api.login(args.auth_service,
args.username,
args.password):
log.info('Failed
Go.
seconds.'.format(args.login_delay))
time.sleep(args.login_delay)
log.info('Login
create_search_threads(num):
search_threads
range(num):
Thread(target=search_thread,
name='search_thread-{}'.format(i),
args=(search_queue,))
search_threads.append(t)
search_thread(q):
threadname
threading.currentThread().getName()
log.debug("Search
{}:
waiting".format(threadname))
q.get()
waiting,
flushing
queue".format(threadname))
{}".format(threadname,
lock:
parse_map(response_dict,
complete".format(threadname,
log.error('Search
dictionary
dictionary\
error.'.format(threadname,
if(failed_consecutive
config['REQ_MAX_FAILED']):
log.error('Niantic
heavy
load.
time.sleep(config['REQ_HEAVY_SLEEP'])
log.info('Map
failed'.format(threadname,
search_loop(args):
starting".format(i))
complete.".format(i))
log.error('Scanning
{0.__class__.__name__}:
args.thread_delay
log.info('Waiting
beginning
scan.'.format(args.thread_delay))
time.sleep(args.thread_delay)
num_steps
log.info('New
config['NEXT_LOCATION']['lat']
config['NEXT_LOCATION']['lon']
config.pop('NEXT_LOCATION',
(config['ORIGINAL_LATITUDE'],
api._auth_provider
api._auth_provider._ticket_expire:
api._auth_provider._ticket_expire/1000
60:
log.info("Skipping
{:.2f}
seconds".format(remaining_time))
step_location
enumerate(generate_location_steps(position,
num_steps),
log.debug("Queue
search_args
lock)
search_queue.put(search_args)
search_queue.empty():
log.debug("Waiting
(remaining:
{})".format(search_queue.qsize()))
search_queue.join()
fake_search_loop():
log.info('Fake
running...')
sqrt,
sin,
cos
6378245.0
0.00669342162296594323
3.14159265358979324
transform_from_wgs_to_gcj(latitude,
transform_lat(longitude
transform_long(longitude
rad_lat
180.0
sin(rad_lat)
sqrt(magic)
(adjust_lat
((a
ee))
(magic
sqrt_magic)
(adjust_lon
(a
cos(rad_lat)
72.004
137.8347
0.8293
55.8271:
transform_lat(x,
-100.0
(160.0
320
30.0))
transform_long(x,
(150.0
30.0
getpass
configargparse
parse_unicode(bytestring):
bytestring.decode(sys.getfilesystemencoding())
verify_config_file_exists(filename):
os.path.exists(fullpath):
log.info("Could
shutil.copy2(fullpath
'.example',
get_args():
configpath
'../config/config.ini')
configargparse.ArgParser(default_config_files=[configpath])
parser.add_argument('-a',
'--auth-service',
type=str.lower,
help='Auth
Service',
default='ptc')
parser.add_argument('-u',
'--username',
help='Username')
parser.add_argument('-p',
'--password',
help='Password')
parser.add_argument('-l',
'--location',
type=parse_unicode,
help='Location,
coordinates')
parser.add_argument('-st',
'--step-limit',
help='Steps',
default=12)
parser.add_argument('-sd',
'--scan-delay',
parser.add_argument('-td',
'--thread-delay',
loop',
parser.add_argument('-ld',
'--login-delay',
attempt',
parser.add_argument('-dc',
'--display-in-console',
Found
Console',
parser.add_argument('-H',
'--host',
host',
default='127.0.0.1')
parser.add_argument('-P',
'--port',
port',
default=5000)
parser.add_argument('-L',
'--locale',
help='Locale
{},\
more)'.
format(config['LOCALE'],
config['LOCALES_DIR']),
default='en')
parser.add_argument('-c',
'--china',
help='Coordinates
transformer
China',
parser.add_argument('-d',
help='Debug
Mode',
parser.add_argument('-m',
'--mock',
help='Mock
mode.
thread.',
parser.add_argument('-ns',
'--no-server',
help='No-Server
searcher
Webserver.',
parser.add_argument('-os',
'--only-server',
help='Server-Only
Webserver
searcher.',
parser.add_argument('-fl',
'--fixed-location',
help='Hides
maps.',
parser.add_argument('-k',
'--gmaps-key',
help='Google
Maps
Javascript
Key',
required=True)
parser.add_argument('-C',
'--cors',
server',
parser.add_argument('-D',
'--db',
help='Database
filename',
default='pogom.db')
'--num-threads',
help='Number
default=1)
parser.add_argument('-np',
'--no-pokemon',
parser.add_argument('-ng',
'--no-gyms',
parser.add_argument('-nk',
'--no-pokestops',
PokeStops
parser.add_argument('--db-type',
help='Type
sqlite)',
default='sqlite')
parser.add_argument('--db-name',
used')
parser.add_argument('--db-user',
help='Username
parser.add_argument('--db-pass',
help='Password
parser.add_argument('--db-host',
help='IP
parser.add_argument('-wh',
'--webhook',
help='Define
URL(s)
POST
webhook
to',
nargs='*',
dest='webhooks')
parser.set_defaults(DEBUG=False)
-l/--location
(args.username
-u/--username,
-l/--location,
-st/--step-limit
getpass.getpass()
insert_mock_data():
num_pokestop
num_gym
log.info('Creating
fake:
gyms'.format(
num_pokemon,
num_pokestop,
num_gym))
.search
generate_location_steps
float(config['ORIGINAL_LATITUDE']),\
float(config['ORIGINAL_LONGITUDE'])
[l
generate_location_steps((latitude,
longitude),
num_pokemon)]
timedelta(hours=1)
detect_time
range(num_pokemon):
Pokemon.create(encounter_id=uuid.uuid4(),
spawnpoint_id='sp{}'.format(i),
pokemon_id=(i+1)
latitude=locations[i][0],
longitude=locations[i][1],
disappear_time=disappear_time,
detect_time=detect_time)
range(num_pokestop):
Pokestop.create(pokestop_id=uuid.uuid4(),
latitude=locations[i+num_pokemon][0],
longitude=locations[i+num_pokemon][1],
lure_expiration=disappear_time
active_pokemon_id=i
range(num_gym):
Gym.create(gym_id=uuid.uuid4(),
team_id=i
guard_pokemon_id=(i+1)
latitude=locations[i
num_pokestop][0],
longitude=locations[i
num_pokestop][1],
gym_points=1000
get_pokemon_name(pokemon_id):
hasattr(get_pokemon_name,
'names'):
config['ROOT_PATH'],
config['LOCALES_DIR'],
'pokemon.{}.json'.format(config['LOCALE']))
get_pokemon_name.names
json.loads(f.read())
get_pokemon_name.names[str(pokemon_id)]
send_to_webhook(message_type,
message_type,
args.webhooks:
webhooks
args.webhooks
webhooks:
requests.post(w,
json=data,
timeout=(None,
requests.exceptions.ReadTimeout:
log.debug('Could
webhook')
requests.exceptions.RequestException
log.debug(e)
requests.packages.urllib3
requests.packages.urllib3.disable_warnings()
Auth:
is_login(self):
get_token(self):
has_ticket(self):
self._ticket_end:
set_ticket(self,
self._ticket_expire,
get_ticket(self):
self.has_ticket():
(self._ticket_expire,
self._ticket_end)
gpsoauth
perform_master_login,
perform_oauth
AuthGoogle(Auth):
GOOGLE_LOGIN_ANDROID_ID
'9774d56d682e549c'
GOOGLE_LOGIN_SERVICE=
'audience:server:client_id:848232511240-7so421jotr2609rmqakceuu1luuq0ptb.apps.googleusercontent.com'
GOOGLE_LOGIN_APP
'com.nianticlabs.pokemongo'
GOOGLE_LOGIN_CLIENT_SIG
'321187995bc7cdc2b5fc91b11a96e2baa8602c62'
'google'
{}'.format(username))
perform_master_login(username,
self.GOOGLE_LOGIN_ANDROID_ID)
perform_oauth(username,
login.get('Token',
self.GOOGLE_LOGIN_ANDROID_ID,
self.GOOGLE_LOGIN_SERVICE,
self.GOOGLE_LOGIN_APP,
self.GOOGLE_LOGIN_CLIENT_SIG)
login.get('Auth')
failed.')
self.log.debug('Google
self._auth_token[:25])
AuthPtc(Auth):
PTC_LOGIN_URL
'https://sso.pokemon.com/sso/login?service=https%3A%2F%2Fsso.pokemon.com%2Fsso%2Foauth2.0%2FcallbackAuthorize'
PTC_LOGIN_OAUTH
'https://sso.pokemon.com/sso/oauth2.0/accessToken'
PTC_LOGIN_CLIENT_SECRET
'w8ScCUXJQc6kXKw8FiOhd8Fixzht18Dq3PEVkUCP5ZPxtgyWsbTvWHFLm2wNY0JR'
'ptc'
{'User-Agent':
'niantic'}
self._session.get(self.PTC_LOGIN_URL,
jdata
json.loads(r.content)
self.log.error('{}...
:('.format(str(e)))
jdata['lt'],
jdata['execution'],
'_eventId':
'submit',
password[:15],
self._session.post(self.PTC_LOGIN_URL,
re.sub('.*ticket=',
r1.history[0].headers['Location'])
Exception,e:
r1.json()['errors'][0])
token!
(%s)',
'client_id':
'mobile-app_pokemon-go',
'redirect_uri':
'https://www.nianticlabs.com/pokemongo/error',
'client_secret':
self.PTC_LOGIN_CLIENT_SECRET,
'grant_type':
'refresh_token',
ticket,
self._session.post(self.PTC_LOGIN_OAUTH,
data=data1)
re.sub('&expires.*',
r2.content)
re.sub('.*access_token=',
access_token)
'-sso.pokemon.com'
access_token:
successful')
self.log.debug('PTC
access_token[:25])
self.log.info('Seems
PTC
Token...
:(')
AuthException(Exception):
NotLoggedInException(Exception):
ServerBusyOrOfflineException(Exception):
f2i
rpc_api
RpcApi
auth_ptc
AuthPtc
auth_google
AuthGoogle
AuthException,
PGoApi:
API_ENTRY
'https://pgorelease.nianticlabs.com/plfe/rpc'
other.log
other._auth_provider
other._api_endpoint
other._position_lat
other._position_lng
other._position_alt
other._req_method_list
list(self._req_method_list)
call(self):
self._auth_provider.is_login():
self.log.info('Not
self.get_position()
RpcApi(self._auth_provider)
self._api_endpoint:
self.API_ENTRY
self.log.info('Execution
request.request(api_endpoint,
self._req_method_list,
self.log.info('Server
busy
offline
again!')
self.log.info('Cleanup
request!')
list_curr_methods(self):
print("{}
({})".format(RpcEnum.RequestMethod.Name(i),i))
set_logger(self,
logger):
self._
get_position(self):
(self._position_lat,
self._position_lng,
self._position_alt)
set_position(self,
alt):
self.log.debug('Set
Position
Lat:
Long:
Alt:
alt)
f2i(lat)
f2i(lng)
f2i(alt)
function(**kwargs):
self.log.info('Create
RpcEnum.RequestMethod.Value(name):
arguments",
self.log.debug("Arguments
\n\r%s",
RpcEnum.RequestMethod.Value(name)
request",
RpcEnum.RequestMethod.keys():
isinstance(username,
isinstance(password,
AuthException("Username/password
correctly
specified")
'ptc':
AuthPtc()
AuthGoogle()
AuthException("Invalid
ptc/google
available.")
self.log.debug('Auth
self._auth_provider.login(username,
self.log.info('Starting
self.get_player()
self.get_hatched_eggs()
self.get_inventory()
self.check_awarded_badges()
self.download_settings(hash="4a2e9bc330dae60e7b74fc85b98868ab4700802e")
self.call()
failed!')
'api_url'
('https://{}/rpc'.format(response['api_url']))
self.log.debug('Setting
self._api_endpoint)
'auth_ticket'
auth_ticket
response['auth_ticket']
self._auth_provider.set_ticket([auth_ticket['expire_timestamp_ms'],
auth_ticket['start'],
auth_ticket['end']])
self.log.error('Login
unexpected
self.log.info('Finished
completed')
NotLoggedInException,
google.protobuf.message
h2f,
to_camel_case,
get_class
protos.RpcEnvelope_pb2
RpcEnvelope
RpcApi:
auth_provider):
self._session.headers.update({'User-Agent':
'Niantic
App'})
auth_provider
get_rpc_id(self):
8145806132888207460
decode_raw(self,
raw):
subprocess.Popen(['protoc',
'--decode_raw'],
process.communicate(raw)
_make_rpc(self,
request_proto_plain):
self.log.debug('Execution
request_proto_serialized
request_proto_plain.SerializeToString()
self._session.post(endpoint,
data=request_proto_serialized)
requests.exceptions.ConnectionError
player_position):
self._auth_provider.is_login()
NotLoggedInException()
request_proto
self._build_main_request(subrequests,
self._make_rpc(endpoint,
request_proto)
self._parse_main_request(response,
_build_main_request(self,
RpcEnvelope.Request()
request.direction
RpcEnum.REQUEST
request.rpc_id
self.get_rpc_id()
request.latitude,
request.longitude,
request.altitude
request.auth.provider
self._auth_provider.get_name()
request.auth.token.contents
self._auth_provider.get_token()
request.auth.token.unknown13
request.unknown12
989
self._build_sub_requests(request,
self.log.debug('Generated
protobuf
\n\r%s',
_build_sub_requests(self,
mainrequest,
subrequest_list):
requests...')
subrequest_list:
entry.items()[0][0]
entry_content
entry[entry_id]
'Request'
subrequest_extension
entry_content.items():
setattr(subrequest_extension,
self.log.info('Argument
proto_name)
subrequest.parameters
subrequest_extension.SerializeToString()
Exception('Unknown
list')
mainrequest
_parse_main_request(self,
response_raw,
subrequests):
response...')
response_raw.status_code
self.log.warning('Unexpected
response_raw.status_code)
self.log.debug('HTTP
\n%s',
response_raw.content)
response_raw.content
self.log.warning('Empty
response_proto
RpcEnvelope.Response()
response_proto.ParseFromString(response_raw.content)
self.log.warning('Could
self.log.debug('Protobuf
structure
rpc
response:\n\r%s',
response_proto)
protobuf_to_dict(response_proto)
self._parse_sub_responses(response_proto,
response_proto_dict)
_parse_sub_responses(self,
response_proto,
subrequests_list,
response_proto_dict):
responses...')
response_proto_dict['responses']
list_len
len(subrequests_list)
subresponse
response_proto.responses:
list_len:
self.log.info("Error
strange
happend...")
subrequests_list[i]
isinstance(request_entry,
request_entry.items()[0][0]
'Protobuf
found'.format(proto_classname)
subresponse_extension:
subresponse_extension.ParseFromString(subresponse)
protobuf_to_dict(subresponse_extension)
"Protobuf
match".format(proto_classname)
response_proto_dict['responses'][entry_name]
CellId,
LatLng
geopy.geocoders
GoogleV3
f2i(float):
struct.unpack('<Q',
float))[0]
f2h(float):
hex(struct.unpack('<Q',
float))[0])
h2f(hex):
struct.unpack('<d',
struct.pack('<Q',
int(hex,16)))[0]
to_camel_case(value):
camelcase():
str.capitalize
camelcase()
"".join(c.next()(x)
value.split("_"))
get_pos_by_name(location_name):
prog
re.compile("^(\-?\d+\.\d+)?,\s*(\-?\d+\.\d+?)$")
prog.match(location_name)
float(res.group(1)),
float(res.group(2)),
location_name:
geolocator
GoogleV3()
geolocator.geocode(location_name)
loc:
loc.latitude,
loc.longitude,
loc.altitude
(latitude,
altitude)
get_class(cls):
module_,
cls.rsplit('.',
getattr(import_module(module_),
class_)
get_cellid(lat,
CellId.from_lat_lng(LatLng.from_degrees(lat,
long)).parent(15)
walk
[origin.id()]
origin.next()
origin.prev()
walk.append(prev.id())
walk.append(next.id())
next.next()
prev.prev()
''.join(map(encode,
sorted(walk)))
encode(cellid):
encoder._VarintEncoder()(output.append,
cellid)
''.join(output)
enum_type_wrapper
name='RpcEnum.proto',
package='RpcEnum',
serialized_pb=_b('\n\rRpcEnum.proto\x12\x07RpcEnum*6\n\x0cRpcDirection\x12\x0b\n\x07UNKNOWN\x10\x00\x12\x0c\n\x08RESPONSE\x10\x01\x12\x0b\n\x07REQUEST\x10\x02*7\n\tTeamColor\x12\x0b\n\x07NEUTRAL\x10\x00\x12\x08\n\x04\x42LUE\x10\x01\x12\x07\n\x03RED\x10\x02\x12\n\n\x06YELLOW\x10\x03*\xc3\x0c\n\rRequestMethod\x12\x10\n\x0cMETHOD_UNSET\x10\x00\x12\x11\n\rPLAYER_UPDATE\x10\x01\x12\x0e\n\nGET_PLAYER\x10\x02\x12\x11\n\rGET_INVENTORY\x10\x04\x12\x15\n\x11\x44OWNLOAD_SETTINGS\x10\x05\x12\x1b\n\x17\x44OWNLOAD_ITEM_TEMPLATES\x10\x06\x12\"\n\x1e\x44OWNLOAD_REMOTE_CONFIG_VERSION\x10\x07\x12\x0f\n\x0b\x46ORT_SEARCH\x10\x65\x12\r\n\tENCOUNTER\x10\x66\x12\x11\n\rCATCH_POKEMON\x10g\x12\x10\n\x0c\x46ORT_DETAILS\x10h\x12\x0c\n\x08ITEM_USE\x10i\x12\x13\n\x0fGET_MAP_OBJECTS\x10j\x12\x17\n\x13\x46ORT_DEPLOY_POKEMON\x10n\x12\x17\n\x13\x46ORT_RECALL_POKEMON\x10o\x12\x13\n\x0fRELEASE_POKEMON\x10p\x12\x13\n\x0fUSE_ITEM_POTION\x10q\x12\x14\n\x10USE_ITEM_CAPTURE\x10r\x12\x11\n\rUSE_ITEM_FLEE\x10s\x12\x13\n\x0fUSE_ITEM_REVIVE\x10t\x12\x10\n\x0cTRADE_SEARCH\x10u\x12\x0f\n\x0bTRADE_OFFER\x10v\x12\x12\n\x0eTRADE_RESPONSE\x10w\x12\x10\n\x0cTRADE_RESULT\x10x\x12\x16\n\x12GET_PLAYER_PROFILE\x10y\x12\x11\n\rGET_ITEM_PACK\x10z\x12\x11\n\rBUY_ITEM_PACK\x10{\x12\x10\n\x0c\x42UY_GEM_PACK\x10|\x12\x12\n\x0e\x45VOLVE_POKEMON\x10}\x12\x14\n\x10GET_HATCHED_EGGS\x10~\x12\x1f\n\x1b\x45NCOUNTER_TUTORIAL_COMPLETE\x10\x7f\x12\x15\n\x10LEVEL_UP_REWARDS\x10\x80\x01\x12\x19\n\x14\x43HECK_AWARDED_BADGES\x10\x81\x01\x12\x11\n\x0cUSE_ITEM_GYM\x10\x85\x01\x12\x14\n\x0fGET_GYM_DETAILS\x10\x86\x01\x12\x15\n\x10START_GYM_BATTLE\x10\x87\x01\x12\x0f\n\nATTACK_GYM\x10\x88\x01\x12\x1b\n\x16RECYCLE_INVENTORY_ITEM\x10\x89\x01\x12\x18\n\x13\x43OLLECT_DAILY_BONUS\x10\x8a\x01\x12\x16\n\x11USE_ITEM_XP_BOOST\x10\x8b\x01\x12\x1b\n\x16USE_ITEM_EGG_INCUBATOR\x10\x8c\x01\x12\x10\n\x0bUSE_INCENSE\x10\x8d\x01\x12\x18\n\x13GET_INCENSE_POKEMON\x10\x8e\x01\x12\x16\n\x11INCENSE_ENCOUNTER\x10\x8f\x01\x12\x16\n\x11\x41\x44\x44_FORT_MODIFIER\x10\x90\x01\x12\x13\n\x0e\x44ISK_ENCOUNTER\x10\x91\x01\x12!\n\x1c\x43OLLECT_DAILY_DEFENDER_BONUS\x10\x92\x01\x12\x14\n\x0fUPGRADE_POKEMON\x10\x93\x01\x12\x19\n\x14SET_FAVORITE_POKEMON\x10\x94\x01\x12\x15\n\x10NICKNAME_POKEMON\x10\x95\x01\x12\x10\n\x0b\x45QUIP_BADGE\x10\x96\x01\x12\x19\n\x14SET_CONTACT_SETTINGS\x10\x97\x01\x12\x15\n\x10GET_ASSET_DIGEST\x10\xac\x02\x12\x16\n\x11GET_DOWNLOAD_URLS\x10\xad\x02\x12\x1c\n\x17GET_SUGGESTED_CODENAMES\x10\x91\x03\x12\x1d\n\x18\x43HECK_CODENAME_AVAILABLE\x10\x92\x03\x12\x13\n\x0e\x43LAIM_CODENAME\x10\x93\x03\x12\x0f\n\nSET_AVATAR\x10\x94\x03\x12\x14\n\x0fSET_PLAYER_TEAM\x10\x95\x03\x12\x1b\n\x16MARK_TUTORIAL_COMPLETE\x10\x96\x03\x12\x16\n\x11LOAD_SPAWN_POINTS\x10\xf4\x03\x12\t\n\x04\x45\x43HO\x10\x9a\x05\x12\x1b\n\x16\x44\x45\x42UG_UPDATE_INVENTORY\x10\xbc\x05\x12\x18\n\x13\x44\x45\x42UG_DELETE_PLAYER\x10\xbd\x05\x12\x17\n\x12SFIDA_REGISTRATION\x10\xa0\x06\x12\x15\n\x10SFIDA_ACTION_LOG\x10\xa1\x06\x12\x18\n\x13SFIDA_CERTIFICATION\x10\xa2\x06\x12\x11\n\x0cSFIDA_UPDATE\x10\xa3\x06\x12\x11\n\x0cSFIDA_ACTION\x10\xa4\x06\x12\x11\n\x0cSFIDA_DOWSER\x10\xa5\x06\x12\x12\n\rSFIDA_CAPTURE\x10\xa6\x06*\xce\x17\n\x0bPokemonMove\x12\x0e\n\nMOVE_UNSET\x10\x00\x12\x11\n\rTHUNDER_SHOCK\x10\x01\x12\x10\n\x0cQUICK_ATTACK\x10\x02\x12\x0b\n\x07SCRATCH\x10\x03\x12\t\n\x05\x45MBER\x10\x04\x12\r\n\tVINE_WHIP\x10\x05\x12\n\n\x06TACKLE\x10\x06\x12\x0e\n\nRAZOR_LEAF\x10\x07\x12\r\n\tTAKE_DOWN\x10\x08\x12\r\n\tWATER_GUN\x10\t\x12\x08\n\x04\x42ITE\x10\n\x12\t\n\x05POUND\x10\x0b\x12\x0f\n\x0b\x44OUBLE_SLAP\x10\x0c\x12\x08\n\x04WRAP\x10\r\x12\x0e\n\nHYPER_BEAM\x10\x0e\x12\x08\n\x04LICK\x10\x0f\x12\x0e\n\nDARK_PULSE\x10\x10\x12\x08\n\x04SMOG\x10\x11\x12\n\n\x06SLUDGE\x10\x12\x12\x0e\n\nMETAL_CLAW\x10\x13\x12\r\n\tVICE_GRIP\x10\x14\x12\x0f\n\x0b\x46LAME_WHEEL\x10\x15\x12\x0c\n\x08MEGAHORN\x10\x16\x12\x0f\n\x0bWING_ATTACK\x10\x17\x12\x10\n\x0c\x46LAMETHROWER\x10\x18\x12\x10\n\x0cSUCKER_PUNCH\x10\x19\x12\x07\n\x03\x44IG\x10\x1a\x12\x0c\n\x08LOW_KICK\x10\x1b\x12\x0e\n\nCROSS_CHOP\x10\x1c\x12\x0e\n\nPSYCHO_CUT\x10\x1d\x12\x0b\n\x07PSYBEAM\x10\x1e\x12\x0e\n\nEARTHQUAKE\x10\x1f\x12\x0e\n\nSTONE_EDGE\x10
\x12\r\n\tICE_PUNCH\x10!\x12\x0f\n\x0bHEART_STAMP\x10\"\x12\r\n\tDISCHARGE\x10#\x12\x10\n\x0c\x46LASH_CANNON\x10$\x12\x08\n\x04PECK\x10%\x12\x0e\n\nDRILL_PECK\x10&\x12\x0c\n\x08ICE_BEAM\x10\'\x12\x0c\n\x08\x42LIZZARD\x10(\x12\r\n\tAIR_SLASH\x10)\x12\r\n\tHEAT_WAVE\x10*\x12\r\n\tTWINEEDLE\x10+\x12\x0e\n\nPOISON_JAB\x10,\x12\x0e\n\nAERIAL_ACE\x10-\x12\r\n\tDRILL_RUN\x10.\x12\x12\n\x0ePETAL_BLIZZARD\x10/\x12\x0e\n\nMEGA_DRAIN\x10\x30\x12\x0c\n\x08\x42UG_BUZZ\x10\x31\x12\x0f\n\x0bPOISON_FANG\x10\x32\x12\x0f\n\x0bNIGHT_SLASH\x10\x33\x12\t\n\x05SLASH\x10\x34\x12\x0f\n\x0b\x42UBBLE_BEAM\x10\x35\x12\x0e\n\nSUBMISSION\x10\x36\x12\x0f\n\x0bKARATE_CHOP\x10\x37\x12\r\n\tLOW_SWEEP\x10\x38\x12\x0c\n\x08\x41QUA_JET\x10\x39\x12\r\n\tAQUA_TAIL\x10:\x12\r\n\tSEED_BOMB\x10;\x12\x0c\n\x08PSYSHOCK\x10<\x12\x0e\n\nROCK_THROW\x10=\x12\x11\n\rANCIENT_POWER\x10>\x12\r\n\tROCK_TOMB\x10?\x12\x0e\n\nROCK_SLIDE\x10@\x12\r\n\tPOWER_GEM\x10\x41\x12\x10\n\x0cSHADOW_SNEAK\x10\x42\x12\x10\n\x0cSHADOW_PUNCH\x10\x43\x12\x0f\n\x0bSHADOW_CLAW\x10\x44\x12\x10\n\x0cOMINOUS_WIND\x10\x45\x12\x0f\n\x0bSHADOW_BALL\x10\x46\x12\x10\n\x0c\x42ULLET_PUNCH\x10G\x12\x0f\n\x0bMAGNET_BOMB\x10H\x12\x0e\n\nSTEEL_WING\x10I\x12\r\n\tIRON_HEAD\x10J\x12\x14\n\x10PARABOLIC_CHARGE\x10K\x12\t\n\x05SPARK\x10L\x12\x11\n\rTHUNDER_PUNCH\x10M\x12\x0b\n\x07THUNDER\x10N\x12\x0f\n\x0bTHUNDERBOLT\x10O\x12\x0b\n\x07TWISTER\x10P\x12\x11\n\rDRAGON_BREATH\x10Q\x12\x10\n\x0c\x44RAGON_PULSE\x10R\x12\x0f\n\x0b\x44RAGON_CLAW\x10S\x12\x13\n\x0f\x44ISARMING_VOICE\x10T\x12\x11\n\rDRAINING_KISS\x10U\x12\x12\n\x0e\x44\x41ZZLING_GLEAM\x10V\x12\r\n\tMOONBLAST\x10W\x12\x0e\n\nPLAY_ROUGH\x10X\x12\x10\n\x0c\x43ROSS_POISON\x10Y\x12\x0f\n\x0bSLUDGE_BOMB\x10Z\x12\x0f\n\x0bSLUDGE_WAVE\x10[\x12\r\n\tGUNK_SHOT\x10\\\x12\x0c\n\x08MUD_SHOT\x10]\x12\r\n\tBONE_CLUB\x10^\x12\x0c\n\x08\x42ULLDOZE\x10_\x12\x0c\n\x08MUD_BOMB\x10`\x12\x0f\n\x0b\x46URY_CUTTER\x10\x61\x12\x0c\n\x08\x42UG_BITE\x10\x62\x12\x0f\n\x0bSIGNAL_BEAM\x10\x63\x12\r\n\tX_SCISSOR\x10\x64\x12\x10\n\x0c\x46LAME_CHARGE\x10\x65\x12\x0f\n\x0b\x46LAME_BURST\x10\x66\x12\x0e\n\nFIRE_BLAST\x10g\x12\t\n\x05\x42RINE\x10h\x12\x0f\n\x0bWATER_PULSE\x10i\x12\t\n\x05SCALD\x10j\x12\x0e\n\nHYDRO_PUMP\x10k\x12\x0b\n\x07PSYCHIC\x10l\x12\r\n\tPSYSTRIKE\x10m\x12\r\n\tICE_SHARD\x10n\x12\x0c\n\x08ICY_WIND\x10o\x12\x10\n\x0c\x46ROST_BREATH\x10p\x12\n\n\x06\x41\x42SORB\x10q\x12\x0e\n\nGIGA_DRAIN\x10r\x12\x0e\n\nFIRE_PUNCH\x10s\x12\x0e\n\nSOLAR_BEAM\x10t\x12\x0e\n\nLEAF_BLADE\x10u\x12\x0e\n\nPOWER_WHIP\x10v\x12\n\n\x06SPLASH\x10w\x12\x08\n\x04\x41\x43ID\x10x\x12\x0e\n\nAIR_CUTTER\x10y\x12\r\n\tHURRICANE\x10z\x12\x0f\n\x0b\x42RICK_BREAK\x10{\x12\x07\n\x03\x43UT\x10|\x12\t\n\x05SWIFT\x10}\x12\x0f\n\x0bHORN_ATTACK\x10~\x12\t\n\x05STOMP\x10\x7f\x12\r\n\x08HEADBUTT\x10\x80\x01\x12\x0f\n\nHYPER_FANG\x10\x81\x01\x12\t\n\x04SLAM\x10\x82\x01\x12\x0e\n\tBODY_SLAM\x10\x83\x01\x12\t\n\x04REST\x10\x84\x01\x12\r\n\x08STRUGGLE\x10\x85\x01\x12\x14\n\x0fSCALD_BLASTOISE\x10\x86\x01\x12\x19\n\x14HYDRO_PUMP_BLASTOISE\x10\x87\x01\x12\x0f\n\nWRAP_GREEN\x10\x88\x01\x12\x0e\n\tWRAP_PINK\x10\x89\x01\x12\x15\n\x10\x46URY_CUTTER_FAST\x10\xc8\x01\x12\x12\n\rBUG_BITE_FAST\x10\xc9\x01\x12\x0e\n\tBITE_FAST\x10\xca\x01\x12\x16\n\x11SUCKER_PUNCH_FAST\x10\xcb\x01\x12\x17\n\x12\x44RAGON_BREATH_FAST\x10\xcc\x01\x12\x17\n\x12THUNDER_SHOCK_FAST\x10\xcd\x01\x12\x0f\n\nSPARK_FAST\x10\xce\x01\x12\x12\n\rLOW_KICK_FAST\x10\xcf\x01\x12\x15\n\x10KARATE_CHOP_FAST\x10\xd0\x01\x12\x0f\n\nEMBER_FAST\x10\xd1\x01\x12\x15\n\x10WING_ATTACK_FAST\x10\xd2\x01\x12\x0e\n\tPECK_FAST\x10\xd3\x01\x12\x0e\n\tLICK_FAST\x10\xd4\x01\x12\x15\n\x10SHADOW_CLAW_FAST\x10\xd5\x01\x12\x13\n\x0eVINE_WHIP_FAST\x10\xd6\x01\x12\x14\n\x0fRAZOR_LEAF_FAST\x10\xd7\x01\x12\x12\n\rMUD_SHOT_FAST\x10\xd8\x01\x12\x13\n\x0eICE_SHARD_FAST\x10\xd9\x01\x12\x16\n\x11\x46ROST_BREATH_FAST\x10\xda\x01\x12\x16\n\x11QUICK_ATTACK_FAST\x10\xdb\x01\x12\x11\n\x0cSCRATCH_FAST\x10\xdc\x01\x12\x10\n\x0bTACKLE_FAST\x10\xdd\x01\x12\x0f\n\nPOUND_FAST\x10\xde\x01\x12\r\n\x08\x43UT_FAST\x10\xdf\x01\x12\x14\n\x0fPOISON_JAB_FAST\x10\xe0\x01\x12\x0e\n\tACID_FAST\x10\xe1\x01\x12\x14\n\x0fPSYCHO_CUT_FAST\x10\xe2\x01\x12\x14\n\x0fROCK_THROW_FAST\x10\xe3\x01\x12\x14\n\x0fMETAL_CLAW_FAST\x10\xe4\x01\x12\x16\n\x11\x42ULLET_PUNCH_FAST\x10\xe5\x01\x12\x13\n\x0eWATER_GUN_FAST\x10\xe6\x01\x12\x10\n\x0bSPLASH_FAST\x10\xe7\x01\x12\x1d\n\x18WATER_GUN_FAST_BLASTOISE\x10\xe8\x01\x12\x12\n\rMUD_SLAP_FAST\x10\xe9\x01\x12\x16\n\x11ZEN_HEADBUTT_FAST\x10\xea\x01\x12\x13\n\x0e\x43ONFUSION_FAST\x10\xeb\x01\x12\x16\n\x11POISON_STING_FAST\x10\xec\x01\x12\x10\n\x0b\x42UBBLE_FAST\x10\xed\x01\x12\x16\n\x11\x46\x45INT_ATTACK_FAST\x10\xee\x01\x12\x14\n\x0fSTEEL_WING_FAST\x10\xef\x01\x12\x13\n\x0e\x46IRE_FANG_FAST\x10\xf0\x01\x12\x14\n\x0fROCK_SMASH_FAST\x10\xf1\x01*\xc9\x05\n\x08ItemType\x12\x10\n\x0cITEM_UNKNOWN\x10\x00\x12\x12\n\x0eITEM_POKE_BALL\x10\x01\x12\x13\n\x0fITEM_GREAT_BALL\x10\x02\x12\x13\n\x0fITEM_ULTRA_BALL\x10\x03\x12\x14\n\x10ITEM_MASTER_BALL\x10\x04\x12\x0f\n\x0bITEM_POTION\x10\x65\x12\x15\n\x11ITEM_SUPER_POTION\x10\x66\x12\x15\n\x11ITEM_HYPER_POTION\x10g\x12\x13\n\x0fITEM_MAX_POTION\x10h\x12\x10\n\x0bITEM_REVIVE\x10\xc9\x01\x12\x14\n\x0fITEM_MAX_REVIVE\x10\xca\x01\x12\x13\n\x0eITEM_LUCKY_EGG\x10\xad\x02\x12\x1a\n\x15ITEM_INCENSE_ORDINARY\x10\x91\x03\x12\x17\n\x12ITEM_INCENSE_SPICY\x10\x92\x03\x12\x16\n\x11ITEM_INCENSE_COOL\x10\x93\x03\x12\x18\n\x13ITEM_INCENSE_FLORAL\x10\x94\x03\x12\x13\n\x0eITEM_TROY_DISK\x10\xf5\x03\x12\x12\n\rITEM_X_ATTACK\x10\xda\x04\x12\x13\n\x0eITEM_X_DEFENSE\x10\xdb\x04\x12\x13\n\x0eITEM_X_MIRACLE\x10\xdc\x04\x12\x14\n\x0fITEM_RAZZ_BERRY\x10\xbd\x05\x12\x14\n\x0fITEM_BLUK_BERRY\x10\xbe\x05\x12\x15\n\x10ITEM_NANAB_BERRY\x10\xbf\x05\x12\x15\n\x10ITEM_WEPAR_BERRY\x10\xc0\x05\x12\x15\n\x10ITEM_PINAP_BERRY\x10\xc1\x05\x12\x18\n\x13ITEM_SPECIAL_CAMERA\x10\xa1\x06\x12#\n\x1eITEM_INCUBATOR_BASIC_UNLIMITED\x10\x85\x07\x12\x19\n\x14ITEM_INCUBATOR_BASIC\x10\x86\x07\x12!\n\x1cITEM_POKEMON_STORAGE_UPGRADE\x10\xe9\x07\x12\x1e\n\x19ITEM_ITEM_STORAGE_UPGRADE\x10\xea\x07*b\n\x14InventoryUpgradeType\x12\x11\n\rUPGRADE_UNSET\x10\x00\x12\x19\n\x15INCREASE_ITEM_STORAGE\x10\x01\x12\x1c\n\x18INCREASE_POKEMON_STORAGE\x10\x02*\xba\x02\n\x10ItemTypeCategory\x12\x12\n\x0eITEM_TYPE_NONE\x10\x00\x12\x16\n\x12ITEM_TYPE_POKEBALL\x10\x01\x12\x14\n\x10ITEM_TYPE_POTION\x10\x02\x12\x14\n\x10ITEM_TYPE_REVIVE\x10\x03\x12\x11\n\rITEM_TYPE_MAP\x10\x04\x12\x14\n\x10ITEM_TYPE_BATTLE\x10\x05\x12\x12\n\x0eITEM_TYPE_FOOD\x10\x06\x12\x14\n\x10ITEM_TYPE_CAMERA\x10\x07\x12\x12\n\x0eITEM_TYPE_DISK\x10\x08\x12\x17\n\x13ITEM_TYPE_INCUBATOR\x10\t\x12\x15\n\x11ITEM_TYPE_INCENSE\x10\n\x12\x16\n\x12ITEM_TYPE_XP_BOOST\x10\x0b\x12\x1f\n\x1bITEM_TYPE_INVENTORY_UPGRADE\x10\x0c*?\n\x10\x45ggIncubatorType\x12\x13\n\x0fINCUBATOR_UNSET\x10\x00\x12\x16\n\x12INCUBATOR_DISTANCE\x10\x01*\xdd\x0c\n\x0fPokemonFamilyId\x12\x10\n\x0c\x46\x41MILY_UNSET\x10\x00\x12\x14\n\x10\x46\x41MILY_BULBASAUR\x10\x01\x12\x15\n\x11\x46\x41MILY_CHARMANDER\x10\x04\x12\x13\n\x0f\x46\x41MILY_SQUIRTLE\x10\x07\x12\x13\n\x0f\x46\x41MILY_CATERPIE\x10\n\x12\x11\n\rFAMILY_WEEDLE\x10\r\x12\x11\n\rFAMILY_PIDGEY\x10\x10\x12\x12\n\x0e\x46\x41MILY_RATTATA\x10\x13\x12\x12\n\x0e\x46\x41MILY_SPEAROW\x10\x15\x12\x10\n\x0c\x46\x41MILY_EKANS\x10\x17\x12\x12\n\x0e\x46\x41MILY_PIKACHU\x10\x19\x12\x14\n\x10\x46\x41MILY_SANDSHREW\x10\x1b\x12\x12\n\x0e\x46\x41MILY_NIDORAN\x10\x1d\x12\x13\n\x0f\x46\x41MILY_NIDORAN2\x10
\x12\x13\n\x0f\x46\x41MILY_CLEFAIRY\x10#\x12\x11\n\rFAMILY_VULPIX\x10%\x12\x15\n\x11\x46\x41MILY_JIGGLYPUFF\x10\'\x12\x10\n\x0c\x46\x41MILY_ZUBAT\x10)\x12\x11\n\rFAMILY_ODDISH\x10+\x12\x10\n\x0c\x46\x41MILY_PARAS\x10.\x12\x12\n\x0e\x46\x41MILY_VENONAT\x10\x30\x12\x12\n\x0e\x46\x41MILY_DIGLETT\x10\x32\x12\x11\n\rFAMILY_MEOWTH\x10\x34\x12\x12\n\x0e\x46\x41MILY_PSYDUCK\x10\x36\x12\x11\n\rFAMILY_MANKEY\x10\x38\x12\x14\n\x10\x46\x41MILY_GROWLITHE\x10:\x12\x12\n\x0e\x46\x41MILY_POLIWAG\x10<\x12\x0f\n\x0b\x46\x41MILY_ABRA\x10?\x12\x11\n\rFAMILY_MACHOP\x10\x42\x12\x15\n\x11\x46\x41MILY_BELLSPROUT\x10\x45\x12\x14\n\x10\x46\x41MILY_TENTACOOL\x10H\x12\x12\n\x0e\x46\x41MILY_GEODUDE\x10J\x12\x11\n\rFAMILY_PONYTA\x10M\x12\x13\n\x0f\x46\x41MILY_SLOWPOKE\x10O\x12\x14\n\x10\x46\x41MILY_MAGNEMITE\x10Q\x12\x14\n\x10\x46\x41MILY_FARFETCHD\x10S\x12\x10\n\x0c\x46\x41MILY_DODUO\x10T\x12\x0f\n\x0b\x46\x41MILY_SEEL\x10V\x12\x11\n\rFAMILY_GRIMER\x10X\x12\x13\n\x0f\x46\x41MILY_SHELLDER\x10Z\x12\x11\n\rFAMILY_GASTLY\x10\\\x12\x0f\n\x0b\x46\x41MILY_ONIX\x10_\x12\x12\n\x0e\x46\x41MILY_DROWZEE\x10`\x12\x11\n\rFAMILY_KRABBY\x10\x62\x12\x12\n\x0e\x46\x41MILY_VOLTORB\x10\x64\x12\x14\n\x10\x46\x41MILY_EXEGGCUTE\x10\x66\x12\x11\n\rFAMILY_CUBONE\x10h\x12\x14\n\x10\x46\x41MILY_HITMONLEE\x10j\x12\x15\n\x11\x46\x41MILY_HITMONCHAN\x10k\x12\x14\n\x10\x46\x41MILY_LICKITUNG\x10l\x12\x12\n\x0e\x46\x41MILY_KOFFING\x10m\x12\x12\n\x0e\x46\x41MILY_RHYHORN\x10o\x12\x12\n\x0e\x46\x41MILY_CHANSEY\x10q\x12\x12\n\x0e\x46\x41MILY_TANGELA\x10r\x12\x15\n\x11\x46\x41MILY_KANGASKHAN\x10s\x12\x11\n\rFAMILY_HORSEA\x10t\x12\x12\n\x0e\x46\x41MILY_GOLDEEN\x10v\x12\x11\n\rFAMILY_STARYU\x10x\x12\x12\n\x0e\x46\x41MILY_MR_MIME\x10z\x12\x12\n\x0e\x46\x41MILY_SCYTHER\x10{\x12\x0f\n\x0b\x46\x41MILY_JYNX\x10|\x12\x15\n\x11\x46\x41MILY_ELECTABUZZ\x10}\x12\x11\n\rFAMILY_MAGMAR\x10~\x12\x11\n\rFAMILY_PINSIR\x10\x7f\x12\x12\n\rFAMILY_TAUROS\x10\x80\x01\x12\x14\n\x0f\x46\x41MILY_MAGIKARP\x10\x81\x01\x12\x12\n\rFAMILY_LAPRAS\x10\x83\x01\x12\x11\n\x0c\x46\x41MILY_DITTO\x10\x84\x01\x12\x11\n\x0c\x46\x41MILY_EEVEE\x10\x85\x01\x12\x13\n\x0e\x46\x41MILY_PORYGON\x10\x89\x01\x12\x13\n\x0e\x46\x41MILY_OMANYTE\x10\x8a\x01\x12\x12\n\rFAMILY_KABUTO\x10\x8c\x01\x12\x16\n\x11\x46\x41MILY_AERODACTYL\x10\x8e\x01\x12\x13\n\x0e\x46\x41MILY_SNORLAX\x10\x8f\x01\x12\x14\n\x0f\x46\x41MILY_ARTICUNO\x10\x90\x01\x12\x12\n\rFAMILY_ZAPDOS\x10\x91\x01\x12\x13\n\x0e\x46\x41MILY_MOLTRES\x10\x92\x01\x12\x13\n\x0e\x46\x41MILY_DRATINI\x10\x93\x01\x12\x12\n\rFAMILY_MEWTWO\x10\x96\x01\x12\x0f\n\nFAMILY_MEW\x10\x97\x01*E\n\x10MapObjectsStatus\x12\x10\n\x0cUNSET_STATUS\x10\x00\x12\x0b\n\x07SUCCESS\x10\x01\x12\x12\n\x0eLOCATION_UNSET\x10\x02*#\n\x08\x46ortType\x12\x07\n\x03GYM\x10\x00\x12\x0e\n\nCHECKPOINT\x10\x01*\x93\x10\n\tPokemonId\x12\r\n\tMISSINGNO\x10\x00\x12\r\n\tBULBASAUR\x10\x01\x12\x0b\n\x07IVYSAUR\x10\x02\x12\x0c\n\x08VENUSAUR\x10\x03\x12\x0e\n\nCHARMENDER\x10\x04\x12\x0e\n\nCHARMELEON\x10\x05\x12\r\n\tCHARIZARD\x10\x06\x12\x0c\n\x08SQUIRTLE\x10\x07\x12\r\n\tWARTORTLE\x10\x08\x12\r\n\tBLASTOISE\x10\t\x12\x0c\n\x08\x43\x41TERPIE\x10\n\x12\x0b\n\x07METAPOD\x10\x0b\x12\x0e\n\nBUTTERFREE\x10\x0c\x12\n\n\x06WEEDLE\x10\r\x12\n\n\x06KAKUNA\x10\x0e\x12\x0c\n\x08\x42\x45\x45\x44RILL\x10\x0f\x12\n\n\x06PIDGEY\x10\x10\x12\r\n\tPIDGEOTTO\x10\x11\x12\x0b\n\x07PIDGEOT\x10\x12\x12\x0b\n\x07RATTATA\x10\x13\x12\x0c\n\x08RATICATE\x10\x14\x12\x0b\n\x07SPEAROW\x10\x15\x12\n\n\x06\x46\x45\x41ROW\x10\x16\x12\t\n\x05\x45KANS\x10\x17\x12\t\n\x05\x41RBOK\x10\x18\x12\x0b\n\x07PIKACHU\x10\x19\x12\n\n\x06RAICHU\x10\x1a\x12\r\n\tSANDSHREW\x10\x1b\x12\x0c\n\x08SANDLASH\x10\x1c\x12\x12\n\x0eNIDORAN_FEMALE\x10\x1d\x12\x0c\n\x08NIDORINA\x10\x1e\x12\r\n\tNIDOQUEEN\x10\x1f\x12\x10\n\x0cNIDORAN_MALE\x10
\x12\x0c\n\x08NIDORINO\x10!\x12\x0c\n\x08NIDOKING\x10\"\x12\x0b\n\x07\x43LEFARY\x10#\x12\x0c\n\x08\x43LEFABLE\x10$\x12\n\n\x06VULPIX\x10%\x12\r\n\tNINETALES\x10&\x12\x0e\n\nJIGGLYPUFF\x10\'\x12\x0e\n\nWIGGLYTUFF\x10(\x12\t\n\x05ZUBAT\x10)\x12\n\n\x06GOLBAT\x10*\x12\n\n\x06ODDISH\x10+\x12\t\n\x05GLOOM\x10,\x12\r\n\tVILEPLUME\x10-\x12\t\n\x05PARAS\x10.\x12\x0c\n\x08PARASECT\x10/\x12\x0b\n\x07VENONAT\x10\x30\x12\x0c\n\x08VENOMOTH\x10\x31\x12\x0b\n\x07\x44IGLETT\x10\x32\x12\x0b\n\x07\x44UGTRIO\x10\x33\x12\n\n\x06MEOWTH\x10\x34\x12\x0b\n\x07PERSIAN\x10\x35\x12\x0b\n\x07PSYDUCK\x10\x36\x12\x0b\n\x07GOLDUCK\x10\x37\x12\n\n\x06MANKEY\x10\x38\x12\x0c\n\x08PRIMEAPE\x10\x39\x12\r\n\tGROWLITHE\x10:\x12\x0c\n\x08\x41RCANINE\x10;\x12\x0b\n\x07POLIWAG\x10<\x12\r\n\tPOLIWHIRL\x10=\x12\r\n\tPOLIWRATH\x10>\x12\x08\n\x04\x41\x42RA\x10?\x12\x0b\n\x07KADABRA\x10@\x12\r\n\tALAKHAZAM\x10\x41\x12\n\n\x06MACHOP\x10\x42\x12\x0b\n\x07MACHOKE\x10\x43\x12\x0b\n\x07MACHAMP\x10\x44\x12\x0e\n\nBELLSPROUT\x10\x45\x12\x0e\n\nWEEPINBELL\x10\x46\x12\x0f\n\x0bVICTREEBELL\x10G\x12\r\n\tTENTACOOL\x10H\x12\x0e\n\nTENTACRUEL\x10I\x12\x0b\n\x07GEODUGE\x10J\x12\x0c\n\x08GRAVELER\x10K\x12\t\n\x05GOLEM\x10L\x12\n\n\x06PONYTA\x10M\x12\x0c\n\x08RAPIDASH\x10N\x12\x0c\n\x08SLOWPOKE\x10O\x12\x0b\n\x07SLOWBRO\x10P\x12\r\n\tMAGNEMITE\x10Q\x12\x0c\n\x08MAGNETON\x10R\x12\r\n\tFARFETCHD\x10S\x12\t\n\x05\x44ODUO\x10T\x12\n\n\x06\x44ODRIO\x10U\x12\x08\n\x04SEEL\x10V\x12\x0b\n\x07\x44\x45WGONG\x10W\x12\n\n\x06GRIMER\x10X\x12\x07\n\x03MUK\x10Y\x12\x0c\n\x08SHELLDER\x10Z\x12\x0c\n\x08\x43LOYSTER\x10[\x12\n\n\x06GASTLY\x10\\\x12\x0b\n\x07HAUNTER\x10]\x12\n\n\x06GENGAR\x10^\x12\x08\n\x04ONIX\x10_\x12\x0b\n\x07\x44ROWZEE\x10`\x12\t\n\x05HYPNO\x10\x61\x12\n\n\x06KRABBY\x10\x62\x12\x0b\n\x07KINGLER\x10\x63\x12\x0b\n\x07VOLTORB\x10\x64\x12\r\n\tELECTRODE\x10\x65\x12\r\n\tEXEGGCUTE\x10\x66\x12\r\n\tEXEGGUTOR\x10g\x12\n\n\x06\x43UBONE\x10h\x12\x0b\n\x07MAROWAK\x10i\x12\r\n\tHITMONLEE\x10j\x12\x0e\n\nHITMONCHAN\x10k\x12\r\n\tLICKITUNG\x10l\x12\x0b\n\x07KOFFING\x10m\x12\x0b\n\x07WEEZING\x10n\x12\x0b\n\x07RHYHORN\x10o\x12\n\n\x06RHYDON\x10p\x12\x0b\n\x07\x43HANSEY\x10q\x12\x0b\n\x07TANGELA\x10r\x12\x0e\n\nKANGASKHAN\x10s\x12\n\n\x06HORSEA\x10t\x12\n\n\x06SEADRA\x10u\x12\x0b\n\x07GOLDEEN\x10v\x12\x0b\n\x07SEAKING\x10w\x12\n\n\x06STARYU\x10x\x12\x0b\n\x07STARMIE\x10y\x12\x0b\n\x07MR_MIME\x10z\x12\x0b\n\x07SCYTHER\x10{\x12\x08\n\x04JYNX\x10|\x12\x0e\n\nELECTABUZZ\x10}\x12\n\n\x06MAGMAR\x10~\x12\n\n\x06PINSIR\x10\x7f\x12\x0b\n\x06TAUROS\x10\x80\x01\x12\r\n\x08MAGIKARP\x10\x81\x01\x12\r\n\x08GYARADOS\x10\x82\x01\x12\x0b\n\x06LAPRAS\x10\x83\x01\x12\n\n\x05\x44ITTO\x10\x84\x01\x12\n\n\x05\x45\x45VEE\x10\x85\x01\x12\r\n\x08VAPOREON\x10\x86\x01\x12\x0c\n\x07JOLTEON\x10\x87\x01\x12\x0c\n\x07\x46LAREON\x10\x88\x01\x12\x0c\n\x07PORYGON\x10\x89\x01\x12\x0c\n\x07OMANYTE\x10\x8a\x01\x12\x0c\n\x07OMASTAR\x10\x8b\x01\x12\x0b\n\x06KABUTO\x10\x8c\x01\x12\r\n\x08KABUTOPS\x10\x8d\x01\x12\x0f\n\nAERODACTYL\x10\x8e\x01\x12\x0c\n\x07SNORLAX\x10\x8f\x01\x12\r\n\x08\x41RTICUNO\x10\x90\x01\x12\x0b\n\x06ZAPDOS\x10\x91\x01\x12\x0c\n\x07MOLTRES\x10\x92\x01\x12\x0c\n\x07\x44RATINI\x10\x93\x01\x12\x0e\n\tDRAGONAIR\x10\x94\x01\x12\x0e\n\tDRAGONITE\x10\x95\x01\x12\x0b\n\x06MEWTWO\x10\x96\x01\x12\x08\n\x03MEW\x10\x97\x01*B\n\x0b\x46ortSponsor\x12\x11\n\rUNSET_SPONSOR\x10\x00\x12\r\n\tMCDONALDS\x10\x01\x12\x11\n\rPOKEMON_STORE\x10\x02*3\n\x11\x46ortRenderingType\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\x11\n\rINTERNAL_TEST\x10\x01')
name='RpcDirection',
full_name='RpcEnum.RpcDirection',
name='UNKNOWN',
name='RESPONSE',
name='REQUEST',
serialized_start=26,
serialized_end=80,
_sym_db.RegisterEnumDescriptor(_RPCDIRECTION)
RpcDirection
enum_type_wrapper.EnumTypeWrapper(_RPCDIRECTION)
name='TeamColor',
full_name='RpcEnum.TeamColor',
name='NEUTRAL',
name='BLUE',
name='RED',
name='YELLOW',
serialized_start=82,
serialized_end=137,
_sym_db.RegisterEnumDescriptor(_TEAMCOLOR)
TeamColor
enum_type_wrapper.EnumTypeWrapper(_TEAMCOLOR)
name='RequestMethod',
full_name='RpcEnum.RequestMethod',
name='METHOD_UNSET',
name='PLAYER_UPDATE',
name='GET_PLAYER',
name='GET_INVENTORY',
name='DOWNLOAD_SETTINGS',
name='DOWNLOAD_ITEM_TEMPLATES',
name='DOWNLOAD_REMOTE_CONFIG_VERSION',
name='FORT_SEARCH',
name='ENCOUNTER',
name='CATCH_POKEMON',
name='FORT_DETAILS',
name='ITEM_USE',
name='GET_MAP_OBJECTS',
name='FORT_DEPLOY_POKEMON',
name='FORT_RECALL_POKEMON',
name='RELEASE_POKEMON',
name='USE_ITEM_POTION',
name='USE_ITEM_CAPTURE',
name='USE_ITEM_FLEE',
name='USE_ITEM_REVIVE',
name='TRADE_SEARCH',
name='TRADE_OFFER',
name='TRADE_RESPONSE',
name='TRADE_RESULT',
name='GET_PLAYER_PROFILE',
name='GET_ITEM_PACK',
name='BUY_ITEM_PACK',
name='BUY_GEM_PACK',
name='EVOLVE_POKEMON',
name='GET_HATCHED_EGGS',
name='ENCOUNTER_TUTORIAL_COMPLETE',
name='LEVEL_UP_REWARDS',
name='CHECK_AWARDED_BADGES',
name='USE_ITEM_GYM',
name='GET_GYM_DETAILS',
name='START_GYM_BATTLE',
name='ATTACK_GYM',
name='RECYCLE_INVENTORY_ITEM',
name='COLLECT_DAILY_BONUS',
name='USE_ITEM_XP_BOOST',
name='USE_ITEM_EGG_INCUBATOR',
name='USE_INCENSE',
name='GET_INCENSE_POKEMON',
name='INCENSE_ENCOUNTER',
name='ADD_FORT_MODIFIER',
name='DISK_ENCOUNTER',
name='COLLECT_DAILY_DEFENDER_BONUS',
name='UPGRADE_POKEMON',
name='SET_FAVORITE_POKEMON',
name='NICKNAME_POKEMON',
name='EQUIP_BADGE',
name='SET_CONTACT_SETTINGS',
name='GET_ASSET_DIGEST',
number=300,
name='GET_DOWNLOAD_URLS',
name='GET_SUGGESTED_CODENAMES',
name='CHECK_CODENAME_AVAILABLE',
name='CLAIM_CODENAME',
name='SET_AVATAR',
name='SET_PLAYER_TEAM',
number=405,
name='MARK_TUTORIAL_COMPLETE',
number=406,
name='LOAD_SPAWN_POINTS',
number=500,
name='ECHO',
number=666,
name='DEBUG_UPDATE_INVENTORY',
number=700,
name='DEBUG_DELETE_PLAYER',
name='SFIDA_REGISTRATION',
number=800,
name='SFIDA_ACTION_LOG',
name='SFIDA_CERTIFICATION',
number=802,
name='SFIDA_UPDATE',
number=803,
name='SFIDA_ACTION',
number=804,
name='SFIDA_DOWSER',
number=805,
name='SFIDA_CAPTURE',
number=806,
serialized_start=140,
serialized_end=1743,
_sym_db.RegisterEnumDescriptor(_REQUESTMETHOD)
RequestMethod
enum_type_wrapper.EnumTypeWrapper(_REQUESTMETHOD)
name='PokemonMove',
full_name='RpcEnum.PokemonMove',
name='MOVE_UNSET',
name='THUNDER_SHOCK',
name='QUICK_ATTACK',
name='SCRATCH',
name='EMBER',
name='VINE_WHIP',
name='TACKLE',
name='RAZOR_LEAF',
name='TAKE_DOWN',
name='WATER_GUN',
name='BITE',
name='POUND',
name='DOUBLE_SLAP',
name='WRAP',
name='HYPER_BEAM',
name='LICK',
name='DARK_PULSE',
name='SMOG',
name='SLUDGE',
name='METAL_CLAW',
name='VICE_GRIP',
name='FLAME_WHEEL',
name='MEGAHORN',
name='WING_ATTACK',
name='FLAMETHROWER',
name='SUCKER_PUNCH',
name='DIG',
name='LOW_KICK',
name='CROSS_CHOP',
name='PSYCHO_CUT',
name='PSYBEAM',
name='EARTHQUAKE',
name='STONE_EDGE',
name='ICE_PUNCH',
name='HEART_STAMP',
name='DISCHARGE',
name='FLASH_CANNON',
name='PECK',
name='DRILL_PECK',
name='ICE_BEAM',
name='BLIZZARD',
name='AIR_SLASH',
name='HEAT_WAVE',
name='TWINEEDLE',
name='POISON_JAB',
name='AERIAL_ACE',
name='DRILL_RUN',
name='PETAL_BLIZZARD',
name='MEGA_DRAIN',
name='BUG_BUZZ',
name='POISON_FANG',
name='NIGHT_SLASH',
name='SLASH',
name='BUBBLE_BEAM',
name='SUBMISSION',
name='KARATE_CHOP',
name='LOW_SWEEP',
name='AQUA_JET',
name='AQUA_TAIL',
name='SEED_BOMB',
name='PSYSHOCK',
name='ROCK_THROW',
name='ANCIENT_POWER',
name='ROCK_TOMB',
name='ROCK_SLIDE',
name='POWER_GEM',
name='SHADOW_SNEAK',
name='SHADOW_PUNCH',
name='SHADOW_CLAW',
name='OMINOUS_WIND',
name='SHADOW_BALL',
name='BULLET_PUNCH',
name='MAGNET_BOMB',
name='STEEL_WING',
name='IRON_HEAD',
name='PARABOLIC_CHARGE',
name='SPARK',
name='THUNDER_PUNCH',
name='THUNDER',
name='THUNDERBOLT',
name='TWISTER',
name='DRAGON_BREATH',
name='DRAGON_PULSE',
name='DRAGON_CLAW',
name='DISARMING_VOICE',
name='DRAINING_KISS',
name='DAZZLING_GLEAM',
name='MOONBLAST',
name='PLAY_ROUGH',
name='CROSS_POISON',
name='SLUDGE_BOMB',
name='SLUDGE_WAVE',
name='GUNK_SHOT',
name='MUD_SHOT',
name='BONE_CLUB',
name='BULLDOZE',
name='MUD_BOMB',
name='FURY_CUTTER',
name='BUG_BITE',
name='SIGNAL_BEAM',
name='X_SCISSOR',
name='FLAME_CHARGE',
name='FLAME_BURST',
name='FIRE_BLAST',
name='BRINE',
name='WATER_PULSE',
name='SCALD',
name='HYDRO_PUMP',
name='PSYCHIC',
name='PSYSTRIKE',
name='ICE_SHARD',
name='ICY_WIND',
name='FROST_BREATH',
name='ABSORB',
name='GIGA_DRAIN',
name='FIRE_PUNCH',
name='SOLAR_BEAM',
name='LEAF_BLADE',
name='POWER_WHIP',
name='SPLASH',
name='ACID',
name='AIR_CUTTER',
name='HURRICANE',
name='BRICK_BREAK',
name='CUT',
name='SWIFT',
name='HORN_ATTACK',
name='STOMP',
name='HEADBUTT',
name='HYPER_FANG',
name='SLAM',
name='BODY_SLAM',
name='REST',
name='STRUGGLE',
name='SCALD_BLASTOISE',
name='HYDRO_PUMP_BLASTOISE',
name='WRAP_GREEN',
name='WRAP_PINK',
name='FURY_CUTTER_FAST',
number=200,
name='BUG_BITE_FAST',
name='BITE_FAST',
name='SUCKER_PUNCH_FAST',
number=203,
name='DRAGON_BREATH_FAST',
number=204,
name='THUNDER_SHOCK_FAST',
number=205,
name='SPARK_FAST',
number=206,
name='LOW_KICK_FAST',
number=207,
name='KARATE_CHOP_FAST',
number=208,
name='EMBER_FAST',
number=209,
name='WING_ATTACK_FAST',
number=210,
name='PECK_FAST',
number=211,
name='LICK_FAST',
number=212,
name='SHADOW_CLAW_FAST',
number=213,
name='VINE_WHIP_FAST',
index=152,
number=214,
name='RAZOR_LEAF_FAST',
index=153,
number=215,
name='MUD_SHOT_FAST',
index=154,
number=216,
name='ICE_SHARD_FAST',
index=155,
number=217,
name='FROST_BREATH_FAST',
index=156,
number=218,
name='QUICK_ATTACK_FAST',
index=157,
number=219,
name='SCRATCH_FAST',
index=158,
number=220,
name='TACKLE_FAST',
index=159,
number=221,
name='POUND_FAST',
index=160,
number=222,
name='CUT_FAST',
index=161,
number=223,
name='POISON_JAB_FAST',
index=162,
number=224,
name='ACID_FAST',
index=163,
number=225,
name='PSYCHO_CUT_FAST',
index=164,
number=226,
name='ROCK_THROW_FAST',
index=165,
number=227,
name='METAL_CLAW_FAST',
index=166,
number=228,
name='BULLET_PUNCH_FAST',
index=167,
number=229,
name='WATER_GUN_FAST',
index=168,
number=230,
name='SPLASH_FAST',
index=169,
number=231,
name='WATER_GUN_FAST_BLASTOISE',
index=170,
number=232,
name='MUD_SLAP_FAST',
index=171,
number=233,
name='ZEN_HEADBUTT_FAST',
index=172,
number=234,
name='CONFUSION_FAST',
index=173,
number=235,
name='POISON_STING_FAST',
index=174,
number=236,
name='BUBBLE_FAST',
index=175,
number=237,
name='FEINT_ATTACK_FAST',
index=176,
number=238,
name='STEEL_WING_FAST',
index=177,
number=239,
name='FIRE_FANG_FAST',
index=178,
number=240,
name='ROCK_SMASH_FAST',
index=179,
number=241,
serialized_start=1746,
serialized_end=4768,
_sym_db.RegisterEnumDescriptor(_POKEMONMOVE)
PokemonMove
enum_type_wrapper.EnumTypeWrapper(_POKEMONMOVE)
name='ItemType',
full_name='RpcEnum.ItemType',
name='ITEM_UNKNOWN',
name='ITEM_POKE_BALL',
name='ITEM_GREAT_BALL',
name='ITEM_ULTRA_BALL',
name='ITEM_MASTER_BALL',
name='ITEM_POTION',
name='ITEM_SUPER_POTION',
name='ITEM_HYPER_POTION',
name='ITEM_MAX_POTION',
name='ITEM_REVIVE',
name='ITEM_MAX_REVIVE',
name='ITEM_LUCKY_EGG',
name='ITEM_INCENSE_ORDINARY',
name='ITEM_INCENSE_SPICY',
name='ITEM_INCENSE_COOL',
name='ITEM_INCENSE_FLORAL',
name='ITEM_TROY_DISK',
number=501,
name='ITEM_X_ATTACK',
number=602,
name='ITEM_X_DEFENSE',
number=603,
name='ITEM_X_MIRACLE',
number=604,
name='ITEM_RAZZ_BERRY',
name='ITEM_BLUK_BERRY',
number=702,
name='ITEM_NANAB_BERRY',
number=703,
name='ITEM_WEPAR_BERRY',
number=704,
name='ITEM_PINAP_BERRY',
number=705,
name='ITEM_SPECIAL_CAMERA',
name='ITEM_INCUBATOR_BASIC_UNLIMITED',
number=901,
name='ITEM_INCUBATOR_BASIC',
number=902,
name='ITEM_POKEMON_STORAGE_UPGRADE',
number=1001,
name='ITEM_ITEM_STORAGE_UPGRADE',
number=1002,
serialized_start=4771,
serialized_end=5484,
_sym_db.RegisterEnumDescriptor(_ITEMTYPE)
ItemType
enum_type_wrapper.EnumTypeWrapper(_ITEMTYPE)
name='InventoryUpgradeType',
full_name='RpcEnum.InventoryUpgradeType',
name='UPGRADE_UNSET',
name='INCREASE_ITEM_STORAGE',
name='INCREASE_POKEMON_STORAGE',
serialized_start=5486,
serialized_end=5584,
_sym_db.RegisterEnumDescriptor(_INVENTORYUPGRADETYPE)
InventoryUpgradeType
enum_type_wrapper.EnumTypeWrapper(_INVENTORYUPGRADETYPE)
name='ItemTypeCategory',
full_name='RpcEnum.ItemTypeCategory',
name='ITEM_TYPE_NONE',
name='ITEM_TYPE_POKEBALL',
name='ITEM_TYPE_POTION',
name='ITEM_TYPE_REVIVE',
name='ITEM_TYPE_MAP',
name='ITEM_TYPE_BATTLE',
name='ITEM_TYPE_FOOD',
name='ITEM_TYPE_CAMERA',
name='ITEM_TYPE_DISK',
name='ITEM_TYPE_INCUBATOR',
name='ITEM_TYPE_INCENSE',
name='ITEM_TYPE_XP_BOOST',
name='ITEM_TYPE_INVENTORY_UPGRADE',
serialized_start=5587,
serialized_end=5901,
_sym_db.RegisterEnumDescriptor(_ITEMTYPECATEGORY)
ItemTypeCategory
enum_type_wrapper.EnumTypeWrapper(_ITEMTYPECATEGORY)
name='EggIncubatorType',
full_name='RpcEnum.EggIncubatorType',
name='INCUBATOR_UNSET',
name='INCUBATOR_DISTANCE',
serialized_start=5903,
serialized_end=5966,
_sym_db.RegisterEnumDescriptor(_EGGINCUBATORTYPE)
EggIncubatorType
enum_type_wrapper.EnumTypeWrapper(_EGGINCUBATORTYPE)
name='PokemonFamilyId',
full_name='RpcEnum.PokemonFamilyId',
name='FAMILY_UNSET',
name='FAMILY_BULBASAUR',
name='FAMILY_CHARMANDER',
name='FAMILY_SQUIRTLE',
name='FAMILY_CATERPIE',
name='FAMILY_WEEDLE',
name='FAMILY_PIDGEY',
name='FAMILY_RATTATA',
name='FAMILY_SPEAROW',
name='FAMILY_EKANS',
name='FAMILY_PIKACHU',
name='FAMILY_SANDSHREW',
name='FAMILY_NIDORAN',
name='FAMILY_NIDORAN2',
name='FAMILY_CLEFAIRY',
name='FAMILY_VULPIX',
name='FAMILY_JIGGLYPUFF',
name='FAMILY_ZUBAT',
name='FAMILY_ODDISH',
name='FAMILY_PARAS',
name='FAMILY_VENONAT',
name='FAMILY_DIGLETT',
name='FAMILY_MEOWTH',
name='FAMILY_PSYDUCK',
name='FAMILY_MANKEY',
name='FAMILY_GROWLITHE',
name='FAMILY_POLIWAG',
name='FAMILY_ABRA',
name='FAMILY_MACHOP',
name='FAMILY_BELLSPROUT',
name='FAMILY_TENTACOOL',
name='FAMILY_GEODUDE',
name='FAMILY_PONYTA',
name='FAMILY_SLOWPOKE',
name='FAMILY_MAGNEMITE',
name='FAMILY_FARFETCHD',
name='FAMILY_DODUO',
name='FAMILY_SEEL',
name='FAMILY_GRIMER',
name='FAMILY_SHELLDER',
name='FAMILY_GASTLY',
name='FAMILY_ONIX',
name='FAMILY_DROWZEE',
name='FAMILY_KRABBY',
name='FAMILY_VOLTORB',
name='FAMILY_EXEGGCUTE',
name='FAMILY_CUBONE',
name='FAMILY_HITMONLEE',
name='FAMILY_HITMONCHAN',
name='FAMILY_LICKITUNG',
name='FAMILY_KOFFING',
name='FAMILY_RHYHORN',
name='FAMILY_CHANSEY',
name='FAMILY_TANGELA',
name='FAMILY_KANGASKHAN',
name='FAMILY_HORSEA',
name='FAMILY_GOLDEEN',
name='FAMILY_STARYU',
name='FAMILY_MR_MIME',
name='FAMILY_SCYTHER',
name='FAMILY_JYNX',
name='FAMILY_ELECTABUZZ',
name='FAMILY_MAGMAR',
name='FAMILY_PINSIR',
name='FAMILY_TAUROS',
name='FAMILY_MAGIKARP',
name='FAMILY_LAPRAS',
name='FAMILY_DITTO',
name='FAMILY_EEVEE',
name='FAMILY_PORYGON',
name='FAMILY_OMANYTE',
name='FAMILY_KABUTO',
name='FAMILY_AERODACTYL',
name='FAMILY_SNORLAX',
name='FAMILY_ARTICUNO',
name='FAMILY_ZAPDOS',
name='FAMILY_MOLTRES',
name='FAMILY_DRATINI',
name='FAMILY_MEWTWO',
name='FAMILY_MEW',
serialized_start=5969,
serialized_end=7598,
_sym_db.RegisterEnumDescriptor(_POKEMONFAMILYID)
PokemonFamilyId
enum_type_wrapper.EnumTypeWrapper(_POKEMONFAMILYID)
name='MapObjectsStatus',
full_name='RpcEnum.MapObjectsStatus',
name='UNSET_STATUS',
name='SUCCESS',
name='LOCATION_UNSET',
serialized_start=7600,
serialized_end=7669,
_sym_db.RegisterEnumDescriptor(_MAPOBJECTSSTATUS)
MapObjectsStatus
enum_type_wrapper.EnumTypeWrapper(_MAPOBJECTSSTATUS)
name='FortType',
full_name='RpcEnum.FortType',
name='GYM',
name='CHECKPOINT',
serialized_start=7671,
serialized_end=7706,
_sym_db.RegisterEnumDescriptor(_FORTTYPE)
FortType
enum_type_wrapper.EnumTypeWrapper(_FORTTYPE)
name='PokemonId',
full_name='RpcEnum.PokemonId',
name='MISSINGNO',
name='BULBASAUR',
name='IVYSAUR',
name='VENUSAUR',
name='CHARMENDER',
name='CHARMELEON',
name='CHARIZARD',
name='SQUIRTLE',
name='WARTORTLE',
name='BLASTOISE',
name='CATERPIE',
name='METAPOD',
name='BUTTERFREE',
name='WEEDLE',
name='KAKUNA',
name='BEEDRILL',
name='PIDGEY',
name='PIDGEOTTO',
name='PIDGEOT',
name='RATTATA',
name='RATICATE',
name='SPEAROW',
name='FEAROW',
name='EKANS',
name='ARBOK',
name='PIKACHU',
name='RAICHU',
name='SANDSHREW',
name='SANDLASH',
name='NIDORAN_FEMALE',
name='NIDORINA',
name='NIDOQUEEN',
name='NIDORAN_MALE',
name='NIDORINO',
name='NIDOKING',
name='CLEFARY',
name='CLEFABLE',
name='VULPIX',
name='NINETALES',
name='JIGGLYPUFF',
name='WIGGLYTUFF',
name='ZUBAT',
name='GOLBAT',
name='ODDISH',
name='GLOOM',
name='VILEPLUME',
name='PARAS',
name='PARASECT',
name='VENONAT',
name='VENOMOTH',
name='DIGLETT',
name='DUGTRIO',
name='MEOWTH',
name='PERSIAN',
name='PSYDUCK',
name='GOLDUCK',
name='MANKEY',
name='PRIMEAPE',
name='GROWLITHE',
name='ARCANINE',
name='POLIWAG',
name='POLIWHIRL',
name='POLIWRATH',
name='ABRA',
name='KADABRA',
name='ALAKHAZAM',
name='MACHOP',
name='MACHOKE',
name='MACHAMP',
name='BELLSPROUT',
name='WEEPINBELL',
name='VICTREEBELL',
name='TENTACOOL',
name='TENTACRUEL',
name='GEODUGE',
name='GRAVELER',
name='GOLEM',
name='PONYTA',
name='RAPIDASH',
name='SLOWPOKE',
name='SLOWBRO',
name='MAGNEMITE',
name='MAGNETON',
name='FARFETCHD',
name='DODUO',
name='DODRIO',
name='SEEL',
name='DEWGONG',
name='GRIMER',
name='MUK',
name='SHELLDER',
name='CLOYSTER',
name='GASTLY',
name='HAUNTER',
name='GENGAR',
name='ONIX',
name='DROWZEE',
name='HYPNO',
name='KRABBY',
name='KINGLER',
name='VOLTORB',
name='ELECTRODE',
name='EXEGGCUTE',
name='EXEGGUTOR',
name='CUBONE',
name='MAROWAK',
name='HITMONLEE',
name='HITMONCHAN',
name='LICKITUNG',
name='KOFFING',
name='WEEZING',
name='RHYHORN',
name='RHYDON',
name='CHANSEY',
name='TANGELA',
name='KANGASKHAN',
name='HORSEA',
name='SEADRA',
name='GOLDEEN',
name='SEAKING',
name='STARYU',
name='STARMIE',
name='MR_MIME',
name='SCYTHER',
name='JYNX',
name='ELECTABUZZ',
name='MAGMAR',
name='PINSIR',
name='TAUROS',
name='MAGIKARP',
name='GYARADOS',
name='LAPRAS',
name='DITTO',
name='EEVEE',
name='VAPOREON',
name='JOLTEON',
name='FLAREON',
name='PORYGON',
name='OMANYTE',
name='OMASTAR',
name='KABUTO',
name='KABUTOPS',
name='AERODACTYL',
name='SNORLAX',
name='ARTICUNO',
name='ZAPDOS',
name='MOLTRES',
name='DRATINI',
name='DRAGONAIR',
name='DRAGONITE',
name='MEWTWO',
name='MEW',
serialized_start=7709,
serialized_end=9776,
_sym_db.RegisterEnumDescriptor(_POKEMONID)
PokemonId
enum_type_wrapper.EnumTypeWrapper(_POKEMONID)
name='FortSponsor',
full_name='RpcEnum.FortSponsor',
name='UNSET_SPONSOR',
name='MCDONALDS',
name='POKEMON_STORE',
serialized_start=9778,
serialized_end=9844,
_sym_db.RegisterEnumDescriptor(_FORTSPONSOR)
FortSponsor
enum_type_wrapper.EnumTypeWrapper(_FORTSPONSOR)
name='FortRenderingType',
full_name='RpcEnum.FortRenderingType',
name='DEFAULT',
name='INTERNAL_TEST',
serialized_start=9846,
serialized_end=9897,
_sym_db.RegisterEnumDescriptor(_FORTRENDERINGTYPE)
FortRenderingType
enum_type_wrapper.EnumTypeWrapper(_FORTRENDERINGTYPE)
UNKNOWN
RESPONSE
REQUEST
NEUTRAL
BLUE
RED
YELLOW
METHOD_UNSET
PLAYER_UPDATE
GET_PLAYER
GET_INVENTORY
DOWNLOAD_SETTINGS
DOWNLOAD_ITEM_TEMPLATES
DOWNLOAD_REMOTE_CONFIG_VERSION
FORT_SEARCH
ENCOUNTER
CATCH_POKEMON
FORT_DETAILS
ITEM_USE
GET_MAP_OBJECTS
FORT_DEPLOY_POKEMON
FORT_RECALL_POKEMON
RELEASE_POKEMON
USE_ITEM_POTION
USE_ITEM_CAPTURE
USE_ITEM_FLEE
USE_ITEM_REVIVE
TRADE_SEARCH
TRADE_OFFER
TRADE_RESPONSE
TRADE_RESULT
GET_PLAYER_PROFILE
GET_ITEM_PACK
BUY_ITEM_PACK
BUY_GEM_PACK
EVOLVE_POKEMON
GET_HATCHED_EGGS
ENCOUNTER_TUTORIAL_COMPLETE
LEVEL_UP_REWARDS
CHECK_AWARDED_BADGES
USE_ITEM_GYM
GET_GYM_DETAILS
START_GYM_BATTLE
ATTACK_GYM
RECYCLE_INVENTORY_ITEM
COLLECT_DAILY_BONUS
USE_ITEM_XP_BOOST
USE_ITEM_EGG_INCUBATOR
USE_INCENSE
GET_INCENSE_POKEMON
INCENSE_ENCOUNTER
ADD_FORT_MODIFIER
DISK_ENCOUNTER
COLLECT_DAILY_DEFENDER_BONUS
UPGRADE_POKEMON
SET_FAVORITE_POKEMON
NICKNAME_POKEMON
EQUIP_BADGE
SET_CONTACT_SETTINGS
GET_ASSET_DIGEST
GET_DOWNLOAD_URLS
GET_SUGGESTED_CODENAMES
CHECK_CODENAME_AVAILABLE
CLAIM_CODENAME
SET_AVATAR
SET_PLAYER_TEAM
MARK_TUTORIAL_COMPLETE
406
LOAD_SPAWN_POINTS
ECHO
666
DEBUG_UPDATE_INVENTORY
700
DEBUG_DELETE_PLAYER
SFIDA_REGISTRATION
800
SFIDA_ACTION_LOG
SFIDA_CERTIFICATION
802
SFIDA_UPDATE
803
SFIDA_ACTION
804
SFIDA_DOWSER
805
SFIDA_CAPTURE
806
MOVE_UNSET
THUNDER_SHOCK
QUICK_ATTACK
SCRATCH
EMBER
VINE_WHIP
TACKLE
RAZOR_LEAF
TAKE_DOWN
WATER_GUN
BITE
POUND
DOUBLE_SLAP
WRAP
HYPER_BEAM
LICK
DARK_PULSE
SMOG
SLUDGE
METAL_CLAW
VICE_GRIP
FLAME_WHEEL
MEGAHORN
WING_ATTACK
FLAMETHROWER
SUCKER_PUNCH
DIG
LOW_KICK
CROSS_CHOP
PSYCHO_CUT
PSYBEAM
EARTHQUAKE
STONE_EDGE
ICE_PUNCH
HEART_STAMP
DISCHARGE
FLASH_CANNON
PECK
DRILL_PECK
ICE_BEAM
BLIZZARD
AIR_SLASH
HEAT_WAVE
TWINEEDLE
POISON_JAB
AERIAL_ACE
DRILL_RUN
PETAL_BLIZZARD
MEGA_DRAIN
BUG_BUZZ
POISON_FANG
NIGHT_SLASH
SLASH
BUBBLE_BEAM
SUBMISSION
KARATE_CHOP
LOW_SWEEP
AQUA_JET
AQUA_TAIL
SEED_BOMB
PSYSHOCK
ROCK_THROW
ANCIENT_POWER
ROCK_TOMB
ROCK_SLIDE
POWER_GEM
SHADOW_SNEAK
SHADOW_PUNCH
SHADOW_CLAW
OMINOUS_WIND
SHADOW_BALL
BULLET_PUNCH
MAGNET_BOMB
STEEL_WING
IRON_HEAD
PARABOLIC_CHARGE
SPARK
THUNDER_PUNCH
THUNDER
THUNDERBOLT
TWISTER
DRAGON_BREATH
DRAGON_PULSE
DRAGON_CLAW
DISARMING_VOICE
DRAINING_KISS
DAZZLING_GLEAM
MOONBLAST
PLAY_ROUGH
CROSS_POISON
SLUDGE_BOMB
SLUDGE_WAVE
GUNK_SHOT
MUD_SHOT
BONE_CLUB
BULLDOZE
MUD_BOMB
FURY_CUTTER
BUG_BITE
SIGNAL_BEAM
X_SCISSOR
FLAME_CHARGE
FLAME_BURST
FIRE_BLAST
BRINE
WATER_PULSE
SCALD
HYDRO_PUMP
PSYCHIC
PSYSTRIKE
ICE_SHARD
ICY_WIND
FROST_BREATH
ABSORB
GIGA_DRAIN
FIRE_PUNCH
SOLAR_BEAM
LEAF_BLADE
POWER_WHIP
SPLASH
ACID
AIR_CUTTER
HURRICANE
BRICK_BREAK
CUT
SWIFT
HORN_ATTACK
STOMP
HEADBUTT
HYPER_FANG
SLAM
BODY_SLAM
REST
STRUGGLE
SCALD_BLASTOISE
HYDRO_PUMP_BLASTOISE
WRAP_GREEN
WRAP_PINK
FURY_CUTTER_FAST
BUG_BITE_FAST
BITE_FAST
SUCKER_PUNCH_FAST
203
DRAGON_BREATH_FAST
204
THUNDER_SHOCK_FAST
205
SPARK_FAST
206
LOW_KICK_FAST
207
KARATE_CHOP_FAST
208
EMBER_FAST
209
WING_ATTACK_FAST
210
PECK_FAST
211
LICK_FAST
212
SHADOW_CLAW_FAST
213
VINE_WHIP_FAST
214
RAZOR_LEAF_FAST
215
MUD_SHOT_FAST
216
ICE_SHARD_FAST
217
FROST_BREATH_FAST
218
QUICK_ATTACK_FAST
219
SCRATCH_FAST
220
TACKLE_FAST
221
POUND_FAST
222
CUT_FAST
223
POISON_JAB_FAST
224
ACID_FAST
225
PSYCHO_CUT_FAST
226
ROCK_THROW_FAST
227
METAL_CLAW_FAST
228
BULLET_PUNCH_FAST
229
WATER_GUN_FAST
230
SPLASH_FAST
231
WATER_GUN_FAST_BLASTOISE
232
MUD_SLAP_FAST
233
ZEN_HEADBUTT_FAST
234
CONFUSION_FAST
235
POISON_STING_FAST
236
BUBBLE_FAST
237
FEINT_ATTACK_FAST
238
STEEL_WING_FAST
239
FIRE_FANG_FAST
ROCK_SMASH_FAST
241
ITEM_UNKNOWN
ITEM_POKE_BALL
ITEM_GREAT_BALL
ITEM_ULTRA_BALL
ITEM_MASTER_BALL
ITEM_POTION
ITEM_SUPER_POTION
ITEM_HYPER_POTION
ITEM_MAX_POTION
ITEM_REVIVE
ITEM_MAX_REVIVE
ITEM_LUCKY_EGG
ITEM_INCENSE_ORDINARY
ITEM_INCENSE_SPICY
ITEM_INCENSE_COOL
ITEM_INCENSE_FLORAL
ITEM_TROY_DISK
501
ITEM_X_ATTACK
602
ITEM_X_DEFENSE
603
ITEM_X_MIRACLE
604
ITEM_RAZZ_BERRY
ITEM_BLUK_BERRY
702
ITEM_NANAB_BERRY
703
ITEM_WEPAR_BERRY
704
ITEM_PINAP_BERRY
705
ITEM_SPECIAL_CAMERA
ITEM_INCUBATOR_BASIC_UNLIMITED
901
ITEM_INCUBATOR_BASIC
902
ITEM_POKEMON_STORAGE_UPGRADE
ITEM_ITEM_STORAGE_UPGRADE
1002
UPGRADE_UNSET
INCREASE_ITEM_STORAGE
INCREASE_POKEMON_STORAGE
ITEM_TYPE_NONE
ITEM_TYPE_POKEBALL
ITEM_TYPE_POTION
ITEM_TYPE_REVIVE
ITEM_TYPE_MAP
ITEM_TYPE_BATTLE
ITEM_TYPE_FOOD
ITEM_TYPE_CAMERA
ITEM_TYPE_DISK
ITEM_TYPE_INCUBATOR
ITEM_TYPE_INCENSE
ITEM_TYPE_XP_BOOST
ITEM_TYPE_INVENTORY_UPGRADE
INCUBATOR_UNSET
INCUBATOR_DISTANCE
FAMILY_UNSET
FAMILY_BULBASAUR
FAMILY_CHARMANDER
FAMILY_SQUIRTLE
FAMILY_CATERPIE
FAMILY_WEEDLE
FAMILY_PIDGEY
FAMILY_RATTATA
FAMILY_SPEAROW
FAMILY_EKANS
FAMILY_PIKACHU
FAMILY_SANDSHREW
FAMILY_NIDORAN
FAMILY_NIDORAN2
FAMILY_CLEFAIRY
FAMILY_VULPIX
FAMILY_JIGGLYPUFF
FAMILY_ZUBAT
FAMILY_ODDISH
FAMILY_PARAS
FAMILY_VENONAT
FAMILY_DIGLETT
FAMILY_MEOWTH
FAMILY_PSYDUCK
FAMILY_MANKEY
FAMILY_GROWLITHE
FAMILY_POLIWAG
FAMILY_ABRA
FAMILY_MACHOP
FAMILY_BELLSPROUT
FAMILY_TENTACOOL
FAMILY_GEODUDE
FAMILY_PONYTA
FAMILY_SLOWPOKE
FAMILY_MAGNEMITE
FAMILY_FARFETCHD
FAMILY_DODUO
FAMILY_SEEL
FAMILY_GRIMER
FAMILY_SHELLDER
FAMILY_GASTLY
FAMILY_ONIX
FAMILY_DROWZEE
FAMILY_KRABBY
FAMILY_VOLTORB
FAMILY_EXEGGCUTE
FAMILY_CUBONE
FAMILY_HITMONLEE
FAMILY_HITMONCHAN
FAMILY_LICKITUNG
FAMILY_KOFFING
FAMILY_RHYHORN
FAMILY_CHANSEY
FAMILY_TANGELA
FAMILY_KANGASKHAN
FAMILY_HORSEA
FAMILY_GOLDEEN
FAMILY_STARYU
FAMILY_MR_MIME
FAMILY_SCYTHER
FAMILY_JYNX
FAMILY_ELECTABUZZ
FAMILY_MAGMAR
FAMILY_PINSIR
FAMILY_TAUROS
FAMILY_MAGIKARP
FAMILY_LAPRAS
FAMILY_DITTO
FAMILY_EEVEE
FAMILY_PORYGON
FAMILY_OMANYTE
FAMILY_KABUTO
FAMILY_AERODACTYL
FAMILY_SNORLAX
FAMILY_ARTICUNO
FAMILY_ZAPDOS
FAMILY_MOLTRES
FAMILY_DRATINI
FAMILY_MEWTWO
FAMILY_MEW
UNSET_STATUS
SUCCESS
LOCATION_UNSET
GYM
CHECKPOINT
MISSINGNO
BULBASAUR
IVYSAUR
VENUSAUR
CHARMENDER
CHARMELEON
CHARIZARD
SQUIRTLE
WARTORTLE
BLASTOISE
CATERPIE
METAPOD
BUTTERFREE
WEEDLE
KAKUNA
BEEDRILL
PIDGEY
PIDGEOTTO
PIDGEOT
RATTATA
RATICATE
SPEAROW
FEAROW
EKANS
ARBOK
PIKACHU
RAICHU
SANDSHREW
SANDLASH
NIDORAN_FEMALE
NIDORINA
NIDOQUEEN
NIDORAN_MALE
NIDORINO
NIDOKING
CLEFARY
CLEFABLE
VULPIX
NINETALES
JIGGLYPUFF
WIGGLYTUFF
ZUBAT
GOLBAT
ODDISH
GLOOM
VILEPLUME
PARAS
PARASECT
VENONAT
VENOMOTH
DIGLETT
DUGTRIO
MEOWTH
PERSIAN
PSYDUCK
GOLDUCK
MANKEY
PRIMEAPE
GROWLITHE
ARCANINE
POLIWAG
POLIWHIRL
POLIWRATH
ABRA
KADABRA
ALAKHAZAM
MACHOP
MACHOKE
MACHAMP
BELLSPROUT
WEEPINBELL
VICTREEBELL
TENTACOOL
TENTACRUEL
GEODUGE
GRAVELER
GOLEM
PONYTA
RAPIDASH
SLOWPOKE
SLOWBRO
MAGNEMITE
MAGNETON
FARFETCHD
DODUO
DODRIO
SEEL
DEWGONG
GRIMER
MUK
SHELLDER
CLOYSTER
GASTLY
HAUNTER
GENGAR
ONIX
DROWZEE
HYPNO
KRABBY
KINGLER
VOLTORB
ELECTRODE
EXEGGCUTE
EXEGGUTOR
CUBONE
MAROWAK
HITMONLEE
HITMONCHAN
LICKITUNG
KOFFING
WEEZING
RHYHORN
RHYDON
CHANSEY
TANGELA
KANGASKHAN
HORSEA
SEADRA
GOLDEEN
SEAKING
STARYU
STARMIE
MR_MIME
SCYTHER
JYNX
ELECTABUZZ
MAGMAR
PINSIR
TAUROS
MAGIKARP
GYARADOS
LAPRAS
DITTO
EEVEE
VAPOREON
JOLTEON
FLAREON
PORYGON
OMANYTE
OMASTAR
KABUTO
KABUTOPS
AERODACTYL
SNORLAX
ARTICUNO
ZAPDOS
MOLTRES
DRATINI
DRAGONAIR
DRAGONITE
MEWTWO
MEW
UNSET_SPONSOR
MCDONALDS
POKEMON_STORE
INTERNAL_TEST
DESCRIPTOR.enum_types_by_name['RpcDirection']
DESCRIPTOR.enum_types_by_name['TeamColor']
DESCRIPTOR.enum_types_by_name['RequestMethod']
DESCRIPTOR.enum_types_by_name['PokemonMove']
DESCRIPTOR.enum_types_by_name['ItemType']
DESCRIPTOR.enum_types_by_name['InventoryUpgradeType']
DESCRIPTOR.enum_types_by_name['ItemTypeCategory']
DESCRIPTOR.enum_types_by_name['EggIncubatorType']
DESCRIPTOR.enum_types_by_name['PokemonFamilyId']
DESCRIPTOR.enum_types_by_name['MapObjectsStatus']
DESCRIPTOR.enum_types_by_name['FortType']
DESCRIPTOR.enum_types_by_name['PokemonId']
DESCRIPTOR.enum_types_by_name['FortSponsor']
DESCRIPTOR.enum_types_by_name['FortRenderingType']
name='RpcEnvelope.proto',
package='',
serialized_pb=_b('\n\x11RpcEnvelope.proto\x1a\rRpcEnum.proto\"\xd8\x04\n\x07Request\x12(\n\tdirection\x18\x01
\x02(\x0e\x32\x15.RpcEnum.RpcDirection\x12\x0e\n\x06rpc_id\x18\x03
\x01(\x03\x12#\n\x08requests\x18\x04
\x03(\x0b\x32\x11.Request.Requests\x12#\n\x08unknown6\x18\x06
\x01(\x0b\x32\x11.Request.Unknown6\x12\x10\n\x08latitude\x18\x07
\x01(\x06\x12\x11\n\tlongitude\x18\x08
\x01(\x06\x12\x10\n\x08\x61ltitude\x18\t
\x01(\x06\x12\x1f\n\x04\x61uth\x18\n
\x01(\x0b\x32\x11.Request.AuthInfo\x12
\n\x0b\x61uth_ticket\x18\x0b
\x01(\x0b\x32\x0b.AuthTicket\x12\x11\n\tunknown12\x18\x0c
\x01(\x03\x1a\x44\n\x08Requests\x12$\n\x04type\x18\x01
\x02(\x0e\x32\x16.RpcEnum.RequestMethod\x12\x12\n\nparameters\x18\x02
\x01(\x0c\x1an\n\x08\x41uthInfo\x12\x10\n\x08provider\x18\x01
\x02(\t\x12$\n\x05token\x18\x02
\x02(\x0b\x32\x15.Request.AuthInfo.JWT\x1a*\n\x03JWT\x12\x10\n\x08\x63ontents\x18\x01
\x02(\t\x12\x11\n\tunknown13\x18\x02
\x02(\x05\x1a\x1c\n\x08Unknown3\x12\x10\n\x08unknown4\x18\x01
\x02(\t\x1ah\n\x08Unknown6\x12\x10\n\x08unknown1\x18\x01
\x02(\x05\x12,\n\x08unknown2\x18\x02
\x02(\x0b\x32\x1a.Request.Unknown6.Unknown2\x1a\x1c\n\x08Unknown2\x12\x10\n\x08unknown1\x18\x01
\x02(\x0c\"\xe2\x02\n\x08Response\x12(\n\tdirection\x18\x01
\x02(\x0e\x32\x15.RpcEnum.RpcDirection\x12\x10\n\x08unknown2\x18\x02
\x01(\x03\x12\x0f\n\x07\x61pi_url\x18\x03
\x01(\t\x12$\n\x08unknown6\x18\x06
\x02(\x0b\x32\x12.Response.Unknown6\x12
\n\x0b\x61uth_ticket\x18\x07
\x01(\x0b\x32\x0b.AuthTicket\x12\x11\n\tresponses\x18\x64
\x03(\x0c\x1ai\n\x08Unknown6\x12\x10\n\x08unknown1\x18\x01
\x02(\x05\x12-\n\x08unknown2\x18\x02
\x02(\x0b\x32\x1b.Response.Unknown6.Unknown2\x1a\x1c\n\x08Unknown2\x12\x10\n\x08unknown1\x18\x01
\x02(\x0c\x1a\x43\n\x08Unknown7\x12\x11\n\tunknown71\x18\x01
\x01(\x0c\x12\x11\n\tunknown72\x18\x02
\x01(\x03\x12\x11\n\tunknown73\x18\x03
\x01(\x0c\"E\n\nAuthTicket\x12\r\n\x05start\x18\x01
\x01(\x0c\x12\x1b\n\x13\x65xpire_timestamp_ms\x18\x02
\x01(\x04\x12\x0b\n\x03\x65nd\x18\x03
\x01(\x0c')
name='Requests',
full_name='Request.Requests',
full_name='Request.Requests.type',
name='parameters',
full_name='Request.Requests.parameters',
serialized_start=321,
serialized_end=389,
name='JWT',
full_name='Request.AuthInfo.JWT',
name='contents',
full_name='Request.AuthInfo.JWT.contents',
full_name='Request.AuthInfo.JWT.unknown13',
serialized_start=459,
name='AuthInfo',
full_name='Request.AuthInfo',
name='provider',
full_name='Request.AuthInfo.provider',
name='token',
full_name='Request.AuthInfo.token',
nested_types=[_REQUEST_AUTHINFO_JWT,
serialized_start=391,
_REQUEST_UNKNOWN3
name='Unknown3',
full_name='Request.Unknown3',
name='unknown4',
full_name='Request.Unknown3.unknown4',
serialized_start=503,
serialized_end=531,
full_name='Request.Unknown6.Unknown2',
full_name='Request.Unknown6.Unknown2.unknown1',
full_name='Request.Unknown6',
full_name='Request.Unknown6.unknown1',
full_name='Request.Unknown6.unknown2',
nested_types=[_REQUEST_UNKNOWN6_UNKNOWN2,
serialized_start=533,
name='Request',
full_name='Request',
full_name='Request.direction',
name='rpc_id',
full_name='Request.rpc_id',
name='requests',
full_name='Request.requests',
full_name='Request.unknown6',
full_name='Request.latitude',
full_name='Request.longitude',
name='altitude',
full_name='Request.altitude',
name='auth',
full_name='Request.auth',
full_name='Request.auth_ticket',
full_name='Request.unknown12',
nested_types=[_REQUEST_REQUESTS,
serialized_start=37,
full_name='Response.Unknown6.Unknown2',
full_name='Response.Unknown6.Unknown2.unknown1',
full_name='Response.Unknown6',
full_name='Response.Unknown6.unknown1',
full_name='Response.Unknown6.unknown2',
nested_types=[_RESPONSE_UNKNOWN6_UNKNOWN2,
serialized_start=820,
serialized_end=925,
_RESPONSE_UNKNOWN7
name='Unknown7',
full_name='Response.Unknown7',
name='unknown71',
full_name='Response.Unknown7.unknown71',
name='unknown72',
full_name='Response.Unknown7.unknown72',
name='unknown73',
full_name='Response.Unknown7.unknown73',
serialized_start=927,
name='Response',
full_name='Response',
full_name='Response.direction',
full_name='Response.unknown2',
name='api_url',
full_name='Response.api_url',
full_name='Response.unknown6',
full_name='Response.auth_ticket',
name='responses',
full_name='Response.responses',
nested_types=[_RESPONSE_UNKNOWN6,
serialized_start=640,
name='AuthTicket',
full_name='AuthTicket',
name='start',
full_name='AuthTicket.start',
name='expire_timestamp_ms',
full_name='AuthTicket.expire_timestamp_ms',
name='end',
full_name='AuthTicket.end',
serialized_start=996,
serialized_end=1065,
_REQUEST_REQUESTS.fields_by_name['type'].enum_type
RpcEnum_pb2._REQUESTMETHOD
_REQUEST_REQUESTS.containing_type
_REQUEST_AUTHINFO_JWT.containing_type
_REQUEST_AUTHINFO.fields_by_name['token'].message_type
_REQUEST_AUTHINFO.containing_type
_REQUEST_UNKNOWN3.containing_type
_REQUEST_UNKNOWN6_UNKNOWN2.containing_type
_REQUEST_UNKNOWN6.fields_by_name['unknown2'].message_type
_REQUEST_UNKNOWN6.containing_type
_REQUEST.fields_by_name['direction'].enum_type
_REQUEST.fields_by_name['requests'].message_type
_REQUEST.fields_by_name['unknown6'].message_type
_REQUEST.fields_by_name['auth'].message_type
_REQUEST.fields_by_name['auth_ticket'].message_type
_RESPONSE_UNKNOWN6_UNKNOWN2.containing_type
_RESPONSE_UNKNOWN6.fields_by_name['unknown2'].message_type
_RESPONSE_UNKNOWN6.containing_type
_RESPONSE_UNKNOWN7.containing_type
_RESPONSE.fields_by_name['direction'].enum_type
_RESPONSE.fields_by_name['unknown6'].message_type
_RESPONSE.fields_by_name['auth_ticket'].message_type
DESCRIPTOR.message_types_by_name['Request']
DESCRIPTOR.message_types_by_name['Response']
DESCRIPTOR.message_types_by_name['AuthTicket']
_reflection.GeneratedProtocolMessageType('Request',
_reflection.GeneratedProtocolMessageType('Requests',
_REQUEST_REQUESTS,
AuthInfo
_reflection.GeneratedProtocolMessageType('AuthInfo',
JWT
_reflection.GeneratedProtocolMessageType('JWT',
_REQUEST_AUTHINFO_JWT,
Unknown3
_reflection.GeneratedProtocolMessageType('Unknown3',
_REQUEST_UNKNOWN6_UNKNOWN2,
_REQUEST,
_sym_db.RegisterMessage(Request)
_sym_db.RegisterMessage(Request.Requests)
_sym_db.RegisterMessage(Request.AuthInfo)
_sym_db.RegisterMessage(Request.AuthInfo.JWT)
_sym_db.RegisterMessage(Request.Unknown3)
_sym_db.RegisterMessage(Request.Unknown6)
_sym_db.RegisterMessage(Request.Unknown6.Unknown2)
_reflection.GeneratedProtocolMessageType('Response',
_RESPONSE_UNKNOWN6_UNKNOWN2,
_RESPONSE_UNKNOWN6,
Unknown7
_reflection.GeneratedProtocolMessageType('Unknown7',
_RESPONSE,
_sym_db.RegisterMessage(Response)
_sym_db.RegisterMessage(Response.Unknown6)
_sym_db.RegisterMessage(Response.Unknown6.Unknown2)
_sym_db.RegisterMessage(Response.Unknown7)
AuthTicket
_reflection.GeneratedProtocolMessageType('AuthTicket',
_AUTHTICKET,
_sym_db.RegisterMessage(AuthTicket)
name='RpcSub.proto',
package='RpcSub',
serialized_pb=_b('\n\x0cRpcSub.proto\x12\x06RpcSub\x1a\rRpcEnum.proto\"G\n\x11GetPlayerResponse\x12\x10\n\x08unknown1\x18\x01
\n\x07profile\x18\x02
\x01(\x0b\x32\x0f.RpcSub.Profile\"\xac\x02\n\x07Profile\x12\x15\n\rcreation_time\x18\x01
\x02(\x03\x12\x10\n\x08username\x18\x02
\x01(\t\x12
\n\x04team\x18\x05
\x01(\x0e\x32\x12.RpcEnum.TeamColor\x12\x10\n\x08tutorial\x18\x07
\x01(\x0c\x12%\n\x06\x61vatar\x18\x08
\x01(\x0b\x32\x15.RpcSub.AvatarDetails\x12\x14\n\x0cpoke_storage\x18\t
\x01(\x05\x12\x14\n\x0citem_storage\x18\n
\x01(\x05\x12\'\n\x0b\x64\x61ily_bonus\x18\x0b
\x01(\x0b\x32\x12.RpcSub.DailyBonus\x12\x11\n\tunknown12\x18\x0c
\x01(\x0c\x12\x11\n\tunknown13\x18\r
\x01(\x0c\x12\"\n\x08\x63urrency\x18\x0e
\x03(\x0b\x32\x10.RpcSub.Currency\"Y\n\nDailyBonus\x12\x1e\n\x16NextCollectTimestampMs\x18\x01
\x01(\x03\x12+\n#NextDefenderBonusCollectTimestampMs\x18\x02
\x01(\x03\"(\n\x08\x43urrency\x12\x0c\n\x04type\x18\x01
\x02(\t\x12\x0e\n\x06\x61mount\x18\x02
\x01(\x05\"X\n\rAvatarDetails\x12\x10\n\x08unknown2\x18\x02
\x01(\x05\x12\x10\n\x08unknown3\x18\x03
\x01(\x05\x12\x10\n\x08unknown9\x18\t
\x01(\x05\x12\x11\n\tunknown10\x18\n
\x01(\x05\"\'\n\x17\x44ownloadSettingsRequest\x12\x0c\n\x04hash\x18\x01
\x01(\t\"X\n\x14GetInventoryResponse\x12\x0f\n\x07success\x18\x01
\x01(\x08\x12/\n\x0finventory_delta\x18\x02
\x01(\x0b\x32\x16.RpcSub.InventoryDelta\"y\n\x0eInventoryDelta\x12\x1d\n\x15original_timestamp_ms\x18\x01
\x01(\x03\x12\x18\n\x10new_timestamp_ms\x18\x02
\x01(\x03\x12.\n\x0finventory_items\x18\x03
\x03(\x0b\x32\x15.RpcSub.InventoryItem\"\x80\x01\n\rInventoryItem\x12\x1d\n\x15modified_timestamp_ms\x18\x01
\x01(\x03\x12\x18\n\x10\x64\x65leted_item_key\x18\x02
\x01(\x03\x12\x36\n\x13inventory_item_data\x18\x03
\x01(\x0b\x32\x19.RpcSub.InventoryItemData\"\xc9\x03\n\x11InventoryItemData\x12
\n\x07pokemon\x18\x01
\x01(\x0b\x32\x0f.RpcSub.Pokemon\x12\x1a\n\x04item\x18\x02
\x01(\x0b\x32\x0c.RpcSub.Item\x12+\n\rpokedex_entry\x18\x03
\x01(\x0b\x32\x14.RpcSub.PokedexEntry\x12)\n\x0cplayer_stats\x18\x04
\x01(\x0b\x32\x13.RpcSub.PlayerStats\x12/\n\x0fplayer_currency\x18\x05
\x01(\x0b\x32\x16.RpcSub.PlayerCurrency\x12+\n\rplayer_camera\x18\x06
\x01(\x0b\x32\x14.RpcSub.PlayerCamera\x12\x35\n\x12inventory_upgrades\x18\x07
\x01(\x0b\x32\x19.RpcSub.InventoryUpgrades\x12+\n\rapplied_items\x18\x08
\x01(\x0b\x32\x14.RpcSub.AppliedItems\x12-\n\x0e\x65gg_incubators\x18\t
\x01(\x0b\x32\x15.RpcSub.EggIncubators\x12-\n\x0epokemon_family\x18\n
\x01(\x0b\x32\x15.RpcSub.PokemonFamily\"\xd8\x05\n\x07Pokemon\x12\n\n\x02id\x18\x01
\x01(\x05\x12(\n\x0cpokemon_type\x18\x02
\x01(\x05\"F\n\x04Item\x12\x1f\n\x04item\x18\x01
\x01(\x0e\x32\x11.RpcEnum.ItemType\x12\r\n\x05\x63ount\x18\x02
\x01(\x05\x12\x0e\n\x06unseen\x18\x03
\x01(\x08\"\x99\x01\n\x0cPokedexEntry\x12\x1c\n\x14pokedex_entry_number\x18\x01
\x01(\x05\x12\x19\n\x11times_encountered\x18\x02
\x01(\x05\x12\x16\n\x0etimes_captured\x18\x03
\x01(\x05\x12\x1e\n\x16\x65volution_stone_pieces\x18\x04
\x01(\x05\x12\x18\n\x10\x65volution_stones\x18\x05
\x01(\x05\"\xed\x04\n\x0bPlayerStats\x12\r\n\x05level\x18\x01
\x01(\x05\x12\x12\n\nexperience\x18\x02
\x01(\x03\x12\x15\n\rprev_level_xp\x18\x03
\x01(\x03\x12\x15\n\rnext_level_xp\x18\x04
\x01(\x03\x12\x11\n\tkm_walked\x18\x05
\x01(\x02\x12\x1c\n\x14pokemons_encountered\x18\x06
\x01(\x05\x12\x1e\n\x16unique_pokedex_entries\x18\x07
\x01(\x05\x12\x19\n\x11pokemons_captured\x18\x08
\x01(\x05\x12\x12\n\nevolutions\x18\t
\x01(\x05\x12\x18\n\x10poke_stop_visits\x18\n
\x01(\x05\x12\x18\n\x10pokeballs_thrown\x18\x0b
\x01(\x05\x12\x14\n\x0c\x65ggs_hatched\x18\x0c
\x01(\x05\x12\x1b\n\x13\x62ig_magikarp_caught\x18\r
\x01(\x05\x12\x19\n\x11\x62\x61ttle_attack_won\x18\x0e
\x01(\x05\x12\x1b\n\x13\x62\x61ttle_attack_total\x18\x0f
\x01(\x05\x12\x1b\n\x13\x62\x61ttle_defended_won\x18\x10
\x01(\x05\x12\x1b\n\x13\x62\x61ttle_training_won\x18\x11
\x01(\x05\x12\x1d\n\x15\x62\x61ttle_training_total\x18\x12
\x01(\x05\x12\x1d\n\x15prestige_raised_total\x18\x13
\x01(\x05\x12\x1e\n\x16prestige_dropped_total\x18\x14
\x01(\x05\x12\x18\n\x10pokemon_deployed\x18\x15
\x01(\x05\x12\x1e\n\x16pokemon_caught_by_type\x18\x16
\x01(\x0c\x12\x1c\n\x14small_rattata_caught\x18\x17
\x01(\x05\"\x1e\n\x0ePlayerCurrency\x12\x0c\n\x04gems\x18\x01
\x01(\x05\")\n\x0cPlayerCamera\x12\x19\n\x11is_default_camera\x18\x01
\x01(\x08\"I\n\x11InventoryUpgrades\x12\x34\n\x12inventory_upgrades\x18\x01
\x03(\x0b\x32\x18.RpcSub.InventoryUpgrade\"\x84\x01\n\x10InventoryUpgrade\x12\x1f\n\x04item\x18\x01
\x01(\x0e\x32\x11.RpcEnum.ItemType\x12\x33\n\x0cupgrade_type\x18\x02
\x01(\x0e\x32\x1d.RpcEnum.InventoryUpgradeType\x12\x1a\n\x12\x61\x64\x64itional_storage\x18\x03
\x01(\x05\"1\n\x0c\x41ppliedItems\x12!\n\x04item\x18\x04
\x01(\x0b\x32\x13.RpcSub.AppliedItem\"\x91\x01\n\x0b\x41ppliedItem\x12$\n\titem_type\x18\x01
\x01(\x0e\x32\x11.RpcEnum.ItemType\x12\x35\n\x12item_type_category\x18\x02
\x01(\x0e\x32\x19.RpcEnum.ItemTypeCategory\x12\x11\n\texpire_ms\x18\x03
\x01(\x03\x12\x12\n\napplied_ms\x18\x04
\x01(\x03\"<\n\rEggIncubators\x12+\n\regg_incubator\x18\x01
\x01(\x0b\x32\x14.RpcSub.EggIncubator\"\xd7\x01\n\x0c\x45ggIncubator\x12\x0f\n\x07item_id\x18\x01
\x01(\t\x12$\n\titem_type\x18\x02
\x01(\x0e\x32\x11.RpcEnum.ItemType\x12\x31\n\x0eincubator_type\x18\x03
\x01(\x0e\x32\x19.RpcEnum.EggIncubatorType\x12\x16\n\x0euses_remaining\x18\x04
\x01(\x05\x12\x12\n\npokemon_id\x18\x05
\x01(\x03\x12\x17\n\x0fstart_km_walked\x18\x06
\x01(\x01\x12\x18\n\x10target_km_walked\x18\x07
\x01(\x01\"K\n\rPokemonFamily\x12+\n\tfamily_id\x18\x01
\x01(\x0e\x32\x18.RpcEnum.PokemonFamilyId\x12\r\n\x05\x63\x61ndy\x18\x02
\x01(\x05\"h\n\x14GetMapObjectsRequest\x12\x0f\n\x07\x63\x65ll_id\x18\x01
\x01(\x0c\x12\x1a\n\x12since_timestamp_ms\x18\x02
\x01(\x0c\x12\x10\n\x08latitude\x18\x03
\x01(\x01\"f\n\x15GetMapObjectsResponse\x12\"\n\tmap_cells\x18\x01
\x03(\x0b\x32\x0f.RpcSub.MapCell\x12)\n\x06status\x18\x02
\x01(\x0e\x32\x19.RpcEnum.MapObjectsStatus\"\xa7\x03\n\x07MapCell\x12\x12\n\ns2_cell_id\x18\x01
\x01(\x04\x12\x1c\n\x14\x63urrent_timestamp_ms\x18\x02
\x01(\x03\x12\x1f\n\x05\x66orts\x18\x03
\x03(\x0b\x32\x10.RpcSub.FortData\x12(\n\x0cspawn_points\x18\x04
\x03(\x0b\x32\x12.RpcSub.SpawnPoint\x12\x17\n\x0f\x64\x65leted_objects\x18\x06
\x03(\t\x12\x19\n\x11is_truncated_list\x18\x07
\x01(\x08\x12+\n\x0e\x66ort_summaries\x18\x08
\x03(\x0b\x32\x13.RpcSub.FortSummary\x12\x32\n\x16\x64\x65\x63imated_spawn_points\x18\t
\x03(\x0b\x32\x12.RpcSub.SpawnPoint\x12*\n\rwild_pokemons\x18\x05
\x03(\x0b\x32\x13.RpcSub.WildPokemon\x12.\n\x12\x63\x61tchable_pokemons\x18\n
\x03(\x0b\x32\x12.RpcSub.MapPokemon\x12.\n\x0fnearby_pokemons\x18\x0b
\x03(\x0b\x32\x15.RpcSub.NearbyPokemon\"\xf8\x03\n\x08\x46ortData\x12\n\n\x02id\x18\x01
\x01(\t\x12\"\n\x1alast_modified_timestamp_ms\x18\x02
\x01(\x01\x12\x0f\n\x07\x65nabled\x18\x08
\x01(\x08\x12\x1f\n\x04type\x18\t
\x01(\x0e\x32\x11.RpcEnum.FortType\x12)\n\rowned_by_team\x18\x05
\x01(\x0e\x32\x12.RpcEnum.TeamColor\x12,\n\x10guard_pokemon_id\x18\x06
\x01(\x0e\x32\x12.RpcEnum.PokemonId\x12\x18\n\x10guard_pokemon_cp\x18\x07
\x01(\x05\x12\x12\n\ngym_points\x18\n
\x01(\x03\x12\x14\n\x0cis_in_battle\x18\x0b
\x01(\x08\x12&\n\x1e\x63ooldown_complete_timestamp_ms\x18\x0e
\x01(\x03\x12%\n\x07sponsor\x18\x0f
\x01(\x0e\x32\x14.RpcEnum.FortSponsor\x12\x32\n\x0erendering_type\x18\x10
\x01(\x0e\x32\x1a.RpcEnum.FortRenderingType\x12\x1c\n\x14\x61\x63tive_fort_modifier\x18\x0c
\x01(\x0c\x12\'\n\tlure_info\x18\r
\x01(\x0b\x32\x14.RpcSub.FortLureInfo\"\x83\x01\n\x0c\x46ortLureInfo\x12\x0f\n\x07\x66ort_id\x18\x01
\x01(\t\x12\x10\n\x08unknown2\x18\x02
\x01(\x01\x12-\n\x11\x61\x63tive_pokemon_id\x18\x03
\x01(\x0e\x32\x12.RpcEnum.PokemonId\x12!\n\x19lure_expires_timestamp_ms\x18\x04
\x01(\x03\"1\n\nSpawnPoint\x12\x10\n\x08latitude\x18\x02
\x01(\x01\x12\x11\n\tlongitude\x18\x03
\x01(\x01\"o\n\x0b\x46ortSummary\x12\x17\n\x0f\x66ort_summary_id\x18\x01
\x01(\x05\x12\"\n\x1alast_modified_timestamp_ms\x18\x02
\x01(\x05\x12\x10\n\x08latitude\x18\x03
\x01(\x05\x12\x11\n\tlongitude\x18\x04
\x01(\x05\"\xcb\x01\n\x0bWildPokemon\x12\x14\n\x0c\x65ncounter_id\x18\x01
\x01(\x06\x12\"\n\x1alast_modified_timestamp_ms\x18\x02
\x01(\x01\x12\x15\n\rspawnpoint_id\x18\x05
\x01(\t\x12)\n\x0cpokemon_data\x18\x07
\x01(\x0b\x32\x13.RpcSub.PokemonData\x12\x1b\n\x13time_till_hidden_ms\x18\x0b
\x01(\x05\"\xda\x05\n\x0bPokemonData\x12\n\n\x02id\x18\x01
\x01(\x05\x12&\n\npokemon_id\x18\x02
\x01(\x05\"\xa7\x01\n\nMapPokemon\x12\x15\n\rspawnpoint_id\x18\x01
\x01(\t\x12\x14\n\x0c\x65ncounter_id\x18\x02
\x01(\x06\x12&\n\npokemon_id\x18\x03
\x01(\x0e\x32\x12.RpcEnum.PokemonId\x12\x1f\n\x17\x65xpiration_timestamp_ms\x18\x04
\x01(\x03\x12\x10\n\x08latitude\x18\x05
\x01(\x01\x12\x11\n\tlongitude\x18\x06
\x01(\x01\"i\n\rNearbyPokemon\x12&\n\npokemon_id\x18\x01
\x01(\x0e\x32\x12.RpcEnum.PokemonId\x12\x1a\n\x12\x64istance_in_meters\x18\x02
\x01(\x02\x12\x14\n\x0c\x65ncounter_id\x18\x03
\x01(\x06\"a\n\x18\x44ownloadSettingsResponse\x12\r\n\x05\x65rror\x18\x01
\x01(\t\x12\x0c\n\x04hash\x18\x02
\x01(\t\x12(\n\x08settings\x18\x03
\x01(\x0b\x32\x16.RpcSub.GlobalSettings\"\xee\x01\n\x0eGlobalSettings\x12+\n\rfort_settings\x18\x02
\x01(\x0b\x32\x14.RpcSub.FortSettings\x12)\n\x0cmap_settings\x18\x03
\x01(\x0b\x32\x13.RpcSub.MapSettings\x12-\n\x0elevel_settings\x18\x04
\x01(\x0b\x32\x15.RpcSub.LevelSettings\x12\x35\n\x12inventory_settings\x18\x05
\x01(\x0b\x32\x19.RpcSub.InventorySettings\x12\x1e\n\x16minimum_client_version\x18\x06
\x01(\t\"\xe4\x01\n\x0c\x46ortSettings\x12
\n\x18interaction_range_meters\x18\x01
\x01(\x01\x12\"\n\x1amax_total_deployed_pokemon\x18\x02
\x01(\x05\x12#\n\x1bmax_player_deployed_pokemon\x18\x03
\x01(\x05\x12!\n\x19\x64\x65ploy_stamina_multiplier\x18\x04
\x01(\x01\x12
\n\x18\x64\x65ploy_attack_multiplier\x18\x05
\x01(\x01\x12$\n\x1c\x66\x61r_interaction_range_meters\x18\x06
\x01(\x01\"\x8f\x02\n\x0bMapSettings\x12\x1d\n\x15pokemon_visible_range\x18\x01
\x01(\x01\x12\x1d\n\x15poke_nav_range_meters\x18\x02
\x01(\x01\x12\x1e\n\x16\x65ncounter_range_meters\x18\x03
\x01(\x01\x12+\n#get_map_objects_min_refresh_seconds\x18\x04
\x01(\x02\x12+\n#get_map_objects_max_refresh_seconds\x18\x05
\x01(\x02\x12+\n#get_map_objects_min_distance_meters\x18\x06
\x01(\x02\x12\x1b\n\x13google_maps_api_key\x18\x07
\x01(\t\"Q\n\rLevelSettings\x12\x1b\n\x13trainer_cp_modifier\x18\x02
\x01(\x01\x12#\n\x1btrainer_difficulty_modifier\x18\x03
\x01(\x01\"\x80\x01\n\x11InventorySettings\x12\x13\n\x0bmax_pokemon\x18\x01
\x01(\x05\x12\x15\n\rmax_bag_items\x18\x02
\x01(\x05\x12\x14\n\x0c\x62\x61se_pokemon\x18\x03
\x01(\x05\x12\x16\n\x0e\x62\x61se_bag_items\x18\x04
\x01(\x05\x12\x11\n\tbase_eggs\x18\x05
\x01(\x05')
name='GetPlayerResponse',
full_name='RpcSub.GetPlayerResponse',
full_name='RpcSub.GetPlayerResponse.unknown1',
name='profile',
full_name='RpcSub.GetPlayerResponse.profile',
serialized_start=39,
serialized_end=110,
name='Profile',
full_name='RpcSub.Profile',
name='creation_time',
full_name='RpcSub.Profile.creation_time',
name='username',
full_name='RpcSub.Profile.username',
name='team',
full_name='RpcSub.Profile.team',
name='tutorial',
full_name='RpcSub.Profile.tutorial',
name='avatar',
full_name='RpcSub.Profile.avatar',
name='poke_storage',
full_name='RpcSub.Profile.poke_storage',
name='item_storage',
full_name='RpcSub.Profile.item_storage',
name='daily_bonus',
full_name='RpcSub.Profile.daily_bonus',
full_name='RpcSub.Profile.unknown12',
full_name='RpcSub.Profile.unknown13',
name='currency',
full_name='RpcSub.Profile.currency',
serialized_start=113,
serialized_end=413,
name='DailyBonus',
full_name='RpcSub.DailyBonus',
name='NextCollectTimestampMs',
full_name='RpcSub.DailyBonus.NextCollectTimestampMs',
name='NextDefenderBonusCollectTimestampMs',
full_name='RpcSub.DailyBonus.NextDefenderBonusCollectTimestampMs',
serialized_start=415,
serialized_end=504,
name='Currency',
full_name='RpcSub.Currency',
full_name='RpcSub.Currency.type',
name='amount',
full_name='RpcSub.Currency.amount',
serialized_start=506,
serialized_end=546,
name='AvatarDetails',
full_name='RpcSub.AvatarDetails',
full_name='RpcSub.AvatarDetails.unknown2',
name='unknown3',
full_name='RpcSub.AvatarDetails.unknown3',
name='unknown9',
full_name='RpcSub.AvatarDetails.unknown9',
name='unknown10',
full_name='RpcSub.AvatarDetails.unknown10',
serialized_start=548,
serialized_end=636,
name='DownloadSettingsRequest',
full_name='RpcSub.DownloadSettingsRequest',
full_name='RpcSub.DownloadSettingsRequest.hash',
serialized_start=638,
serialized_end=677,
name='GetInventoryResponse',
full_name='RpcSub.GetInventoryResponse',
name='success',
full_name='RpcSub.GetInventoryResponse.success',
name='inventory_delta',
full_name='RpcSub.GetInventoryResponse.inventory_delta',
serialized_start=679,
serialized_end=767,
name='InventoryDelta',
full_name='RpcSub.InventoryDelta',
name='original_timestamp_ms',
full_name='RpcSub.InventoryDelta.original_timestamp_ms',
name='new_timestamp_ms',
full_name='RpcSub.InventoryDelta.new_timestamp_ms',
name='inventory_items',
full_name='RpcSub.InventoryDelta.inventory_items',
serialized_start=769,
serialized_end=890,
name='InventoryItem',
full_name='RpcSub.InventoryItem',
name='modified_timestamp_ms',
full_name='RpcSub.InventoryItem.modified_timestamp_ms',
name='deleted_item_key',
full_name='RpcSub.InventoryItem.deleted_item_key',
name='inventory_item_data',
full_name='RpcSub.InventoryItem.inventory_item_data',
serialized_start=893,
serialized_end=1021,
name='InventoryItemData',
full_name='RpcSub.InventoryItemData',
name='pokemon',
full_name='RpcSub.InventoryItemData.pokemon',
full_name='RpcSub.InventoryItemData.item',
name='pokedex_entry',
full_name='RpcSub.InventoryItemData.pokedex_entry',
name='player_stats',
full_name='RpcSub.InventoryItemData.player_stats',
name='player_currency',
full_name='RpcSub.InventoryItemData.player_currency',
name='player_camera',
full_name='RpcSub.InventoryItemData.player_camera',
full_name='RpcSub.InventoryItemData.inventory_upgrades',
name='applied_items',
full_name='RpcSub.InventoryItemData.applied_items',
name='egg_incubators',
full_name='RpcSub.InventoryItemData.egg_incubators',
name='pokemon_family',
full_name='RpcSub.InventoryItemData.pokemon_family',
serialized_start=1024,
serialized_end=1481,
name='Pokemon',
full_name='RpcSub.Pokemon',
full_name='RpcSub.Pokemon.id',
name='pokemon_type',
full_name='RpcSub.Pokemon.pokemon_type',
full_name='RpcSub.Pokemon.cp',
full_name='RpcSub.Pokemon.stamina',
full_name='RpcSub.Pokemon.stamina_max',
full_name='RpcSub.Pokemon.move_1',
full_name='RpcSub.Pokemon.move_2',
full_name='RpcSub.Pokemon.deployed_fort_id',
full_name='RpcSub.Pokemon.owner_name',
full_name='RpcSub.Pokemon.is_egg',
full_name='RpcSub.Pokemon.egg_km_walked_target',
full_name='RpcSub.Pokemon.egg_km_walked_start',
full_name='RpcSub.Pokemon.origin',
full_name='RpcSub.Pokemon.height_m',
full_name='RpcSub.Pokemon.weight_kg',
full_name='RpcSub.Pokemon.individual_attack',
full_name='RpcSub.Pokemon.individual_defense',
full_name='RpcSub.Pokemon.individual_stamina',
full_name='RpcSub.Pokemon.cp_multiplier',
full_name='RpcSub.Pokemon.pokeball',
full_name='RpcSub.Pokemon.captured_cell_id',
full_name='RpcSub.Pokemon.battles_attacked',
full_name='RpcSub.Pokemon.battles_defended',
full_name='RpcSub.Pokemon.egg_incubator_id',
full_name='RpcSub.Pokemon.creation_time_ms',
full_name='RpcSub.Pokemon.num_upgrades',
full_name='RpcSub.Pokemon.additional_cp_multiplier',
full_name='RpcSub.Pokemon.favorite',
full_name='RpcSub.Pokemon.nickname',
full_name='RpcSub.Pokemon.from_fort',
serialized_start=1484,
serialized_end=2212,
name='Item',
full_name='RpcSub.Item',
full_name='RpcSub.Item.item',
name='count',
full_name='RpcSub.Item.count',
name='unseen',
full_name='RpcSub.Item.unseen',
serialized_start=2214,
serialized_end=2284,
name='PokedexEntry',
full_name='RpcSub.PokedexEntry',
name='pokedex_entry_number',
full_name='RpcSub.PokedexEntry.pokedex_entry_number',
name='times_encountered',
full_name='RpcSub.PokedexEntry.times_encountered',
name='times_captured',
full_name='RpcSub.PokedexEntry.times_captured',
name='evolution_stone_pieces',
full_name='RpcSub.PokedexEntry.evolution_stone_pieces',
name='evolution_stones',
full_name='RpcSub.PokedexEntry.evolution_stones',
serialized_start=2287,
serialized_end=2440,
name='PlayerStats',
full_name='RpcSub.PlayerStats',
name='level',
full_name='RpcSub.PlayerStats.level',
name='experience',
full_name='RpcSub.PlayerStats.experience',
name='prev_level_xp',
full_name='RpcSub.PlayerStats.prev_level_xp',
name='next_level_xp',
full_name='RpcSub.PlayerStats.next_level_xp',
name='km_walked',
full_name='RpcSub.PlayerStats.km_walked',
name='pokemons_encountered',
full_name='RpcSub.PlayerStats.pokemons_encountered',
name='unique_pokedex_entries',
full_name='RpcSub.PlayerStats.unique_pokedex_entries',
name='pokemons_captured',
full_name='RpcSub.PlayerStats.pokemons_captured',
name='evolutions',
full_name='RpcSub.PlayerStats.evolutions',
name='poke_stop_visits',
full_name='RpcSub.PlayerStats.poke_stop_visits',
name='pokeballs_thrown',
full_name='RpcSub.PlayerStats.pokeballs_thrown',
name='eggs_hatched',
full_name='RpcSub.PlayerStats.eggs_hatched',
name='big_magikarp_caught',
full_name='RpcSub.PlayerStats.big_magikarp_caught',
name='battle_attack_won',
full_name='RpcSub.PlayerStats.battle_attack_won',
name='battle_attack_total',
full_name='RpcSub.PlayerStats.battle_attack_total',
name='battle_defended_won',
full_name='RpcSub.PlayerStats.battle_defended_won',
name='battle_training_won',
full_name='RpcSub.PlayerStats.battle_training_won',
name='battle_training_total',
full_name='RpcSub.PlayerStats.battle_training_total',
name='prestige_raised_total',
full_name='RpcSub.PlayerStats.prestige_raised_total',
name='prestige_dropped_total',
full_name='RpcSub.PlayerStats.prestige_dropped_total',
name='pokemon_deployed',
full_name='RpcSub.PlayerStats.pokemon_deployed',
name='pokemon_caught_by_type',
full_name='RpcSub.PlayerStats.pokemon_caught_by_type',
name='small_rattata_caught',
full_name='RpcSub.PlayerStats.small_rattata_caught',
serialized_start=2443,
serialized_end=3064,
name='PlayerCurrency',
full_name='RpcSub.PlayerCurrency',
name='gems',
full_name='RpcSub.PlayerCurrency.gems',
serialized_start=3066,
serialized_end=3096,
name='PlayerCamera',
full_name='RpcSub.PlayerCamera',
name='is_default_camera',
full_name='RpcSub.PlayerCamera.is_default_camera',
serialized_start=3098,
serialized_end=3139,
name='InventoryUpgrades',
full_name='RpcSub.InventoryUpgrades',
full_name='RpcSub.InventoryUpgrades.inventory_upgrades',
serialized_start=3141,
serialized_end=3214,
name='InventoryUpgrade',
full_name='RpcSub.InventoryUpgrade',
full_name='RpcSub.InventoryUpgrade.item',
name='upgrade_type',
full_name='RpcSub.InventoryUpgrade.upgrade_type',
name='additional_storage',
full_name='RpcSub.InventoryUpgrade.additional_storage',
serialized_start=3217,
serialized_end=3349,
name='AppliedItems',
full_name='RpcSub.AppliedItems',
full_name='RpcSub.AppliedItems.item',
serialized_start=3351,
serialized_end=3400,
name='AppliedItem',
full_name='RpcSub.AppliedItem',
full_name='RpcSub.AppliedItem.item_type',
name='item_type_category',
full_name='RpcSub.AppliedItem.item_type_category',
name='expire_ms',
full_name='RpcSub.AppliedItem.expire_ms',
name='applied_ms',
full_name='RpcSub.AppliedItem.applied_ms',
serialized_start=3403,
serialized_end=3548,
name='EggIncubators',
full_name='RpcSub.EggIncubators',
name='egg_incubator',
full_name='RpcSub.EggIncubators.egg_incubator',
serialized_start=3550,
serialized_end=3610,
name='EggIncubator',
full_name='RpcSub.EggIncubator',
name='item_id',
full_name='RpcSub.EggIncubator.item_id',
full_name='RpcSub.EggIncubator.item_type',
name='incubator_type',
full_name='RpcSub.EggIncubator.incubator_type',
name='uses_remaining',
full_name='RpcSub.EggIncubator.uses_remaining',
full_name='RpcSub.EggIncubator.pokemon_id',
name='start_km_walked',
full_name='RpcSub.EggIncubator.start_km_walked',
name='target_km_walked',
full_name='RpcSub.EggIncubator.target_km_walked',
serialized_start=3613,
serialized_end=3828,
name='PokemonFamily',
full_name='RpcSub.PokemonFamily',
name='family_id',
full_name='RpcSub.PokemonFamily.family_id',
name='candy',
full_name='RpcSub.PokemonFamily.candy',
serialized_start=3830,
serialized_end=3905,
name='GetMapObjectsRequest',
full_name='RpcSub.GetMapObjectsRequest',
name='cell_id',
full_name='RpcSub.GetMapObjectsRequest.cell_id',
name='since_timestamp_ms',
full_name='RpcSub.GetMapObjectsRequest.since_timestamp_ms',
full_name='RpcSub.GetMapObjectsRequest.latitude',
full_name='RpcSub.GetMapObjectsRequest.longitude',
serialized_start=3907,
serialized_end=4011,
name='GetMapObjectsResponse',
full_name='RpcSub.GetMapObjectsResponse',
name='map_cells',
full_name='RpcSub.GetMapObjectsResponse.map_cells',
name='status',
full_name='RpcSub.GetMapObjectsResponse.status',
serialized_start=4013,
serialized_end=4115,
name='MapCell',
full_name='RpcSub.MapCell',
name='s2_cell_id',
full_name='RpcSub.MapCell.s2_cell_id',
name='current_timestamp_ms',
full_name='RpcSub.MapCell.current_timestamp_ms',
name='forts',
full_name='RpcSub.MapCell.forts',
name='spawn_points',
full_name='RpcSub.MapCell.spawn_points',
name='deleted_objects',
full_name='RpcSub.MapCell.deleted_objects',
name='is_truncated_list',
full_name='RpcSub.MapCell.is_truncated_list',
name='fort_summaries',
full_name='RpcSub.MapCell.fort_summaries',
name='decimated_spawn_points',
full_name='RpcSub.MapCell.decimated_spawn_points',
name='wild_pokemons',
full_name='RpcSub.MapCell.wild_pokemons',
name='catchable_pokemons',
full_name='RpcSub.MapCell.catchable_pokemons',
name='nearby_pokemons',
full_name='RpcSub.MapCell.nearby_pokemons',
serialized_start=4118,
serialized_end=4541,
name='FortData',
full_name='RpcSub.FortData',
full_name='RpcSub.FortData.id',
full_name='RpcSub.FortData.last_modified_timestamp_ms',
full_name='RpcSub.FortData.latitude',
full_name='RpcSub.FortData.longitude',
name='enabled',
full_name='RpcSub.FortData.enabled',
full_name='RpcSub.FortData.type',
name='owned_by_team',
full_name='RpcSub.FortData.owned_by_team',
name='guard_pokemon_id',
full_name='RpcSub.FortData.guard_pokemon_id',
name='guard_pokemon_cp',
full_name='RpcSub.FortData.guard_pokemon_cp',
name='gym_points',
full_name='RpcSub.FortData.gym_points',
name='is_in_battle',
full_name='RpcSub.FortData.is_in_battle',
name='cooldown_complete_timestamp_ms',
full_name='RpcSub.FortData.cooldown_complete_timestamp_ms',
name='sponsor',
full_name='RpcSub.FortData.sponsor',
name='rendering_type',
full_name='RpcSub.FortData.rendering_type',
name='active_fort_modifier',
full_name='RpcSub.FortData.active_fort_modifier',
name='lure_info',
full_name='RpcSub.FortData.lure_info',
serialized_start=4544,
serialized_end=5048,
name='FortLureInfo',
full_name='RpcSub.FortLureInfo',
name='fort_id',
full_name='RpcSub.FortLureInfo.fort_id',
full_name='RpcSub.FortLureInfo.unknown2',
name='active_pokemon_id',
full_name='RpcSub.FortLureInfo.active_pokemon_id',
name='lure_expires_timestamp_ms',
full_name='RpcSub.FortLureInfo.lure_expires_timestamp_ms',
serialized_start=5051,
serialized_end=5182,
name='SpawnPoint',
full_name='RpcSub.SpawnPoint',
full_name='RpcSub.SpawnPoint.latitude',
full_name='RpcSub.SpawnPoint.longitude',
serialized_start=5184,
serialized_end=5233,
name='FortSummary',
full_name='RpcSub.FortSummary',
name='fort_summary_id',
full_name='RpcSub.FortSummary.fort_summary_id',
full_name='RpcSub.FortSummary.last_modified_timestamp_ms',
full_name='RpcSub.FortSummary.latitude',
full_name='RpcSub.FortSummary.longitude',
serialized_start=5235,
serialized_end=5346,
name='WildPokemon',
full_name='RpcSub.WildPokemon',
full_name='RpcSub.WildPokemon.encounter_id',
full_name='RpcSub.WildPokemon.last_modified_timestamp_ms',
full_name='RpcSub.WildPokemon.latitude',
full_name='RpcSub.WildPokemon.longitude',
full_name='RpcSub.WildPokemon.spawnpoint_id',
name='pokemon_data',
full_name='RpcSub.WildPokemon.pokemon_data',
name='time_till_hidden_ms',
full_name='RpcSub.WildPokemon.time_till_hidden_ms',
serialized_start=5349,
serialized_end=5552,
name='PokemonData',
full_name='RpcSub.PokemonData',
full_name='RpcSub.PokemonData.id',
full_name='RpcSub.PokemonData.pokemon_id',
full_name='RpcSub.PokemonData.cp',
full_name='RpcSub.PokemonData.stamina',
full_name='RpcSub.PokemonData.stamina_max',
full_name='RpcSub.PokemonData.move_1',
full_name='RpcSub.PokemonData.move_2',
full_name='RpcSub.PokemonData.deployed_fort_id',
full_name='RpcSub.PokemonData.owner_name',
full_name='RpcSub.PokemonData.is_egg',
full_name='RpcSub.PokemonData.egg_km_walked_target',
full_name='RpcSub.PokemonData.egg_km_walked_start',
full_name='RpcSub.PokemonData.origin',
full_name='RpcSub.PokemonData.height_m',
full_name='RpcSub.PokemonData.weight_kg',
full_name='RpcSub.PokemonData.individual_attack',
full_name='RpcSub.PokemonData.individual_defense',
full_name='RpcSub.PokemonData.individual_stamina',
full_name='RpcSub.PokemonData.cp_multiplier',
full_name='RpcSub.PokemonData.pokeball',
full_name='RpcSub.PokemonData.captured_cell_id',
full_name='RpcSub.PokemonData.battles_attacked',
full_name='RpcSub.PokemonData.battles_defended',
full_name='RpcSub.PokemonData.egg_incubator_id',
full_name='RpcSub.PokemonData.creation_time_ms',
full_name='RpcSub.PokemonData.num_upgrades',
full_name='RpcSub.PokemonData.additional_cp_multiplier',
full_name='RpcSub.PokemonData.favorite',
full_name='RpcSub.PokemonData.nickname',
full_name='RpcSub.PokemonData.from_fort',
serialized_start=5555,
serialized_end=6285,
name='MapPokemon',
full_name='RpcSub.MapPokemon',
full_name='RpcSub.MapPokemon.spawnpoint_id',
full_name='RpcSub.MapPokemon.encounter_id',
full_name='RpcSub.MapPokemon.pokemon_id',
name='expiration_timestamp_ms',
full_name='RpcSub.MapPokemon.expiration_timestamp_ms',
full_name='RpcSub.MapPokemon.latitude',
full_name='RpcSub.MapPokemon.longitude',
serialized_start=6288,
serialized_end=6455,
name='NearbyPokemon',
full_name='RpcSub.NearbyPokemon',
full_name='RpcSub.NearbyPokemon.pokemon_id',
name='distance_in_meters',
full_name='RpcSub.NearbyPokemon.distance_in_meters',
full_name='RpcSub.NearbyPokemon.encounter_id',
serialized_start=6457,
serialized_end=6562,
name='DownloadSettingsResponse',
full_name='RpcSub.DownloadSettingsResponse',
name='error',
full_name='RpcSub.DownloadSettingsResponse.error',
full_name='RpcSub.DownloadSettingsResponse.hash',
name='settings',
full_name='RpcSub.DownloadSettingsResponse.settings',
serialized_start=6564,
serialized_end=6661,
name='GlobalSettings',
full_name='RpcSub.GlobalSettings',
name='fort_settings',
full_name='RpcSub.GlobalSettings.fort_settings',
name='map_settings',
full_name='RpcSub.GlobalSettings.map_settings',
name='level_settings',
full_name='RpcSub.GlobalSettings.level_settings',
name='inventory_settings',
full_name='RpcSub.GlobalSettings.inventory_settings',
name='minimum_client_version',
full_name='RpcSub.GlobalSettings.minimum_client_version',
serialized_start=6664,
serialized_end=6902,
name='FortSettings',
full_name='RpcSub.FortSettings',
name='interaction_range_meters',
full_name='RpcSub.FortSettings.interaction_range_meters',
name='max_total_deployed_pokemon',
full_name='RpcSub.FortSettings.max_total_deployed_pokemon',
name='max_player_deployed_pokemon',
full_name='RpcSub.FortSettings.max_player_deployed_pokemon',
name='deploy_stamina_multiplier',
full_name='RpcSub.FortSettings.deploy_stamina_multiplier',
name='deploy_attack_multiplier',
full_name='RpcSub.FortSettings.deploy_attack_multiplier',
name='far_interaction_range_meters',
full_name='RpcSub.FortSettings.far_interaction_range_meters',
serialized_start=6905,
serialized_end=7133,
name='MapSettings',
full_name='RpcSub.MapSettings',
name='pokemon_visible_range',
full_name='RpcSub.MapSettings.pokemon_visible_range',
name='poke_nav_range_meters',
full_name='RpcSub.MapSettings.poke_nav_range_meters',
name='encounter_range_meters',
full_name='RpcSub.MapSettings.encounter_range_meters',
name='get_map_objects_min_refresh_seconds',
full_name='RpcSub.MapSettings.get_map_objects_min_refresh_seconds',
name='get_map_objects_max_refresh_seconds',
full_name='RpcSub.MapSettings.get_map_objects_max_refresh_seconds',
name='get_map_objects_min_distance_meters',
full_name='RpcSub.MapSettings.get_map_objects_min_distance_meters',
name='google_maps_api_key',
full_name='RpcSub.MapSettings.google_maps_api_key',
serialized_start=7136,
serialized_end=7407,
name='LevelSettings',
full_name='RpcSub.LevelSettings',
name='trainer_cp_modifier',
full_name='RpcSub.LevelSettings.trainer_cp_modifier',
name='trainer_difficulty_modifier',
full_name='RpcSub.LevelSettings.trainer_difficulty_modifier',
serialized_start=7409,
serialized_end=7490,
name='InventorySettings',
full_name='RpcSub.InventorySettings',
name='max_pokemon',
full_name='RpcSub.InventorySettings.max_pokemon',
name='max_bag_items',
full_name='RpcSub.InventorySettings.max_bag_items',
name='base_pokemon',
full_name='RpcSub.InventorySettings.base_pokemon',
name='base_bag_items',
full_name='RpcSub.InventorySettings.base_bag_items',
name='base_eggs',
full_name='RpcSub.InventorySettings.base_eggs',
serialized_start=7493,
serialized_end=7621,
_GETPLAYERRESPONSE.fields_by_name['profile'].message_type
_PROFILE.fields_by_name['team'].enum_type
_PROFILE.fields_by_name['avatar'].message_type
_PROFILE.fields_by_name['daily_bonus'].message_type
_PROFILE.fields_by_name['currency'].message_type
_GETINVENTORYRESPONSE.fields_by_name['inventory_delta'].message_type
_INVENTORYDELTA.fields_by_name['inventory_items'].message_type
_INVENTORYITEM.fields_by_name['inventory_item_data'].message_type
_INVENTORYITEMDATA.fields_by_name['pokemon'].message_type
_INVENTORYITEMDATA.fields_by_name['item'].message_type
_INVENTORYITEMDATA.fields_by_name['pokedex_entry'].message_type
_INVENTORYITEMDATA.fields_by_name['player_stats'].message_type
_INVENTORYITEMDATA.fields_by_name['player_currency'].message_type
_INVENTORYITEMDATA.fields_by_name['player_camera'].message_type
_INVENTORYITEMDATA.fields_by_name['inventory_upgrades'].message_type
_INVENTORYITEMDATA.fields_by_name['applied_items'].message_type
_INVENTORYITEMDATA.fields_by_name['egg_incubators'].message_type
_INVENTORYITEMDATA.fields_by_name['pokemon_family'].message_type
_POKEMON.fields_by_name['pokemon_type'].enum_type
_POKEMON.fields_by_name['move_1'].enum_type
_POKEMON.fields_by_name['move_2'].enum_type
_ITEM.fields_by_name['item'].enum_type
_INVENTORYUPGRADES.fields_by_name['inventory_upgrades'].message_type
_INVENTORYUPGRADE.fields_by_name['item'].enum_type
_INVENTORYUPGRADE.fields_by_name['upgrade_type'].enum_type
RpcEnum_pb2._INVENTORYUPGRADETYPE
_APPLIEDITEMS.fields_by_name['item'].message_type
_APPLIEDITEM.fields_by_name['item_type'].enum_type
_APPLIEDITEM.fields_by_name['item_type_category'].enum_type
RpcEnum_pb2._ITEMTYPECATEGORY
_EGGINCUBATORS.fields_by_name['egg_incubator'].message_type
_EGGINCUBATOR.fields_by_name['item_type'].enum_type
_EGGINCUBATOR.fields_by_name['incubator_type'].enum_type
RpcEnum_pb2._EGGINCUBATORTYPE
_POKEMONFAMILY.fields_by_name['family_id'].enum_type
RpcEnum_pb2._POKEMONFAMILYID
_GETMAPOBJECTSRESPONSE.fields_by_name['map_cells'].message_type
_GETMAPOBJECTSRESPONSE.fields_by_name['status'].enum_type
RpcEnum_pb2._MAPOBJECTSSTATUS
_MAPCELL.fields_by_name['forts'].message_type
_MAPCELL.fields_by_name['spawn_points'].message_type
_MAPCELL.fields_by_name['fort_summaries'].message_type
_MAPCELL.fields_by_name['decimated_spawn_points'].message_type
_MAPCELL.fields_by_name['wild_pokemons'].message_type
_MAPCELL.fields_by_name['catchable_pokemons'].message_type
_MAPCELL.fields_by_name['nearby_pokemons'].message_type
_FORTDATA.fields_by_name['type'].enum_type
RpcEnum_pb2._FORTTYPE
_FORTDATA.fields_by_name['owned_by_team'].enum_type
_FORTDATA.fields_by_name['guard_pokemon_id'].enum_type
_FORTDATA.fields_by_name['sponsor'].enum_type
RpcEnum_pb2._FORTSPONSOR
_FORTDATA.fields_by_name['rendering_type'].enum_type
RpcEnum_pb2._FORTRENDERINGTYPE
_FORTDATA.fields_by_name['lure_info'].message_type
_FORTLUREINFO.fields_by_name['active_pokemon_id'].enum_type
_WILDPOKEMON.fields_by_name['pokemon_data'].message_type
_POKEMONDATA.fields_by_name['pokemon_id'].enum_type
_POKEMONDATA.fields_by_name['move_1'].enum_type
_POKEMONDATA.fields_by_name['move_2'].enum_type
_MAPPOKEMON.fields_by_name['pokemon_id'].enum_type
_NEARBYPOKEMON.fields_by_name['pokemon_id'].enum_type
_DOWNLOADSETTINGSRESPONSE.fields_by_name['settings'].message_type
_GLOBALSETTINGS.fields_by_name['fort_settings'].message_type
_GLOBALSETTINGS.fields_by_name['map_settings'].message_type
_GLOBALSETTINGS.fields_by_name['level_settings'].message_type
_GLOBALSETTINGS.fields_by_name['inventory_settings'].message_type
DESCRIPTOR.message_types_by_name['GetPlayerResponse']
DESCRIPTOR.message_types_by_name['Profile']
DESCRIPTOR.message_types_by_name['DailyBonus']
DESCRIPTOR.message_types_by_name['Currency']
DESCRIPTOR.message_types_by_name['AvatarDetails']
DESCRIPTOR.message_types_by_name['DownloadSettingsRequest']
DESCRIPTOR.message_types_by_name['GetInventoryResponse']
DESCRIPTOR.message_types_by_name['InventoryDelta']
DESCRIPTOR.message_types_by_name['InventoryItem']
DESCRIPTOR.message_types_by_name['InventoryItemData']
DESCRIPTOR.message_types_by_name['Pokemon']
DESCRIPTOR.message_types_by_name['Item']
DESCRIPTOR.message_types_by_name['PokedexEntry']
DESCRIPTOR.message_types_by_name['PlayerStats']
DESCRIPTOR.message_types_by_name['PlayerCurrency']
DESCRIPTOR.message_types_by_name['PlayerCamera']
DESCRIPTOR.message_types_by_name['InventoryUpgrades']
DESCRIPTOR.message_types_by_name['InventoryUpgrade']
DESCRIPTOR.message_types_by_name['AppliedItems']
DESCRIPTOR.message_types_by_name['AppliedItem']
DESCRIPTOR.message_types_by_name['EggIncubators']
DESCRIPTOR.message_types_by_name['EggIncubator']
DESCRIPTOR.message_types_by_name['PokemonFamily']
DESCRIPTOR.message_types_by_name['GetMapObjectsRequest']
DESCRIPTOR.message_types_by_name['GetMapObjectsResponse']
DESCRIPTOR.message_types_by_name['MapCell']
DESCRIPTOR.message_types_by_name['FortData']
DESCRIPTOR.message_types_by_name['FortLureInfo']
DESCRIPTOR.message_types_by_name['SpawnPoint']
DESCRIPTOR.message_types_by_name['FortSummary']
DESCRIPTOR.message_types_by_name['WildPokemon']
DESCRIPTOR.message_types_by_name['PokemonData']
DESCRIPTOR.message_types_by_name['MapPokemon']
DESCRIPTOR.message_types_by_name['NearbyPokemon']
DESCRIPTOR.message_types_by_name['DownloadSettingsResponse']
DESCRIPTOR.message_types_by_name['GlobalSettings']
DESCRIPTOR.message_types_by_name['FortSettings']
DESCRIPTOR.message_types_by_name['MapSettings']
DESCRIPTOR.message_types_by_name['LevelSettings']
DESCRIPTOR.message_types_by_name['InventorySettings']
GetPlayerResponse
_reflection.GeneratedProtocolMessageType('GetPlayerResponse',
_GETPLAYERRESPONSE,
_sym_db.RegisterMessage(GetPlayerResponse)
Profile
_reflection.GeneratedProtocolMessageType('Profile',
_PROFILE,
_sym_db.RegisterMessage(Profile)
DailyBonus
_reflection.GeneratedProtocolMessageType('DailyBonus',
_DAILYBONUS,
_sym_db.RegisterMessage(DailyBonus)
Currency
_reflection.GeneratedProtocolMessageType('Currency',
_CURRENCY,
_sym_db.RegisterMessage(Currency)
AvatarDetails
_reflection.GeneratedProtocolMessageType('AvatarDetails',
_AVATARDETAILS,
_sym_db.RegisterMessage(AvatarDetails)
DownloadSettingsRequest
_reflection.GeneratedProtocolMessageType('DownloadSettingsRequest',
_DOWNLOADSETTINGSREQUEST,
_sym_db.RegisterMessage(DownloadSettingsRequest)
GetInventoryResponse
_reflection.GeneratedProtocolMessageType('GetInventoryResponse',
_GETINVENTORYRESPONSE,
_sym_db.RegisterMessage(GetInventoryResponse)
InventoryDelta
_reflection.GeneratedProtocolMessageType('InventoryDelta',
_INVENTORYDELTA,
_sym_db.RegisterMessage(InventoryDelta)
InventoryItem
_reflection.GeneratedProtocolMessageType('InventoryItem',
_INVENTORYITEM,
_sym_db.RegisterMessage(InventoryItem)
InventoryItemData
_reflection.GeneratedProtocolMessageType('InventoryItemData',
_INVENTORYITEMDATA,
_sym_db.RegisterMessage(InventoryItemData)
_reflection.GeneratedProtocolMessageType('Pokemon',
_POKEMON,
_sym_db.RegisterMessage(Pokemon)
_reflection.GeneratedProtocolMessageType('Item',
_ITEM,
_sym_db.RegisterMessage(Item)
PokedexEntry
_reflection.GeneratedProtocolMessageType('PokedexEntry',
_POKEDEXENTRY,
_sym_db.RegisterMessage(PokedexEntry)
PlayerStats
_reflection.GeneratedProtocolMessageType('PlayerStats',
_PLAYERSTATS,
_sym_db.RegisterMessage(PlayerStats)
PlayerCurrency
_reflection.GeneratedProtocolMessageType('PlayerCurrency',
_PLAYERCURRENCY,
_sym_db.RegisterMessage(PlayerCurrency)
PlayerCamera
_reflection.GeneratedProtocolMessageType('PlayerCamera',
_PLAYERCAMERA,
_sym_db.RegisterMessage(PlayerCamera)
InventoryUpgrades
_reflection.GeneratedProtocolMessageType('InventoryUpgrades',
_INVENTORYUPGRADES,
_sym_db.RegisterMessage(InventoryUpgrades)
InventoryUpgrade
_reflection.GeneratedProtocolMessageType('InventoryUpgrade',
_INVENTORYUPGRADE,
_sym_db.RegisterMessage(InventoryUpgrade)
AppliedItems
_reflection.GeneratedProtocolMessageType('AppliedItems',
_APPLIEDITEMS,
_sym_db.RegisterMessage(AppliedItems)
AppliedItem
_reflection.GeneratedProtocolMessageType('AppliedItem',
_APPLIEDITEM,
_sym_db.RegisterMessage(AppliedItem)
EggIncubators
_reflection.GeneratedProtocolMessageType('EggIncubators',
_EGGINCUBATORS,
_sym_db.RegisterMessage(EggIncubators)
EggIncubator
_reflection.GeneratedProtocolMessageType('EggIncubator',
_EGGINCUBATOR,
_sym_db.RegisterMessage(EggIncubator)
PokemonFamily
_reflection.GeneratedProtocolMessageType('PokemonFamily',
_POKEMONFAMILY,
_sym_db.RegisterMessage(PokemonFamily)
GetMapObjectsRequest
_reflection.GeneratedProtocolMessageType('GetMapObjectsRequest',
_GETMAPOBJECTSREQUEST,
_sym_db.RegisterMessage(GetMapObjectsRequest)
GetMapObjectsResponse
_reflection.GeneratedProtocolMessageType('GetMapObjectsResponse',
_GETMAPOBJECTSRESPONSE,
_sym_db.RegisterMessage(GetMapObjectsResponse)
MapCell
_reflection.GeneratedProtocolMessageType('MapCell',
_MAPCELL,
_sym_db.RegisterMessage(MapCell)
FortData
_reflection.GeneratedProtocolMessageType('FortData',
_FORTDATA,
_sym_db.RegisterMessage(FortData)
FortLureInfo
_reflection.GeneratedProtocolMessageType('FortLureInfo',
_FORTLUREINFO,
_sym_db.RegisterMessage(FortLureInfo)
SpawnPoint
_reflection.GeneratedProtocolMessageType('SpawnPoint',
_SPAWNPOINT,
_sym_db.RegisterMessage(SpawnPoint)
FortSummary
_reflection.GeneratedProtocolMessageType('FortSummary',
_FORTSUMMARY,
_sym_db.RegisterMessage(FortSummary)
WildPokemon
_reflection.GeneratedProtocolMessageType('WildPokemon',
_WILDPOKEMON,
_sym_db.RegisterMessage(WildPokemon)
PokemonData
_reflection.GeneratedProtocolMessageType('PokemonData',
_POKEMONDATA,
_sym_db.RegisterMessage(PokemonData)
MapPokemon
_reflection.GeneratedProtocolMessageType('MapPokemon',
_MAPPOKEMON,
_sym_db.RegisterMessage(MapPokemon)
NearbyPokemon
_reflection.GeneratedProtocolMessageType('NearbyPokemon',
_NEARBYPOKEMON,
_sym_db.RegisterMessage(NearbyPokemon)
DownloadSettingsResponse
_reflection.GeneratedProtocolMessageType('DownloadSettingsResponse',
_DOWNLOADSETTINGSRESPONSE,
_sym_db.RegisterMessage(DownloadSettingsResponse)
GlobalSettings
_reflection.GeneratedProtocolMessageType('GlobalSettings',
_GLOBALSETTINGS,
_sym_db.RegisterMessage(GlobalSettings)
FortSettings
_reflection.GeneratedProtocolMessageType('FortSettings',
_FORTSETTINGS,
_sym_db.RegisterMessage(FortSettings)
MapSettings
_reflection.GeneratedProtocolMessageType('MapSettings',
_MAPSETTINGS,
_sym_db.RegisterMessage(MapSettings)
LevelSettings
_reflection.GeneratedProtocolMessageType('LevelSettings',
_LEVELSETTINGS,
_sym_db.RegisterMessage(LevelSettings)
InventorySettings
_reflection.GeneratedProtocolMessageType('InventorySettings',
_INVENTORYSETTINGS,
_sym_db.RegisterMessage(InventorySettings)
iterbytes(buf):
(ord(byte)
buf)
b85decode
_b85alphabet
(b"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
b"abcdefghijklmnopqrstuvwxyz!#$%&()*+-;<=>?@^_`{|}~")
b85decode(b):
_b85dec
256
enumerate(iterbytes(_b85alphabet)):
(-len(b))
b'~'
packI
struct.Struct('!I').pack
len(b),
b[i:i
iterbytes(chunk):
enumerate(iterbytes(chunk)):
base85
out.append(packI(acc))
struct.error:
ValueError('base85
overflow
b''.join(out)
result[:-padding]
bootstrap(tmpdir=None):
pip
pip.commands.install
InstallCommand
pip.req
InstallRequirement
CertInstallCommand(InstallCommand):
parse_args(self,
self.parser.get_default_values().cert:
self.parser.defaults["cert"]
below
super(CertInstallCommand,
self).parse_args(args)
pip.commands_dict["install"]
CertInstallCommand
"--no-setuptools"
os.environ.get("PIP_NO_SETUPTOOLS"):
"--no-setuptools"]
"--no-wheel"
os.environ.get("PIP_NO_WHEEL"):
"--no-wheel"]
wheel
InstallRequirement.from_line(arg)
"pip":
"setuptools":
"wheel":
implicit_pip:
["pip"]
["setuptools"]
["wheel"]
"cacert.pem")
open(cert_path,
cert.write(pkgutil.get_data("pip._vendor.requests",
"cacert.pem"))
sys.exit(pip.main(["install",
"--upgrade"]
pip_zip
"pip.zip")
open(pip_zip,
fp.write(b85decode(DATA.replace(b"\n",
b"")))
pip_zip)
bootstrap(tmpdir=tmpdir)
DATA
parser.add_argument("-lat",
"--lat",
help="latitude")
parser.add_argument("-lon",
"--lon",
help="longitude")
parser.add_argument("-st",
"--steps",
help="steps")
parser.add_argument("-lp",
"--leaps",
help="like
'steps'
workers
scans")
6378137.0
149.9497/2.0
(int)(args.steps)
wst
(int)(args.leaps)
wst):
6*(i)
lat[0]
math.radians((float)(args.lat))
lon[0]
math.radians((float)(args.lon))
turn_count
(wst
jump_points[0]
jump_points[1]
range(2,wst
jump_points[i]
jump_points[i-1]
*(i-1)
total_workers):
lat[i
lon[i
lat[jump_points[jump-1]]
lon[jump_points[jump-1]]
math.sin(lat1)*math.cos(d/R)
math.cos(lat1)*math.sin(d/R)*math.cos(brng))
math.atan2(math.sin(brng)*math.sin(d/R)*math.cos(lat1),
math.cos(d/R)-math.sin(lat1)*math.sin(lat2))
lat[i]
lon[i]
jump_points:
math.radians(60)
turn_steps:
math.radians(60.0)
range(total_workers):
str(math.degrees(lat[i]))
str(math.degrees(lon[i]))
scripts
"boto.fps",
"boto.emr",
%s/%s'
Config()
'endpoints.json')
format_string=None):
format_string:
format_string
"%(asctime)s
%(name)s
[%(levelname)s]:%(message)s"
logging.getLogger(name)
logger.setLevel(level)
fh
fh.setLevel(level)
logging.Formatter(format_string)
fh.setFormatter(formatter)
logger.addHandler(fh)
S3Connection(aws_access_key_id,
gs_secret_access_key=None,
gs_secret_access_key,
EC2Connection(aws_access_key_id,
port=8773,
'euca_access_key_id',
'euca_secret_access_key',
config.get('Boto',
'is_secure'
calling_format=OrdinaryCallingFormat(),
boto.sts
boto.cloudsearch.layer2
boto.cloudsearch2.layer2
boto.route53.domains.layer1
Route53DomainsConnection
boto.cognito.identity.layer1
CognitoIdentityConnection
boto.cognito.sync.layer1
CognitoSyncConnection
boto.codedeploy.layer1
CodeDeployConnection
boto.configservice.layer1
ConfigServiceConnection
boto.cloudhsm.layer1
CloudHSMConnection
end_scheme_idx
InvalidUriError('Invalid
md
match.groupdict()
versionless_uri_str
md['versionless_uri_str']
versionless_uri_str.split('/',
object_name,
suppress_consec_slashes=suppress_consec_slashes,
is_latest=is_latest)
key.bucket.name,
boto.auth_handler
parse_qs_safe
sha1
'.eu-central',
'-ap-northeast-2',
'.ap-south-1',
digestmod=sha)
self._hmac_256:
digestmod
string_to_sign):
copy.copy(self.__dict__)
dct):
['anon']
self).update_provider(provider)
http_request.auth_path
boto.utils.canonical_string(method,
auth_hdr
self._provider.auth_header
("%s
%s:%s"
(auth_hdr,
b64_hmac))
boto.log.debug('Signature:\n%s'
headers['Authorization']
self.sign_string(headers['Date'])
'route53',
AWSAccessKeyId=%s,"
lname.startswith('x-amz'):
canonical_headers(self,
headers_to_sign])
string_to_sign(self,
self.headers_to_sign(http_request)
canonical_headers
http_request.auth_path,
canonical_headers,
'X-Amzn-Authorization'
req.headers['X-Amz-Security-Token']
self.algorithm()
hmac.new(key,
msg.encode('utf-8'),
self.host_header(self.host,
http_request)
host_header_value}
host_header(self,
http_request.port
http_request.protocol
((port
secure)
(port
secure)):
pname
safe='-_~'))
canonical_query_string(self,
sorted(http_request.params):
boto.utils.get_utf8_value(http_request.params[param])
(urllib.parse.quote(param,
safe='-_.~'),
urllib.parse.quote(value,
safe='-_.~')))
'&'.join(l)
c_value
n.lower().strip()
';'.join(l)
canonical_uri(self,
payload(self,
hasattr(body,
'seek')
scope.append(http_request.timestamp)
scope.append(http_request.region_name)
scope.append(http_request.service_name)
scope.append('aws4_request')
'/'.join(scope)
determine_region_name(self,
parts[1]
parts[0]
determine_service_name(self,
req.headers['Content-Type']
req.headers['Content-Length']
str(len(req.body))
req.path.split('?')[0]
self.canonical_request(req)
canonical_request)
self.string_to_sign(req,
self.signature(req,
signature)
self.headers_to_sign(req)
's3'
modified_req
copy_params
req.headers['x-amz-content-sha256']
iso_date=None):
['sts-anon']
urllib.parse.quote(value)
list(params.keys())
keys.sort(key=lambda
http_request.params
qs)
self.SignatureVersion
params['Timestamp']
boto.utils.get_ts()
boto.log.debug('query_string:
Signature:
signature))
http_request.path
urllib.parse.quote_plus(signature))
params['Action']
urllib.parse.quote(val))
base64.b64encode(hmac.digest()))
'ecs',
'sdb',
'iam',
'rds',
b64)
['mws']
requested_capability=None):
auth_handlers
ready
credentials'
_wrapper(self):
'use-sigv4',
'region'):
SIGV4_DETECT:
func(self)
zip
boto.vendored.six.moves.urllib.parse
unquote_plus
ConfigParser,
NoSectionError
parse_qs
is_text_type:
qs_dict
decoded_value
HAVE_HTTPS_CONNECTION
https_connection
DEFAULT_CA_CERTS_FILE
len(self.queue)
self.clean()
self.queue.pop(0)
clean(self):
ConnectionPool.STALE_DURATION
config.getfloat('Boto',
get_http_connection(self,
self.host_to_pool:
put_http_connection(self,
self.params
'Transfer-Encoding'
headers['Transfer-Encoding']
'chunked'
self.headers['Transfer-Encoding']
self.path,
self.params,
'_headers_quoted',
self.headers:
self.headers[key]
amt
'https_validate_certificates',
available.
'ca_certificates_file',
DEFAULT_CA_CERTS_FILE)
self.ca_certificates_file
six.integer_types):
self.auth_service_name
self.host)
getattr(self._auth_handler,
'service_name')
'region_name')
path[-1]
need_trailing
sys.version[:3]
'2.5')
'proxy_user',
'proxy_pass',
self.no_proxy
hostonly
host.split(':')[0]
http_connection_kwargs['port']
http_connection_kwargs)
self.https_connection_factory:
self.is_secure:
socket.create_connection((self.proxy,
%s\r\n"
sock.sendall("\r\n")
"wrapping
"CA
file=%s"
"using
certs"
ssl.wrap_socket(sock,
cert_reqs=ssl.CERT_REQUIRED,
0)[0]
self.proxy_pass)
self.port))
request.headers['Host']
override_num_retries
self.get_http_connection(request.host,
min(random.random()
self.provider.security_token)
self._required_auth_capability():
connection.getresponse()
getattr(response,
%3.1f
body.decode('utf-8')
conn_header_value
self.request_hook.handle_request_data(request,
request.host,
boto.log.debug('encountered
self.new_http_connection(request.host,
unretryable
e.__class__.__name__)
self.get_path(auth_path)
boto.utils.find_matching_headers('host',
self.build_base_http_request(method,
sender,
compat
self).__init__(
profile_name=profile_name,
provider=provider)
self.APIVersion
params['%s.%d'
names):
full_key
(current_prefix,
params[full_key]
get_list(self,
ResultSet(markers)
self).__init__(reason,
'BotoClientError:
super(BotoServerError,
'Error'
self.body.get('message',
json.loads(self.body)
parsed:
parsed['Error']:
'Message'
'error_message':
%s\n%s'
ConsoleOutput(object):
'output':
super(StorageCreateError,
'BucketName':
self.detail
'SQSDecodeError:
'Resource':
super(EC2ResponseError,
disposition):
self).__init__(message,
self.disposition
self.disposition)
"credentials
XmlHandler(xml.sax.ContentHandler):
self.nodes
[('root',
root_node)]
new_node
self.nodes[-1][1].startElement(name,
self.nodes.append((name,
self.nodes[-1][1].endElement(name,
self.current_text,
self.nodes[-1][0]
self.nodes.pop()
ValidateCertificateHostname(cert,
default_port
self.ca_certs
socket.create_connection((self.host,
utils
s.encode('utf-8')
element_name=None,
item_marker=('member',
'item'),
pythonize_name=False):
self.element_name
self.list_marker
self.item_marker
self.stack
self.pythonize_name
self.pythonize_name:
utils.pythonize_name(name)
lm
self.list_marker:
name.endswith(lm):
ListElement(self.connection,
len(self.stack)
Element(self.connection,
isinstance(self.parent,
self.get_name(name),
requested_capability:
InvalidInstanceMetadataError
CannedGSACLStrings
NO_CREDENTIALS_PROVIDED
('aws_access_key_id',
'gs_secret_access_key',
Policy,
HEADER_PREFIX_KEY:
METADATA_PREFIX_KEY:
'meta-',
ACL_HEADER_KEY:
AUTH_HEADER_KEY:
COPY_SOURCE_HEADER_KEY:
'copy-source',
COPY_SOURCE_VERSION_ID_HEADER_KEY:
'copy-source-version-id',
COPY_SOURCE_RANGE_HEADER_KEY:
DATE_HEADER_KEY:
'date',
DELETE_MARKER_HEADER_KEY:
'delete-marker',
METADATA_DIRECTIVE_HEADER_KEY:
'metadata-directive',
RESUMABLE_UPLOAD_HEADER_KEY:
SECURITY_TOKEN_HEADER_KEY:
'security-token',
SERVER_SIDE_ENCRYPTION_KEY:
VERSION_ID_HEADER_KEY:
'version-id',
STORAGE_CLASS_HEADER_KEY:
MFA_HEADER_KEY:
RESTORE_HEADER_KEY:
STORAGE_COPY_ERROR:
STORAGE_CREATE_ERROR:
STORAGE_DATA_ERROR:
STORAGE_PERMISSIONS_ERROR:
STORAGE_RESPONSE_ERROR:
access_key=None,
secret_key=None,
self.shared_credentials
Config(do_load=False)
seconds_left
delta.days
3600)
profile_name_name
shared.has_option(profile_name,
shared.get(profile_name,
"file
profile
ProfileNotFoundError('Profile
found!'
shared.has_option('default',
shared.get('default',
data='meta-data/iam/security-credentials/')
"%Y-%m-%dT%H:%M:%SZ")
header_info_map[
get_provider_name(self):
merge_endpoints(defaults,
service,
load_endpoint_json(boto.ENDPOINTS_PATH)
os.environ['BOTO_ENDPOINTS']
additional
region_cls
region_objs
region_name,
{}).items():
self.connection_cls
connection_cls
'regionName':
handle_request_data(self,
error=False):
marker_elem=None):
self.next_key_marker
self.next_upload_id_marker
self.next_version_id_marker
self.next_generation_marker
self.markers:
User()
'NextMarker':
self.upload_id_marker
'Success')
'nextToken':
connection.box_usage
'True')
__nonzero__(self):
equals(self,
uri.uri
'Attempt
(e.g.,
off
-R
ls
issubclass(type(self),
bucket-less
object-less
secret_access_key,
**connection_args)
boto.file.connection
FileConnection
has_version(self):
(issubclass(type(self),
((self.version_id
(self.generation
self._check_object_uri('delete_key')
list_bucket(self,
validate,
'bucket',
bucket.new_key(self.object_name)
get_contents_to_stream(self,
self.get_key(None,
res_download_handler,
response_headers,
response_headers)
acl_class(self):
acl_class
canned_acls(self):
canned_acls
bucket_name=None,
object_name=None,
int(generation)
self.is_version_specific
'%s#%s'
self.version_id)
('%s://%s/'
self.bucket_name))
('%s://'
'version_id',
'generation',
'is_latest',
BucketStorageUri(
debug=self.debug,
clone_replace_key(self,
key.version_id
key.generation
key.provider.get_provider_name(),
bucket_name=key.bucket.name,
object_name=key.name,
suppress_consec_slashes=self.suppress_consec_slashes,
bucket.get_acl(key_name,
self.check_response(acl,
'cors',
cors,
'URIs.'
self.object_name,
ValueError('add_group_email_grant()
key.add_group_email_grant(permission,
bool(self.object_name)
self.bucket_name)
self.names_singleton()
conn.create_bucket(self.bucket_name,
policy)
set_def_xml_acl(self,
set_def_canned_acl(self,
reduced_redundancy=False):
self.new_key(headers=headers)
ignored
key.set_contents_from_string(
self._update_from_key(key)
rewind=False,
key.set_contents_from_file(
res_upload_handler:
src_generation=None):
src_generation:
dst_bucket.copy_key(
new_key_name=self.object_name,
src_key_name=src_key_name,
src_version_id=src_version_id,
target_prefix=None,
target_prefix,
main_page_suffix=None,
error_key,
get_versioning_config(self,
self.get_bucket(False,
set_metadata(self,
metadata_plus,
compose(self,
components,
content_type=None,
suri
components:
'lifecycle',
sha512
_hashfn
interesting_headers['date']
expires:
"%s\n"
len(t)
qsa]
metadata_prefix
provider.metadata_prefix
final_headers
k.lower()
num_retries=10,
opener
if(not
boto.log.error('Unable
num_retries=self._num_retries,
data.split('\n')
self._leaves[key]
'{':
json.loads(val)
"encountered
e.__class__.__name__,
self).__repr__()
urllib.error.URLError:
boto.log.exception("Exception
iid
'%Y-%m-%dT%H:%M:%S.%fZ'
locale.setlocale(locale.LC_ALL)
m)
dme_id,
dme_url
file=None,
password=None):
Retrieving
self.fail_fast
boto.log.info('running:%s'
cwd=cwd)
setReadOnly(self,
getStatus(self):
property(getStatus,
fromaddr,
toaddrs,
capacity):
len(self._dict)
self._dict[key]
self._update_item(item)
item.value
self._manage_size()
self.head.previous
_update_item(self,
hashfunc=None):
self.str
self.hashfunc
str(self.str)
attachments
to_string:
to_string
email.mime.multipart.MIMEMultipart()
boto.config.get_value("Notification",
server.ehlo()
six.text_type(value)
con
mime_con
rtype
compute_md5(fp,
buf_size=8192,
compute_hash(fp,
buf_size:
fp.read(size)
fp.read(buf_size)
base64_digest
'\n':
find_matching_headers(name,
matching_headers
host_is_ipv6(hostname):
event_source,
'/2014-11-13/event-source-mappings/'
batch_size
params['Parameters']
expected_status=204)
uuid):
'/2014-11-13/event-source-mappings/{0}'.format(uuid)
'/2014-11-13/functions/{0}/configuration'.format(function_name)
OSError,
IOError):
"File-like
seekable."
event_source_arn
function_name
query_params['Marker']
query_params['MaxItems']
memory_size=None):
query_params['Role']
query_params['Handler']
query_params['Description']
query_params['Timeout']
query_params['MemorySize']
mode,
simple_e
e.body)
Layer1(AWSQueryConnection):
'2010-12-01'
_encode_bool(self,
bool(v)
"false"}[v]
create_application(self,
tier_name=None,
tier_type=None,
tier_version='1.0'):
environment_name}
tier_version:
params['Tier.Name']
params['Tier.Type']
params['Tier.Version']
tier_version
params={})
delete_application(self,
application_names=None):
request_id=None,
info_type='tail',
{'InfoType':
info_type}
update_application(self,
self._repr_by_type(value)
self.response_metadata
str(response['RequestId'])
response['ConfigurationTemplates']:
response['Versions']:
self.source_bundle
self.regex
response['ValueOptions']:
self.option_name
str(response['OptionName'])
response['AutoScalingGroups']:
response['Instances']:
response['LaunchConfigurations']:
response['LoadBalancers']:
response['Triggers']:
self.severity
str(response['Severity'])
response['Listeners']:
self.label
response['PermittedFileTypes']:
self.s3_bucket
str(response['S3Bucket'])
self.available
response['Application']:
ApplicationDescription(response['Application'])
response['ApplicationVersion']:
ApplicationVersionDescription(response['ApplicationVersion'])
response['ApplicationVersions']:
response['Applications']:
response['Options']:
response['ConfigurationSettings']:
self.environment_resources
response['Environments']:
response['Events']:
response['SolutionStackDetails']:
response['SolutionStacks']:
response['EnvironmentInfo']:
response['Messages']:
boto.beanstalk.response
boto.beanstalk.exception
cls_name)
Layer1(*args,
'CREATE_IN_PROGRESS',
on_failure,
use_previous_template=None,
stack_policy_during_update_body=None,
template_body:
boto.log.warning("If
TemplateURL
are"
specified,
honored
API")
len(parameters)
enumerate(parameters):
params['Parameters.member.%d.ParameterKey'
params['Parameters.member.%d.ParameterValue'
on_failure
params['StackPolicyBody']
params['StackPolicyURL']
stack_policy_during_update_body
stack_policy_during_update_url
create_stack(self,
notification_arns=None,
timeout_in_minutes=None,
capabilities=None,
self._build_create_or_update_params(stack_name,
update_stack(self,
delete_stack(self,
logical_resource_id):
logical_resource_id=None,
physical_resource_id=None):
describe_stacks(self,
verb="POST")
self.timeout_in_minutes
"Outputs":
"Capabilities":
Capability)])
'CreationTime':
'StackStatus':
"member":
next_token=next_token
self.template_description
Parameter(object):
"ParameterKey":
(self.value)
self[self._current_key]
"StackId":
"StackName":
"Timestamp":
(self.logical_resource_id,
self.resource_type)
self.event_id
self.resource_properties
self.capabilities_reason
self.no_echo
self.parameter_key
Distribution,
DistributionConfig
InvalidationListResultSet
'2010-11-01'
response_headers.keys():
'etag':
response_headers[key]
[('DistributionSummary',
DistributionSummary)]
rs_class
rs_kwargs
dist_class):
'/%s/%s/%s'
dist_class(connection=self)
'/%s/%s/%s/config'
distribution_id)
{'If-Match':
self._get_info(distribution_id,
Distribution)
self._get_config(distribution_id,
self._set_config(distribution_id,
origin,
trusted_signers=None):
trusted_signers=trusted_signers)
self._delete_object(distribution_id,
StreamingDistribution)
access_id):
OriginAccessIdentity)
access_id,
handler.XmlHandler(paths,
request_id,
request_id)
marker=marker)
boto.cloudfront.logging
LoggingInfo
boto.cloudfront.origin
CustomOrigin
origin=None,
trusted_signers=None,
logging=None):
caller_reference:
xmlns="http://cloudfront.amazonaws.com/doc/2010-07-15/">\n'
self.origin:
self.origin.to_xml()
self.cnames:
<CNAME>%s</CNAME>\n'
<Enabled>'
self.enabled:
'</Enabled>\n'
'<TrustedSigners>\n'
<AwsAccountNumber>%s</AwsAccountNumber>\n'
'</TrustedSigners>\n'
self.logging:
'<Logging>\n'
<Bucket>%s</Bucket>\n'
self.logging.bucket
<Prefix>%s</Prefix>\n'
self.logging.prefix
'</Logging>\n'
dro
'TrustedSigners':
TrustedSigners()
'S3Origin':
S3Origin()
'CustomOrigin':
CustomOrigin()
'CNAME':
self.cnames.append(value)
last_modified_time
self.streaming
'LastModifiedTime':
self.in_progress_invalidation_batches
Object
DistributionConfig()
self.config.origin,
self.config.enabled,
self.config.cnames,
self.config.comment,
new_config.enabled
new_config.cnames
isinstance(self.config.origin,
S3Origin):
s3.get_bucket(bucket_name)
keypair_id,
expire_time=None,
valid_after_time=None,
ip_address=None,
policy_url=None,
"?"
signed_url_params
policy_url
encoded_policy
encoded_signature
86400
{"AWS:EpochTime":
ip_address:
rsa
private_key_string:
private_key,
super(StreamingDistribution,
s3_user_id='',
s3_user_id
'S3CanonicalUserId':
self.paths
self.distribution
distribution
"/*")
"Id":
distribution_id=None,
inval
result_set._inval_cache:
origin_access_identity
dns_name=None,
dns_name
self.origin_access_identity
<DNSName>%s</DNSName>\n'
'AwsAccountNumber':
"InvalidRequestException":
exceptions.InvalidRequestException,
ssh_key,
iam_role_arn,
subscription_type,
eni_ip=None,
syslog_ip=None):
params['EniIp']
params['SyslogIp']
label=None):
params['Label']
hapg_arn):
{'HsmArn':
hsm_arn
hsm_serial_number
client_arn
certificate_fingerprint
client_version,
'ClientArn':
partition_serial_list
iam_role_arn
'cloudsearch',
EncodingError(Exception):
ContentTooLongError(Exception):
DocumentServiceConnection(object):
self.endpoint:
domain.doc_service_endpoint
get_sdf(self):
json.dumps(self.documents_batch)
clear_sdf(self):
add_sdf_from_s3(self,
key_obj):
key_obj.get_contents_as_string()
self.get_sdf()
null'
sdf:
boto.log.error('null
detected.
probably
error.')
sdf.index(':
null')
boto.log.error(sdf[index
100:index
100])
(self.endpoint)
requests.adapters.HTTPAdapter(
pool_connections=20,
pool_maxsize=50,
max_retries=5
session.mount('http://',
session.mount('https://',
session.post(url,
data=sdf,
'application/json'})
CommitResponse(r,
CommitResponse(object):
doc_service,
self.doc_service
doc_service
self.sdf
response.content.decode('utf-8')
documents.\nResponse
self.sdf))
boto.exception.BotoServerError(self.response.status_code,
body=_body)
self.content['status']
[e.get('message')
self.content.get('errors',
self.errors:
character"
EncodingError("Illegal
document")
long":
ContentTooLongError("Content
long")
self.adds
self.content['adds']
self.content['deletes']
self._check_num_ops('add',
self.adds)
self._check_num_ops('delete',
self.deletes)
_check_num_ops(self,
response_num):
commit_num
len([d
self.doc_service.documents_batch
d['type']
type_])
response_num
commit_num:
CommitMismatchError(
'Incorrect
{0}s
returned.
Commit:
.format(type_,
commit_num,
IndexFieldStatus
ServicePoliciesStatus
SearchConnection
handle_bool(value):
'True',
'TRUE',
1]:
self.update_from_data(data)
update_from_data(self,
self.processing
self.requires_index_documents
self.search_instance_count
self.search_instance_type
self.search_partition_count
self._doc_service
self._search_service
doc_service_endpoint(self):
search_service_endpoint(self):
created(self):
@created.setter
created(self,
deleted(self):
@deleted.setter
deleted(self,
processing(self):
@processing.setter
processing(self,
requires_index_documents(self):
@requires_index_documents.setter
requires_index_documents(self,
search_partition_count(self):
@search_partition_count.setter
search_partition_count(self,
search_instance_count(self):
@search_instance_count.setter
search_instance_count(self,
self._num_searchable_docs
self.layer1.delete_domain(self.name)
get_access_policies(self):
index_documents(self):
self.layer1.index_documents(self.name)
get_index_fields(self,
self.layer1.describe_index_fields(self.name,
field_names)
[IndexFieldStatus(self,
create_index_field(self,
result=False,
self.layer1.define_index_field(self.name,
IndexFieldStatus(self,
self.layer1.describe_index_fields)
rank_names=None):
fn(self.name,
expression):
expression)
get_document_service(self):
DocumentServiceConnection(domain=self)
get_search_service(self):
SearchConnection(domain=self)
'<Domain:
verb='GET',
pythonize_name=True)
'domain_status')
define_index_field(self,
'index_field')
'literal':
do_bool(facet)
do_bool(result)
'rank_expression')
rank_name}
delete_index_field(self,
'default_search_field')
describe_domains(self,
describe_index_fields(self,
rank_name
describe_service_access_policies(self,
'access_policies')
'stems')
'stopwords')
'synonyms')
index_documents(self,
update_service_access_policies(self,
access_policies):
access_policies,
boto.cloudsearch.domain
security_token=session_token,
self.layer1.describe_domains(domain_names)
[Domain(self.layer1,
domain_data]
self.layer1.create_domain(domain_name)
Domain(self.layer1,
self.list_domains(domain_names=[domain_name])
len(domains)
domains[0]
OptionStatus(dict):
refresh_fn=None,
save_fn=None):
self.refresh_fn
refresh_fn
self.save_fn
save_fn
_update_status(self,
self.refresh_fn:
self.refresh_fn(self.domain.name)
to_json(self):
json.dumps(self)
'State':
'Options':
self.save_fn:
self.save_fn(self.domain.name,
self.to_json())
state):
IndexFieldStatus(OptionStatus):
self.update(options)
ServicePoliciesStatus(OptionStatus):
new_statement(self,
Docs
say
GET,
denies
unless
"IpAddress":
"aws:SourceIp":
[ip]
_allow_ip(self,
self['Statement']
[s]
condition['aws:SourceIp'].append(ip)
add_statement:
self['Statement'].append(s)
allow_search_ip(self,
self.domain.search_service_arn
allow_doc_ip(self,
self.domain.doc_service_arn
_disallow_ip(self,
condition['aws:SourceIp'].remove(ip)
need_update:
disallow_search_ip(self,
disallow_doc_ip(self,
ceil
SearchResults(object):
**attrs):
self.rid
self.time_ms
self.hits
attrs['hits']['found']
self.docs
attrs['hits']['hit']
attrs['hits']['start']
self.rank
attrs['query']
self.search_service
attrs['search_service']
self.facets
'facets'
(facet,
attrs['facets'].items():
self.facets[facet]
map(lambda
(x['value'],
x['count']),
self.num_pages_needed
ceil(self.hits
self.query.real_size)
len(self.docs)
iter(self.docs)
next_page(self):
self.num_pages_needed:
self.query.start
self.query.real_size
self.search_service(self.query)
RESULTS_PER_PAGE
self.bq
self.return_fields
return_fields
self.facet
self.update_size(size)
update_size(self,
new_size):
new_size
self.real_size
(self.size
to_params(self):
params['facet']
SearchConnection(object):
domain.search_service_endpoint
build_query(self,
Query(q=q,
bq=bq,
facet_constraints=facet_constraints,
facet_sort=facet_sort,
facet_top_n=facet_top_n,
t=t)
self.build_query(q=q,
re.search('<html><body><h1>403
Forbidden</h1>([^<]+)<',
(g.groups()[0].strip())
SearchServiceException('Authentication
Amazon%s'
SearchServiceException("Got
non-json
Amazon.
'messages'
data['messages']:
m['severity']
'fatal':
"=>
(params,
m['message']),
SearchServiceException("Unknown
json.dumps(data),
data['query']
SearchResults(**data)
get_all_paged(self,
per_page):
query.update_size(per_page)
get_all_hits(self,
get_num_hits(self,
query.update_size(1)
self(query).hits
self.domain.layer1.use_proxy:
getattr(self.domain.layer1,
'sign_request',
self.domain.layer1
self.domain_connection
host=self.endpoint,
aws_access_key_id=layer1.aws_access_key_id,
aws_secret_access_key=layer1.aws_secret_access_key,
region=layer1.region,
provider=layer1.provider
(self.endpoint,
'2013-01-01'
self.domain.layer1.APIVersion
self.signed_request:
self._service_arn
'HighlightEnabled':
analysis_scheme:
analysis_scheme
"2013-01-01"
"cloudsearch.us-east-1.amazonaws.com"
sign_request
multi_az,
value.items():
params['%s.%s'
isinstance(region,
self.refresh_key:
expr=None,
self.fq:
self.expr:
six.iteritems(self.expr):
six.iteritems(self.facet):
json.dumps(v)
self.highlight:
six.iteritems(self.highlight):
self.options:
params['partial']
self.sort:
params['sort']
','.join(self.sort)
parser=parser,
fq=fq,
highlight=highlight,
sort=sort,
partial=partial,
options=options)
_status_code
'cloudsearch'
kwargs.get('host',
cursor
filter_query
query_params['q']
query_options
query_parser
query_params['size']
suggester
boto.cloudtrail
s3_bucket_name,
s3_key_prefix=None,
sns_topic_name=None,
include_global_service_events=None,
cloud_watch_logs_log_group_arn=None,
cloud_watch_logs_role_arn=None):
params['SnsTopicName']
params['IncludeGlobalServiceEvents']
params['CloudWatchLogsLogGroupArn']
params['CloudWatchLogsRoleArn']
trail_name_list
lookup_attributes
s3_bucket_name
InvalidRoleException(BotoServerError):
"InvalidRoleException":
exceptions.InvalidRoleException,
application_names
deployment_ids=None):
create_deployment(self,
deployment_group_name=None,
params['deploymentGroupName']
revision
ignore_application_stop_failures
self.make_request(action='CreateDeployment',
minimum_healthy_hosts
ec_2_tag_filters=None,
auto_scaling_groups=None,
service_role_arn=None):
params['ec2TagFilters']
params['autoScalingGroups']
params['serviceRoleArn']
deployment_config_name):
deployment_group_name):
'revision':
deployment_id):
sort_by=None,
sort_by
s_3_bucket
s_3_key_prefix
instance_status_filter
params['applicationName']
include_only_statuses
create_time_range
new_application_name
current_deployment_group_name,
new_deployment_group_name
ResourceConflictException(BotoServerError):
TooManyRequestsException(BotoServerError):
NotAuthorizedException(BotoServerError):
"2014-06-30"
"ResourceConflictException":
exceptions.ResourceConflictException,
"TooManyRequestsException":
exceptions.TooManyRequestsException,
"InternalErrorException":
exceptions.InternalErrorException,
"NotAuthorizedException":
exceptions.NotAuthorizedException,
supported_login_providers=None,
developer_provider_name=None,
open_id_connect_provider_ar_ns=None):
'AllowUnauthenticatedIdentities':
params['SupportedLoginProviders']
params['DeveloperProviderName']
params['OpenIdConnectProviderARNs']
account_id,
logins=None):
params['Logins']
identity_id=None,
'Logins':
params['IdentityId']
token_duration
developer_user_identifier
source_user_identifier,
destination_user_identifier,
'DeveloperProviderName':
'IdentityId':
dataset_name):
'/identitypools/{0}/configuration'.format(identity_pool_id)
last_sync_count
sync_session_token
token):
token,
push_sync
device_id):
'/identitypools/{0}/identities/{1}/datasets/{2}/subscriptions/{3}'.format(
device_id)
sync_session_token,
device_id
record_patches
headers['x-amz-Client-Context']
delivery_channel_name):
delivery_channel_name,
configuration_recorder_names=None):
params['ConfigurationRecorderNames']
delivery_channel_names=None):
params['DeliveryChannelNames']
'resourceType':
'resourceId':
later_time
earlier_time
chronological_order
configuration_recorder_name):
'ConfigurationRecorderName':
configuration_recorder_name,
xml_attrs=None):
set_body(self,
get_body(self):
"DataPipeline"
delete_pipeline(self,
'objectIds':
evaluate_expressions
expression,
list_pipelines(self,
worker_group,
instance_identity=None):
params['hostname']
'pipelineObjects':
sphere,
query=None,
{'taskId':
taskrunner_id,
worker_group
task_status,
error_id
error_stack_trace
'connectionName':
connection_id):
{'connectionId':
virtual_interface_id):
{'virtualInterfaceId':
'location':
interconnect_name,
interconnect_id):
{'interconnectId':
params['connectionId']
interconnect_id
virtual_interface_id
get_regions('dynamodb',
self.table.layer2.build_key_from_values(self.table.schema,
batch_dict['ConsistentRead']
puts=None,
deletes=None):
layer2):
self.unprocessed
add_batch(self,
r))
submit(self):
batch.to_dict()
self.v1
self.v2)
[dynamize_value(v)
'{0}({1})'.format(self.__class__.__name__,
BEGINS_WITH(ConditionOneArg):
BotoServerError,
Item(dict):
hash_key=None,
attrs=None):
self._range_key_name
self[self._hash_key_name]
self._range_key_name:
hash_key_name(self):
range_key_name(self):
dynamodb_exceptions
'ConditionalCheckFailedException'
'ValidationException'
NumberRetries
boto.config.get('DynamoDB',
boto.config.getbool(
'DynamoDB',
'validate_checksums',
validate_checksums)
{'X-Amz-Target':
(self.ServiceName,
self.Version,
'application/x-amz-json-1.0',
str(len(body))}
override_num_retries=self.NumberRetries,
json.loads(response_body,
self.NumberRetries:
response.getheader('x-amz-crc32')
boto.log.debug('Validating
crc32(response.read())
int(expected_crc32)
expected_crc32:
("The
"checksum
(actual_crc32,
expected_crc32))
min(0.05
start_table:
provisioned_throughput}
update_table(self,
key}
data['ConsistentRead']
'Item'
request_items}
object_hook=None,
data['Count']
data['ScanIndexForward']
data['ExclusiveStartKey']
boto.dynamodb.layer1
boto.dynamodb.item
get_dynamodb_type,
Dynamizer,
remaining
count(self):
self.next_response()
self.kwargs['limit']
self.dynamizer
Dynamizer()
NonBooleanDynamizer()
{"Action":
{'Exists':
val}
last_evaluated_key:
self.dynamizer.encode(range_key)
range_key=None):
dynamodb_key
list(dynamodb_value.keys())[0]
tables
len(tables)
self.layer1.describe_table(name)
schema):
Table(self,
write_units})
update_throughput(self,
table.update_from_response(response)
table):
batch_list):
request_items
batch_list.to_dict()
return_values,
self.build_key_from_values(item.table.schema,
item.hash_key,
item.range_key)
request_limit=None,
item_class=Item,
rkc
*exclusive_start_key)
{'table_name':
table.name,
request_limit,
'consistent_read':
'exclusive_start_key':
esk,
'object_hook':
self.dynamizer.decode}
TableGenerator(table,
self.range_key_name:
reconstructed
BatchList
self.table.name
Table(object):
layer2,
self.update_from_response(response)
status(self):
self._schema
has_item(self,
Overflow,
Inexact,
DynamoDBNumberError
DYNAMODB_CONTEXT
ctx.divide(numerator,
denominator)
is_str(n):
(isinstance(n,
issubclass(n,
is_binary(n):
Binary
int(s)
'BOOL'
'M'
'L'
val]}
encode(self):
'Binary(%r)'
_get_dynamodb_type(self,
dynamodb_type.lower())
_encode_n(self,
_encode_ns(self,
dict([(k,
attr.items()])
_decode_n(self,
_decode_ns(self,
set(map(self._decode_n,
LimitExceededException(JSONResponseError):
ResourceInUseException(JSONResponseError):
InternalServerError(JSONResponseError):
definition(self):
self.data_type,
self.parts:
'INCLUDE'
self.includes_fields
super(IncludeIndex,
self).schema()
kwargs.pop('throughput',
super(GlobalBaseIndexField,
int(self.throughput['read']),
int(self.throughput['write']),
throughput:
self._loaded:
self._is_storable(self._data[key]):
alterations['deletes'].append(key)
self._determine_alterations()
key_fields
raw_key_data
NEWVALUE)
current_value
self._dynamizer.encode(self._data[key])
self.get_keys()
final_data:
fieldname
expects=expects)
self.mark_clean()
"ProvisionedThroughputExceededException":
exceptions.ProvisionedThroughputExceededException,
"InternalServerError":
exceptions.InternalServerError,
attribute_definitions,
local_secondary_indexes
global_secondary_indexes
consistent_read=None,
exclusive_start_table_name
key_conditions,
filter_expression=None,
index_name
params['Select']
query_filter
scan_index_forward
params['ExclusiveStartKey']
params['FilterExpression']
total_segments
segment
update_expression
global_secondary_index_updates=None,
attribute_definitions
global_secondary_index_updates
host=self.host)
boto.log.debug("Saw
self.the_callable
self.call_args
self.call_kwargs
self._results
self._fetches
len(self._results):
self.fetch_more()
fetch_more(self):
self._reset()
self.call_args[:]
self.call_kwargs.copy()
kwargs['limit']
self.the_callable(*args,
self._results.extend(results['results'])
len(results['results'])
self._keys_left
len(self._keys_left)
AllIndex,
IncludeIndex,
GlobalAllIndex,
GlobalIncludeIndex)
boto.dynamodb2.results
BatchGetResultSet
FILTER_OPERATORS,
indexes=None,
global_indexes=None,
self.indexes
DynamoDBConnection()
raw_schema
seen_attrs.add(field.name)
attr_defs.append(field.definition())
raw_throughput
table_indexes:
raw_attributes:
schema.append(
data_type=data_type)
seen,
unknown.
"https://github.com/boto/boto/issues."
raw_indexes,
map_indexes_projection.get('ALL')
self._introspect_all_indexes(
result['Table']['ProvisionedThroughput']
gsi_throughput
global_indexes.items():
"Update":
int(gsi_throughput['read']),
int(gsi_throughput['write']),
raw_key[key]
self._encode_keys(kwargs)
attributes_to_get=attributes,
self.get_item(**kwargs)
Item(self,
expects=None):
kwargs['expected']
using=FILTER_OPERATORS)
'null':
lookup['ComparisonOperator']
'NOT_NULL'
'in':
reverse
consistent=consistent,
attributes=attributes,
ResultSet(
max_page_size=max_page_size
filter_kwargs.copy()
kwargs.update({
consistent,
select,
self.connection.query(
limit=limit,
kwargs['exclusive_start_key']
exclusive_start_key.items():
kwargs['exclusive_start_key'][key]
raw_results.get('Items',
raw_results.get('LastEvaluatedKey',
raw_results['LastEvaluatedKey'].items():
last_key[key]
last_key,
'segment':
segment,
'total_segments':
total_segments,
'unprocessed_keys':
self._to_delete
self._unprocessed
self._to_delete:
self.should_flush():
batch_data
self.table.table_name:
batch_data[self.table.table_name].append({
'DeleteRequest':
self.table.connection.batch_write_item(batch_data)
self.handle_unprocessed(resp)
resp):
boto.log.info(
len(self._unprocessed)
to_resend
'eq':
'lte':
'LT',
'gte':
'gt':
'beginswith':
'IN',
get_regions('ec2',
self.allocation_id
self.association_id
self.network_interface_owner_id
self.allocation_id:
self.connection.release_address(
allocation_id=self.allocation_id,
self.connection.disassociate_address(
self.attribute_name
self.enable_dns_hostnames
self.enable_dns_support
self.ephemeral_name
no_device
'deleteontermination':
'iops':
BlockDeviceType
self.current_name
prefix=pre)
params['%s.DeviceName'
block_dev.iops
params['%s.Ebs.Encrypted'
self.upload_policy
self.upload_policy_signature
'startTime':
'updateTime':
'c1.xlarge',
'm2.2xlarge',
get_instance_type(self,
StringProperty(name='instance_type',
verbose_name='Instance
Type',
choices=InstanceTypes)
params['instance_type']
get_quantity(self,
IntegerProperty(name='quantity',
verbose_name='Number
Instances')
self.get_instance_type(params)
self.get_quantity(params)
offerings
offering
offering.describe()
unit_price
answer
detect_potential_sigv4
boto.ec2.image
Image,
ImageAttribute,
Reservation,
ConsoleOutput,
boto.ec2.keypair
KeyPair
boto.ec2.volume
VolumeAttribute
ReservedInstance
ReservedInstancesConfiguration
boto.ec2.spotinstancerequest
boto.ec2.instancestatus
get_params(self):
filters):
isinstance(filters,
{'image-type':
filter)
dry_run=dry_run)[0]
virtualization_type=None,
root_vol
block_device_map.ec2_build_list_params(params)
params['VirtualizationType']
delete_snapshot=False,
no_reboot=False,
img.id
operation='add',
'OperationType':
operation}
user_ids:
'UserId')
'UserGroup')
self.get_all_reservations(instance_ids=instance_ids,
filters=filters,
dry_run=dry_run,
reservations
11:
(sg-*)
name.
UserWarning)
max_results:
min_count=1,
disable_api_termination=False,
instance_initiated_shutdown_behavior=None,
tenancy=None,
network_interfaces=None,
min_count,
params['KeyName']
isinstance(user_data,
user_data.encode('utf-8')
params['UserData']
base64.b64encode(user_data).decode('utf-8')
addressing_type:
addressing_type
placement:
placement_group:
tenancy
monitoring_enabled:
subnet_id:
private_ip_address:
params['AdditionalInfo']
instance_profile_name:
instance_profile_arn:
instance_profile_arn
ebs_optimized:
network_interfaces:
get_console_output(self,
params['Attribute']
'sourcedestcheck',
'groupset':
isinstance(sg,
sg.id
also
attachment_id
ValueError('Unknown
params['%s.Value'
request_ids:
'SpotInstanceRequestId')
'launch.group-id'
"group
product_description:
price,
SpotDatafeedSubscription,
zones:
Address,
secondary_private_ip_address_count=None,
private_ip_addresses,
'PrivateIpAddress')
params['AllocationId']
public_ip=public_ip,
allocation_id=allocation_id,
association_id
volume_ids=None,
volume_ids:
volume_ids,
'VolumeId')
create_volume(self,
encrypted=False,
snapshot.id
params['SnapshotId']
params['VolumeType']
iops:
params['KmsKeyId']
attach_volume(self,
detach_volume(self,
restorable_by=None,
create_snapshot(self,
Snapshot,
volume_name:
'SourceSnapshotId':
trim_snapshots(self,
now.day)
target_backup_times
oldest_snapshot_date
one_day
snaps_for_each_volume[volume_name]
snap_date
(snap.tags['Name'],
snap.start_time))
'KeyName')
self.ResponseError
key_name}
KeyPair,
groupnames=None,
groupnames,
'GroupName')
description}
params['SourceSecurityGroupName']
params['SourceSecurityGroupOwnerId']
params['IpProtocol']
params['FromPort']
params['ToPort']
params['CidrIp']
self.get_status('AuthorizeSecurityGroupIngress',
src_security_group_group_id=None,
src_security_group_name,
src_security_group_owner_id)
group_id:
'IpPermissions.1.Groups.1.GroupName'
'IpPermissions.1.Groups.1.UserId'
src_security_group_group_id:
'IpPermissions.1.Groups.1.GroupId'
src_security_group_group_id
isinstance(cidr_ip,
[cidr_ip]
src_group_id=None,
params['IpPermissions.1.Groups.1.GroupId']
self.get_status('RevokeSecurityGroupIngress',
region_names:
params['InstanceTenancy']
params['IncludeMarketplace']
'ReservedInstancesId')
reserved_instances_offering_id,
instance_count=1,
'ReservedInstancesOfferingId':
instance_count,
'ReservedInstancesId':
term
ReservedInstanceListing)],
target_configurations):
tc.availability_zone
tc.platform
tc.instance_count
tc.instance_type
reserved_instance_ids,
InstanceInfo)],
s3_bucket,
s3_prefix,
self.aws_access_key_id
BundleInstanceTask,
get_all_tags(self,
self.get_list('DescribeTags',
'ResourceId')
self.build_tag_param_list(params,
isinstance(tags,
self.get_status('DeleteTags',
{'SubnetId':
subnet_id}
source_image_id,
encrypted=None,
attribute_names=None,
attribute_names,
modify_vpc_attribute(self,
enable_dns_support=None,
enable_dns_hostnames=None,
self.get_status('ModifyVpcAttribute',
'tagSet':
add_tag(self,
dry_run)
add_tags(self,
remove_tags(self,
EC2Object,
'productCode':
super(Image,
self.ownerId
ProductCodes()
self.billing_products
self.instance_lifecycle
self.sriov_net_support
'productCodes':
Exception(
'Unexpected
'architecture':
'rootDeviceType':
'rootDeviceName':
'virtualizationType':
'hypervisor':
group_names=None,
self.connection.modify_image_attribute(self.id,
group_names,
'remove',
'kernel',
'ramdisk',
self.attrs['block_device_mapping']
self.attrs['groups'].append(value)
self.attrs['groups']
'user_ids'
self.attrs['user_ids'].append(value)
self.attrs['user_ids']
self._parent
InstanceState(object):
code=0,
'%s(%d)'
zone=None,
self.tenancy
self.instances:
self.public_dns_name
self.private_dns_name
self.launch_time
self.ami_launch_index
self.monitoring_state
self.spot_instance_request_id
self.requester_id
InstanceState()
self._placement
self._previous_state:
'monitoring':
"eventsSet":
'iamInstanceProfile':
SubParse('iamInstanceProfile')
'previousState':
'placement':
'spotInstanceRequestId':
'ipAddress':
'clientToken':
'ebsOptimized':
terminate(self,
not_before=None,
not_after=None):
not_before
not_after
'Event:%s'
'notBefore':
'notAfter':
Status(object):
details:
self.details
EventSet(list):
Event()
self.append(event)
events
self.state_code
self.state_name
self.system_status
self.instance_status
'eventsSet':
EventSet()
self.append(status)
self.cores
self.memory
self.disk
self.fingerprint
self.material
'%s.pem'
BotoClientError('%s
os.chmod(file_path,
0o600)
copy_to_region(self,
BotoClientError('Unable
conn_params
self.connection.get_params()
rconn
region.connect(**conn_params)
self.placement
Attachment(object):
self.instance_owner_id
'attachTime':
super(NetworkInterface,
self.mac_address
ENI
self.primary
full_prefix
spec.delete_on_termination
(full_prefix,
ip_addr
params[query_param_key_prefix
ip_addr.primary
"Only
"'associate_public_ip_address'."
associate_public_ip_address
self.strategy
parse_ts
fixed_price=None,
usage_price=None,
currency_code=None,
self.duration
self.fixed_price
self.usage_price
self.offering_type
'instanceTenancy':
'currencyCode':
self.marketplace
self.instance_type)
Price=%s'
self.description)
price=None,
count=None):
super(ReservedInstance,
self.end
'reservedInstancesId':
create_date=None,
update_date=None,
client_token=None):
self.listing_id
update_date
self.instance_counts
self.price_schedules
'createDate':
'updateDate':
self.term
'active':
modification_id=None,
self.effective_date
('item',
super(SecurityGroup,
self.rules
IPPermissionsList()
self.rules_egress
self.connection.delete_security_group(
rule.ip_protocol
rule.from_port
rule.to_port
ValueError("The
target_rule
target_grant
rule.grants:
grant.owner_id
grant.cidr_ip
src_group=None,
src_group:
src_group.owner_id
src_group.name
hasattr(src_group,
'group_id'):
src_group.group_id
src_group.id
revoke(self,
grant_nom
rs.extend(self.connection.get_all_reservations(
self.id},
self.ip_protocol
self.from_port,
self.to_port)
self.group_id
(value.lower()
self.connection.modify_snapshot_attribute(self.id,
'fault':
SpotInstanceStateFault()
(self.code,
self.message)
super(SpotInstanceRequest,
self.valid_from
self.valid_until
self.launch_group
self.launched_availability_zone
self.availability_zone_group
'spotPrice':
'createTime':
self.res_id
self.res_type
'attachmentSet':
unfiltered_rs
self.attach_data.instance_id
self.attach_data.device
self.attach_data.status
mine
self._key_name
self.attrs[self._key_name]
zone=None):
self.volume_status
boto.ec2.autoscale.request
AutoScalingGroup
ProcessType
boto.ec2.autoscale.activity
Activity
boto.ec2.autoscale.scheduled
ScheduledUpdateGroupAction
use_block_device_types
isinstance(items[i
1],
vv
as_group):
as_group.desired_capacity
params['DesiredCapacity']
instance_ids):
decrement_capacity=True):
force_delete=False):
'true'}
params['InstanceMonitoring.Enabled']
launch_config.instance_profile_name
launch_config.associate_public_ip_address
params['AssociatePublicIpAddress']
params['DeleteOnTermination']
scaling_policy.min_adjustment_step
scaling_policy.cooldown
get_all_groups(self,
names,
activity_ids=None,
activity_ids,
autoscale_group=None):
autoscale_group:
terminate_instance(self,
Activity)
MetricCollectionTypes)
suspend_processes(self,
scaling_processes:
scaling_processes,
'ScalingProcesses')
resume_processes(self,
desired_capacity=None,
min_size=None,
max_size=None,
start_time.isoformat()
end_time.isoformat()
recurrence
time:
metrics=None):
metrics:
metrics,
'Metrics')
honor_cooldown:
params['HonorCooldown']
put_notification_configuration(self,
notification_types):
delete_notification_configuration(self,
'HealthStatus':
params['ShouldRespectGracePeriod']
enumerate(tags):
self.activity_id
self.cause
'ProcessName':
metric=None,
metric
'Metric':
TerminationPolicies(list):
launch_config
lbs
load_balancers
self.health_check_type
self.placement_group
self.autoscaling_group_arn
vpc_zone_identifier
self.vpc_zone_identifier
termination_policies
self.termination_policies
self.enabled_metrics
self.suspended_processes
scaling_processes)
self.health_status
self.lifecycle_state
(self.instance_id,
self.device_name
device_name
self.virtual_name
self.spot_price
self.instance_profile_name
self.launch_configuration_arn
self.classic_link_vpc_id
self.classic_link_vpc_security_groups
'SecurityGroups':
'InstanceMonitoring':
'EbsOptimized':
'AssociatePublicIpAddress':
'VolumeType':
'DeleteOnTermination':
'ClassicLinkVPCId':
self.max_autoscaling_groups
self.max_launch_configurations
self.val)
self.as_name
self.scaling_adjustment
self.cooldown
self.min_adjustment_step
self.alarms
'MinAdjustmentStep':
self.action_arn
self.as_group
self.recurrence
'Recurrence':
'PropagateAtLaunch']
boto.ec2.cloudwatch.alarm
dim_value
dim_value:
timestamp=None,
statistics=None):
unit,
len(a)
isinstance(a,
Exception('Must
%d.'
{'MetricName':
supplied
namespace}
timestamp=timestamp,
unit=unit,
dimensions=dimensions,
describe_alarms(self,
history_item_type=None,
period
statistic
'Statistic':
'Threshold':
'EvaluationPeriods':
'Period':
alarm.actions_enabled
put_metric_alarm
alarm_name,
state_reason,
'StateReason':
'StateValue':
alarm_names):
boto.ec2.cloudwatch.dimension
Dimension
metric_alarm
alarm_actions=None,
insufficient_data_actions=None,
self.statistic
self.last_updated
self.state_value
self.unit
'Dimensions':
Dimension()
['Average',
'Maximum',
'Sum',
unit)
comparison,
threshold,
evaluation_periods,
alarm
boto.ec2.elb.loadbalancer
boto.ec2.elb.healthcheck
listeners=None,
complex_listeners=None):
'Scheme':
listeners:
enumerate(listeners):
enumerate(complex_listeners):
InstanceProtocol
listener[3].upper()
params['Listeners.member.%d.InstanceProtocol'
listener[4]
'SecurityGroups.member.%d')
LoadBalancer)
LoadBalancerZones)
obj.zones
'crosszoneloadbalancing':
'accesslog':
value.enabled
'connectiondraining':
'connectingsettings':
boto.ec2.elb.attributes
LbAttributes
register_instances(self,
deregister_instances(self,
instances=None):
configure_health_check(self,
health_check):
ssl_certificate_id):
'SSLCertificateId':
create_app_cookie_stickiness_policy(self,
cookie_expiration_period
create_lb_policy(self,
index]
policies,
'PolicyNames.member.%d')
params['PolicyNames']
policies:
security_groups):
self.idle_timeout
self.s3_bucket_name
self.s3_bucket_prefix
self.enabled,
'Timeout':
self.cross_zone_load_balancing
self.access_log
self.connection_draining
self.connecting_settings
HealthCheck(object):
target=None,
load_balancer=None,
self.reason_code
self.policy_names
')'
'PolicyNames':
self.load_balancer_port,
boto.ec2.elb.listener
Listener
OtherPolicy)])
self.zones
self.canonical_hosted_zone_name
self.canonical_hosted_zone_name_id
'VPCId':
zones):
isinstance(zones,
[zones]
zones)
self.connection.modify_lb_attribute(
'crossZoneLoadBalancing',
self._attributes:
self._attributes.cross_zone_load_balancing.enabled
isinstance(instances,
[instances]
listeners):
listeners)
outPort
policies)
isinstance(subnets,
[subnets]
subnets)
new_sgs
self.cookie_name
(self.policy_name,
self.cookie_expiration_period
boto.ec2containerservice
create_cluster(self,
cluster_name
action='CreateCluster',
delete_cluster(self,
action='DeleteCluster',
container_instance,
task_definition):
describe_clusters(self,
clusters
action='DescribeClusters',
'containerInstances.member')
tasks,
params['containerInstance']
list_clusters(self,
family_prefix
family
instance_identity_document
instance_identity_document_signature
('name',
family,
'command',
params['overrides']
task=None,
params['task']
container_name
params['status']
exit_code
params['reason']
AWSQueryConnection,
params['Operation']
self.make_request(None,
itemSet
xml.sax.parseString(body.encode('utf-8'),
self.get(name)
ResponseGroup.__init__(self,
self.objs
self.total_results
self.total_pages
self.is_valid
'True':
boto.elasticache.layer1
ec2_security_group_owner_id):
ec2_security_group_owner_id,
num_cache_nodes=None,
replication_group_id=None,
cache_subnet_group_name=None,
'CacheClusterId':
params['NumCacheNodes']
params['ReplicationGroupId']
params['CacheSubnetGroupName']
preferred_availability_zone
'CacheParameterGroupFamily':
cache_subnet_group_description,
primary_cluster_id,
{'CacheSubnetGroupName':
{'ReplicationGroupId':
cache_cluster_id
cache_parameter_group_family
default_only
params['DefaultOnly']
default_only).lower()
cache_security_group_name
describe_engine_default_parameters(self,
action='DescribeEngineDefaultParameters',
reserved_cache_node_id=None,
reserved_cache_nodes_offering_id=None,
params['ReservedCacheNodeId']
params['ReservedCacheNodesOfferingId']
notification_topic_status=None,
params['NotificationTopicStatus']
'ParameterNameValues.member',
'ParameterValue'))
cache_subnet_group_description
replication_group_description
primary_cluster_id
reserved_cache_nodes_offering_id,
cache_node_count
boto.elastictranscoder
super(ElasticTranscoderConnection,
'/2012-09-25/jobs/{0}'.format(id)
pipeline_id=None,
output=None,
params['PipelineId']
input_name
output_key_prefix
playlists
output_bucket=None,
notifications=None,
content_config=None,
thumbnail_config=None):
params['OutputBucket']
params['ContentConfig']
params['ThumbnailConfig']
video
audio
thumbnails
'/2012-09-25/presets/{0}'.format(id)
params['Status']
topics
params['Id']
status=None):
boto.emr.connection
boto.emr.step
Step,
JarStep
boto.emr.emrobject
AddInstanceGroupsResponse,
BootstrapActionList,
Cluster,
ClusterSummaryList,
InstanceGroupList,
InstanceList,
JobFlow,
ModifyInstanceGroupsResponse,
RunJobFlowResponse,
EmrResponseError
jobflow_id):
created_after=None,
jobflow_ids,
created_after:
params['CreatedAfter']
created_after.strftime(
created_before:
params['CreatedBefore']
created_before.strftime(
'StepId':
steps):
isinstance(steps,
[steps]
params['JobFlowId']
jobflow_id
[self._build_step_args(step)
steps]
params.update(self._build_step_list(step_args))
instance_groups):
isinstance(instance_groups,
[instance_groups]
ig
enumerate(instance_groups):
(k+1)
keep_alive,
instance_params
params.update(instance_params)
main_class=None,
bootstrap_action
job_flow_role
service_role
'RunJobFlow',
"true")
"false"
[jobflow_id],
bootstrap_action_params
step_params
key_value
current_prefix]
'Args',
'Key',
'Jar',
'Properties':
'BidPrice',
'InstanceType',
'Market',
'LogUri',
'MasterPublicDnsName',
'NormalizedInstanceHours',
'TerminationProtected',
'VisibleToAllUsers',
'Steps':
'InstanceGroups':
'BootstrapActions':
BootstrapAction)])
self.supported_products
self.statechangereason
'Config':
StepConfig()
'Name'
self.role
self.market
num_instances=%r,
role=%r,
type=%r,
self.num_instances,
self.role,
self._main_class
self.action_on_failure
isinstance(step_args,
[step_args]
self.step_args
self.step_args:
args.extend(self.step_args)
self.input:
args.extend(('-input',
self.cache_files:
self.cache_archives:
BaseArgs
'--base-path',
'Install
step_args.extend(['--pig-versions',
pig_versions])
'--args',
'-f',
hive_versions='latest',
step_args.extend(['--hive-versions',
hive_versions])
boto.file.key
boto.file.bucket
BucketListResultSet
Bucket(object):
self.contained_key
iter(BucketListResultSet(self))
key_type=Key.KEY_REGULAR_FILE):
'-',
open(key_name,
dir_name
sys.stdin
torrent=False):
BotoClientError('Stream
open(self.full_path,
key_file.close()
decorated_attrs
('action',
to):
setattr(to,
kw.pop(field)
kw[field
getattr(amount,
self.currencycode)
requires(*groups):
'action',
"{0}\nRequired:
''.join(api
map(str.capitalize,
func.__name__.split('_')))
func.__doc__)
kw.setdefault('host',
@requires(['CreditInstrumentId',
'SenderTokenId',
self.aws_access_key_id)
@requires(['SenderTokenId',
@complex_amounts('RefundAmount')
refund(self,
ResponseError)
ResponseError(BotoServerError):
self.retry
'(Retriable)'
RetriableResponseError(ResponseError):
_action
ResponseElement)
do_show
pair[0].startswith('_')
filter(do_show,
self.__dict__.items())
'.join(map(render,
attrs)))
Response(ResponseElement):
ComplexAmount(ResponseElement):
{1}'.format(self.CurrencyCode,
float(self.Value)
str(self.Value)
('CurrencyCode',
'Value'):
ComplexAmount'.format(name)
super(Transaction,
super(GetAccountActivityResult,
'Transaction':
super(GetTokensResult,
bytes_to_hex
TreeHashDoesNotMatchError
_END_SENTINEL
total_size):
min_part_size_required
minimum
size.
self._threads:
thread.should_continue
self).__init__(part_size,
num_threads)
self._api
self._vault_name
total_size
self._calculate_required_part_size(total_size)
hash_chunks
self._add_work_items_to_queue(total_parts,
total_parts)
UploadArchiveError
upload.")
total_size)
_wait_for_upload_threads(self,
hash_chunks,
result_queue.get()
Exception):
tree_sha256
hash_chunks[part_number]
_start_upload_threads(self,
log.debug("Starting
thread.start()
self._threads.append(thread)
self._cleanup()
_cleanup(self):
time_between_retries=5,
retry_exceptions=Exception):
self).__init__(worker_queue,
self._time_between_retries
time_between_retries
log.error("Exception
work[0],
time.sleep(self._time_between_retries)
(start_byte,
job,
self._job
DownloadArchiveError
_wait_for_download_threads(self,
actual_hash
f.flush()
final_hash
"Tree
match,
"expected:
got:
_start_download_threads(self,
worker_queue):
response['TreeHash'],
'Expected
self.code,
DefaultPartSize
ResponseDataElements
False),
('CreationDate',
'creation_date',
('VaultARN',
'arn',
response_data=None):
response_data:
byte_range=None,
actual_tree_hash
tree_hash_from_str(data)
actual_tree_hash:
"expected
actual_tree_hash,
byte_range))
chunk_size):
float(chunk_size)))
chunk_size=DefaultPartSize,
verify_hashes=True,
retry_exceptions=(socket.error,)):
num_chunks
self._calc_num_chunks(chunk_size)
self._download_to_fileob(output_file,
fileobj,
retry_exceptions):
((i
account_id
create_vault(self,
list_jobs(self,
completed=None,
'vaults/%s/jobs'
job_id):
[('x-amz-job-id',
[('x-amz-sha256-tree-hash',
'bytes=%d-%d'
byte_range}
upload_archive(self,
[('x-amz-archive-id',
u'ArchiveId'),
u'TreeHash')]
{'x-amz-content-sha256':
'x-amz-sha256-tree-hash':
headers['x-amz-archive-description']
archive):
hasattr(archive,
delete_archive(self,
archive_id):
initiate_multipart_upload(self,
'vaults/%s/multipart-uploads'
complete_multipart_upload(self,
sha256_treehash,
upload_id):
list_multipart_uploads(self,
upload_part(self,
response_data)
response_headers):
http_response.status
MAXIMUM_NUMBER_OF_PARTS
size_in_bytes
default_part_size
[hashlib.sha256(b'').digest()]
fileobj.read(chunk_size)
boto.glacier.writer
_GIGABYTE
tree_hash
fileobj.seek(0)
part_size=part_size)
file_obj:
file_obj
(start
part_size),
"upload
sns_topic=None,
job_data
job_data['SNSTopic']
job_data['Description']
self.layer1.initiate_job(self.name,
job_data)
end_date
rparams
result['Marker']
compute_hashes_from_fileobj
self.part_size:
self._send_part()
self.upload_id
self.archive_id
tree_hash(chunk_hashes(part_data,
self.chunk_size))
self._insert_tree_hash(part_index,
part_tree_hash)
hex_tree_hash
len(part_data))
self.upload_id,
hex_tree_hash,
part_tree_hash,
generate_parts_from_fobj(fobj,
fobj.read(part_size)
fobj,
part_hash_map,
_Uploader(vault,
part_data
part_size)):
chunk_size))
self.next_part_index
boto.gs.user
InvalidAclError
SCOPE
TYPE
['private',
'public-read-write',
'authenticated-read',
'bucket-owner-read',
['READ',
'WRITE',
'FULL_CONTROL']
ACL(object):
Entries(self)
'owner'):
acl_entries
acl_entries:
entries_repr.append(e.__repr__())
'.join(entries_repr)
email_address=email_address,
user_id):
add_group_grant(self,
group_id):
OWNER.lower():
ENTRIES.lower():
self.owner.to_xml()
ENTRY.lower():
email_address=None,
scope:
Scope(self,
SCOPE.lower():
PERMISSION.lower():
ALL_AUTHENTICATED_USERS.lower()
GROUP_BY_EMAIL.lower()
DISPLAY_NAME.lower(),
EMAIL_ADDRESS.lower(),
GROUP_BY_ID.lower()
[DISPLAY_NAME.lower(),
ID.lower(),
email_address
(SCOPE,
TYPE,
self.type))
self.domain:
type="%s">'
(NAME,
NAME)
VersionedBucketListResultSet
boto.gs.cors
Cors
boto.gs.lifecycle
LifecycleConfig
boto.gs.key
GSKey
DEF_OBJ_ACL
VersioningBody
super(Bucket,
query_args_l.append('generation=%s'
generation)
rk,
six.iteritems(response_headers):
query_args_l.append('%s=%s'
(rk,
self._get_key_internal(key_name,
query_args_l=query_args_l)
e.reason
key_name))
list_versions(self,
VersionedBucketListResultSet(self,
marker,
validate_get_all_versions_params(self,
'marker',
self._delete_key_internal(key_name,
mfa_token=mfa_token,
InvalidAclError('Attempt
GS
ACL')
ACL):
self.set_xml_acl(acl_or_str.to_xml(),
self.set_canned_acl(acl_or_str,
query_args):
self._get_xml_acl_helper(key_name,
ACL(self)
if_generation,
if_metageneration,
headers[self.connection.provider.acl_header]
ValueError("Received
argument.
CannedACLStrings:
ValueError("Provided
valid."
acl_str)
canned=True)
query_args=CORS_ARG,
Cors()
handler.XmlHandler(cors,
ResultSet(self)
acl.add_email_grant(permission,
acl.add_user_grant(permission,
acl.add_group_email_grant(permission,
self.set_subresource('logging',
xml_str,
isinstance(target_bucket,
Bucket):
target_bucket.name
(xml_str
target_bucket)
target_prefix)
configure_website(self,
main_page_frag
error_frag
error_key
query_args='websiteConfig',
get_website_configuration(self,
self.get_website_configuration_with_xml(headers)[0]
get_website_configuration_with_xml(self,
delete_website_configuration(self,
resp_json
req_body
query_args=LIFECYCLE_ARG,
lifecycle_config.to_xml()
versioned_bucket_lister(bucket,
bucket.get_all_versions(prefix=prefix,
marker=marker,
rs.next_marker
generation_marker
VersionedBucketListResultSet(object):
versioned_bucket_lister(self.bucket,
marker=self.marker,
Location(object):
EU
'EU'
QueryString
location=Location.DEFAULT,
check_lowercase_bucketname(bucket_name)
headers[self.provider.acl_header]
policy}
storage_class_elem
self.provider.storage_create_error(
ORIGINS
METHODS
self.collections
self.elements
(tag,
CORS_CONFIG:
CORS:
self.legal_collections:
self.legal_elements:
self.legal_collections[self.collection]:
self.collection))
InvalidCorsError('Unsupported
InvalidCorsError('Mismatched
self.collections.append((name,
value.strip()))
compute_hash
self.component_count
ver_str
self.metageneration)
'<Key:
ver_str)
self.metageneration
handle_version_headers(self,
handle_restore_headers(self,
handle_addl_headers(self,
query_args='',
self._get_file_internal(fp,
res_download_handler:
res_download_handler.get_file(self,
hex_digest,
b64_digest,
b64_digest)
self._send_file_internal(fp,
self.bucket.delete_key(self.name,
version_id=self.version_id,
res_upload_handler=None,
rewind:
AttributeError('fp
EOF.
'or
seek()
start.')
hasattr(fp,
'name'):
fp.name
fp.getkey()
(re.match('^"[a-fA-F0-9]{32}"$',
key.etag)):
key.etag.strip('"')
base64.b64encode(binascii.unhexlify(etag)))
self.compute_md5(fp,
md5[0]
num_cb)
if_generation=if_generation)
self.bucket.get_acl(self.name,
generation=generation)
self.bucket.get_xml_acl(self.name,
self.bucket.set_xml_acl(acl_str,
'GCS
inter-bucket
composing')
generation_tag
ACTION:
RULE)
'Only
rule')
'Tag
CONDITION:
RULE:
self.action_params:
('<'
self.has_root_tag
LIFECYCLE_CONFIG:
self.has_root_tag:
XML')
Rule()
action_params,
conditions):
conditions)
8192
RETRYABLE_EXCEPTIONS
(httplib.HTTPException,
socket.gaierror)
tracker_file_name=None,
num_retries=None):
self.tracker_file_name
tracker_file_name:
f.readline().strip()
errno.ENOENT:
'upload
e.strerror))
self.tracker_file_name:
%s.\nThis
happen'
you\'re
incorrectly
tool\n'
'(e.g.,
'unwritable
directory)'
e.strerror),
self.tracker_uri:
_remove_tracker_file(self):
(self.tracker_file_name
os.path.exists(self.tracker_file_name)):
os.unlink(self.tracker_file_name)
file_length):
put_headers['Content-Range']
file_length))
put_headers['Content-Length']
path=self.tracker_uri_path,
headers=put_headers,
host=self.tracker_uri_host)
self._query_server_state(conn,
got_valid_response
server_start
self.SERVER_HAS_NOTHING
'start'
key.name,
ResumableTransferDisposition.WAIT_BEFORE_RETRY)
http_conn,
fp.read(self.BUFFER_SIZE)
self._build_content_range_header(
'*',
(total_bytes_uploaded,
http_conn.endheaders()
http_conn.set_debuglevel(0)
upload:
EOF
http_conn.getresponse()
ResumableUploadException('Got
bytes_to_go
len(chunk)
print('Resuming
print('Unable
self._start_new_resumable_upload(key,
key.md5
doesn\'t
ResumableTransferDisposition.ABORT_CUR_PROCESS):
(%s);
'aborting
ResumableTransferDisposition.ABORT):
self.digesters
self.digesters_before_attempt
self.num_retries:
'Too
'progress.
later',
sleep_time_secs
print('Got
retryable
(%d
progress-less
row).\n'
'Sleeping
re-trying'
sleep_time_secs))
time.sleep(sleep_time_secs)
CT
headers[CT]
headers['User-Agent']
fp.getkey().size
(alg,
Retry
we're
making
progress.
print('Resumable
self.RETRYABLE_EXCEPTIONS
e.__repr__())
errno.EPIPE:
parent.owner
element_name='Owner'):
self.type:
connection_cls=IAMConnection
regions.append(
name='universal',
'Statement':
['sts:AssumeRole']
path_prefix:
list_marker='Groups')
get_group(self,
list_marker='Users')
policy_json):
policy_json}
create_user(self,
get_user(self,
{'AccessKeyId':
status}
{'CertificateId':
cert_name):
auth_code_2):
'AuthenticationCode1':
'AuthenticationCode2':
auth_code_2}
'Password':
password}
alias):
{'AccountAlias':
alias}
Exception('No
account.
iam.create_account_alias()
first.')
instance_profile_name):
instance_profile_name})
policy_name})
list_marker='InstanceProfiles')
policy_document):
policy_document})
saml_provider_arn):
{'SAMLProviderArn':
saml_provider_arn}
saml_provider_arn,
'VirtualMFADeviceName':
max_password_age
minimum_password_length
password_reuse_prevention
policy_arn):
get_policy(self,
'ListPolicies',
'ListPolicyVersions',
entity_filter
boto.kinesis
exclusive_start_shard_id
shard_iterator,
shard_iterator_type,
starting_sequence_number
exclusive_start_stream_name
exclusive_start_tag_key
shard_to_merge,
partition_key,
b64_encode=True):
explicit_hash_key
sequence_number_for_ordering
b64_encode:
params['Data']
params['Records'][i]['Data']
tag_keys):
tag_keys,
shard_to_split,
grantee_principal,
constraints=None,
'KeyId':
retiring_principal
operations
key_usage
isinstance(ciphertext_blob,
``ciphertext_blob``
ciphertext_blob
base64.b64encode(ciphertext_blob)
number_of_bytes=None,
key_spec=None,
params['KeySpec']
destination_key_id,
source_encryption_context
destination_encryption_context
boto.logs
log_stream_name):
'filterName':
log_group_name_prefix
log_stream_name_prefix
filter_name_prefix
start_from_head
log_events,
sequence_token
'filterPattern':
retention_in_days):
'retentionInDays':
retention_in_days,
batch_prediction_data_source_id,
output_uri,
'BatchPredictionId':
batch_prediction_name
rds_data,
'RoleARN':
'DataSpec':
evaluation_data_source_id,
'EvaluationId':
evaluation_name
ml_model_type,
training_data_source_id,
ml_model_name=None,
params['MLModelName']
recipe
recipe_uri
batch_prediction_id):
{'BatchPredictionId':
{'DataSourceId':
evaluation_id):
{'EvaluationId':
verbose=None):
params['Verbose']
score_threshold
boto.mashups.interactive
interactive_shell
uname='root',
self.host_key_file
host_key_file
paramiko.SSHClient()
self._ssh_client.load_system_host_keys()
self._ssh_client.load_host_keys(os.path.expanduser(host_key_file))
self._ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
seconds')
then
mode='r',
isdir(self,
self.run('[
||
"FALSE"'
status[1].startswith('FALSE'):
shell(self):
interactive_shell(channel)
boto.log.debug('running:%s
self.server.instance_id))
std_err)
shutil.copyfile(src,
log_fp
process.poll()
process.communicate()
log_fp.write(t[0])
log_fp.write(t[1])
log_fp.getvalue())
'instance-id',
SSHClient(server)
prompt
valid:
min,
max))
choice'
prompt)
print('A
CalculatedProperty
closing
SSHClient(server,
print('\tcopying
pk
os.path.split(key_file)
self.remote_key_file)
os.path.split(cert_file)
bundle_image(self,
ssh_key):
'root':
"sudo
'-c
-k
'-u
'-p
'-d
self.server.instance_type
'm1.small'
'c1.medium':
i386'
x86_64'
upload_bundle(self,
'ec2-upload-bundle
'-m
/mnt/%s.manifest.xml
'-b
'-a
iobject.get_string('Name
iobject.get_string('Prefix
cert_file:
iobject.get_int('Size
(in
MB)
image')
fp.write('sudo
mv
fp.write('mv
ssh_key))
fp.write(';
t[1])
print('registering
prefix))
CommandLineGetter(object):
my_amis
self.cls.find_property('region_name')
self.cls.find_property('name')
params['name']
params.get('ami',
ami:
params['ami']
keypair:
self.get_name(params)
Server(Model):
StringProperty(unique=True,
verbose_name="Name")
StringProperty(verbose_name="Description")
Region
Name")
private_hostname
launch_time
console_output
aws_secret_access_key):
cfg.set('Credentials',
cfg.set('DB_Server',
'SimpleDB')
config_file=None,
CommandLineGetter()
getter.get(cls,
params.get('region')
params.get('zone')
ami.run(min_count=1,
elastic
params.get('name')
s._reservation
self._setup_ec2()
self.ec2.get_all_reservations([self.instance_id])
self._reservation.groups
wait(self):
self.ssh_key_file:
ssh_file
self.key_name)
OpenSSH
get_ssh_client(self,
DateTimeProperty,
StringProperty(required=True)
DateTimeProperty()
cls.all():
self.hourly
self.hour
self.daily
self.last_executed:
int(self.hour)
self.now.hour:
max(
(int(self.hour)-self.now.hour),
(self.now.hour-int(self.hour))
)*60*60
self.last_executed
delta.seconds
current_timeout
vtimeout
current_timeout))
vtimeout=60):
queue.new_message(self.id)
self.message_id
new_msg.id
boto.lookup('sqs',
queue_name)
msg.id
boto.connect_sqs()
m.id))
boto.manage.server
boto.manage.volume
Creating
print('---->
Run
"df
-k"
server.run('df
-k')
ReferenceProperty,
Zone')
State",
calculated_type=int,
ebs_volume
ec2.create_volume(size,
v.ec2
v.volume_id
ebs_volume.id
v.name
v.mount_point
v.device
v.region_name
v.zone_name
v.put()
snapshot,
self.size:
ec2.get_all_volumes([self.volume_id])[0]
self.mount_point
self.__size
xfsdump')
self.attachment_state
print('already
ec2.attach_volume(self.volume_id,
self.server.instance_id,
use_cmd:
boto.log.info('%s
boto.log.info('make_fs...')
xfs
boto.log.info('handle_mount_point')
boto.log.info('making
directory')
boto.log.info('directory
already')
cmd.run('mount
-l')
line.split()
t[2]
self.mount_point:
self.device:
/tmp'
777
/tmp')
self.attach()
self.server.run("/usr/sbin/xfs_freeze
-u
boto.log.info('Snapshot
one_week
midnight)
has_termios
sys.stdin.read(1)
item_list:
choices.append(obj)
len(choices)))
search_str
int_val
prompt):
self.userdata[key]
z
self.config.set('Credentials',
get_domain()
self.config.set('Pyami',
'server_sdb_domain',
'server_sdb_name',
set_config(self,
self.region)
self.items.append(item)
SDB
ServerSet()
states
[i.state
[i.update()
is_callable
StringProperty(verbose_name="Instance
'Retrieve
self._config:
local_file
'%s.ini'
config.write(fp)
self._config.set("Credentials",
ec2.aws_access_key_id)
remotepath):
cert_file,
remote_key_file)
hasattr(volume,
"id"):
volume.id
instance_id=self.instance_id,
'sandbox')
'mechanicalturk.sandbox.amazonaws.com'
approval_delay=None,
Title=title,
Description=description,
keywords:
params['Keywords']
self.get_keywords_as_string(keywords)
approval_delay
self.duration_as_seconds(approval_delay)
'REST',
queue_url,
event_types=None,
event_types,
destination,
request_type,
hit_type}
num
hit_type=None,
neither
hit_layout
(single
(list
hit_type
sort_by='Expiration',
page_number=1):
list(range(1,
page_nums
self._get_pages(page_size,
total_records)
page_nums)
[('Assignment',
'QuestionIdentifier':
hit_id):
(assignments_increment
assignments_increment
about,
{'WorkerId':
worker_ids,
retry_delay=None,
test=None,
answer_key=None,
test_duration=None,
params['RetryDelayInSeconds']
assert(isinstance(test,
QuestionForm))
params['Test']
test.get_as_xml()
params['TestDurationInSeconds']
isinstance(answer_key,
params['AnswerKey']
params['AutoGranted']
params['AutoGrantedValue']
qualification_type_id
page_number
[('Qualification',
Qualification)])
qualification_type_id}
auto_granted
qualification_request_id,
subject_id,
value=1,
'IntegerValue'
{'QualificationTypeId'
'SubjectId'
marker_elems=None):
property,
Expiration
super(QualificationRequest,
self.answers
'Answer':
answer_rs
ResultSet([('Answer',
QuestionFormAnswer)])
handler.XmlHandler(answer_rs,
connection.get_utf8_value(value)
xml.sax.parseString(value,
self.answers.append(answer_rs)
super(Assignment,
self.qid
((n+1),
self.signature
NotificationMessage.OPERATION_NAME
events_dict
events_dict:
signature_calc
integer_value=None,
"QualificationTypeId":
self.qualification_type_id,
"Comparator":
self.comparator,
('In',
'NotIn'):
self.required_to_preview:
params['RequiredToPreview']
"true"
self.locale
params['LocaleValue.%d.Country'
params['LocaleValue.Country']
self.__dict__.update(vars())
self.self
ValidatingXML(object):
external_url,
frame_height):
external_url
self.frame_height
frame_height
super(JavaApplet,
self).get_inner_content(content)
super(Flash,
get_attributes(self):
zip(self.attribute_names,
self.attribute_values)
'.join(
max_length=None):
max_length
self.constraints
min_bytes
max_bytes
(count_xml,
style_xml,
selections_xml)
value_xml)
non-empty
self.min_selections
TYPE_TAG
'text'
selection_xml
style_xml
XmlHandler
('2009-01-01',
'Merchant',
'/'),
('2014-03-01',
'response',
suffix
members
i:
requires(*args,
len(list(filter(hasgroup,
requires.__doc__
to=requires)
any(i
[f
quota,
restore,
connection=self)
self.Merchant
}.items():
parser,
self._response_error_factory(response.status,
contenttype,
handler)
@structured_lists('MarketplaceIdList.Id')
@structured_lists('FeedSubmissionIdList.Id',
'FeedProcessingStatusList.Status')
@structured_lists('ReportRequestIdList.Id',
'ReportProcessingStatusList.Status')
@structured_objects('InboundShipmentHeader',
'InboundShipmentItems')
'Items'])
'Items')
@requires(['SellerFulfillmentOrderId'])
@exclusive(['CreatedAfter'],
dont
include
@requires(['AmazonOrderId'])
@api_action('Sellers',
'Subscription'])
@structured_objects('Subscription',
'NotificationType',
@requires(['AmazonOrderReferenceId',
@requires(['AmazonAuthorizationId'])
_value
self._value,
_hint=None,
self._hint
setattr(self._parent,
self._name,
teardown(self,
self._hint(parent=self._parent,
self._value.append(value)
self._value.member
_name
hasattr(scope,
suffix)
self.element_factory(action
self.find_element(action,
_namespace
attr:
FeedSubmissionInfo
ReportRequestInfo
_amount
super(ComplexWeight,
self._dimensions:
EstimatedShippingWeight
Element(ComplexWeight)
MemberList(SimpleList)
Element(ComplexDimensions)
Identifiers
Element(MarketplaceASIN=Element(),
SKUIdentifier=Element())
OrderTotal
Element(CartItem=ElementList(CartItem))
Destination
boto.opsworks
layer_ids):
'LayerIds':
'ElasticLoadBalancerName':
'LayerId':
source_stack_id,
default_instance_profile_arn=None,
use_opsworks_security_groups=None,
default_root_device_type=None):
'ServiceRoleArn':
params['DefaultInstanceProfileArn']
clone_permissions
clone_app_ids
shortname=None,
data_sources=None,
app_source=None,
domains=None,
enable_ssl=None,
ssl_configuration=None,
environment=None):
params['Shortname']
params['DataSources']
params['AppSource']
params['Domains']
params['EnableSsl']
params['SslConfiguration']
params['Environment']
app_id=None,
params['AppId']
params['InstanceIds']
auto_scaling_type=None,
os=None,
ami_id=None,
ssh_key_name=None,
ebs_optimized=None):
params['AutoScalingType']
params['Os']
params['AmiId']
params['SshKeyName']
root_device_type
shortname,
custom_instance_profile_arn=None,
custom_security_group_ids=None,
packages=None,
volume_configurations=None,
enable_auto_healing=None,
auto_assign_elastic_ips=None,
auto_assign_public_ips=None,
custom_recipes=None,
use_ebs_optimized_instances=None,
lifecycle_event_configuration=None):
params['CustomInstanceProfileArn']
params['CustomSecurityGroupIds']
params['Packages']
params['VolumeConfigurations']
params['EnableAutoHealing']
params['AutoAssignElasticIps']
params['AutoAssignPublicIps']
params['CustomRecipes']
params['UseEbsOptimizedInstances']
params['LifecycleEventConfiguration']
default_instance_profile_arn,
'Region':
ssh_username=None,
ssh_public_key=None,
allow_self_management=None):
params['SshUsername']
params['AllowSelfManagement']
{'AppId':
delete_elastic_ip
delete_volumes
elastic_ip):
{'RdsDbInstanceArn':
volume_id):
app_ids
deployment_id
command_ids
ips
layer_ids=None):
layer_id
iam_user_arn
raid_array_ids
rds_db_instance_arns
service_error_ids
stack_ids
iam_user_arns
raid_array_id
volume_ids
private_ip
rsa_public_key
rsa_public_key_fingerprint
ec_2_volume_id
up_scaling
down_scaling
allow_ssh
allow_sudo
auto_scaling_schedule
db_password
get_instance_userdata
self.working_dir
get_instance_userdata()
self.working_dir)
directory:
update.find(':')
update.split(':')
'boto_location',
self.run('git
cwd=location)
pull
self.run('easy_install
exit_on_error=False)
'/var/log/boto.log')
bs
bs.main()
full_path
has_option(self,
ConfigParser()
self.has_section(section):
self.add_section(section)
int(default)
self.sections():
self.options(section):
sdb.lookup(domain_name)
item.save()
'working_dir')
self.log_file
'%s.log'
self.log_path
os.path.join(self.wdir,
self.log_file)
boto.set_file_logger(self.name,
self.log_path)
boto.config.getbool(self.name,
self.src
self.src:
self.dst
'copy_acls',
key.name))
key.set_contents_from_filename(self.log_path)
self.notify('%s
Starting'
self.instance_id),
reload:
params['script_name'])
print('Reservation
instances:'
r.id)
i.id)
wait:
body=''):
os.mkdir(path)
self.run('umount
self.last_command.status
self.last_command.output))
'scripts')
mod_name
Script:
script:
add_cron(self,
hour,
mday,
month,
wday,
who,
env=None):
add_init_script(self,
add_env(self,
attempt_attach
EBS
(self.volume_id,
os.path.exists(self.device):
has_fs
+x
self.run('mount
minute=minute,
hour=hour)
'random':
self.run('cp
self.run('/etc/init.d/%s
data_dir
fresh_install
fp.write('#
self.start('mysql')
/var/log/boto.log")
cnf.write("NameVirtualHost
*:80>\n")
cnf.write("\tServerAdmin
"server_admin").strip())
cnf.write("\tServerName
"home").strip())
if(env[0]
"."):
cnf.write("\t\tPythonOption
%s/%s\n"
env))
DBInstance
boto.rds.dbsnapshot
DBSnapshot
boto.rds.dbsubnetgroup
DBSubnetGroup
boto.rds.logfile
LogFile,
LogFileObject
RDSConnection(AWSQueryConnection):
super(RDSConnection,
port=3306,
engine='MySQL5.1',
str(auto_minor_version_upgrade).lower()
backup_retention_period,
'CharacterSetName':
(param_group.name
isinstance(param_group,
str(multi_az).lower()
'PreferredBackupWindow':
preferred_backup_window,
'PreferredMaintenanceWindow':
preferred_maintenance_window,
DBSecurityGroup):
vpc_grp
isinstance(vpc_grp,
VPCSecurityGroupMembership):
l.append(vpc_grp.vpc_group)
l.append(vpc_grp)
instance_class:
instance_class
promote_read_replica(self,
preferred_backup_window=None):
preferred_backup_window:
master_password=None,
apply_immediately=False,
params['AllocatedStorage']
params['NewDBInstanceIdentifier']
skip_final_snapshot=False,
final_snapshot_id=''):
params['FinalDBSnapshotIdentifier']
groupname=None,
groupname:
ParameterGroup)])
source:
len(parameters)):
parameters[i]
parameter.merge(params,
i+1)
params['DBSecurityGroupName']
DBSecurityGroup)])
ec2_security_group_name:
ec2_security_group_owner_id:
params['DBSnapshotIdentifier']
file_size=None,
filename_contains=None,
dbinstance_id}
params['FileSize']
params['FilenameContains']
params['FileLastWritten']
params['NumberOfLines']
db_subnet_group_name=None):
source_instance_id,
restore_time=None,
params['UseLatestRestorableTime']
params['RestoreTime']
create_db_subnet_group(self,
delete_db_subnet_group(self,
modify_db_subnet_group(self,
params['DBSubnetGroupDescription']
create_option_group(self,
OptionGroup)
delete_option_group(self,
describe_option_groups(self,
major_engine_version:
int(max_records)
describe_option_group_options(self,
self.auto_minor_version_upgrade
self.instance_class
self.backup_retention_period
self.preferred_backup_window
self.preferred_maintenance_window
self.latest_restorable_time
self.multi_az
self.character_set_name
'PendingModifiedValues':
'DBInstanceStatus':
'InstanceCreateTime':
self._in_endpoint:
validate=False):
self.__dict__.update(i.__dict__)
cidr
ec2_grp
'DBSecurityGroupName':
'DBSecurityGroupDescription':
ec2_group=None):
isinstance(ec2_group,
ec2_group.name
ec2_group.owner_id
self.connection.revoke_dbsecurity_group(
self.snapshot_create_time
self.instance_create_time
self.option_group_name
self.percent_progress
self.snapshot_type
self.source_region
self.subnet_ids
self.source_identifier
self.source_type
self.last_written
vpc_id=None):
permanent=False,
persistent=False,
self.permanent
'Permanent':
'Persistent':
allowed_values=None,
apply_type=None,
allowed_values
apply_type
is_modifiable
'VpcSecurityGroupId':
self.min_minor_engine_version
self.default_port
param.name
'string'
'Source':
self.allowed_values:
self.allowed_values)
'string':
'integer':
TypeError('unknown
self.status_type
InvalidSubnet(JSONResponseError):
InvalidRestore(JSONResponseError):
AuthorizationQuotaExceeded(JSONResponseError):
InvalidVPCNetworkState(JSONResponseError):
SNSTopicArnNotFound(JSONResponseError):
SNSNoAuthorization(JSONResponseError):
SNSInvalidTopic(JSONResponseError):
SourceNotFound(JSONResponseError):
SubscriptionCategoryNotFound(JSONResponseError):
EventSubscriptionQuotaExceeded(JSONResponseError):
SubscriptionAlreadyExist(JSONResponseError):
AuthorizationNotFound(JSONResponseError):
SubscriptionNotFound(JSONResponseError):
AuthorizationAlreadyExists(JSONResponseError):
SubnetAlreadyInUse(JSONResponseError):
boto.rds2
"InvalidSubnet":
exceptions.InvalidSubnet,
"InvalidRestore":
exceptions.InvalidRestore,
"AuthorizationQuotaExceeded":
exceptions.AuthorizationQuotaExceeded,
"InvalidVPCNetworkState":
exceptions.InvalidVPCNetworkState,
"SNSTopicArnNotFound":
exceptions.SNSTopicArnNotFound,
"SNSNoAuthorization":
exceptions.SNSNoAuthorization,
"SNSInvalidTopic":
exceptions.SNSInvalidTopic,
"SourceNotFound":
exceptions.SourceNotFound,
"SubscriptionCategoryNotFound":
exceptions.SubscriptionCategoryNotFound,
"EventSubscriptionQuotaExceeded":
exceptions.EventSubscriptionQuotaExceeded,
"SubscriptionAlreadyExist":
exceptions.SubscriptionAlreadyExist,
"AuthorizationNotFound":
exceptions.AuthorizationNotFound,
"SubscriptionNotFound":
exceptions.SubscriptionNotFound,
"AuthorizationAlreadyExists":
exceptions.AuthorizationAlreadyExists,
"SubnetAlreadyInUse":
exceptions.SubnetAlreadyInUse,
source_identifier):
source_identifier,
ec2_security_group_id=None,
params['EC2SecurityGroupId']
source_db_snapshot_identifier,
target_db_snapshot_identifier,
'SourceDBSnapshotIdentifier':
db_instance_class,
db_security_groups,
db_security_group_description,
db_subnet_group_description,
create_event_subscription(self,
'SnsTopicArn':
action='CreateEventSubscription',
option_group_description,
final_db_snapshot_identifier
delete_event_subscription(self,
subscription_name):
action='DeleteEventSubscription',
db_parameter_group_family
db_instance_identifier=None,
db_security_group_name
snapshot_type=None,
db_snapshot_identifier
params['SnapshotType']
describe_event_categories(self,
source_type=None):
action='DescribeEventCategories',
describe_event_subscriptions(self,
subscription_name=None,
params['SubscriptionName']
action='DescribeEventSubscriptions',
reserved_db_instance_id=None,
reserved_db_instances_offering_id=None,
params['ReservedDBInstanceId']
params['ReservedDBInstancesOfferingId']
master_user_password=None,
new_db_instance_identifier
parameters):
'ApplyType',
'MinimumEngineVersion',
'ApplyMethod'))
db_subnet_group_description
modify_event_subscription(self,
sns_topic_arn=None,
params['SnsTopicArn']
action='ModifyEventSubscription',
reserved_db_instances_offering_id,
db_instance_count
reset_all_parameters=None,
target_db_instance_identifier,
restore_time
boto.redshift
'AccountWithRestoreAccess':
source_snapshot_identifier,
target_snapshot_identifier,
source_snapshot_cluster_identifier
node_type,
cluster_type=None,
automated_snapshot_retention_period=None,
number_of_nodes=None,
params['ClusterType']
params['NumberOfNodes']
params['ElasticIp']
hsm_client_certificate_identifier):
'HsmClientCertificateIdentifier':
hsm_client_certificate_identifier,
hsm_ip_address,
hsm_partition_name,
hsm_partition_password,
'HsmConfigurationIdentifier':
final_cluster_snapshot_identifier
parameter_group_name
cluster_security_group_name
cluster_identifier=None,
params['ClusterIdentifier']
snapshot_identifier
params['OwnerAccount']
cluster_parameter_group_family
node_type=None,
params['NodeType']
reserved_node_offering_id
reserved_node_id
destination_region,
retention_period
new_cluster_identifier
'MinimumEngineVersion'))
reserved_node_offering_id,
node_count
sys.excepthook
self.line
'%s\t'
self.printed
getattr(cls,
encode_string
Description
'int',
self.http_response.status
self.args['debug']
self.Params+self.Args:
param.long_name.replace('-',
l.append('(%s)'
'object':
fmt['properties']:
self.Filters:
print('Available
self.args['url']
self.args['aws_access_key_id']
self.args['aws_secret_access_key']
get_usage(self):
%prog
self.Args:
self.Params:
'append'
action=action,
type=ptype,
choices=param.choices,
help=param.doc)
self.parser.parse_args()
d[p_name]
self.cli_options.filter:
print(e)
self.Regions:
self.args['host']
'path'
self.args['path']
'port'
self.args['port']
os.path.expanduser(path)
name.strip()
self.args['is_secure']
rslt.path
os.path.isdir(value):
'--%s'
'-%s'
connection_cls=Route53Connection
HZXML
super(Route53Connection,
'/%s/hostedzone'
hosted_zone_id):
'/%s/hostedzone/%s'
boto.jsonresponse.Element(list_marker='NameServers',
item_marker=('NameServer',))
self.get_all_hosted_zones()
private_zone=False,
vpc_region=None):
'comment':
comment,
'xmlns':
self.XMLNameSpace}
204):
'/%s/healthcheck'
maxitems
hosted_zone_id,
'/%s/hostedzone/%s/rrset'
new_list
err.error_code,
self.fqdn
self.string_match
'resource_path':
'AAAA',
'TXT',
self.next_record_name
self.next_record_type
self.next_record_identifier
self.changes:
record_list
ttl=600,
alias_hosted_zone_id=None,
alias_dns_name=None,
weight=None,
alias_evaluate_target_health=None,
health_check=None,
failover=None):
weight=weight,
self.changes.append([action,
change])
changesXML
changeParams
{"action":
truncated
self.ttl
alias_hosted_zone_id
alias_dns_name
alias_evaluate_target_health
self.resource_records.append(value)
eval_target_health
"weight":
"region":
route53connection,
self.route53connection
route53connection
TooManyRecordsException
int(identifier[1])
identifier[0]
region=region)
self._new_record(changes,
new_ttl
r.identifier
identifier[0])]
len(results)
register_domain(self,
idn_lang_code=None,
auto_renew=None,
privacy_protect_admin_contact=None,
privacy_protect_registrant_contact=None,
privacy_protect_tech_contact=None):
'DurationInYears':
'AdminContact':
'RegistrantContact':
'TechContact':
params['AutoRenew']
params['PrivacyProtectAdminContact']
params['PrivacyProtectRegistrantContact']
params['PrivacyProtectTechContact']
'Nameservers':
auth_code
admin_contact
registrant_contact
tech_contact
admin_privacy
registrant_privacy
tech_privacy
g.display_name
g.uri
g.email_address
(u,
'AccessControlList':
grant):
Grant(permission=permission,
grant.to_xml()
display_name
'Grantee':
'DisplayName':
xsi:type="%s">'
'<DisplayName>%s</DisplayName>'
MultiPartUpload
boto.s3.multidelete
Lifecycle
boto.s3.bucketlogging
BucketLogging
self.key_class
key_class
'<Bucket:
"When
query_args_l.append('versionId=%s'
'&'.join(query_args_l)
Key.base_fields:
k.size
k.handle_encryption_headers(response)
k.handle_addl_headers(response.getheaders())
list(self,
key_marker,
'maxkeys':
['maxkeys',
'max_keys',
'encoding_type'])
self.key_class),
Prefix)],
Prefix),
('DeleteMarker',
'key_marker',
MultiPartUpload),
ValueError('Empty
allowed')
force_http=force_http,
headers[provider.mfa_header]
query_args=None):
preserve_acl:
self.connection.get_bucket(
'%s/%s'
provider.storage_class_header
headers[provider.metadata_directive_header]
subresource:
subresource=None')
make_public(self,
self.set_canned_acl('public-read',
S3Permissions:
policy.acl.add_email_grant(permission,
policy.acl.add_user_grant(permission,
logging_str
self.set_xml_logging(blogging.to_xml(),
BucketLogging()
type='Group',
uri=self.LoggingGroup)
query_args='requestPayment',
mfa_delete=False,
'Suspended'
data=fp.getvalue(),
suffix=None,
redirect_all_requests_to=None,
xml,
self.get_website_configuration_xml(headers=headers)
'.'.join(l)
handler.XmlHandler(resp,
contains_error
Tags()
query_args='tagging',
rs.next_key_marker
unquote_str(key_marker)
version_id_marker
key_marker=self.key_marker,
upload_id_marker
"<BucketLoggingStatus:
%s/%s
detect_potential_s3sigv4
build_url_base(self,
self.build_host(server,
connection.get_path(self.build_path_base(bucket,
'/%s'
build_path_base(self,
@assert_case_insensitive
'us-west-2'
anon=False,
no_host_provided
self.bucket_class
bucket_class
super(S3Connection,
@detect_potential_s3sigv4
self.provider.security_token:
query_auth=True,
expires_in_absolute=False,
query_auth:
response_headers.items():
extra_qp.append("%s=%s"
urllib.parse.quote(v)))
extra_qp:
'&'.join(extra_qp)
err.error_message
'Access
boto.log.debug('path=%s'
boto.log.debug('auth_path=%s'
auth_path)
allowed_header=None,
max_age_seconds=None,
expose_header=None):
'<Rule:
allowed_origin,
isinstance(allowed_origin,
[allowed_origin]
find_matching_headers
merge_headers_by_name
DefaultContentType
self.metadata
self.DefaultContentType
self.source_version_id
self.ongoing_restore
self.expiry_date
u'<Key:
%s,%s>'
self.local_hashes['md5']:
self.local_hashes:
isinstance(md5,
parts:
self.resp.status
self.__dict__[name.lower().replace('-',
override_num_retries=None):
BotoClientError('Invalid
size=0):
validate_dst_bucket=True):
new_storage_class
self.copy(bucket_name,
validate_dst_bucket=validate_dst_bucket)
dst_key,
dst_bucket.copy_key(dst_key,
self.metadata,
self.read_from_stream
digesters
dict((alg,
provider.storage_data_error(
'Cannot
(1024
self.BufferSize:
fp.read(bytes_togo)
chunk.encode('utf-8')
http_conn.send(chunk)
http_conn.send('\r\n')
self.local_hashes[alg]
digesters[alg].digest()
(cb_count
provider.storage_response_error(
find_matching_headers('Content-Encoding',
find_matching_headers('Content-Language',
find_matching_headers('Content-Type',
headers[content_type_headers[0]]
server_side_encryption_customer_algorithm
md5:
compute_md5(self,
provider.supports_chunked_transfer():
destination
reduced_redundancy,
encrypt_key=encrypt_key)
hash_algs=None,
torrent:
self.get_contents_to_file(fp,
underscore_name
underscore_name.replace('_',
metadata[h]
rewritten_metadata
rewritten_h
days,
StorageResponseError
self.key.size
IOError("Invalid
argument")
self.key.open_read(headers={"Range":
"bytes=%d-"
pos})
(%d)
readline(self):
transition=None):
Transitions()
'Expiration':
'Days':
"on:
"in:
days"
'<Days>%s</Days>'
'<Date>%s</Date>'
date=None,
prop)
status='Enabled',
self.delete_marker_version_id
'<Deleted:
%s.%s>'
'<Error:
'<Part
part_number_marker
self.initiated
self.part_number_marker
self.next_part_number_marker
self.max_parts
user.User(self)
part_num,
part_num
ValueError('Part
zero')
'uploadId=%s&partNumber=%d'
part_num)
key.set_contents_from_file(fp,
rng
end)
get_cur_file_size(fp,
cur_pos
'download
f.write('%s\n'
key.size:
download.')
GSKey):
override_num_retries=0,
override_num_retries=0)
fp.flush()
get_cur_file_size(fp)
self.append(tag)
self.append(tag_set)
self.suffix
self.error_key
self.redirect_all_requests_to
routing_rules
container=self)
encoding="UTF-8"?>',
''.join(parts)
self.container
xml_key,
self.translator:
'hostname'),
('Protocol',
'protocol'),
super(RedirectLocation,
self.condition
'Condition':
key_prefix=None,
http_error_code=None):
protocol=None,
replace_key=None,
replace_key_prefix=None,
http_redirect_code=None):
super(Condition,
super(Redirect,
boto.sdb.regioninfo
SDBRegionInfo
Domain,
SDBConnection()
item_names
self.item_cls
replace=False,
self.converter.encode(v)
params['%s.%d.Replace'
self.converter.encode(value)
params['Expected.1.Exists']
attr_names
attr_names:
params['Item.%d.Attribute.%d.Name'
params['Item.%d.Attribute.%d.Value'
params['Item.%d.Attribute.%d.Replace'
attribute_names:
%f
self._build_name_value_list(params,
self._build_expected_value(params,
batch_put_attributes(self,
self._build_batch_list(params,
item=None):
attr_names)
batch_delete_attributes(self,
'replace')
six.text_type(value,
self.item_count
self.item_names_size
self.attr_name_count
self.attr_names_size
self.attr_value_count
self.attr_values_size
self.item_id
attrs['id']
self.uploader.start()
self.last_key
self[self.last_key]
self.converter.decode(value)
load(self):
del_attrs
len(del_attrs)
del_attrs)
attr_names=None):
self.query,
self.max_items:
self._file
self._file:
hasattr(self.file,
"get_contents_as_string"):
self.file.get_contents_as_string()
Names
parents
boto.sdb.db.query
cls._manager
prop.__class__.__name__.startswith('_'):
manager=None):
manager:
cls.__dict__.keys():
cls.__dict__[key]
len(cls.__bases__)
cls.__bases__[0]
boto.sdb.db.manager.xmlmanager
XMLManager
cls._xmlmanager
XMLManager(cls,
fp):
self.properties(hidden=False):
self._manager
boto.log.exception(e)
self._manager.load_object(self)
assert(isinstance(attrs,
"Argument
self.reload()
props[prop.name]
props,
cls.__name__:
cls.__sub_classes__:
boto.sdb.db.blob
verbose_name
"on_set_%s"
self.slot_name,
__property_config__(self,
property_name):
model_class
property'
make_value_from_datastore(self,
1024:
maxlength')
super(TextProperty,
hashfunc=self.hashfunc)
self).__get__(obj,
re.match(self.validate_regex,
"s3://%s/%s"
default=0,
ValueError('Maximum
ValueError('Minimum
super(LongProperty,
bool
super(FloatProperty,
auto_now=False,
auto_now_add=False,
auto_now
self.auto_now_add
auto_now_add
self.auto_now_add:
self.now()
self).default_value()
self.auto_now:
setattr(model_instance,
self.now())
now(self):
super(TimeProperty,
"id")
model
model_instance,
model_instance)
self.calculated_type
self.use_method:
super(ListProperty,
required=True,
**kwds)
ValueError('Items
integers.'
instances'
self.item_type.__name__))
super(MapProperty,
ValueError('Values
Map
self.offset
self.sort_by
self.__local_iter__
self.filters,
self.sort_by,
self.select)
self._next_token
"%s%s"
self._inc(last_value))
lv=None):
domain_name=None,
self.last_value
new_val['timestamp']
expected_value=expected_value)
boto.config.get("DB",
MapProperty
logging.basicConfig()
log.setLevel(logging.DEBUG)
nums
ListProperty(int)
TestBasic()
t.foo
log.debug('saving
log.debug('now
it')
tt.id
t.value
test_basic()
t.nums
TestUnique()
t1
'db_user',
'db_passwd',
'db_table',
'db_host',
boto.config.getint('DB',
'db_port',
'enable_ssl',
db_section
'DB_'
boto.config.has_section(db_section):
db_type)
cls._db_name
sql_dir,
self.type_map
(self.encode_bool,
self.decode_bool),
(self.encode_int,
self.decode_int),
(self.encode_datetime,
self.type_map[long]
(self.encode_long,
self.decode_long)
self.type_map[item_type][0]
encode(value)
self.type_map[item_type][1]
decode(value)
self.encode(item_type,
self.encode_map(prop,
encoded_value
encode_prop(self,
ListProperty):
MapProperty):
self.encode(prop.data_type,
self.decode_map_element(item_type,
ret_value
self.decode(item_type,
decode_prop(self,
self.decode(prop.data_type,
encode_int(self,
2147483648
decode_int(self,
integer"
encode_long(self,
9223372036854775808
decode_long(self,
decode_bool(self,
exponent[1:].rjust(3,
999
int(exponent)
'2'
(10
float(mantissa))
mantissa.ljust(18,
'5':
(float(mantissa)
encode_datetime(self,
value.strftime(ISO8601)
value.isoformat()
decode_datetime(self,
value.split("-")
date(int(value[0]),
int(value[1]),
int(value[2]))
encode_reference(self,
decode_reference(self,
(key.bucket.name,
re.match("^s3:\/\/([^\/]*)\/(.*)$",
self.manager.get_s3_connection()
ddl_dir,
self.db_user
self.db_passwd
self.db_host
self.db_port
self.db_table
self.ddl_dir
ddl_dir
self.enable_ssl
boto.sdb.regions()
self._domain:
_object_lister(self,
encode_value(self,
prop:
self.converter.encode_prop(prop,
self.converter.decode_prop(prop,
get_s3_connection(self):
self.s3:
bucket_name=None):
self.domain.name)
s3.create_bucket(bucket_name)
load_object(self,
obj._loaded:
'__type__'
a[prop.name])
a['__type__'])
query_str
"select
(self.domain.name,
select=None):
null"
"not
order_by[1:]
select:
query_parts.append("(%s)"
"WHERE
AND
`__type__`
["__id__",
"itemName()"]:
itemName()
(order_by,
order_by_method)
filter_parts
filter_props
types.TypeType(value)
filter_parts_sub.append(self._build_filter(property,
filter_parts.append(self._build_filter(property,
decendents
query_gql(self,
query_string,
NotImplementedError("GQL
queries
save_object(self,
property.get_value_for_datastore(obj)
obj2
next(obj.find(**args))
obj2.id
SDBPersistenceError("Error:
unique!"
property.name)
except(StopIteration):
self.encode_value(prop,
delete_object(self,
set_property(self,
get_property(self,
set_key_value(self,
delete_key_value(self,
get_key_value(self,
a[name]
get_raw_item(self,
self.domain.get_item(obj.id)
item_node)
long(value)
val_node
self.impl.createDocument(None,
'objects',
base64string
authheader
cls(id)
obj.find_property(prop_name)
items_node
doc.getElementsByTagName('object')[0]
id:
NotImplementedError("Can't
connection")
self._make_request('GET',
parse(resp)
Exception("Error:
resp.status)
self.get_object_from_doc(cls,
parts.append("['%s'
type))
Node):
text_node
propname
cls=None,
parseString(fp)
parse(fp)
boto.services.servicedef
ServiceDef
service',
generated
stored
help="your
Access
Key")
submit
launched
instance(s)")
iq
iq:
iq.read()
print('deleted
messages'
ob
ob.name
ib.name:
k.delete()
keys'
self.options.path:
os.path.exists(self.options.path):
self.parser.error('Invalid
service')
self.sd.set('Credentials',
print('The
'do_%s'
self['OriginalLocation']
self['OriginalFileName']
t[1]
self['Bucket']
mimetype_files=None):
sd
self.total_time
self.min_time
self.max_time
self.earliest_time
self.latest_time
self.sd.get_obj('output_queue')
self.log_fp:
self.log_fp.write(line+'\n')
get_file=True):
'OutputBucket'
bucket.lookup(key_name)
print('retrieving
key.get_contents_to_filename(file_name)
get_file=True,
delete_msg=True):
self.queue.read()
self.num_files)
self.retry_count
self.output_domain
message:
(bucket_name,
process_file(self,
in_file_name,
input_message,
input_message:
write_message(self,
message['Host']
boto.log.info('Writing
self.output_domain:
shutdown(self):
on_completion
self.notify('Service:
output_message)
Service
super(SonOfMMM,
'/usr/local/bin/ffmpeg
self.output_ext
boto.log.info('Queueing
m.for_key(key,
num_cb=0,
prefix='/'):
ignore_dirs:
super(SESConnection,
"Address
blacklisted."
"Email
"Daily
quota
rate
ends
dot."
"Local
address"
response.reason
to_addresses,
Warning("You've
choose
other.")
'Domain':
dkim_enabled
identities):
identities,
'Identities.member')
connection_cls=SNSConnection)
prefix]
account_ids,
'Label':
{'Message':
target_arn
message_structure
sorted(message_attributes.keys())
enumerate(keys,
message_attributes[name]
sid_exists
queue.get_attributes('Policy')
platform_application_arn=None):
custom_user_data
endpoint_arn=None):
self[self.current_key]
'MessageId':
ResultEntry()
super(BigMessage,
s3_url):
SQSDecodeError(msg,
s3_conn
s3_conn.get_bucket(bucket_name)
boto.sqs.attributes
Attributes
'sqs'
queue_name,
{'QueueName':
queue_name}
Queue)
number_messages=1,
wait_time_seconds
self.get_status('DeleteMessage',
delete_message_batch(self,
'%s.%i.Id'
'%s.%i.ReceiptHandle'
'string_list_value'
attribute['string_list_value']
'binary_list_value'
attribute['binary_list_value']
receipt_handle,
change_message_visibility_batch(self,
[('QueueUrl',
Queue)])
SQSError:
action_name):
base64.b64encode(value.encode('utf-8')).decode('utf-8')
self.receipt_handle
self.message_attributes
self.md5_message_attributes
'MessageAttribute':
visibility_timeout)
self._body[key]
self.set_body(self._body)
super(EncodedMHMessage,
self.message_class
message_class
self.url:
wait_time_seconds=wait_time_seconds,
message_attributes=message_attributes)
fp.write(m.get_body())
fp.write(sep)
self.delete_message(m)
sep)
'%s/'
self.write(m)
connection_cls=STSConnection)
Credentials,
FederationToken,
Provider('aws',
NO_CREDENTIALS_PROVIDED,
token_key
duration:
params['SerialNumber']
params['TokenCode']
principal_arn,
saml_assertion,
duration_seconds=None):
'PrincipalArn':
'SAMLAssertion':
web_identity_token,
provider_id
'AssumeRoleWithWebIdentity',
self.session_token
file_path):
self.federated_user_arn
self.federated_user_id
self.packed_policy_size
'Arn':
self.assume_role_id
self.decoded_message
attachments,
cc_email_addresses=None,
params['caseId']
params['ccEmailAddresses']
language=None,
service_code
severity_code
category_code
issue_type
after_time=None,
before_time=None,
case_id_list
display_id
params['afterTime']
params['beforeTime']
include_resolved_cases
include_communications
case_id,
service_code_list
check_ids):
{'checkIds':
check_ids,
{'checkId':
swf_exceptions
data[item]
'identity':
decisions=None,
decisions,
task_start_to_close_timeout=None):
child_policy,
'tagList':
default_task_start_to_close_timeout=None,
'defaultTaskList':
'defaultTaskStartToCloseTimeout':
default_task_start_to_close_timeout,
activity_version):
activity_version}
workflow_version):
workflow_execution_retention_period_in_days,
name})
workflow_version=None):
latest_date},
workflow_version=None,
start_latest_date=None,
start_oldest_date=None,
close_latest_date=None,
close_oldest_date=None,
close_status=None,
start_oldest_date,
start_latest_date},
'closeTimeFilter':
close_oldest_date,
close_latest_date},
'closeStatusFilter':
close_status},
count_pending_decision_tasks(self,
count_pending_activity_tasks(self,
activity_type_name,
activity_type_version,
start_to_close_timeout=None,
input=None):
attrs['activityId']
heartbeat_timeout
schedule_to_close_timeout
schedule_to_start_timeout
attrs['childPolicy']
attrs['executionStartToCloseTimeout']
attrs['tagList']
attrs['taskStartToCloseTimeout']
workflow_type_version
attrs['timerId']
timer_id
attrs['runId']
workflow_type_name,
workflow_type_version,
task_start_to_close_timeout
Layer1Decisions
rep_str
status='REGISTERED',
act_objects
act_args['activityType']
wf_objects
wf_args['workflowType']
exe_objects
exe_args[nested_key]
task_list)
complete(self,
self.task_list
'task_list'
kwargs.get('task_list')
kwargs['task_list']
decisions
decision_task
'default_task_start_to_close_timeout':
'300',
args.update(kwargs)
**args)
kwargs['workflow_id']
signame,
self.runId,
PY34
sys.version_info[0:2]
class_types
binary_type
31)
self._resolve()
_resolve(self):
_import_module(self.mod)
__dir__(self):
self.attr
self.known_modules[self.name
fullname]
self.__get_module(fullname)
MovedModule):
get_code
mark
"xrange",
"range"),
"tkinter.filedialog"),
"tkinter.simpledialog"),
".moves.urllib"),
"robotparser",
"urllib.robotparser"),
setattr(_MovedItems,
_urllib_parse_moved_attributes
_urllib_error_moved_attributes
_urllib_request_moved_attributes
_urllib_response_moved_attributes
_urllib_robotparser_moved_attributes
_meth_func
_meth_self
_func_closure
_func_code
_func_defaults
_func_globals
advance_iterator
klass
get_unbound_function(unbound):
create_unbound_method(func,
types.MethodType(func,
iterkeys(d,
itervalues(d,
iteritems(d,
iterlists(d,
viewkeys
viewvalues
viewitems
b(s):
u(s):
chr
int2byte
_assertCountEqual
"assertRaisesRegexp"
"assertRegexpMatches"
getattr(moves.builtins,
print_(*args,
sys.stdout)
isinstance(sep,
string")
isinstance(end,
want_unicode:
flush
klass.__str__
451
CustomerGateway
RouteTable
InternetGateway
VpnGateway,
Attachment
DhcpOptions
VpnConnection
VpcPeeringConnection
vpc_ids=None,
vpc_ids:
vpc_ids,
'VpcId')
cidr_block}
VPC)
self._replace_route_table_association(
gateway_id=None,
interface_id=None,
vpc_peering_connection_id=None,
params['GatewayId']
params['VpcPeeringConnectionId']
vpc_id:
egress=None,
icmp_code=None,
icmp_type=None,
port_range_from=None,
port_range_to=None):
cidr_block
params['Icmp.Code']
params['Icmp.Type']
params['PortRange.From']
params['PortRange.To']
'BgpAsn':
vpn_gateway_id}
Attachment)
params[key_name]
'domain-name',
'domain-name-servers',
'ntp-servers',
'netbios-name-servers',
'netbios-node-type',
{'DhcpOptionsId':
VpnConnection)
peer_owner_id
VpcPeeringConnection)
self.bgp_asn
'customerGatewayId':
super(DhcpOptions,
'dhcpOptionsId':
super(InternetGateway,
super(NetworkAcl,
'associationSet':
'networkAclId':
self.rule_action
self.egress
self.port_range
self.icmp
self.network_acl_id
super(RouteTable,
'routeTableId':
self.gateway_id
self.interface_id
self.vpc_peering_connection_id
'destinationCidrBlock':
'vpcPeeringConnectionId':
self.route_table_id
self.main
self.available_ip_address_count
self.dhcp_options_id
self.is_default
(self.id,))
self._get_status_then_update_vpc(
validate=validate,
vpc_id=self.id,
super(VpcPeeringConnection,
self.accepter_vpc_info
VpcInfo()
self.requester_vpc_info
self.expiration_time
self._status
self.static_routes_only
self.last_status_change
self.accepted_route_count
super(VpnConnection,
self.customer_gateway_configuration
self.customer_gateway_id
self.vpn_gateway_id
'vpnGatewayId':
super(VpnGateway,
att
inliner.document.settings.env.app
options)
commit,
re.I),
self._resolver
self.get_available_partitions():
endpoint_prefix,
partition_data
self._get_partition_data(partition_name)
self._endpoint_data['partitions']:
partition['partition']
endpoints_for_service
endpoints[region_name]
unittest2
tests.
remaining_args
strs
ListProperty(str)
cls.objs
cls.objs:
o.delete()
[5,
"D",
"Foo"]
SimpleListModel.get_by_id(t.id)
SimpleListModel.all().filter("strs
hmac.new('mysecret',
PasswordProperty
MyModel(Model):
myhashfunc
obj.password
self.assertTrue(obj.password
self.assertEquals(obj.password,
self.assertEquals(str(obj.password),
self.assertEquals('foo',
curdir
srcroot
SimpleModel()
"Sub
SequenceGenerator
assert(gen("")
assert(gen("A")
assert(gen("B")
"C")
assert(gen("C")
"AA")
Sequence(s.id)
Sequence(fnc=increment_string)
fib
34,
89,
144]:
lv)
"Z"
AMAZON_USER_TOKEN
logging_bucket
c.create_bucket(bucket_name
bucket.enable_logging(target_bucket=logging_bucket,
open('foobar',
open('foobar')
fp.read(),
file.read(),
os.unlink('foobar')
len(all)
bucket.lookup('foo/bar',
'has_metadata'
rs1
rs1:
bucket.get_acl(headers=DEVPAY_HEADERS)
k.get_acl(headers=DEVPAY_HEADERS)
bucket.add_email_grant('foobar',
S3PermissionsError:
devpath
os.path.relpath(os.path.join('..',
'..'),
start=os.path.dirname(__file__))
[devpath]
tests;
advanced:
self.assertTrue(hasattr(response,
accountbalance
response.GetAccountBalanceResult.AccountBalance
self.assertTrue(hasattr(accountbalance,
@unittest.skip('cosmetic')
special_access_required
snippet
str(random.randint(1,
1000000))
'app-'
'version-'
'template-'
'environment-'
Layer1Wrapper()
self.beanstalk.create_application(application_name=self.app_name)
self.beanstalk.create_application_version(
solution_stack_name='32bit
env_name,
environment_name=self.environment,
info_type='tail')
"Test
Parameter
"String"
"Ref":
"Public
"Private
DNSName
self.stack_name,
template_body=json.dumps(BASIC_EC2_TEMPLATE),
parameters=[('Parameter1',
self.connection.describe_stacks(self.stack_name)
[(p.key,
p.value)
self.assertEquals([('Parameter1',
range(30):
stack.update()
stack.stack_status.find("PROGRESS")
CloudSearchCertVerificationTest(unittest.TestCase,
conn.describe_domains()
CloudSearchLayer1Test(unittest.TestCase):
super(CloudSearchLayer1Test,
self.layer1.create_domain(self.domain_name)
self.addCleanup(self.layer1.delete_domain,
CloudSearchLayer2Test(unittest.TestCase):
super(CloudSearchLayer2Test,
self.layer2.create_domain(self.domain_name)
self.addCleanup(domain.delete)
self.assertTrue(domain.created,
self.assertEqual(domain.domain_name,
test_initialization_regression(self):
us_west_2
endpoint='cloudsearch.us-west-2.amazonaws.com'
Layer2(
region=us_west_2,
host='cloudsearch.us-west-2.amazonaws.com'
self.layer2.layer1.host,
'cloudsearch.us-west-2.amazonaws.com'
cloudtrail
cloudtrail.describe_trails()
cloudtrail.get_trail_status(name='test')
tests.integration.cognito
CognitoTest
self.cognito_identity.describe_identity_pool(
test_resource_not_found_exception(self):
identity_pool_id='us-east-0:c09e640-b014-4822-86b9-ec77c40d8d6f'
self.cognito_sync.describe_identity_pool_usage(
self.configservice
self.configservice.describe_configuration_recorders()
self.assertIn('ConfigurationRecorders',
datapipeline
conn.list_pipelines()
'workerGroup',
'Default',
'Default'},
'Schedule'},
'refValue':
self.connection.put_pipeline_definition(self.sample_pipeline_objects,
'Default')
self.get_pipeline_state(pipeline_id)
DynamoDBKeyNotFoundError
DynamoDBConditionalCheckFailedError
'forum_name'
'subject'
self.read_units,
provisioned_throughput)
table_name)
DynamoDB
hash_key_name
hash_key_type
range_key_type
result_thruput
result_thruput['ReadCapacityUnits']
result_thruput['WriteCapacityUnits']
c.list_tables()
item1_data
item1_range},
item1_data)
c.get_item(table_name,
self.assertRaises(DynamoDBKeyNotFoundError,
['Message',
'Views']
consistent_read=True,
self.assertRaises(DynamoDBConditionalCheckFailedError,
attribute_updates=attribute_updates)
item2_key},
key2
item3_key
item3_range
item3_key},
'Count'
result['Count']
self.create_table(self.table_name,
c.describe_table(self.table_name)
{self.hash_key_type:
{self.range_key_type:
'BinaryData':
c.create_schema(self.hash_key_name,
table.read_units
table.write_units
table2_name
item1_attrs
item1_attrs[self.hash_key_name]
foobar_item.hash_key
item1_attrs[self.range_key_name]
foobar_item.range_key
item1
item1.put()
range_key=item1_range,
consistent_read=True))
item1.delete(expected_value=expected)
replies_by_set)
retvals
item1_updated)
item3
item3_range,
item3.put()
range_key_condition=BEGINS_WITH('DynamoDB'))
set([1.1,
2.2,
3.3,
set(['foo',
c.new_batch_list()
c.new_batch_write_list()
0}
set([Binary(b'\x01\x02'),
self.assertEqual(retrieved['BinaryData'],
self.dynamodb.use_decimals()
item['decimalvalue']
retrieved)
self.dynamodb.create_table(
boto.dynamodb2.table
NUMBER,
test_integration(self):
indexes=[
self.assertEqual(len(users.schema),
self.assertEqual(users.throughput['read'],
users_hit_api
users_hit_api.describe()
users.batch_write()
batch.put_item({
jane['username']
jane['first_name']
'Jane'
jane['friend_count']
self.assertTrue(jane.save(overwrite=True))
'Feed
last_name__eq='Doe',
index='LastNameIndex',
self.assertEqual(list(res.keys()),
attributes=('first_name',)
c_results
c_results:
self.assertTrue(res['first_name']
self.assertEqual(next(filtered_users)['username'],
johndoe
users.batch_get(keys=[
4},
['Bob',
'Jane'])
'mau5',
attributes=['username',
HashKey('user_id'),
global_indexes={
'7',
'24',
'35',
sorted([user['username']
rs])
posts
HashKey('thread'),
RangeKey('posted_on')
self.addCleanup(posts.delete)
test_data_path
'forum_test_data.json'
open(test_data_path,
test_data:
json.load(test_data)
posts.batch_write()
batch.put_item(post)
'joe',
posted_on__gte='2013-12-24T00:00:00'
posts.query(
GlobalAllIndex('EmailGSIIndex',
HashKey('email')
email__eq='johndoe@johndoe.com',
index='EmailGSIIndex'
rs_item
self.assertEqual(rs_item['username'],
'EmailGSIIndex':
global_index=GlobalAllIndex(
self.hash_key_type,
self.provisioned_throughput,
record_1
self.assertEqual(record_1['Item']['username']['S'],
self.assertEqual(record_1['Item']['first_name']['S'],
self.dynamodb.batch_write_item({
self.dynamodb.scan(self.table_name)
['jane',
'johndoe'])
time.sleep(20)
self.dynamodb.scan(self.table_name,
total_segments=2)
self.assertTrue(results['Count']
image.get_launch_permissions()
group_desc
testing'
group1
group_desc)
c.get_all_security_groups()
c.authorize_security_group(group1.name,
group2.owner_id)
c.revoke_security_group(group1.name,
group2.owner_id,
22)
socket.error:
instance.terminate()
c.get_all_key_pairs()
demo_paid_ami_product_code
c.run_instances(
instance_ids=[rs.instances[0].id],
AutoScaleConnection()
activity
c.get_all_launch_configurations()
lc_name
'lc-%s'
image_id='ami-2272864b',
c.create_launch_configuration(lc)
c.get_all_tags()
tag.resource_id
tag.key
instance_type='t1.micro',
DESCRIBE_ALARMS_BODY
test_build_list_params(self):
['thing1',
'thing3'],
'ThingName%d')
'ThingName1':
'thing1',
'ThingName2':
'ThingName3':
'thing3'
dimensions={"D":
"V"})
name=["N",
"M"],
'MetricData.member.2.Value':
'MetricData.member.2.Dimensions.member.1.Name':
'MetricData.member.2.Dimensions.member.1.Value':
"V"),
OrderedDict((("D1",
c.build_dimension_param(dimensions,
'Dimensions.member.1.Name':
'Dimensions.member.1.Value':
'Dimensions.member.2.Name':
'Dimensions.member.2.Value':
24,
now,
{u'Job':
[(80,
self.conn.create_load_balancer(
self.availability_zones,
self.listeners)
self.conn.get_all_load_balancers()
sorted(l.get_tuple()
sorted(self.listeners
mod_balancers
load_balancer_names=[mod_name])
mod_balancers],
[mod_name])
'lb-policy'
self.conn.create_lb_cookie_stickiness_policy(
lb_policy_name)
app_policy_name)
8081
backend_port,
self.assertEqual(len(balancers[0].policies.other_policies),
other_policy_name)
self.assertEqual(len(balancers[0].backends),
(8080,
(2525,
'TCP',
'TCP'),
attributes.access_log.enabled)
self.conn.get_lb_attribute(self.balancer.name,
attributes.connection_draining.enabled)
attributes.connection_draining.timeout)
self.balancer.name,
256)
NetworkInterfaceCollection
NetworkInterfaceSpecification
PrivateIPAddress
device_index=0,
primary=False),
self.assertEqual(len(retrieved),
retrieved_instances
retrieved[0].instances
self.assertEqual(len(retrieved_instances),
retrieved_instance
retrieved_instances[0]
self.assertEqual(len(retrieved_instance.interfaces),
retrieved_instance.interfaces[0]
interface.private_ip_addresses
associate_public_ip_address=True,
delete_on_termination=True
self.api.run_instances(
network_interfaces=interfaces
self.addCleanup(self.terminate_instances)
test_list_clusters(self):
test_handle_not_found_exception(self):
600
self.elasticache.describe_cache_clusters(cluster_id)
self.fail('Timeout
self.sns
boto.connect_sns()
'boto-pipeline-%s'
self.addCleanup(self.s3.delete_bucket,
self.api.create_pipeline(
self.pipeline_name,
self.input_bucket,
self.output_bucket,
self.role_arn,
''})
pipeline['Pipeline']['Id']
'Paused')
self.assertEqual(vault.name,
self.assertEqual(vault.number_of_archives,
self.fail_after_n_bytes
self.exception
self.fp_to_change
self.fp_change_pos
total_bytes_transferred)
'\s*<Permission>FULL_CONTROL</Permission>\s*</Entry>\s*<Entry>'
open(fpath)
'foobar')
open(fpath2,
bucket.get_all_keys(prefix='foo')
bucket.get_all_keys(maxkeys=5)
bucket.lookup('notthere')
self._MakeKey(bucket=bucket)
u'föö'
mdkey3
'meta3'
k.set_metadata(mdkey3,
self.assertEqual(k.get_metadata(mdkey1),
self.assertEqual(k.get_metadata(mdkey2),
self.assertEqual(k.get_metadata(mdkey3),
bucket.set_acl('public-read')
bucket.set_acl('private')
k.set_acl('public-read')
k.set_acl('private')
version='1.0'
bucket.disable_logging()
bucket1
bucket2
self.assertIsNotNone(re.search(PROJECT_PRIVATE_RE,
acl.to_xml()))
public_read_acl
bucket.set_def_acl('private')
uri.set_def_acl('private')
bucket.get_cors().to_xml())
CORS_EMPTY)
CORS_DOC)
uri.get_cors().to_xml())
bucket.get_lifecycle_config().to_xml()
LIFECYCLE_EMPTY)
lifecycle_config.add_rule('Delete',
LIFECYCLE_CONDITIONS)
LIFECYCLE_DOC)
uri.get_lifecycle_config().to_xml()
sax
tempfile.NamedTemporaryFile(prefix="boto-gs-test",
delete=False)
k.set_contents_from_filename(fname1,
k.set_contents_from_filename(fname2,
b.set_canned_acl("public-read",
acl.to_xml()
boto.s3.resumable_download_handler
cb_test_harness
CallbackTestHarness
SMALL_KEY_SIZE
LARGE_KEY_SIZE
make_tracker_file(self,
ResumableDownloadHandler()
ResumableDownloadException')
open(tracker_file_name)
test_retryable_exception_recovery(self):
test_broken_pipe_recovery(self):
IOError(errno.EPIPE,
"Broken
pipe")
test_non_retryable_exception_handling(self):
exception=OSError(errno.EACCES,
'Permission
denied'))
OSError')
OSError
self.assertEqual(e.errno,
test_multiple_in_process_failures_then_succeed(self):
test_multiple_in_process_failures_then_succeed_with_tracker_file(self):
fail_after_n_bytes=LARGE_KEY_SIZE/2,
num_times_to_fail=2)
larger_src_key_as_string
os.urandom(LARGE_KEY_SIZE)
larger_src_key
self._MakeKey(data=larger_src_key_as_string)
self.assertEqual(larger_src_key_as_string,
larger_src_key.get_contents_as_string())
len(harness.transferred_seq_before_first_failure)
self.make_dst_fp(tmp_dir)
save_mod
os.stat(tmp_dir).st_mode
tracker_file_name=tracker_file_name)
e.message.find('Couldn\'t
file'),
save_mod)
self.fail("should
rewind=True)
res_upload_handler.get_tracker_uri())
self.assertEqual(larger_src_file_as_string,
self.assertTrue(len(harness.transferred_seq_before_first_failure)
meta',
test_file_size
test_file
self.build_input_file(test_file_size)[1]
test_file,
Execute():
KB
self.assertFalse(uri.has_version())
b.new_key("obj")
k.set_contents_from_string("stringdata")
storage_uri("gs://%s/"
bucket_uri.object_name
all_users_read_permission
storage_uri("gs://%s"
self.assertGreater(key_uri.generation,
k.generation)
self.assertFalse(b.get_versioning_status())
b.configure_versioning(True)
self.assertEqual(versions[1].name,
generations
[k.generation
versions]
self.assertIn(g1,
self.assertIn(g2,
b.get_key("foo",
"foo",
r'[0-9]+')
self.assertEqual(k.metageneration,
self._conn
self._MakeTempName()
set_contents:
mdelay
try_one_last_time
mtries
'boto-test-user-%d'
iam.create_user(username)
iam.delete_user(username)
boto.exception.BotoServerError
srv_error:
srv_error.status
srv_error
test_min_length
Exception("Failed
policy")
['get_account_password_policy_result']['password_policy']
iam.list_entities_for_policy(policy.arn)[
'list_entities_for_policy_response'][
'list_entities_for_policy_result']
len(result['policy_roles'])
Exception("Roles
len(result['policy_groups'])
Exception("Groups
len(result['policy_users'])
Exception("Users
rolename)
groupname)
kinesis
self.kinesis
aborting...')
kinesis.list_tags_for_stream(stream_name='test')
self.assertEqual(len(response['Tags']),
shard_iterator
'127.0.0.1
frank
"GET
/apache_pb.gif
HTTP/1.0"
self._marketplace
self.mws.list_marketplace_participations()
response.ListMarketplaceParticipationsResult
'144930544X'
ASINList=[asin])
opsworks
test_describe_stacks(self):
'us-east-1',
connect_to_region('us-east-1')
RDSCertVerificationTest(unittest.TestCase,
_is_ok(subnet_group,
VPCConnection()
grp_name,
[subnet[0].id,subnet[1].id]):
rds_api.modify_db_subnet_group(grp_name,
subnet_grps
_is_ok(subnet_grps[0],
Exception("modifying
self.conn.get_all_dbinstances()
self.replicaDB_name,
renaming
self.conn.get_all_dbinstances(self.renamedDB_name)
self.db_name,
time.sleep(60
self.assertEqual(inst['AllocatedStorage'],
redshift
boto.redshift.exceptions
self.cluster_prefix
self.wait_time
self.cluster_id()
self.api.create_cluster(
self.node_type,
self.master_username,
self.master_password,
db_name=self.db_name,
number_of_nodes=3
self.api.delete_cluster(cluster_id,
skip_final_cluster_snapshot=True)
test_as_much_as_possible_before_teardown(self):
self.assertEqual(response['CreateClusterSnapshotResponse']\
['CreateClusterSnapshotResult']['Snapshot']\
snapshot_id)
'manual')
super(Route53TestCase,
self.conn.create_zone(self.base_domain)
boto.route53.exception
DNSServerError
super(TestRoute53AliasResourceRecordSets,
alias_dns_name="target.%s"
alias_hosted_zone_id=self.zone.id,
new.set_alias(self.zone.id,
delete.set_alias(self.zone.id,
tests.integration.route53
Route53TestCase
boto.route53.healthcheck
hc_type="HTTPS",
result[u'CreateHealthCheckResponse'][u'HealthCheck'][u'HealthCheckConfig'])
result['ListHealthChecksResponse']['HealthChecks']:
resource_path="/testing",
string_match="test")
self.assertEquals(result[u'CreateHealthCheckResponse'][u'HealthCheck'][u'HealthCheckConfig'][u'SearchString'],
'HTTPS_STR_MATCH')
ttl=30,
identifier='test',
weight=1,
health_check=result['CreateHealthCheckResponse']['HealthCheck']['Id'])
records.commit()
hc_params
HealthCheck(**hc_params)
hc_config
(result[u'CreateHealthCheckResponse']
[u'HealthCheck'][u'HealthCheckConfig'])
rrs.add_change("CREATE",
"vpn.%s."
hostid
range(hosts):
rec
".%s"
rec,
'192.168.0.'
(hostid
all_records:
setUpClass(self):
self.zone.get_nameservers()
self.zone.get_a(self.base_domain)
u'%s.'
'www.%s'
self.zone.get_cname('www.%s'
u'www.%s.'
'10
self.zone.get_mx(self.base_domain)
self.assertEquals(set(record.resource_records),
set([u'10
self.base_domain]))
self.zone.add_a('wrr.%s'
self.zone.find_records(
all=True
self.zone.add_a('lbr.%s'
'4.3.2.1',
'8.7.6.5',
self.zone.delete_a('lbr.%s'
self.zone.add_a('exception.%s'
tearDownClass(self):
time_str
test_create_private_zone(self):
private_zone=True,
self.route53domains.check_domain_availability(
GMT'
'<MockKey:
fp.write(self.data)
override_num_retries=NOT_IMPL):
len(self.data)
size=NOT_IMPL):
reduced_redundancy=NOT_IMPL):
hex_md5
MockAcl()
src_version_id=NOT_IMPL,
storage_class=NOT_IMPL,
preserve_acl=NOT_IMPL,
encrypt_key=NOT_IMPL,
self.acls[self.name]
self.subresources[subresource]
mfa_token=NOT_IMPL):
boto.exception.StorageResponseError(404,
'Not
key_or_prefix
MockAcl(acl_or_str)
debug=NOT_IMPL,
location=NOT_IMPL,
storage_class=NOT_IMPL):
own
self.buckets[bucket_name]
'NoSuchBucket',
MockConnection()
self.new_key()
self.get_key()
key.get_contents_to_file(fp)
Transition
RedirectLocation
key.set_contents_from_string(key_name)
"b")
self.assertEqual(rs.next_marker,
"a/")
expected.pop(0))
bls
sb.get_logging_status()
self.assertEqual(bls.target,
self.assertEqual(response[0][0].key,
self.assertEqual(response[0][0].value,
S3ResponseError,
raised.")
'avalue')
'anothervalue')
self.bucket.configure_website(
'WebsiteConfiguration':
'HostName':
lifecycle.add_rule('myid',
actual_lifecycle
self.assertEqual(actual_lifecycle.prefix,
Transition(days=30,
'GLACIER')
'GLACIER'
storage_class=sc))
self.bucket.configure_lifecycle(lifecycle)
readlifecycle
self.bucket.get_lifecycle_config();
readlifecycle:
self.assertEqual(rule.expiration.date,
self.assertEqual(rule.transition.storage_class,
sc)
days)
connect_to_region
connect_to_region('us-east-1',
connect_to_region('us-west-2',
connect_to_region('us-west-2')
self.assertEquals('s3-us-west-2.amazonaws.com',
c.create_bucket(bucket_name)
force_http=True)
k.generate_url(60,
reduced_redundancy=True)
bucket.new_key('has_metadata')
k.get_metadata(mdkey3)
next(iter(anon_bucket.list()))
expose_header='x-amz-server-side-encryption',
id='foobar_rule')
self.bucket.get_cors()
test_1_versions(self):
encrypt_key=True)
@attr('notdefault',
self.do_test_valid_cert()
self.do_test_invalid_signature()
do_test_invalid_host(self):
'gs_host',
's3_host',
self.assertConnectionThrows(
self.do_test_invalid_host()
self.assertEqual(sfp.tell(),
content[5:10])
good_md5
k.compute_md5(sfp)
md5=good_md5)
self.assertRaises(key.provider.storage_response_error):
self.assertEqual(check.content_type,
plus
(+)')
u'filename=Schöne
GMT')
'public,%20max-age=500')
'filename=Sch%C3%B6ne%20Zeit.txt')
'Thu,%2001%20Dec%201994%2016:00:00%20GMT')
self.bucket.new_key("testkey_for_sse_c")
self.conn.create_bucket(self.bucket_name,
Key(self.bucket)
k.key
k.set_contents_from_string(body)
from_s3_key
self.assertEqual(from_s3_key.get_contents_as_string().decode('utf-8'),
validate=True)
self.bucket.configure_versioning(True)
Key(self.bucket,
'testkey')
second_key
self.bucket.get_key('copiedkey')
self.bucket.delete_key(k.name,
version_id=k.version_id)
8):
time.sleep(2**i)
d['MfaDelete'])
v1'
version_id=v1)
self.assertEqual('Suspended',
self.bucket.delete_keys(self.bucket.list_versions())
key_names
['key-%03d'
self.assertEqual(n,
nkeys)
'multipart-%d'
mpu.upload_part_from_file(fp,
part_num=1)
cmpu
mpu.complete_upload()
self.assertEqual(cmpu.key_name,
self.assertNotEqual(cmpu.etag,
self.bucket.list_multipart_uploads()
mpus.append(self.bucket.initiate_multipart_upload(key_name))
"k"
"01234567890123456789"
StringIO(contents)
part_num=2,
part_num=3,
part_num=4,
part.size)
etags[pn]
self.assertEqual(pn,
self.assertEqual(etags[1],
etags[3])
self.assertEqual(etags[2],
etags[4])
self.assertNotEqual(etags[1],
etags[2])
uparts.append(mpu.upload_part_from_file(fp,
print("Running
s3.create_bucket('test-%d'
out.size
small_names
range(5)
self.bucket.configure_versioning(versioning=True)
self.bucket.new_key("foobar")
self.assertEqual(s2,
len(rs))
versions)
iter(self.bucket.get_all_versions())
listed_kv2
self.assertEqual(listed_kv2.version_id,
kv2.version_id)
self.assertEqual(listed_kv2.is_latest,
c.get_all_domains()
num_domains
same_value,
'select
domain.select(query,
{'item3':
domain.get_attributes('item3',
ses
response['GetIdentityDkimAttributesResponse']
getheader(self,
self.connection.list_platform_applications()
mpo(https,
topic_name
(now)
self.sqsc.create_queue(queue_name,
self.addCleanup(self.sqsc.delete_queue,
queue_arn
self.snsc.create_topic(topic_name)
topic['CreateTopicResponse']['CreateTopicResult']\
self.addCleanup(self.snsc.delete_topic,
found_expected_sid
Timer
boto.sqs.bigmessage
BigMessage
TestBigMessage(unittest.TestCase):
c.get_all_queues()
queue_1.get_timeout()
time.sleep(90)
queue_1.read()
num_msgs
[(i,
range(num_msgs)]
queue_1.write_batch(msgs)
queue_2.write(message)
c.create_queue(queue_name)
poll_seconds
total_time
self.assertTrue(total_time
poll_seconds,
"SQS
seconds:
(poll_seconds,
total_time))
self.assertEqual(message.get_body(),
initial_count
self.assertEqual(len(conn.get_all_queues()),
m1
m1.set_body('This
message.')
current_timestamp
self.create_temp_queue(conn)
time.sleep(65)
test.get_messages(
self.assertEqual(msg.attributes['ApproximateReceiveCount'],
first_rec
msg.attributes['ApproximateFirstReceiveTimestamp']
alt_key
Credentials
STSConnection()
test_assume_role_with_web_identity(self):
'arn:aws:iam::000240903217:role/FederatedWebIdentityRole'
wit
'b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9'
role_session_name='guestuser',
web_identity_token=wit,
provider_id='www.amazon.com',
403)
err.body)
self.api.describe_cases()
communication_body="This
BOTO_SWF_UNITTEST_DOMAIN
workflow
description'
_default_task_start_to_close_timeout
'30'
default_task_start_to_close_timeout=
self._default_task_start_to_close_timeout,
swf_exceptions.SWFTypeAlreadyExistsError:
self._activity_type_name,
self._activity_type_version,
'REGISTERED')
r['typeInfos']:
latest_date
oldest_date
tag='ig')
workflow_id='ig')
workflow_name='ig',
workflow_version='ig')
self._task_list,
decision
occurred'
tevent
decisions.fail_workflow_execution(
raised'
atask['taskToken'],
'wfid-%.2f'
(time.time(),)
self.conn.start_workflow_execution(self._domain,
execution_start_to_close_timeout='20',
input='[600,
r['runId']
self.run_worker()
self.conn.get_workflow_execution_history(self._domain,
reverse_order=True)['events'][0]
doctest
create_hit_with_qualifications
hits_to_process)
total_hits
hits_processed
SetHostMTurkConnection,
qn
response_groups=['Minimal',
'HITDetail',
'HITQuestion',
conn.create_hit(question=q,
title="Boto
0.05,
assert(create_hit_rs.status
question=self.get_question(),
self.create_hit_result()
new_result
pickle.loads(pickle.dumps(result))
selenium
selenium(*sel_args)
'Server
identity
self.actual_request
status_code,
reason='',
header=[],
self.default_body()
dict(header)
self.environ
self.config_patch
mock.patch('boto.provider.config.get',
self.get_config)
self.has_config_patch
mock.patch('boto.provider.config.has_option',
self.has_config)
self.environ_patch
self.environ)
self.config_patch.start()
self.has_config_patch.start()
self.environ_patch.start()
self.config_patch.stop()
self.has_config_patch.stop()
self.environ_patch.stop()
has_config(self,
self.assertDictEqual({
suppress_consec_slashes=False
'/folder//image.jpg')
self.assertEqual(conn.proxy,
'127.0.0.1')
self.assertEqual(conn.proxy_port,
proxy="127.0.0.1",
proxy_user="john.doe",
proxy_pass="p4ssw0rd",
proxy_port="8180"
V4AuthConnection(
'testhost',
conn.build_base_http_request(
host=None)
conn.set_host_header(request)
self.assertEqual(request.headers['Host'],
proxy="NON_EXISTENT_HOSTNAME",
[b'foo'])
[b'baz'])
resp1
con2)
con3)
content_type='text/xml')
{'Some-Header':
HTTPRequest('PUT',
'Body')
request.authorize(mock_connection)
bse.message)
self.assertEqual(bse.error_code,
@httprettified
self.assertEqual([],
HTTPretty.latest_requests)
'Your
self.assertEqual(jre.error_message,
self.assertEqual(jre.code,
'us-west-1':
'ec2.us-west-1.amazonaws.com',
's3.amazonaws.com'
'ec2.auto-resolve.amazonaws.com',
'ec2.us-west-2.amazonaws.com',
self.assertTrue('us-east-1'
ec2_regions
self.assertTrue(len(ec2_regions)
ec2_regions:
self.assertNotEqual(west_2,
"Couldn't
us-west-2
region!")
self.assertTrue(isinstance(west_2,
RegionInfo))
self.assertEqual(west_2.name,
self.assertEqual(west_2.endpoint,
'ec2.us-west-2.amazonaws.com')
self.assertEqual(west_2.connection_cls,
'/-/vaults/foo/archives',
copy.copy(self.request)
auth.add_auth(req)
'two
c"
test_canonical_query_string(self):
u'We
\u2665
test_canonical_uri(self):
8080,
'glacier.us-east-1.amazonaws.com')
HmacAuthV4Handler('queue.amazonaws.com',
self.awesome_bucket_request
'X-AMZ-Content-sha256':
'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855',
'X-AMZ-Date':
'20130605T193245Z',
provider=self.provider,
host='s3-us-west-2.amazonaws.com',
self.assertEqual(auth.region_name,
test_add_auth(self):
self.request.headers)
'hello%2Bworld.txt')
'why
there',
mod_req
self.assertEqual(mod_req.path,
self.assertEqual(mod_req.auth_path,
self.assertEqual(mod_req.params,
authed_req
self.assertEqual(authed_req,
['nope']
test_sigv4_opt_out(self):
FakeS3Connection(host='s3.amazonaws.com')
['nope'])
test_sigv4_non_optional(self):
FakeS3Connection(host='s3'
fake._required_auth_capability(),
test_sigv4_opt_in_config(self):
test_sigv4_opt_in_env(self):
name='cn-north-1',
'ProviderId':
'Action=AssumeRoleWithWebIdentity'
'&ProviderId=2012-06-01&RoleSessionName=web-identity-federation'
'&Version=2011-06-15&WebIdentityToken=Atza%7CIQEBLjAsAhRkcxQ')
function_data
b'This
my
self.assertEqual(self.actual_request.body,
function_data)
self.actual_request.headers['Content-Length'],
str(len(function_data))
'/2014-11-13/functions/my-function?Handler=myhandler&Mode'
'=event&Role=myrole&Runtime=nodejs'
open(full_path,
simple(error)
self.assertEqual(exception.__class__.__name__,
'TooManyApplications')
{u'PermittedFileTypes':
u'SolutionStackName':
[u'32bit
api_response['ListAvailableSolutionStacksResponse']\
['ListAvailableSolutionStacksResult']\
1343067094.342,
u'Description':
'mybucket',
self.service_connection.create_environment(
option_settings=[
('aws:autoscaling:launchconfiguration',
'mykeypair'),
('aws:elasticbeanstalk:application:environment',
'CreateEnvironment',
'OptionSettings.member.1.Namespace':
'aws:autoscaling:launchconfiguration',
'OptionSettings.member.1.OptionName':
'OptionSettings.member.1.Value':
'mykeypair',
'OptionSettings.member.2.Namespace':
'aws:elasticbeanstalk:application:environment',
'OptionSettings.member.2.OptionName':
'OptionSettings.member.2.Value':
'VALUE1',
'1.0',
{u'StackId':
self.stack_id},
template_url='http://url',
tags={'TagKey':
'TagValue'},
notification_arns=['arn:notify1',
'arn:notify2'],
disable_rollback=True,
timeout_in_minutes=20,
'CreateStack',
'NotificationARNs.member.1':
'arn:notify1',
'NotificationARNs.member.2':
'arn:notify2',
'Parameters.member.1.ParameterKey':
'KeyName',
'Parameters.member.1.ParameterValue':
'myKeyName',
'TagKey',
'TagValue',
'TimeoutInMinutes':
{"Code":
"Message":
arg."}}')
self.service_connection.update_stack(
'UpdateStack',
self.service_connection.delete_stack('stack_name')
json.dumps('fake
response').encode('utf-8')
self.service_connection.describe_stack_resource(
'resource_id')
'fake
self.service_connection.get_template('stack_name')
self.assertEqual(first.logical_resource_id,
self.assertEqual(first.physical_resource_id,
self.assertEqual(first.resource_status,
self.assertEqual(first.resource_status_reason,
self.assertEqual(first.resource_type,
self.assertEqual(first.stack_id,
self.assertEqual(first.stack_name,
self.assertIsNotNone(first.timestamp)
self.assertEqual(second.logical_resource_id,
self.assertEqual(second.physical_resource_id,
self.assertEqual(second.resource_status,
self.assertEqual(second.resource_status_reason,
self.assertEqual(second.resource_type,
self.assertEqual(second.stack_id,
self.assertEqual(second.stack_name,
self.assertIsNotNone(second.timestamp)
'logical_resource_id',
self.assertEqual(len(stacks),
16,
'REASON')
'CAPABILITY_IAM')
'MyStack',
47,
Description.')
'{}',
'stack-id',
boto.cloudformation.stack.Stack)
xml.sax.parseString(SAMPLE_XML,
rs[0].creation_time
28,
19,
57,
self.assertFalse(disable_rollback)
self.assertTrue(disable_rollback)
"EEEEEEEEEEEEE")
"abcdef12345678.cloudfront.net")
self.assertEqual(response[0].status,
"http-only")
Distribution))
self.assertTrue(isinstance(response.config,
self.assertTrue(isinstance(response.config.origin,
self.assertEqual(response.config.origin.dns_name,
self.assertEqual(response.config.origin.http_port,
self.assertEqual(response.config.origin.https_port,
self.assertEqual(response.config.origin.origin_protocol_policy,
self.assertEqual(response.config.cnames,
self.assertEqual(response.domain_name,
self.assertEqual(response.in_progress_invalidation_batches,
self.assertTrue(not
self.dist
INVAL_SUMMARY_TEMPLATE
len(accumulator)
self._get_mock_responses(num=num_invals,
self.cf.make_request
mock.Mock(side_effect=responses)
self.cf.get_invalidation_requests('dist-id-here',
list(ir)
self.assertEqual(len(all_invals),
num_invals)
"Condition":{
}]
'}\n')
("eyAKICAgIlN0YXRlbWVudCI6IFt7IAogICAgICAiUmVzb3VyY2Ui"
pk_file.seek(0)
private_key_file=pk_file)
valid_after
date_greater_than
len(date_greater_than.keys()))
date_greater_than["AWS:EpochTime"]
self.assertEqual(valid_after,
self.assertTrue("DateLessThan"
condition)
"192.168.0.0/24"
self.assertEqual(ip_range,
"http://d604721fxaaqy9.cloudfront.net/horizon.jpg?large=yes&license=yes"
1258237200
self.pk_id,
expire_time,
TestCloudSearchCreateDomain(AWSMockServiceTestCase):
test_cloudsearch_connect_result_endpoints(self):
domain.doc_service_endpoint,
"doc-demo-userdomain.us-east-1.cloudsearch.amazonaws.com")
domain.search_service_endpoint,
"search-demo-userdomain.us-east-1.cloudsearch.amazonaws.com")
test_cloudsearch_connect_result_statuses(self):
self.assertEqual(domain.created,
self.assertEqual(domain.processing,
self.assertEqual(domain.requires_index_documents,
self.assertEqual(domain.deleted,
test_cloudsearch_connect_result_details(self):
self.assertEqual(domain.id,
"1234567890/demo")
self.assertEqual(domain.name,
"demo")
test_cloudsearch_documentservice_creation(self):
document.endpoint,
test_cloudsearch_searchservice_creation(self):
search.endpoint,
CloudSearchConnectionDeletionTest(AWSMockServiceTestCase):
test_cloudsearch_deletion(self):
self.service_connection.delete_domain('demo')
CloudSearchConnectionIndexDocumentTest(AWSMockServiceTestCase):
test_cloudsearch_index_documents(self):
test_cloudsearch_index_documents_resp(self):
['average_score',
'brand_id',
'colors',
'context',
'context_owner',
'created_at',
'creator_id',
'file_size',
'format',
'has_logo',
'has_messaging',
'height',
'image_id',
'ingested_from',
'is_advertising',
'is_photo',
'is_reviewed',
'modified_at',
'subject_date',
'tags',
'title',
'width'])
CommitMismatchError,
EncodingError,
ContentTooLongError,
CloudSearchDocumentTest(unittest.TestCase):
HTTPretty.register_uri(
HTTPretty.POST,
("http://doc-demo-userdomain.us-east-1.cloudsearch.amazonaws.com/"
body=json.dumps(self.response).encode('utf-8'),
content_type="application/json")
CloudSearchDocumentSingleTest(CloudSearchDocumentTest):
test_cloudsearch_add_single_basic(self):
self.assertEqual(args['version'],
test_cloudsearch_add_single_fields(self):
self.assertEqual(args['fields']['category'],
['cat_a',
'cat_b',
'cat_c'])
self.assertEqual(args['fields']['id'],
self.assertEqual(args['fields']['title'],
'Title
test_cloudsearch_add_single_result(self):
self.assertEqual(doc.doc_service,
CloudSearchDocumentMultipleAddTest(CloudSearchDocumentTest):
'1234':
"cat_c"]}},
'1235':
"1235",
["cat_b",
"cat_c",
"cat_d"]}},
'1236':
"1236",
["cat_e",
"cat_f",
"cat_g"]}},
obj['version'],
self.assertTrue(arg['id']
self.objs)
self.assertEqual(arg['fields']['id'],
self.objs[arg['id']]['fields']['id'])
self.assertEqual(arg['fields']['title'],
self.objs[arg['id']]['fields']['title'])
self.assertEqual(arg['fields']['category'],
self.objs[arg['id']]['fields']['category'])
test_cloudsearch_add_results(self):
len(self.objs))
CloudSearchDocumentDelete(CloudSearchDocumentTest):
test_cloudsearch_delete(self):
test_cloudsearch_delete_results(self):
CloudSearchDocumentDeleteMultiple(CloudSearchDocumentTest):
test_cloudsearch_delete_multiples(self):
self.assertEqual(len(args),
self.assertEqual(arg['type'],
arg['id']
CloudSearchSDFManipulation(CloudSearchDocumentTest):
test_cloudsearch_initial_sdf_is_blank(self):
test_cloudsearch_single_document_sdf(self):
self.assertNotEqual(document.get_sdf(),
document.clear_sdf()
CloudSearchBadSDFTesting(CloudSearchDocumentTest):
test_cloudsearch_erroneous_sdf(self):
self.assertNotEqual(len(boto.log.error.call_args_list),
CloudSearchDocumentErrorBadUnicode(CloudSearchDocumentTest):
'Illegal
document'}]
test_fake_bad_unicode(self):
self.assertRaises(EncodingError,
CloudSearchDocumentErrorDocsTooBig(CloudSearchDocumentTest):
long'}]
test_fake_docs_too_big(self):
self.assertRaises(ContentTooLongError,
CloudSearchDocumentErrorMismatch(CloudSearchDocumentTest):
'Something
wrong'}]
self.assertRaises(CommitMismatchError,
HOSTNAME,
CloudSearchSearchBaseTest
fake_loads_value_error(content,
ValueError("HAHAHA!
Totally
JSON.")
fake_loads_json_error(content,
json.JSONDecodeError('Using
JSON.',
CloudSearchJSONExceptionTest(CloudSearchSearchBaseTest):
test_no_simplejson_value_error(self):
fake_loads_value_error):
@unittest.skipUnless(hasattr(json,
'JSONDecodeError'),
'requires
simplejson')
test_simplejson_jsondecodeerror(self):
fake_loads_json_error):
"search-demo-userdomain.us-east-1.cloudsearch.amazonaws.com"
FULL_URL
CloudSearchSearchBaseTest(unittest.TestCase):
'12342',
'12343',
'12344',
4',
5',
'12346',
'12347',
"text/xml"
get_args(self,
requestline):
requestline.split(b"
request.split(b"?",
six.moves.urllib.parse.parse_qs(request)
json.dumps(body).encode('utf-8')
FULL_URL,
content_type=self.content_type,
status=self.response_status)
CloudSearchSearchTest(CloudSearchSearchBaseTest):
'info':
test_cloudsearch_qsearch(self):
[b"0"])
[b"10"])
test_cloudsearch_search_details(self):
size=50,
start=20)
[b"50"])
[b"20"])
self.assertEqual(args[b'facet'],
test_cloudsearch_facet_constraint_single(self):
facet_constraints={'author':
Smith'"})
self.assertEqual(args[b'facet-author-constraints'],
test_cloudsearch_facet_constraint_multiple(self):
Smith'",
'category':
"'News','Reviews'"})
[b"'News','Reviews'"])
test_cloudsearch_facet_sort_single(self):
facet_sort={'author':
self.assertEqual(args[b'facet-author-sort'],
[b'alpha'])
test_cloudsearch_facet_sort_multiple(self):
'alpha',
facet_top_n={'author':
self.assertEqual(args[b'facet-author-top-n'],
self.assertEqual(args[b'rank'],
test_cloudsearch_result_fields_single(self):
return_fields=['author'])
self.assertEqual(args[b'return-fields'],
[b'author'])
test_cloudsearch_result_fields_multiple(self):
return_fields=['author',
'title'])
[b'author,title'])
t={'year':
self.assertEqual(args[b't-year'],
[b'2001..2007'])
test_cloudsearch_results_meta(self):
test_cloudsearch_results_info(self):
self.assertEqual(results.num_pages_needed,
3.0)
test_cloudsearch_results_matched(self):
search(query)
self.assertEqual(results.search_service,
search)
self.assertEqual(results.query,
test_cloudsearch_results_hits(self):
x['id'],
results.docs))
hits,
["12341",
test_cloudsearch_results_iterator(self):
results_correct
iter(["12341",
self.assertEqual(x['id'],
next(results_correct))
test_cloudsearch_results_internal_consistancy(self):
self.assertEqual(len(results),
len(results.docs))
test_cloudsearch_search_nextpage(self):
query1
query2
search(query2)
self.assertEqual(results.next_page().query.start,
query1.start
query1.size)
self.assertEqual(query1.q,
query2.q)
CloudSearchSearchFacetTest(CloudSearchSearchBaseTest):
'facets':
'tags':
'animals':
[{'count':
'fish'},
{'count':
'lions'}]},
test_cloudsearch_search_facets(self):
self.assertTrue('tags'
results.facets)
self.assertEqual(results.facets['animals'],
{u'lions':
u'fish':
u'2'})
CloudSearchNonJsonTest(CloudSearchSearchBaseTest):
b'<html><body><h1>500
Internal
Error</h1></body></html>'
self.assertRaises(SearchServiceException):
CloudSearchUnauthorizedTest(CloudSearchSearchBaseTest):
b'<html><body><h1>403
Forbidden</h1>foo
baz</body></html>'
baz'):
super(CloudSearchConnectionTest,
SearchConnection(
endpoint='test-domain.cloudsearch.amazonaws.com'
test_expose_additional_error_info(self):
b'Nopenopenope'
mpo(requests,
self.conn.search(q='not_gonna_happen')
self.assertTrue('non-json
self.assertTrue('Nopenopenope'
"Something
Oops."
}).encode('utf-8')
self.conn.search(q='no_luck_here')
self.assertTrue('Unknown
self.assertTrue('went
Oops'
"Endpoint":
"doc-demo.us-east-1.cloudsearch.amazonaws.com")
"search-demo.us-east-1.cloudsearch.amazonaws.com")
tests.unit.cloudsearch2
test_proxy(self):
conn.proxy
"127.0.0.1"
conn.proxy_user
"john.doe"
conn.proxy_pass="p4ssw0rd"
conn.proxy_port="8180"
conn.use_proxy
Domain(conn,
DEMO_DOMAIN_DATA)
'http://john.doe:p4ssw0rd@127.0.0.1:8180'})
"alpha"}'])
self.assertEqual(args[b'return'],
mpo(self.conn.session,
search_service
document_service
body=json.dumps(response).encode('utf-8'))
self.actual_request.headers
self.assertIsNotNone(headers.get('Authorization'))
TestDescribeTrails(AWSMockServiceTestCase):
"IncludeGlobalServiceEvents":
false,
"test",
"SnsTopicName":
"cloudtrail-1",
"S3BucketName":
"cloudtrail-1"
len(api_response['trailList']))
api_response['trailList'][0]['Name'])
self.assertTrue('DescribeTrails'
self.assertEqual('cloudtrail-1',
'Table':
'CreationDateTime':
1349910554.478,
'TableSizeBytes':
'TableStatus':
'ACTIVE'}
Layer2('access_key',
Table(self.layer2,
Batch(self.table,
'k1'}}],
['bar'],
DESCRIBE_TABLE
self.assertEqual(table.name,
'footest')
'ACTIVE')
self.assertEqual(table.schema,
self.layer2.create_schema('foo',
self.assertEqual(schema.hash_key_name,
self.assertEqual(schema.hash_key_type,
Schema.create(hash_key=('bar',
s3)
range_key=('baz',
'1.1'})
{'NS':
'54'},
'1.1'}),
self.assertEqual(dynamizer.decode({'NS':
Decimal('1452525162'),
self.assertEqual(b'\x01',
bytes(data))
self.assertEqual(b'\x88',
types.Binary(1)
b'\x00')
types.Binary(u'\x01')
b'\x01')
self.assertEqual(dynamodb.region.name,
'HASH')
NUMBER)
'alt',
all_index
self.assertEqual(all_index.name,
'AllKeys')
all_index.parts],
self.assertEqual(all_index.projection_type,
'ALL')
self.assertEqual(all_index.definition(),
self.assertEqual(all_index.schema(),
'AllKeys',
keys_only
self.assertEqual(keys_only.name,
'KeysOnly')
keys_only.parts],
self.assertEqual(keys_only.projection_type,
'KEYS_ONLY')
self.assertEqual(keys_only.definition(),
self.assertEqual(keys_only.schema(),
'KeysOnly',
self.assertEqual(include_index.name,
'IncludeKeys')
include_index.parts],
self.assertEqual(include_index.projection_type,
'INCLUDE')
self.assertEqual(include_index.definition(),
GlobalIncludeIndex('IncludeKeys',
self.johndoe
test_initialization(self):
empty_item
Item(self.table)
self.table)
12345)
set(['jane',
self.assertEqual(self.johndoe.prepare_full(),
'johndoe'})
self.johndoe.prepare_partial()
self.assertEqual(final_data,
self.johndoe['new_attr']
'never_seen_before'
self.assertTrue(mock_put_item.called)
mock_put_item.assert_called_once_with({
'never_seen_before'},
expects={
'_update_item',
start_key
end_cap:
test_fetch_more(self):
#7',
#8',
#9',
#10',
#11',
self.assertFalse(self.results._results_left)
test_iteration(self):
#6')
'johndoe':
Table('users',
HashKey))
RangeKey))
raw_attributes_2
"MostRecentlyJoinedIndex",
raw_indexes_2
FakeDynamoDBConnection()
mock.patch.object(conn,
'create_table',
mock_create_table:
connection=conn)
self.assertTrue(retval)
self.assertTrue(mock_create_table.called)
mock_create_table.assert_called_once_with(attribute_definitions=[
table_name='users',
key_schema=[
RangeKey('friend_count')
'describe_table',
mock_describe:
self.assertEqual(self.users.schema,
self.assertEqual(len(self.users.schema),
mock_describe.assert_called_once_with('users')
self.users.update(throughput={
mock_update.call_args
self.assertEqual(args,
('users',))
kwargs['global_secondary_index_updates'][:]
update.sort(key=lambda
x['Update']['IndexName'])
update[0],
update[1],
'JustCreatedIndex',
self.users.update_global_secondary_index(global_indexes={
'A_IndexToBeUpdated':
'A_IndexToBeUpdated',
mock_get_item.assert_called_once_with('users',
'johndoe'}
'first_name'])
self.users.schema
data_type=NUMBER),
mock_get_item.assert_called_once_with(
username=
'put_item',
mock_put_item.assert_called_once_with('users',
expected={
'data'})
'some':
mock_delete_item.assert_called_once_with('users',
'23456'
conditional_operator=None)
['username',
self.assertFalse(mock_batch.called)
NOES')
self.users._build_filters({
'username__eq':
'date_joined__gte':
1234567,
'last_name__between':
['danzig',
'only'],
'first_name__null':
'1234567',
'danzig'},
'only'}],
'gender':
'darling__die':
"Count":
"Items":
"ScannedCount":
self.users._query(
username__between=['aaa',
'aaa'},
'mmm'}],
expected['LastEvaluatedKey']
mock_query_2:
consistent=True,
conditional_operator='AND'
'scan',
mock_scan:
self.users._scan(
limit=2,
scan_filter={
'2'}],
mock_scan_2:
limit=3,
self.users._query)
'Foo',
self.assertEqual(list(res_1.keys()),
self.users._scan)
'Zoey',
'zoeydoe')
self.assertIn('scan_index_forward',
mock_query.call_args[1]['scan_index_forward'])
self.assertIn('limit',
mock_query.call_args[1]['limit'])
'LastEvaluatedKey':
'Responses':
"UnprocessedKeys":
mock_batch_get:
mock_batch_get_2:
self.assertEqual(results._keys_left,
self.address.associate(1)
instance_id=1,
self.address.allocation_id
"aid1"
BlockDeviceType,
BlockDeviceType()
[("volumeId",
("snapshotId",
"snapshot_id"),
"size"),
("status",
status",
"status"),
self.block_device_type.endElement("NoDevice",
self.assertEqual(self.block_device_type.no_device,
self.block_device_type.endElement("deleteOnTermination",
self.assertEqual(self.block_device_type.delete_on_termination,
self.block_device_type.endElement("Encrypted",
self.assertEqual(self.block_device_type.encrypted,
self.block_device_type_eq(retval,
BlockDeviceType(self.block_device_mapping))
self.assertEqual(self.block_device_mapping.current_name,
"/dev/null")
TestLaunchConfiguration(AWSMockServiceTestCase):
self.service_connection.run_instances(
security_groups=['group1',
'group2'],
'RunInstances',
'/dev/sdf',
'snap-12345',
'SecurityGroup.1':
'SecurityGroup.2':
'group2',
'Timestamp'
TestEC2ConnectionBase(AWSMockServiceTestCase):
super(TestEC2ConnectionBase,
TestReservedInstanceOfferings(TestEC2ConnectionBase):
next_token='next_token',
'InstanceTenancy':
'10',
'offering_id',
test_minimal(self):
self.ec2.create_image(
'CreateImage',
'name'},
cancellation
self.assertEqual(cancellation.status,
self.assertEqual(cancellation.status_message,
self.assertEqual(len(cancellation.instance_counts),
cancellation.instance_counts[0]
self.assertEqual(first.state,
'Available')
self.assertEqual(first.instance_count,
self.assertEqual(len(cancellation.price_schedules),
cancellation.price_schedules[0]
self.assertEqual(schedule.term,
self.assertEqual(schedule.price,
self.assertEqual(schedule.currency_code,
self.assertEqual(schedule.active,
'ami-id')
'KmsKeyId':
self.assertEquals("111111111111",
self.assertEquals("Windows
'SourceDestCheck.Value':
'Attachment.AttachmentId':
'Attachment.DeleteOnTermination':
'groupSet',
TestConnectToRegion(unittest.TestCase):
Mock(spec=http_client.HTTPSConnection)
Mock(return_value=self.https_connection),
test_aws_region(self):
self.ec2.host)
orig
'get_all_snapshots':
self.ec2.get_all_snapshots,
'delete_snapshot':
self._get_snapshots()
MagicMock(return_value=snaps)
self.ec2.get_all_snapshots.called)
orig['get_all_snapshots']
orig['delete_snapshot']
self.ec2.modify_reserved_instances(
'a-token-goes-here',
reserved_instance_ids=[
'2567o137-8a55-48d6-82fb-7258506bb497',
target_configurations=[
ReservedInstancesConfiguration(
availability_zone='us-west-2c',
platform='EC2-VPC',
instance_count=3,
instance_type='c3.large'
'ReservedInstancesConfigurationSetItemType.0.AvailabilityZone':
'us-west-2c',
'ReservedInstancesConfigurationSetItemType.0.InstanceCount':
'ReservedInstancesConfigurationSetItemType.0.Platform':
'EC2-VPC',
'ReservedInstancesConfigurationSetItemType.0.InstanceType':
'c3.large',
'ReservedInstancesId.1':
'rimod-3aae219d-3d63-47a9-a7e9-e764example')
'processing',
image_location='s3://foo',
'snap-12345678',
test_default_behavior(self):
test_max_results(self):
'DescribeInstances',
test_unchanged(self):
test_switched(self):
test_associate_address(self):
self.ec2.associate_address(instance_id='i-1234',
self.assertTrue(result[0].encrypted)
self.assertEqual(result[1].id,
self.assertFalse(result[1].encrypted)
'DescribeSnapshots',
'SnapshotId.1':
'SnapshotId.2':
'snap-1a2b3c4d')
self.ec2.create_volume(80,
snapshot='snap-1a2b3c4d',
'CreateVolume',
self.assertEqual(result.id,
self.assertTrue(result.encrypted)
'hello'})
filters={'GroupId':
'sg-9b4343fe'},
'GroupId',
'sg-9b4343fe',
CREATE_TAGS_RESPONSE
DELETE_TAGS_RESPONSE
taggedEC2Object.tags["already_present_key"]
"already_present_value"
'CreateTags',
"already_present_key":
"already_present_value",
taggedEC2Object.remove_tag("key1",
test_remove_tags(self):
DESCRIBE_INSTANCE_VPC
RUN_INSTANCE_RESPONSE
base64.b64encode(b'#!/bin/bash').decode('utf-8'),
TestInstanceStatusResponseParsing(unittest.TestCase):
all_statuses
ec2.make_request.call_args[0][1])
self.assertEqual(all_statuses.next_token,
'page-2')
self.assertEqual(instance_type.name,
self.assertEqual(instance_type.cores,
self.assertEqual(instance_type.disk,
self.assertEqual(instance_type.memory,
status"
NetworkInterface()
"one_status"
self.eni_two.connection
self.eni_two.id
"two_status"
self.eni_two.attachment
test_update_with_validate_true_raises_value_error(self):
self.eni_one.connection.get_all_network_interfaces.return_value
ID$"):
test_update_with_result_set_greater_than_0_updates_dict(self):
"one_status",
'eni-1',
test_update_returns_status(self):
"two_status")
"instance_id",
self.eni_one.connection.detach_network_interface.assert_called_with(
'eni-attach-1',
test_detach_with_no_attach_data(self):
subnet_id='subnet_id2',
description='description2',
groups=['group_id1',
'group_id2'],
private_ip_address='10.0.1.54',
private_ip_addresses=[self.private_ip_address3,
NetworkInterfaceCollection(self.network_interfaces_spec1,
self.network_interfaces_spec2)
'description1',
'10.0.0.54',
'subnet_id',
'10.0.0.10',
'10.0.0.11',
'LaunchSpecification.NetworkInterface.0.DeviceIndex':
'LaunchSpecification.NetworkInterface.0.DeleteOnTermination':
'LaunchSpecification.NetworkInterface.0.Description':
'LaunchSpecification.NetworkInterface.0.PrivateIpAddress':
'LaunchSpecification.NetworkInterface.0.SubnetId':
'LaunchSpecification.NetworkInterface.0.PrivateIpAddresses.0.Primary':
'LaunchSpecification.NetworkInterface.0.PrivateIpAddresses.0.PrivateIpAddress':
'LaunchSpecification.NetworkInterface.0.PrivateIpAddresses.1.Primary':
'LaunchSpecification.NetworkInterface.0.PrivateIpAddresses.1.PrivateIpAddress':
NetworkInterfaceCollection(self.network_interfaces_spec3)
self.assertEqual(len(sg.rules),
'c3.large'},
'q5GwEl5bMGjKq6YmhpDLJ7hEwyWU54jJC2GQ93n61vZV4s1+fzZ674xzvUlTihrl')
'c3.large')
self.volume_two.connection
self.volume_two.status
self.volume_two.size
self.volume_two.snapshot_id
self.volume_two.attach_data
self.volume_two.zone
attrs",
attach_data
obj_value
self.volume_one.connection.get_all_volumes.return_value
self.volume_one.connection.detach_volume.assert_called_with(
self.volume_one.connection.create_snapshot.assert_called_with(
Snapshot()
self.volume_one.connection.get_all_snapshots.return_value
test_endElement_with_name_volumeId_sets_id(self):
test_endElement_with_other_name_sets_other_name_attribute(self):
self.volume_attribute._key_name
self.volume_attribute.endElement("value",
test_autoscaling_group_with_termination_policies(self):
'OldestInstance',
'vpc_zone_1,vpc_zone_2',
'scheduled-foo',
31),
self.service_connection.get_all_launch_configurations()
LaunchConfiguration))
self.assertEqual(response[0].associate_public_ip_address,
self.assertEqual(response[0].name,
"my-test-lc")
"m1.small")
self.assertEqual(response[0].launch_configuration_arn,
"arn:aws:autoscaling:us-east-1:803981987763:launchConfiguration:9dbbbf87-6141-428a-a409-0752edbe6cad:launchConfigurationName/my-test-lc")
self.assertEqual(response[0].image_id,
"ami-514ac838")
self.assertTrue(isinstance(response[0].instance_monitoring,
launchconfig.InstanceMonitoring))
self.assertEqual(response[0].instance_monitoring.enabled,
self.assertEqual(response[0].ebs_optimized,
test_get_all_configuration_limited(self):
self.service_connection.get_all_launch_configurations(max_records=10,
names=["my-test1",
"my-test2"])
'LaunchConfigurationNames.member.1':
'my-test1',
'LaunchConfigurationNames.member.2':
'my-test2'
adjustment_type='PercentChangeInCapacity',
scaling_adjustment=50,
min_adjustment_step=30)
'PercentChangeInCapacity',
name='ana',
'ana',
Tag(
resource_id='sg-00000000',
resource_type='auto-scaling-group',
'sg-00000000',
self.assertEqual(getattr(tag,
'inst4']
'inst4'],
'io1')
35),
'maximum':
'minimum':
'samplecount':
'sum':
ATTRIBUTE_GET_CS_RESPONSE
'ModifyLoadBalancerAttributes',
{'LoadBalancerAttributes.CrossZoneLoadBalancing.Enabled':
'test_elb'},
mock.ANY)
[('cross_zone_load_balancing.enabled',
LoadBalancer(elb,
attr_result
attr_tests
ATTRIBUTE_TESTS:
elb.get_all_lb_attributes('test_elb')
self._verify_attributes(attributes,
attr_tests)
ATTRIBUTE_GET_TRUE_CZL_RESPONSE),
instance_port=80,
self.assertEqual(listener[0],
self.assertEqual(listener[1],
self.assertEqual(listener[2],
self.assertEqual(listener[3],
DISABLE_RESPONSE
['sample-zone'])
elb.get_all_load_balancers()
'EnableProxyProtocol')
DETACH_RESPONSE
ECSConnection
'2009-03-31',
ClusterStatus))
'2014-01-24T01:21:21Z')
date.strftime(boto.utils.ISO8601),
'CreatedAfter':
'WAITING',
cluster_id='j-123')
self.service_connection.list_instances(
'ListSteps',
StepSummaryList))
self.assertEqual(response.steps[0].name,
'Step
'FAILED',
analytics')
'1.0.3')
'ec2-184-0-0-1.us-west-1.compute.amazonaws.com')
'Bar')
input_dict
self.service_connection._build_tag_list(input_dict)
res)
'Two'
input_tags
'ZzzNoValue':
self.service_connection.add_tags()
self.service_connection.add_tags('j-123')
self.assertRaises(AssertionError):
self.service_connection.describe_jobflows()
JobFlow))
'12')
group')
'JobFlowIds.member.1':
self.service_connection.run_jobflow(
'EmrCluster',
[jobflow]
emrobject.JobFlow)])
self._assert_fields(jobflow,
self.assertEquals(6,
self.assertEquals(2,
self.assertEquals('1.1',
instance_group.bidprice)
results_queue,
self.results_queue
results_queue
self.worker_queue
ConcurrentUploader(mock.Mock(),
uploader._calculate_required_part_size(
self.assertEqual(total_parts,
self.assertEqual(part_size,
uploader.upload('foofile')
[q.get()
range(q.qsize())]
self.assertEqual(items[0],
self.assertEqual(items[1],
mock.Mock(
fileobj
job_queue
upload_thread
UploadWorkerThread(
job_queue,
time_between_retries=0)
api.upload_part.side_effect
job_queue.put((0,
job_queue.put(_END_SENTINEL)
upload_thread.run()
num_retries=2,
self.job
GlacierResponse(mock.Mock(),
validate_checksum=True)
test_list_vaults(self):
u'RequestId':
test_delete_vault(self):
b'\xe2'
self.service_connection.upload_part(
u'unicode_vault_name',
'tree_hash',
(1,2),
FIXTURE_VAULTS
'arn:aws:glacier:us-east-1:686406519478:vaults/vault0',
'vault0',
'arn:aws:glacier:us-east-1:686406519478:vaults/vault3',
'vault3',
'arn:aws:glacier:us-east-1:686406519478:vaults/vault2',
FIXTURE_ARCHIVE_JOB
"archive
"OW2fM5iVylEpFEMM9_HpKowRapC3vn5sSL39_396UW9zLFUWVrnRHaPjUJddQ5OxSHVXjYtrN47NBZ-khxOjyEXAMPLE",
4194304,
"4194304-8388607",
"arn:aws:glacier:us-east-1:012345678901:vaults/demo1-vault"
"0-4194303",
Mock(spec=Layer1)
self.mock_layer1.describe_vault.return_value
Vault")
self.layer2.list_vaults()
self.assertEqual(vaults[0].name,
"vault0")
self.assertEqual(len(vaults),
Vault(self.mock_layer1,
FIXTURE_VAULT)
"examplevault",
timedelta(0)
tzinfo=UTC()),
parts_result
self.vault.list_all_parts(sentinel.upload_id)
[call('examplevault',
self.mock_layer1.list_parts.call_args_list)
self.assertEquals(EXAMPLE_PART_LIST_COMPLETE,
parts_result)
mock_list_parts
'RangeInBytes':
'SHA256TreeHash':
100))
Vault._range_string_to_part_index,
bytes_to_hex,
four_meg_bytestring
binary!')
ANY
self.assertEqual(archive_id,
'archive_id')
self.vault.DefaultPartSize
part_size=self.vault.DefaultPartSize)
'something'
mock.patch.object(self.vault.layer1,
'HkF9p6')
vault.Vault(None,
mock.patch('boto.glacier.vault.ConcurrentUploader')
c.return_value.upload.return_value
'archive_id'
v.concurrent_create_archive_from_file(
num_threads=10,
part_size=1024
tree_hash(
sentinel.vault_name,
create_mock_vault()
calculate_mock_vault_calls(
check_mock_vault_calls(
len(data))
test_returns_archive_id(self):
self.assertEquals(sentinel.archive_id,
self.writer.write(b'1234')
self.writer.write(b'567')
self.writer.write(b'22i3uy')
self.assertEqual(final_hash,
self.assertEqual(final_size,
'valid_until':
'create_date':
self.service_connection.create_role('a_name')
'AssumeRolePolicyDocument'])
self.assertDictEqual(json.loads(self.actual_request.params["AssumeRolePolicyDocument"]),
[{"Action":
["sts:AssumeRole"],
"Principal":
{"Service":
self.service_connection.host
'iam.cn-north-1.amazonaws.com.cn'
self.service_connection.create_role(
'a_name',
'{"hello":
"policy"}',
['policy_version']
self.assertEqual(response['get_policy_response']
['get_policy_result']
['list_policies_result']
'Dev')
'Dev'},
'DevRole')
'DevRole'},
'Alice')
TestKinesis(AWSMockServiceTestCase):
self.service_connection.put_record('stream-name',
b'\x00\x01\x02\x03\x04\x05',
'partition-key')
self.assertEqual(body['Data'],
'ZGF0YQ==')
'partition-key'
b'\x00\x01\x02\x03\x04\x05'
self.service_connection.encrypt(key_id='foo',
plaintext=data)
ml_endpoint
'mymlmodel.amazonaws.com'
self.service_connection.predict(
ml_model_id='foo',
record={'Foo':
self.assertEqual(self.actual_request.host,
paramiko.SSHClient
LocaleRequirement
test_requirement1
test_requirement2
'CA')])
qualifications.add(test_requirement1)
qualifications.add(test_requirement2)
'00000000000000000071',
'QualificationRequirement.2.Comparator':
'QualificationRequirement.2.LocaleValue.1.Country':
'QualificationRequirement.2.LocaleValue.1.Subdivision':
'QualificationRequirement.2.QualificationTypeId':
QUAL_NO_ONE_ELSE_HAS_ID
qualification_type_id=QUAL_NO_ONE_ELSE_HAS_ID,
'333333333333333333333333333333'},
mws
{'A':
'two',
'five'},
'six',
'Prefix.2':
{'Prefix':
{'Prefix.A':
'Prefix.B':
'__wrapped__',
self.assertEqual(func,
connection._response_factory(action,
connection._parse_response(parser,
self.assertEqual(response._action,
action)
self.assertEqual(response.__class__.__name__,
self.assertEqual(response._result.__class__,
self.assertRaises(AttributeError)
obj._result.Item
useful
x[0].startswith('_')
dict(filter(useful,
list(map(int,
u'6'],
[e.Value
obj._result.Item],
['One',
'Six'],
getattr(x,
['Bar',
'Bif',
'Baz'])
'allowall':
u'iam_access_key',
u'Code':
u'Success',
u'2012-09-01T03:57:34Z',
u'LastUpdated':
u'2012-08-31T21:43:40Z',
u'SecretAccessKey':
u'iam_token',
u'Type':
u'AWS-HMAC'
provider.Config,
self.shared_config[section_name][key]
'security_token')
'access_key')
'env_security_token')
provider.NO_CREDENTIALS_PROVIDED,
'prod_access_key',
'prod_secret_key',
'default_access_key',
'default_secret_key'
'prod_access_key')
'prod_secret_key')
'cfg_security_token',
'cfg_security_token')
sys.modules['keyring']
'shared_access_key')
'shared_secret_key')
self.environ.clear()
self.shared_config.clear()
'iam_access_key')
'iam_secret_key')
'iam_token')
'rolename':
'gs_access_key_id':
'gs_secret_access_key':
exists.assert_called_once_with(path)
load_from_path.assert_called_once_with(path)
self.assertFalse(self.config.getbool('Boto',
self.assertEqual(self.config.getint('Boto',
'DescribeDBInstances',
'mydbinstance2')
'2012-10-03T22:01:51.047Z')
'backing-up')
'awsuser')
self.assertEqual(db.backup_retention_period,
'10:30-11:00')
'wed:06:30-wed:07:00')
'read
'5.5.27')
{'MasterUserPassword':
'****'})
self.service_connection.restore_dbinstance_from_point_in_time(
'RestoreDBInstanceToPointInTime',
'UseLatestRestorableTime':
'active',
vpc_security_groups=vpc_security_groups)
'VpcSecurityGroupIds.member.1':
'VpcSecurityGroupIds.member.2':
'sg-2'
'DescribeDBLogFiles',
LogFile))
self.assertEqual(response[0].log_filename,
'error/mysql-error-running.log')
self.assertEqual(response[0].last_written,
'1364403600000')
self.assertEqual(response[0].size,
self.service_connection.get_log_file('db1',
'foo.log')
LogFileObject))
'DownloadDBLogFilePortion',
'simcoprod01'
'mydbsnapshot')
'mydbsnapshot',
'mycopieddbsnapshot')
'myrestoreddbinstance',
'3306',
'active'
header=[
['Code',
self.assertRaises(DNSServerError)
self.assertTrue('It
failed.'
body=)
self.do_retry_handler()
_wrapper(*args,
self.assertEqual(self.calls['count'],
Zone))
self.assertEqual(response['CreateHostedZoneResponse']
['DelegationSet']['NameServers'],
test_list_zones(self):
test_get_all_rr_sets(self):
self.service_connection.get_all_rrsets("Z1111",
'abcdefgh-abcd-abcd-abcd-abcdefghijkl')
resource_path='/health_check',
hc_xml
hc.to_xml()
self.service_connection.create_health_check(hc)
hc_resp
response['CreateHealthCheckResponse']['HealthCheck']['HealthCheckConfig']
self.assertEqual(hc_resp['Type'],
self.assertEqual(hc_resp['Port'],
self.assertEqual(response['CreateHealthCheckResponse']['HealthCheck']['Id'],
re.sub(r"\s*[\r\n]+",
"\n",
mock_rrs
rr_name
'mybucket_constructor')
bucket.new_key('mykey')
'mykey')
bukket._get_all_query_args({
qa,
('Contents',
inner_method,
self._test_patched_lister_encoding(
host='s3.cn-north-1.amazonaws.com.cn'
url_enabled
query_auth=True)
url_disabled
query_auth=False)
self.assertIn('Signature=',
url_enabled)
self.assertNotIn('Signature=',
url_disabled)
host='s3.amazonaws.com',
key='test.txt')
self.assertTrue(url.startswith(
self.assertIn('host',
self).create_service_connection(**kwargs)
'<AllowedMethod>PUT</AllowedMethod>'
'<AllowedMethod>POST</AllowedMethod>'
'<AllowedMethod>DELETE</AllowedMethod>'
'<AllowedOrigin>http://www.example.com</AllowedOrigin>'
'<ExposeHeader>x-amz-server-side-encryption</ExposeHeader>'
'<AllowedMethod>GET</AllowedMethod>'
'<AllowedOrigin>*</AllowedOrigin>'
cfg.add_rule(['PUT',
cfg.add_rule('GET',
Key()
self.assertIsNone(k.expiry_date)
header=[('x-amz-restore',
2012
k.set_contents_from_string('test')
k.bucket.list.assert_not_called()
k.copy
k.bucket.name
'mybucket'
self.assertEqual(k.storage_class,
k.copy.assert_called_with(
'fookey',
validate_dst_bucket=True,
_wrapper.count
MockConnection
MockBucket
self.keyfile.close()
self.contents[5:])
argument')
self.keyfile.key.data
self.keyfile.key.set_etag()
self.assertEqual(self.keyfile.key.etag,
'098f6bcd4621d373cade4e832627b4f6')
self._get_bucket_lifecycle_config()[2]
self.assertEqual(rule.status,
self._get_bucket_lifecycle_config()[1]
self._get_bucket_lifecycle_config()[0].transition[0]
self.assertEquals(transition.days,
self.assertIsNone(transition.date)
self._get_bucket_lifecycle_config()[1].transition[0]
self.assertEquals(transition.date,
self.assertIsNone(transition.days)
self._get_bucket_lifecycle_config()[1].transition
'STANDARD_IA')
self.assertIn('<Expiration><Days>30</Days></Expiration>',
transition=t)
self.assertNotEqual(t1,
uri_str,
self.assertEqual('gs://bucket/obj/a/b',
tempfile.tempdir
urllib.request.pathname2url(tmp_dir)
self.assertEqual(tmp_dir,
'version_id'))
'generation'))
'is_version_specific'))
self.assertEqual('file://%s'
RoutingRules
WebsiteConfiguration(redirect_all_requests_to=location)
('<RedirectAllRequestsTo><HostName>'
Condition(http_error_code=404)
Redirect(hostname='example.com',
replace_key_prefix='report-404/')
entry_key
self.assertEqual(entry_key,
.set_identity_notification_topic(
sns_topic='arn:aws:sns:us-east-1:123456789012:example')
response['SetIdentityNotificationTopicResponse']
response['SetIdentityNotificationTopicResult']
.set_identity_feedback_forwarding_enabled(
response['SetIdentityFeedbackForwardingEnabledResponse']
response['SetIdentityFeedbackForwardingEnabledResult']
QUEUE_POLICY
queue.get_attributes.return_value
'arn:aws:sqs:us-east-1:idnum:queuename'
self.service_connection.subscribe_sqs_queue('topic_arn',
'Subscribe',
'arn:aws:sqs:us-east-1:idnum:queuename',
'topic_arn',
'2010-03-31',
ignore_params_values=[])
actual_policy
json.loads(queue.set_attribute.call_args[0][1])
self.assertEqual(len(actual_policy['Statement']),
self.service_connection.publish('topic',
self.service_connection.publish(topic='topic',
message='message',
'PlatformCredential':
'PlatformCredential',
'PlatformPrincipal',
platform_application_arn='arn:myapp',
'PlatformApplicationArn':
'arn:myapp',
self.service_connection.publish(
message=json.dumps({
'Ignored.',
'GCM':
'data':
'goes
here',
message_structure='json',
'MessageStructure':
'json',
{"data":
"goes
message_attributes={
'name1':
'Bob'
self.service_connection.auth_service_name
SQSRegionInfo(name='us-west-2',
endpoint='us-west-2.queue.amazonaws.com')
SQSConnection(
'599169622985')
test_send_message_attributes(self):
'2012-11-05'
{'data_type':
self.assertRaises(SQSDecodeError)
context:
sample_value)
message.set_body(body)
self.assertEqual(message.get_body_encoded(),
connection.region.name
connection=connection,
url='https://sqs.us-east-1.amazonaws.com/id/queuename')
'AssumeRole',
'mysession',
'access_key':
'this'
't2'],
task_list='test_list')
1379019427.953,
'startedEventId':
'workflowExecution':
self.decider.poll()
self.worker.poll()
self.worker._swf.poll_for_activity_task.assert_called_with('test',
'test_list')
self.decider._swf.poll_for_decision_task.assert_called_with('test',
'some_other_tasklist')
ActivityType,
WorkflowType,
WorkflowExecution
'typeInfos':
'GrayscaleTransform',
'S3Download',
'S3Upload',
'SepiaTransform',
expected_names
expected_names)
self.assertEquals(self.domain.aws_access_key_id,
self.assertEquals(self.domain.aws_secret_access_key,
self.assertEquals(self.domain.name,
WorkflowExecution)
wf_type
WorkflowType(name='name',
self.assertEquals(password,
hashed)
hmac_hashfunc
b'foo').hexdigest())
test_empty_string(self):
'http://10.0.1.5',
set_normal_response(self,
test_value
test_value)
boto.utils.retry_url
invalid_data
'{"invalid_json_format"
true,}'
valid_data
{"valid_json_format":
true}}'
"/".join(["http://169.254.169.254",
key_data])
self.set_normal_response([key_data,
invalid_data,
num_retries)
self.set_normal_response(['foo'])
userdata
self.assertEqual('foo',
userdata)
boto.utils.retry_url.assert_called_with(
'http://169.254.169.254/latest/user-data',
'bf1d:cb48:4513:d1f1:efdd:b290:9ff9:64be'
'[bf1d:cb48:4513:d1f1:efdd:b290:9ff9:64be]'
'[bf1d:cb48:4513:d1f1:efdd:b290:9ff9:64be]:8080'
'bf1d:cb48:4513:d1f1:efdd:b290:9ff9:64be')
'available']),
'12.1.2.3',
65534)
'ipsec.1')
self.assertEquals(api_response[0].id,
'dopt-7a8b9c2d')
['example.com'])
['10.2.5.1',
'10.2.5.2'])
'DhcpOptionsId':
'igw-eaad4883EXAMPLE',
'igw-eaad4883')
'vpc-11ad4878')
'acl-5566953c',
self.https_connection.getresponse.side_effect
body=self.get_all_network_acls_subnet_body),
self.create_response(status_code=200)
'ReplaceNetworkAclAssociation',
'aclassoc-5c659635'},
'aclassoc-17b85d7e')
test_create_network_acl(self):
test_delete_network_acl(self):
self.service_connection.create_network_acl_entry(
egress=False,
'CreateNetworkAclEntry',
'PortRange.From':
'PortRange.To':
icmp_code=-1,
icmp_type=8)
'Icmp.Code':
'Icmp.Type':
8},
self.service_connection.replace_network_acl_entry(
'ReplaceNetworkAclEntry',
'rtb-13ad487a')
'CreateRouteTable')
'blackhole')
'rtbassoc-faad4893')
'subnet-15ad487c')
'ReplaceRouteTableAssociation',
'rtb-f9ad4890'},
gateway_id='igw-eaad4883')
instance_id='i-1a2b3c4d')
'i-1a2b3c4d'},
interface_id='eni-1a2b3c4d')
'eni-1a2b3c4d'},
vpc_peering_connection_id='pcx-1a2b3c4d')
'pcx-1a2b3c4d'},
['subnet-9d4a7b6c',
'subnet-9d4a7b6c',
'subnet-9d4a7b6c')
'10.0.1.0/24',
'us-east-1a'},
self.assertEquals(api_response.vpc_id,
self.assertEquals(api_response.cidr_block,
DESCRIBE_VPCS
b'''<?xml
encoding="UTF-8"?>
xmlns="http://ec2.amazonaws.com/doc/2013-02-01/">
test_get_vpcs(self):
api_response[0]
self.service_connection.modify_vpc_attribute(
'ModifyVpcAttribute',
'DescribeVpcClassicLink',
'VpcId.1':
self.vpc.attach_classic_instance(
'AttachClassicLinkVpc',
'sg-foo',
'sg-bar',
test_enable_classic_link(self):
DESCRIBE_VPC_PEERING_CONNECTIONS=
self.DESCRIBE_VPC_PEERING_CONNECTIONS
'pending-acceptance')
'Pending
Acceptance
'10.0.0.0/28')
DELETE_VPC_PEERING_CONNECTION=
self.DELETE_VPC_PEERING_CONNECTION
test_delete_vpc_peering_connection(self):
vpc_conn.make_request
vpc_conn.make_request.call_args_list[0][0])
DESCRIBE_VPNCONNECTIONS
<customerGatewayConfiguration>
&lt;?xml
encoding="UTF-8"?&gt;
</customerGatewayConfiguration>
<type>ipsec.1</type>
<vgwTelemetry>
</vgwTelemetry>
<options>
<staticRoutesOnly>true</staticRoutesOnly>
</options>
<routes>
<source>static</source>
</routes>
<state>pending</state>
'UP')
'vpn-83ad48ea'},
'vgw-8db04f81')
test_delete_vpn_gateway(self):
'rtb-c98a35a0',
'vgw-d8e09e8a')
'vgw-d8e09e8a',
'rtb-c98a35a0'},
_py_files(folder):
"/*.py")
"/*/*.py")
(twisted_version.major,
twisted_version.minor,
twisted_version.micro)
collect_ignore
_py_files("scrapy/xlib/tx")
open('tests/py3-ignores.txt'):
file_path[0]
collect_ignore.append(file_path)
@pytest.fixture()
chdir(tmpdir):
tmpdir.chdir()
setup,
find_packages
open(join(dirname(__file__),
'scrapy/VERSION'),
f.read().decode('ascii').strip()
sys.path.append(path.join(path.dirname(__file__),
"_ext"))
path.dirname(path.dirname(__file__)))
'scrapydocs',
'sphinx.ext.autodoc'
u'Scrapy'
u'2008-2016,
developers'
scrapy.version_info[:2]))
scrapy.__version__
'en'
['.build']
'sphinx_rtd_theme'
sphinx_rtd_theme
[sphinx_rtd_theme.get_html_theme_path()]
html_last_updated_fmt
'%b
%Y'
html_use_smartypants
html_copy_source
'Scrapydoc'
[('index',
'Scrapy.tex',
ur'Scrapy
Documentation',ur'Scrapy
developers',
'manual'),]
linkcheck_ignore
['http://localhost:\d+',
'http://hg.scrapy.org','http://directory.google.com/']
sphinx.util.compat
Directive
sphinx.util.nodes
make_refnode
settingslist_node(nodes.General,
nodes.Element):
SettingsListDirective(Directive):
[settingslist_node('')]
is_setting_index(node):
node.tagname
entry_type
'pair'
info.endswith(';
get_setting_target(node):
node.parent[node.parent.index(node)
get_setting_name_and_refid(node):
info.replace(';
setting',
collect_scrapy_settings_refs(app,
doctree):
hasattr(env,
'scrapy_all_settings'):
env.scrapy_all_settings
doctree.traverse(is_setting_index):
targetnode
get_setting_target(node)
isinstance(targetnode,
nodes.target),
"Next
target"
get_setting_name_and_refid(node)
env.scrapy_all_settings.append({
'docname':
env.docname,
'setting_name':
'refid':
refid,
make_setting_element(setting_data,
make_refnode(app.builder,
fromdocname,
todocname=setting_data['docname'],
targetid=setting_data['refid'],
child=nodes.Text(setting_data['setting_name']))
nodes.paragraph()
nodes.list_item()
replace_settingslist_nodes(app,
doctree,
doctree.traverse(settingslist_node):
settings_list
nodes.bullet_list()
settings_list.extend([make_setting_element(d,
fromdocname)
sorted(env.scrapy_all_settings,
key=itemgetter('setting_name'))
fromdocname
d['docname']])
node.replace_self(settings_list)
setting",
signal",
command",
reqmeta",
app.add_role('source',
source_role)
app.add_role('commit',
commit_role)
issue_role)
app.add_role('rev',
rev_role)
app.add_node(settingslist_node)
app.add_directive('settingslist',
SettingsListDirective)
app.connect('doctree-read',
collect_scrapy_settings_refs)
app.connect('doctree-resolved',
replace_settingslist_nodes)
source_role(name,
'https://github.com/scrapy/scrapy/blob/master/'
issue_role(name,
'https://github.com/scrapy/scrapy/issues/'
'issue
commit_role(name,
'https://github.com/scrapy/scrapy/commit/'
'commit
rev_role(name,
'http://hg.scrapy.org/scrapy/changeset/'
line_re
re.compile(ur'(.*)\:\d+\:\s\[(.*)\]\s(?:(.*)\sto\s(.*)|(.*))')
open("build/linkcheck/output.txt")
output_lines
out.readlines()
print("linkcheck
found;
linkcheck
first.")
exit(1)
output_lines:
re.match(line_re,
errortype
errortype.lower()
["broken",
"local"]:
Fixed:
open(_filename,
_file.write(_contents)
open(_filename)
_file.read()
_contents.replace(match.group(3),
match.group(4))
Understood:
deque(maxlen=100)
_reset_stats(self):
self.tail.clear()
self.tail.appendleft(delta)
len(self.tail)
sum(self.tail)
print('samplesize={0}
concurrent={1}
qps={2:0.2f}'.format(len(self.tail),
self.concurrent,
qps))
'latency'
float(request.args['latency'][0])
reactor.callLater(latency,
self._finish,
_finish(self,
request.finished
request._disconnected:
reactor.listenTCP(8880,
QPSSpider(Spider):
'qps'
benchurl
'http://localhost:8880/'
max_concurrent_requests
as:
super(QPSSpider,
float(self.qps)
float(self.download_delay)
self.benchurl
self.latency
'?latency={0}'.format(self.latency)
int(self.slots)
[url.replace('localhost',
'127.0.0.%d'
xrange(slots)]
urls[idx
len(urls)]
['__version__',
'version_info',
'twisted_version',
'Spider',
'Request',
'FormRequest',
'Selector',
'Item',
'Field']
pkgutil.get_data(__package__,
'VERSION').decode('ascii').strip()
version_info
tuple(int(v)
v.isdigit()
__version__.split('.'))
2.7"
__version__)
warnings.filterwarnings('ignore',
module='twisted')
_txv
(_txv.major,
_txv.minor,
_txv.micro)
copyreg
urlparse('s3://bucket/key').netloc
uses_netloc
uses_netloc.append('s3')
urlparse('s3://bucket/key?key=value').query
'key=value':
uses_query
uses_query.append('s3')
frozenset(copyreg.dispatch_table.items()):
str(getattr(k,
'')).startswith('twisted')
str(getattr(v,
'')).startswith('twisted'):
copyreg.dispatch_table.pop(k)
inside_project,
scrapy.settings.deprecated
check_deprecated_settings
_iter_command_classes(module_name):
walk_modules(module_name):
vars(module).values():
ScrapyCommand)
module.__name__:
_get_commands_from_module(module,
_iter_command_classes(module):
cmd.requires_project:
cmd.__module__.split('.')[-1]
d[cmdname]
cmd()
_get_commands_from_entry_points(inproject,
group='scrapy.commands'):
entry_point
pkg_resources.iter_entry_points(group):
entry_point.load()
inspect.isclass(obj):
cmds[entry_point.name]
obj()
Exception("Invalid
point
entry_point.name)
_get_commands_from_module('scrapy.commands',
cmds.update(_get_commands_from_entry_points(inproject))
cmds_module
settings['COMMANDS_MODULE']
cmds_module:
cmds.update(_get_commands_from_module(cmds_module,
inproject))
_pop_command_name(argv):
argv[1:]:
arg.startswith('-'):
argv[i]
project:
(scrapy.__version__,
settings['BOT_NAME']))
project\n"
print("Usage:")
[args]\n")
commands:")
cmdclass
sorted(cmds.items()):
%-13s
cmdclass.short_desc()))
More
commands
directory")
-h"
print("Unknown
cmdname)
commands')
func(*a,
parser.error(str(e))
e.print_help:
parser.print_help()
execute(argv=None,
'scrapy.conf'
hasattr(conf,
'settings'):
check_deprecated_settings(settings)
warnings.simplefilter("ignore",
inside_project()
_pop_command_name(argv)
optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),
conflict_handler='resolve')
cmdname:
cmds:
cmds[cmdname]
parser.usage
cmd.syntax())
parser.description
cmd.long_desc()
settings.setdict(cmd.default_settings,
priority='command')
cmd.settings
cmd.add_options(parser)
parser.parse_args(args=argv[1:])
cmd.process_options,
cmd.crawler_process
CrawlerProcess(settings)
_run_command,
sys.exit(cmd.exitcode)
_run_command(cmd,
cmd.run(args,
sys.stderr.write("scrapy:
%r\n"
opts.profile)
locals()
cProfile.Profile()
p.runctx('cmd.run(args,
opts)',
loc)
p.dump_stats(opts.profile)
execute()
`scrapy.command`
`scrapy.commands`
'scrapy.cmdline'
`scrapy.conf`
`crawler.settings`
verifyClass,
DoesNotImplement
CachingThreadedResolver
scrapy.extension
ExtensionManager
scrapy.signalmanager
SignalManager
scrapy.utils.ossignal
install_shutdown_handlers,
configure_logging,
log_scrapy_info
Crawler(object):
settings.copy()
self.spidercls.update_settings(self.settings)
SignalManager(self)
load_object(self.settings['STATS_CLASS'])(self)
LogCounterHandler(self,
level=settings.get('LOG_LEVEL'))
self.__remove_handler
logging.root.removeHandler(handler)
self.signals.connect(self.__remove_handler,
lf_cls
load_object(self.settings['LOG_FORMATTER'])
lf_cls.from_crawler(self)
ExtensionManager.from_crawler(self)
'_spiders'):
warnings.warn("Crawler.spiders
"CrawlerRunner.spider_loader
"scrapy.spiderloader.SpiderLoader
"settings.",
_get_spider_loader(self.settings.frozencopy())
self.crawling,
"Crawling
taking
place"
self._create_spider(*args,
self._create_engine()
iter(self.spider.start_requests())
self.engine.open_spider(self.spider,
start_requests)
defer.maybeDeferred(self.engine.start)
self.engine.close()
six.reraise(*exc_info)
_create_spider(self,
self.spidercls.from_crawler(self,
_create_engine(self):
ExecutionEngine(self,
self.stop())
self.crawling:
defer.maybeDeferred(self.engine.stop)
CrawlerRunner(object):
crawlers
property(
self._crawlers,
doc="Set
:class:`crawlers
<scrapy.crawler.Crawler>`
":meth:`crawl`
managed
class."
_get_spider_loader(settings)
self._crawlers
self._active
warnings.warn("CrawlerRunner.spiders
renamed
"CrawlerRunner.spider_loader.",
crawler_or_spidercls,
self.create_crawler(crawler_or_spidercls)
self._crawl(crawler,
_crawl(self,
self.crawlers.add(crawler)
crawler.crawl(*args,
self._active.add(d)
_done(result):
self.crawlers.discard(crawler)
self._active.discard(d)
d.addBoth(_done)
create_crawler(self,
crawler_or_spidercls):
isinstance(crawler_or_spidercls,
Crawler):
crawler_or_spidercls
self._create_crawler(crawler_or_spidercls)
_create_crawler(self,
isinstance(spidercls,
self.spider_loader.load(spidercls)
Crawler(spidercls,
defer.DeferredList([c.stop()
list(self.crawlers)])
join(self):
self._active:
defer.DeferredList(self._active)
CrawlerProcess(CrawlerRunner):
super(CrawlerProcess,
install_shutdown_handlers(self._signal_shutdown)
configure_logging(self.settings)
log_scrapy_info(self.settings)
_signal_shutdown(self,
install_shutdown_handlers(self._signal_kill)
logger.info("Received
%(signame)s,
shutting
gracefully.
Send
reactor.callFromThread(self._graceful_stop_reactor)
_signal_kill(self,
install_shutdown_handlers(signal.SIG_IGN)
logger.info('Received
%(signame)s
twice,
forcing
unclean
shutdown',
reactor.callFromThread(self._stop_reactor)
stop_after_crawl=True):
stop_after_crawl:
self.join()
d.called:
reactor.installResolver(self._get_dns_resolver())
tp
reactor.getThreadPool()
tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))
self.stop)
reactor.run(installSignalHandlers=False)
blocking
_get_dns_resolver(self):
self.settings.getbool('DNSCACHE_ENABLED'):
self.settings.getint('DNSCACHE_SIZE')
CachingThreadedResolver(
reactor=reactor,
cache_size=cache_size,
timeout=self.settings.getfloat('DNS_TIMEOUT')
_graceful_stop_reactor(self):
_stop_reactor(self,
_=None):
reactor.stop()
_get_spider_loader(settings):
settings.get('SPIDER_MANAGER_CLASS'):
warnings.warn('SPIDER_MANAGER_CLASS
SPIDER_LOADER_CLASS.',category=ScrapyDeprecationWarning,
cls_path
settings.get('SPIDER_MANAGER_CLASS',settings.get('SPIDER_LOADER_CLASS'))
loader_cls
load_object(cls_path)
verifyClass(ISpiderLoader,
loader_cls)
DoesNotImplement:
warnings.warn('SPIDER_LOADER_CLASS
(previously
SPIDER_MANAGER_CLASS)
''not
errors.',category=ScrapyDeprecationWarning,
loader_cls.from_settings(settings.frozencopy())
`scrapy.dupefilter`
""use
`scrapy.dupefilters`
instead",ScrapyDeprecationWarning,
BaseDupeFilter(object):
open(self):
filtered
RFPDupeFilter(BaseDupeFilter):
self.fingerprints
'requests.seen'),
'a+')
self.file.seek(0)
self.fingerprints.update(x.rstrip()
settings.getbool('DUPEFILTER_DEBUG')
cls(job_dir(settings),
self.request_fingerprint(request)
self.fingerprints:
self.fingerprints.add(fp)
self.file.write(fp
self.file.close()
"Filtered
self.logdupes:
("Filtered
duplicates
shown"
(see
DUPEFILTER_DEBUG
duplicates)")
spider.crawler.stats.inc_value('dupefilter/filtered',
NotConfigured(Exception):
IgnoreRequest(Exception):
DontCloseSpider(Exception):
CloseSpider(Exception):
super(CloseSpider,
DropItem(Exception):
NotSupported(Exception):
UsageError(Exception):
kw.pop('print_help',
super(UsageError,
ScrapyDeprecationWarning(Warning):
ContractFail(AssertionError):
XMLGenerator
['BaseItemExporter',
'PprintItemExporter',
'PickleItemExporter',
'CsvItemExporter',
'XmlItemExporter',
'JsonLinesItemExporter',
'JsonItemExporter',
'MarshalItemExporter']
BaseItemExporter(object):
options.pop('encoding',
options.pop('fields_to_export',
options.pop('export_empty_fields',
dont_fail
TypeError("Unexpected
'.join(options.keys()))
_get_serialized_fields(self,
include_empty=None):
six.iterkeys(item.fields)
six.iterkeys(item)
include_empty:
field_iter:
item.fields[field_name]
self.serialize_field(field,
item[field_name])
JsonLinesItemExporter(BaseItemExporter):
JsonItemExporter(BaseItemExporter):
self.file.write(b"[\n")
self.file.write(b"\n]")
self.first_item:
self.file.write(b',\n')
XmlItemExporter(BaseItemExporter):
self.item_element
kwargs.pop('item_element',
self.root_element
kwargs.pop('root_element',
'items')
self.xg
XMLGenerator(file,
self.xg.startDocument()
self.xg.startElement(self.root_element,
self.xg.startElement(self.item_element,
default_value=''):
self._export_xml_field(name,
self.xg.endElement(self.item_element)
self.xg.endElement(self.root_element)
self.xg.endDocument()
_export_xml_field(self,
self.xg.startElement(name,
hasattr(serialized_value,
subname,
serialized_value.items():
self._export_xml_field(subname,
is_listlike(serialized_value):
serialized_value:
self._export_xml_field('value',
self._xg_characters(serialized_value)
self._xg_characters(str(serialized_value))
self.xg.endElement(name)
sys.version_info[:3]
serialized_value
serialized_value.decode(self.encoding)
pragma:
cover
CsvItemExporter(BaseItemExporter):
include_headers_line=True,
join_multivalued=',',
self.include_headers_line
include_headers_line
io.TextIOWrapper(
line_buffering=False,
write_through=True,
encoding=self.encoding
self.csv_writer
csv.writer(self.stream,
self._join_multivalued
join_multivalued
self._join_if_needed)
_join_if_needed(self,
self._join_multivalued.join(value)
self._headers_not_written:
self._write_headers_and_set_fields_to_export(item)
default_value='',
include_empty=True)
list(self._build_row(x
fields))
self.csv_writer.writerow(values)
_build_row(self,
to_native_str(s,
_write_headers_and_set_fields_to_export(self,
self.include_headers_line:
self.fields_to_export:
list(item.keys())
list(item.fields.keys())
list(self._build_row(self.fields_to_export))
self.csv_writer.writerow(row)
PickleItemExporter(BaseItemExporter):
protocol=2,
pickle.dump(d,
self.file,
MarshalItemExporter(BaseItemExporter):
marshal.dump(dict(self._get_serialized_fields(item)),
PprintItemExporter(BaseItemExporter):
self.file.write(to_bytes(pprint.pformat(itemdict)
PythonItemExporter(BaseItemExporter):
options.pop('binary',
super(PythonItemExporter,
self)._configure(options,
dont_fail)
"PythonItemExporter
drop
export
future",
self._serialize_value)
_serialize_value(self,
self.export_item(value)
dict(self._serialize_dict(value))
is_listlike(value):
[self._serialize_value(v)
encode_func
encode_func(value,
_serialize_dict(self,
to_bytes(key)
self._serialize_value(val)
dict(self._serialize_dict(result))
ExtensionManager(MiddlewareManager):
'extension'
build_component_list(settings.getwithbase('EXTENSIONS'))
Interface
ISpiderLoader(Interface):
from_settings(settings):
load(spider_name):
list():
find_by_request(request):
ISpiderManager
abc
ABCMeta
BaseItem(object_ref):
Field(dict):
ItemMeta(ABCMeta):
__new__(mcs,
new_bases
tuple(base._class
hasattr(base,
'_class'))
'x_'
new_bases,
'fields',
new_attrs
dir(_class):
Field):
fields[n]
new_attrs[n]
attrs[n]
new_attrs['fields']
new_attrs['_class']
new_attrs)
DictItem(MutableMapping,
self._values
six.iteritems(dict(*args,
**kwargs)):
KeyError("%s
AttributeError(name)
super(DictItem,
len(self._values)
iter(self._values)
__hash__
BaseItem.__hash__
self._values.keys()
pformat(dict(self))
@six.add_metaclass(ItemMeta)
Item(DictItem):
Link(object):
'text',
'fragment',
'nofollow']
text='',
nofollow=False):
warnings.warn("Link
objects.
"Assuming
utf-8
(which
wrong)")
url.__class__.__name__
TypeError("Link
objects,
got)
nofollow
other.url
other.text
other.fragment
other.nofollow
hash(self.url)
hash(self.text)
hash(self.fragment)
hash(self.nofollow)
'Link(url=%r,
text=%r,
fragment=%r,
nofollow=%r)'
(self.url,
self.text,
self.fragment,
self.nofollow)
`scrapy.linkextractor`
`scrapy.log`
relies
builtin
logging.
Read
updated
"logging
learn
more.",
logging.DEBUG
INFO
logging.INFO
WARNING
logging.WARNING
logging.ERROR
logging.CRITICAL
SILENT
level_names
logging.DEBUG:
"DEBUG",
logging.INFO:
"INFO",
logging.WARNING:
"WARNING",
logging.ERROR:
"ERROR",
logging.CRITICAL:
"CRITICAL",
SILENT:
"SILENT",
msg(message=None,
_level=logging.INFO,
warnings.warn('log.msg
'log
_level)
kw.pop('format',
err(_stuff=None,
_why=None,
warnings.warn('log.err
kw.pop('failure',
_stuff)
kw.pop('why',
_why)
failure.value
exc_info=failure_to_exc_info(failure))
SCRAPEDMSG
u"Scraped
%(src)s"
DROPPEDMSG
u"Dropped:
%(exception)s"
CRAWLEDMSG
u"Crawled
(%(status)s)
%(referer)s)%(flags)s"
LogFormatter(object):
crawled(self,
str(response.flags)
CRAWLEDMSG,
referer_str(request),
'flags':
flags,
scraped(self,
response.getErrorMessage()
SCRAPEDMSG,
'src':
dropped(self,
logging.WARNING,
DROPPEDMSG,
COMMASPACE,
six.moves.email_mime_multipart
MIMEMultipart
six.moves.email_mime_text
MIMEText
six.moves.email_mime_base
MIMEBase
email.MIMENonMultipart
email.mime.nonmultipart
MailSender(object):
smtphost='localhost',
mailfrom='scrapy@localhost',
smtpuser=None,
smtppass=None,
smtpport=25,
smtptls=False,
smtpssl=False,
self.smtphost
smtphost
self.smtpport
smtpport
self.smtpuser
smtpuser
self.smtppass
smtppass
self.smtptls
smtptls
self.smtpssl
smtpssl
mailfrom
cls(settings['MAIL_HOST'],
settings['MAIL_FROM'],
settings['MAIL_USER'],
settings['MAIL_PASS'],
settings.getint('MAIL_PORT'),
settings.getbool('MAIL_TLS'),
settings.getbool('MAIL_SSL'))
cc=None,
attachs=(),
mimetype='text/plain',
charset=None,
_callback=None):
MIMEMultipart()
MIMENonMultipart(*mimetype.split('/',
COMMASPACE.join(to)
formatdate(localtime=True)
rcpts
to[:]
rcpts.extend(cc)
msg['Cc']
COMMASPACE.join(cc)
charset:
msg.set_charset(charset)
msg.attach(MIMEText(body,
'plain',
'us-ascii'))
attach_name,
MIMEBase(*mimetype.split('/'))
part.set_payload(f.read())
Encoders.encode_base64(part)
part.add_header('Content-Disposition',
filename="%s"'
attach_name)
msg.set_payload(body)
_callback:
_callback(to=to,
cc=cc,
attach=attachs,
msg=msg)
logger.debug('Debug
len(attachs)})
self._sendmail(rcpts,
dfd.addCallbacks(self._sent_ok,
self._sent_failed,
callbackArgs=[to,
len(attachs)],
errbackArgs=[to,
len(attachs)])
dfd)
_sent_ok(self,
logger.info('Mail
nattachs})
_sent_failed(self,
errstr
str(failure.value)
mail:
Attachs=%(mailattachs)d'
'-
%(mailerr)s',
nattachs,
'mailerr':
errstr})
_sendmail(self,
twisted.mail.smtp
ESMTPSenderFactory
StringIO(msg)
ESMTPSenderFactory(self.smtpuser,
self.smtppass,
self.mailfrom,
heloFallback=True,
requireAuthentication=False,
requireTransportSecurity=self.smtptls)
factory.noisy
self.smtpssl:
reactor.connectSSL(self.smtphost,
ssl.ClientContextFactory())
reactor.connectTCP(self.smtphost,
process_chain_both
MiddlewareManager(object):
*middlewares):
self.middlewares
self.methods
middlewares:
self._add_middleware(mw)
mwlist
cls._get_mwlist_from_settings(settings)
mwlist:
mwcls
load_object(clspath)
'from_crawler'):
mwcls.from_crawler(crawler)
'from_settings'):
mwcls.from_settings(settings)
mwcls()
middlewares.append(mw)
enabled.append(clspath)
e.args:
clsname
clspath.split('.')[-1]
logger.warning("Disabled
%(clsname)s:
%(eargs)s",
{'clsname':
clsname,
'eargs':
e.args[0]},
logger.info("Enabled
%(componentname)ss:\n%(enabledlist)s",
{'componentname':
cls.component_name,
'enabledlist':
pprint.pformat(enabled)},
cls(*middlewares)
cls.from_settings(crawler.settings,
'open_spider'):
self.methods['open_spider'].append(mw.open_spider)
'close_spider'):
self.methods['close_spider'].insert(0,
mw.close_spider)
_process_parallel(self,
process_parallel(self.methods[methodname],
_process_chain(self,
process_chain(self.methods[methodname],
_process_chain_both(self,
cb_methodname,
eb_methodname,
process_chain_both(self.methods[cb_methodname],
self.methods[eb_methodname],
self._process_parallel('open_spider',
self._process_parallel('close_spider',
ImportError()
twisted.internet.base
ThreadedResolver
LocalCache
LocalCache(10000)
CachingThreadedResolver(ThreadedResolver):
cache_size,
self).__init__(reactor)
dnscache.limit
getHostByName(self,
dnscache:
defer.succeed(dnscache[name])
self).getHostByName(name,
d.addCallback(self._cache_result,
_cache_result(self,
dnscache[name]
MimeTypes
get_data
ResponseTypes(object):
CLASSES
'text/html':
'application/atom+xml':
'application/rdf+xml':
'application/rss+xml':
'application/xhtml+xml':
'application/vnd.wap.xhtml+xml':
'application/xml':
'application/x-json':
'application/javascript':
'application/x-javascript':
'text/xml':
'text/*':
self.classes
self.mimetypes
MimeTypes()
mimedata
get_data('scrapy',
'mime.types').decode('utf8')
self.mimetypes.readfp(StringIO(mimedata))
six.iteritems(self.CLASSES):
load_object(cls)
from_mimetype(self,
mimetype):
self.classes:
basetype
"%s/*"
mimetype.split('/')[0]
self.classes.get(basetype,
from_content_type(self,
content_type,
content_encoding=None):
to_native_str(content_type).split(';')[0].strip().lower()
from_content_disposition(self,
content_disposition):
to_native_str(content_disposition,
errors='replace').split(';')[1].split('=')[1]
filename.strip('"\'')
from_headers(self,
b'Content-Type'
self.from_content_type(
content_type=headers[b'Content-type'],
content_encoding=headers.get(b'Content-Encoding')
b'Content-Disposition'
self.from_content_disposition(headers[b'Content-Disposition'])
from_filename(self,
self.mimetypes.guess_type(filename)
encoding:
from_body(self,
body[:5000]
to_bytes(chunk)
binary_is_text(chunk):
self.from_mimetype('application/octet-stream')
b"<html>"
self.from_mimetype('text/html')
b"<?xml"
self.from_mimetype('text/xml')
self.from_mimetype('text')
from_args(self,
self.from_headers(headers)
self.from_filename(url)
self.from_body(body)
ResponseTypes()
threads,
threadable
any_to_uri
Crawler
start_python_console
get_config
Shell(object):
relevant_classes
(Crawler,
update_vars=None,
code=None):
self.update_vars
update_vars
load_object(crawler.settings['DEFAULT_ITEM_CLASS'])
self.inthread
threadable.isInIOThread()
self.vars
signal.SIG_IGN)
self.fetch(url,
self.fetch(request,
self.populate_vars()
print(eval(self.code,
self.vars))
'settings',
os.environ.get('SCRAPY_PYTHON_SHELL')
env.strip().lower().split(',')
cfg.has_option(section,
[cfg.get(section,
option).strip().lower()]
['python']
start_python_console(self.vars,
shells=shells,
banner=self.vars.pop('banner',
_schedule(self,
self._open_spider(request,
_request_deferred(request)
(x,
self.crawler.engine.crawl(request,
_open_spider(self,
self.spider:
self.crawler._create_spider()
self.crawler.engine.open_spider(spider,
close_if_idle=False)
request_or_url,
isinstance(request_or_url,
request_or_url
any_to_uri(request_or_url)
threads.blockingCallFromThread(
self._schedule,
populate_vars(self,
self.vars['crawler']
self.vars['item']
self.item_class()
self.vars['settings']
self.vars['spider']
self.vars['request']
self.vars['response']
self.vars['sel']
_SelectorProxy(response)
self.vars['fetch']
self.fetch
self.vars['view']
self.vars['shelp']
self.update_vars(self.vars)
self.vars['banner']
self.get_help()
print_help(self):
print(self.get_help())
get_help(self):
b.append("Available
objects:")
sorted(self.vars.items()):
self._is_relevant(v):
%-10s
b.append("Useful
shortcuts:")
shelp()
(print
help)")
fetch(req_or_url)
Fetch
(or
URL)
"update
objects")
view(response)
View
browser")
"\n".join("[s]
_is_relevant(self,
self.relevant_classes)
inspect_response(response,
Shell(spider.crawler).start(response=response)
_request_deferred(request):
_restore_callbacks(result):
d.addBoth(_restore_callbacks)
request.callback:
d.addCallbacks(request.callback,
request.callback,
d.errback
_SelectorProxy(object):
self._proxiedresponse
warnings.warn('"sel"
shortcut
"response.xpath()",
'"response.css()"
"response.selector"
getattr(self._proxiedresponse.selector,
_signal
SignalManager(object):
sender=dispatcher.Anonymous):
self.sender
dispatcher.connect(receiver,
disconnect(self,
dispatcher.disconnect(receiver,
send_catch_log(self,
_signal.send_catch_log(signal,
send_catch_log_deferred(self,
_signal.send_catch_log_deferred(signal,
disconnect_all(self,
_signal.disconnect_all(signal,
engine_started
engine_stopped
spider_idle
spider_error
request_dropped
response_received
response_downloaded
item_dropped
stats_spider_opened
stats_spider_closing
stats_spider_closed
item_passed
request_received
`scrapy.spider`
@implementer(ISpiderLoader)
SpiderLoader(object):
self.spider_modules
settings.getlist('SPIDER_MODULES')
self._load_all_spiders()
_load_spiders(self,
module):
self._spiders[spcls.name]
_load_all_spiders(self):
self.spider_modules:
walk_modules(name):
self._load_spiders(module)
cls(settings)
spider_name):
self._spiders[spider_name]
KeyError("Spider
{}".format(spider_name))
find_by_request(self,
[name
self._spiders.items()
cls.handles_request(request)]
list(self):
list(self._spiders.keys())
SpiderManager
create_deprecated_class('SpiderManager',
SpiderLoader)
`scrapy.squeue`
`scrapy.squeues`
queuelib
_serializable_queue(queue_class,
serialize,
deserialize):
SerializableQueue(queue_class):
push(self,
serialize(obj)
self).push(s)
pop(self):
self).pop()
deserialize(s)
SerializableQueue
_pickle_serialize(obj):
pickle.dumps(obj,
(pickle.PicklingError,
AttributeError)
ValueError(str(e))
PickleFifoDiskQueue
MarshalFifoDiskQueue
MarshalLifoDiskQueue
FifoMemoryQueue
queue.FifoMemoryQueue
LifoMemoryQueue
queue.LifoMemoryQueue
ImportError("scrapy.stats
obsolete,
"`crawler.stats`
`scrapy.statscol`
`scrapy.statscollectors`
StatsCollector(object):
self._dump
crawler.settings.getbool('STATS_DUMP')
self._stats.get(key,
get_stats(self,
d[key]
d.setdefault(key,
max(self._stats.setdefault(key,
min(self._stats.setdefault(key,
clear_stats(self,
self._stats.clear()
self._dump:
stats:\n"
pprint.pformat(self._stats),
self._persist_stats(self._stats,
MemoryStatsCollector(StatsCollector):
super(MemoryStatsCollector,
self).__init__(crawler)
self.spider_stats
self.spider_stats[spider.name]
DummyStatsCollector(StatsCollector):
`scrapy.telnet`
`scrapy.extensions.telnet`
scrapy.extensions.telnet
OptionGroup
ScrapyCommand(object):
crawler_process
exitcode
scrapy.cmdline
'_crawler'),
"crawler
self.short_desc()
help(self):
self.long_desc()
OptionGroup(parser,
Options")
group.add_option("--logfile",
omitted
used")
group.add_option("-L",
"--loglevel",
metavar="LEVEL",
self.settings['LOG_LEVEL'])
group.add_option("--nolog",
help="disable
completely")
group.add_option("--profile",
group.add_option("--pidfile",
group.add_option("-s",
"--set",
help="set/override
group.add_option("--pdb",
help="enable
failure")
parser.add_option_group(group)
self.settings.setdict(arglist_to_dict(opts.set),
opts.logfile:
self.settings.set('LOG_FILE',
opts.logfile,
opts.loglevel:
self.settings.set('LOG_LEVEL',
opts.loglevel,
opts.nolog:
opts.pidfile:
open(opts.pidfile,
f.write(str(os.getpid())
opts.pdb:
failure.startDebugMode()
'LOG_LEVEL':
'CLOSESPIDER_TIMEOUT':
quick
benchmark
test"
_BenchServer():
self.crawler_process.crawl(_BenchSpider,
total=100000)
_BenchServer(object):
pargs
'scrapy.utils.benchserver']
subprocess.Popen(pargs,
_BenchSpider(scrapy.Spider):
baseurl
'http://localhost:8998'
self.total,
self.show}
'{}?{}'.format(self.baseurl,
doseq=1))
[scrapy.Request(url,
dont_filter=True)]
scrapy.Request(link.url,
TextTestRunner,
_TextTestResult
TextTestResult(_TextTestResult):
printSummary(self,
stop):
self.stream.write
writeln
self.stream.writeln
self.testsRun
plural
"s"
writeln(self.separator2)
writeln("Ran
contract%s
%.3fs"
(run,
plural,
start))
writeln()
infos
self.wasSuccessful():
write("FAILED")
errored
map(len,
(self.failures,
self.errors))
infos.append("failures=%d"
failed)
errored:
infos.append("errors=%d"
errored)
write("OK")
infos:
writeln("
(",
".join(infos),))
write("\n")
"Check
contracts"
help="only
contracts,
checking
them")
spiders")
build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))
conman
ContractsManager(load_object(c)
contracts)
TextTestRunner(verbosity=2
opts.verbose
TextTestResult(runner.stream,
runner.descriptions,
runner.verbosity)
contract_reqs
spidername
spider_loader.list():
spider_loader.load(spidername)
spidercls.start_requests
conman.from_spider(s,
tested_methods
conman.tested_methods_from_spidercls(spidercls)
contract_reqs[spidercls.name].append(method)
self.crawler_process.crawl(spidercls)
sorted(contract_reqs.items()):
print(spider)
sorted(methods):
result.printErrors()
result.printSummary(start,
stop)
int(not
result.wasSuccessful())
self.settings.getwithbase('FEED_EXPORTERS'))
UsageError("running
'scrapy
crawl'
self.crawler_process.crawl(spname,
"<spider>"
setting"
_err(self,
sys.stderr.write(msg
self.settings['EDITOR']
self.crawler_process.spider_loader.load(args[0])
self._err("Spider
args[0])
sys.modules[spidercls.__module__].__file__
sfile.replace('.pyc',
os.system('%s
(editor,
sfile))
downloader"
stdout.
--nolog
logging"
parser.add_option("--headers",
dest="headers",
_print_headers(self,
self._print_bytes(prefix
opts.headers:
self._print_headers(response.request.headers,
b'>')
print('>')
self._print_headers(response.headers,
b'<')
self._print_bytes(response.body)
_print_bytes(self,
bytes_):
bytes_writer
bytes_writer.write(bytes_
b'\n')
self._print_response(x,
Request(args[0],
spidercls)
start_requests=lambda:
[request])
splitext
sanitize_module_name(module_name):
module_name.replace('-',
'_').replace('.',
module_name[0]
string.ascii_letters:
"a"
<name>
<domain>"
"Generate
pre-defined
templates"
help="List
templates")
parser.add_option("-e",
"--edit",
dest="edit",
help="Edit
it")
"--dump",
dest="dump",
metavar="TEMPLATE",
help="Dump
"--template",
dest="template",
default="basic",
help="Uses
template.")
parser.add_option("--force",
dest="force",
help="If
template")
self._list_templates()
opts.dump:
self._find_template(opts.dump)
open(template_file,
"r")
print(f.read())
args[0:2]
sanitize_module_name(name)
self.settings.get('BOT_NAME')
print("Cannot
self.crawler_process.spider_loader.load(name)
opts.force:
print("Spider
module:"
spidercls.__module__)
self._find_template(opts.template)
self._genspider(module,
opts.template,
template_file)
opts.edit:
os.system('scrapy
edit
_genspider(self,
template_file):
tvars
'project_name':
self.settings.get('BOT_NAME'),
'ProjectName':
string_camelcase(self.settings.get('BOT_NAME')),
'module':
'classname':
'%sSpider'
''.join(s.capitalize()
module.split('_'))
self.settings.get('NEWSPIDER_MODULE'):
import_module(self.settings['NEWSPIDER_MODULE'])
abspath(dirname(spiders_module.__file__))
spider_file
"%s.py"
join(spiders_dir,
shutil.copyfile(template_file,
spider_file)
render_templatefile(spider_file,
**tvars)
print("Created
template_name),
end=(''
spiders_module:
print("in
module:\n
%s.%s"
(spiders_module.__name__,
module))
_find_template(self,
join(self.templates_dir,
'%s.tmpl'
exists(template_file):
print("Unable
template:
--list"
templates.')
_list_templates(self):
templates:")
sorted(os.listdir(self.templates_dir)):
filename.endswith('.tmpl'):
splitext(filename)[0])
'spiders')
"List
sorted(self.crawler_process.spider_loader.list()):
spidercls_for_request
first_response
"Parse
(using
results"
parser.add_option("--pipelines",
help="process
pipelines")
parser.add_option("--nolinks",
dest="nolinks",
(extracted
requests)")
parser.add_option("--noitems",
dest="noitems",
items")
parser.add_option("--nocolour",
dest="nocolour",
help="avoid
parser.add_option("-r",
"--rules",
dest="rules",
discover
"--callback",
dest="callback",
parsing,
"--depth",
dest="depth",
type="int",
default=1,
help="maximum
[default:
%default]")
max_level(self):
list(self.items.keys())
max(levels)
add_items(self,
new_items):
self.items[lvl]
new_items
add_requests(self,
new_reqs):
self.requests[lvl]
new_reqs
print_items(self,
[item
self.items.values()
lst]
Scraped
Items
"-"*60)
display.pprint([dict(x)
items],
print_requests(self,
self.requests[max(levels)]
"-"*65)
display.pprint(requests,
print_results(self,
colour
opts.nocolour
self.max_level+1):
LEVEL:
level)
self.print_items(level,
self.print_requests(level,
LEVEL
self.max_level)
self.print_items(colour=colour)
self.print_requests(colour=colour)
run_callback(self,
cb):
iterate_spider_output(cb(response)):
items.append(x)
requests.append(x)
get_callback_from_rules(self,
'rules',
spider.rules:
rule.link_extractor.matches(response.url)
rule.callback:
%(spider)r,
parsing',
set_spidercls(self,
opts.spider})
Request(url))
self.spidercls:
opts.callback)
[self.prepare_request(s,
opts)]
self.spidercls.start_requests
start_parsing(self,
self.crawler_process.crawl(self.spidercls,
self.pcrawler
list(self.crawler_process.crawlers)[0]
downloaded
prepare_request(self,
callback(response):
response.meta['_callback']
opts.rules
self.get_callback_from_rules(spider,
callable(cb_method):
logger.error('Cannot
%(callback)r
{'callback':
response.meta['_depth']
self.run_callback(response,
opts.pipelines:
itemproc
self.pcrawler.engine.scraper.itemproc
itemproc.process_item(item,
self.add_items(depth,
self.add_requests(depth,
requests)
opts.depth:
requests:
req.meta['_depth']
req.meta['_callback']
request.meta['_depth']
request.meta['_callback']
self.set_spidercls(url,
opts.depth
self.start_parsing(url,
self.print_results(opts)
_import_file(filepath):
os.path.abspath(filepath)
os.path.split(abspath)
fname,
os.path.splitext(file)
'.py':
ValueError("Not
abspath)
[dirname]
import_module(fname)
sys.path.pop(0)
<spider_file>"
self-contained
(without
project)"
file"
without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))
os.path.exists(filename):
UsageError("File
_import_file(filename)
(ImportError,
UsageError("Unable
%r:
(filename,
spclasses
list(iter_spider_classes(module))
spclasses:
UsageError("No
spclasses.pop()
"[options]"
"Get
values"
parser.add_option("--get",
dest="get",
parser.add_option("--getbool",
dest="getbool",
boolean")
parser.add_option("--getint",
dest="getint",
integer")
parser.add_option("--getfloat",
dest="getfloat",
float")
parser.add_option("--getlist",
dest="getlist",
list")
self.crawler_process.settings
opts.get:
settings.get(opts.get)
print(json.dumps(s.copy_to_dict()))
opts.getbool:
print(settings.getbool(opts.getbool))
opts.getint:
print(settings.getint(opts.getint))
opts.getfloat:
print(settings.getfloat(opts.getfloat))
opts.getlist:
print(settings.getlist(opts.getlist))
scrapy.shell
guess_scheme
'KEEP_ALIVE':
'DUPEFILTER_CLASS':
'scrapy.dupefilters.BaseDupeFilter',
"[url|file]"
console"
url"
dest="code",
help="evaluate
shell,
exit")
update_vars(self,
vars):
guess_scheme(url)
Request(url),
log_multiple=True)
self.crawler_process._create_crawler(spidercls)
crawler.engine
crawler._create_engine()
crawler.engine.start()
self._start_crawler_thread()
Shell(crawler,
update_vars=self.update_vars,
code=opts.code)
shell.start(url=url)
_start_crawler_thread(self):
Thread(target=self.crawler_process.start,
kwargs={'stop_after_crawl':
ignore_patterns,
copy2,
copystat
TEMPLATES_TO_RENDER
('scrapy.cfg',),
'settings.py.tmpl'),
'items.py.tmpl'),
'pipelines.py.tmpl'),
ignore_patterns('*.pyc',
'.svn')
"<project_name>
[project_dir]"
"Create
project"
_is_valid_name(self,
_module_exists(module_name):
import_module(module_name)
re.search(r'^[_a-zA-Z]\w*$',
Project
begin
letter
contain'\
only\nletters,
underscores')
_module_exists(project_name):
project_name)
_copytree(self,
os.listdir(src)
ignored_names
ignore(src,
os.path.exists(dst):
os.makedirs(dst)
ignored_names:
srcname
os.path.join(src,
dstname
os.path.join(dst,
os.path.isdir(srcname):
self._copytree(srcname,
copy2(srcname,
copystat(src,
args[1]
exists(join(project_dir,
'scrapy.cfg')):
self._is_valid_name(project_name):
self._copytree(self.templates_dir,
move(join(project_dir,
'module'),
project_name))
TEMPLATES_TO_RENDER:
join(*paths)
tplfile
string.Template(path).substitute(project_name=project_name))
render_templatefile(tplfile,
project_name=project_name,
ProjectName=string_camelcase(project_name))
print("New
in:"
(project_name,
self.templates_dir))
print("You
with:")
cd
project_dir)
example
example.com")
"[-v]"
"Print
version"
parser.add_option("--verbose",
"-v",
help="also
twisted/python/platform
(useful
reports)")
lxml_version
lxml.etree.LXML_VERSION))
libxml2_version
lxml.etree.LIBXML_VERSION))
print("lxml
lxml_version)
print("libxml2
libxml2_version)
print("Twisted
twisted.version.short())
print("Python
sys.version.replace("\n",
"-
"))
print("pyOpenSSL
self._get_openssl_version())
print("Platform
platform.platform())
_get_openssl_version(self):
OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\
.decode('ascii',
version'
'{}
({})'.format(OpenSSL.version.__version__,
openssl)
fetch,
Command(fetch.Command):
"Open
browser,
Scrapy"
"contents
browser"
open_in_browser(response)
get_spec
ContractsManager(object):
contracts):
self.contracts[contract.name]
tested_methods_from_spidercls(self,
vars(spidercls).items():
(callable(value)
value.__doc__
re.search(r'^\s*@',
value.__doc__,
re.MULTILINE)):
methods.append(key)
extract_contracts(self,
method.__doc__.split('\n'):
line.startswith('@'):
re.match(r'@(\w+)\s*(.*)',
line).groups()
re.split(r'\s+',
contracts.append(self.contracts[name](method,
*args))
from_spider(self,
self.tested_methods_from_spidercls(type(spider)):
bound_method
spider.__getattribute__(method)
requests.append(self.from_method(bound_method,
results))
from_method(self,
self.extract_contracts(method)
get_spec(Request.__init__)
kwargs['callback']
contract.adjust_request_args(kwargs)
args.remove('self')
set(args).issubset(set(kwargs)):
Request(**kwargs)
reversed(contracts):
contract.add_pre_hook(request,
contract.add_post_hook(request,
self._clean_req(request,
_clean_req(self,
cb_wrapper(response):
cb(response)
list(iterate_spider_output(output))
'callback')
eb_wrapper(failure):
'errback')
failure.type,
failure.getTracebackObject()
exc_info)
cb_wrapper
eb_wrapper
Contract(object):
self.testcase_pre
pre-hook'
self.testcase_post
post-hook'
add_pre_hook(self,
'pre_process'):
results.startTest(self.testcase_pre)
self.pre_process(response)
results.stopTest(self.testcase_pre)
results.addFailure(self.testcase_pre,
results.addError(self.testcase_pre,
results.addSuccess(self.testcase_pre)
add_post_hook(self,
'post_process'):
results.startTest(self.testcase_post)
self.post_process(output)
results.stopTest(self.testcase_post)
results.addFailure(self.testcase_post,
results.addError(self.testcase_post,
results.addSuccess(self.testcase_post)
desc):
method.__self__.name
ContractTestCase(TestCase):
__str__(_self):
method.__name__,
desc)
'%s_%s'
method.__name__)
setattr(ContractTestCase,
ContractTestCase(name)
ContractFail
Contract
UrlContract(Contract):
args['url']
ReturnsContract(Contract):
'returns'
'requests':
super(ReturnsContract,
self.obj_name
self.obj_type
self.objects[self.obj_name]
int(self.args[1])
int(self.args[2])
float('inf')
self.obj_type):
(self.min_bound
assertion:
self.max_bound:
'%s..%s'
(self.min_bound,
ContractFail("Returned
(occurrences,
self.obj_name,
ScrapesContract(Contract):
'scrapes'
ContractFail("'%s'
missing"
arg)
`scrapy.contrib.closespider`
`scrapy.extensions.closespider`
scrapy.extensions.closespider
`scrapy.contrib.corestats`
`scrapy.extensions.corestats`
scrapy.extensions.corestats
`scrapy.contrib.debug`
`scrapy.extensions.debug`
scrapy.extensions.debug
`scrapy.contrib.feedexport`
`scrapy.extensions.feedexport`
`scrapy.contrib.httpcache`
`scrapy.extensions.httpcache`
scrapy.extensions.httpcache
`scrapy.contrib.logstats`
`scrapy.extensions.logstats`
scrapy.extensions.logstats
`scrapy.contrib.memdebug`
`scrapy.extensions.memdebug`
scrapy.extensions.memdebug
`scrapy.contrib.memusage`
`scrapy.extensions.memusage`
scrapy.extensions.memusage
`scrapy.contrib.spiderstate`
`scrapy.extensions.spiderstate`
`scrapy.contrib.statsmailer`
`scrapy.extensions.statsmailer`
scrapy.extensions.statsmailer
`scrapy.contrib.throttle`
`scrapy.extensions.throttle`
`scrapy.contrib.downloadermiddleware.ajaxcrawl`
`scrapy.downloadermiddlewares.ajaxcrawl`
`scrapy.contrib.downloadermiddleware.chunked`
`scrapy.downloadermiddlewares.chunked`
scrapy.downloadermiddlewares.chunked
`scrapy.contrib.downloadermiddleware.cookies`
`scrapy.downloadermiddlewares.cookies`
`scrapy.contrib.downloadermiddleware.decompression`
`scrapy.contrib.downloadermiddleware.defaultheaders`
`scrapy.downloadermiddlewares.defaultheaders`
`scrapy.contrib.downloadermiddleware.downloadtimeout`
`scrapy.downloadermiddlewares.downloadtimeout`
`scrapy.contrib.downloadermiddleware.httpauth`
`scrapy.downloadermiddlewares.httpauth`
`scrapy.contrib.downloadermiddleware.httpcache`
`scrapy.downloadermiddlewares.httpcache`
`scrapy.contrib.downloadermiddleware.httpcompression`
`scrapy.downloadermiddlewares.httpcompression`
`scrapy.contrib.downloadermiddleware.httpproxy`
`scrapy.downloadermiddlewares.httpproxy`
`scrapy.contrib.downloadermiddleware.redirect`
`scrapy.downloadermiddlewares.redirect`
`scrapy.contrib.downloadermiddleware.retry`
`scrapy.downloadermiddlewares.retry`
`scrapy.contrib.downloadermiddleware.robotstxt`
`scrapy.downloadermiddlewares.robotstxt`
`scrapy.contrib.downloadermiddleware.stats`
`scrapy.downloadermiddlewares.stats`
`scrapy.contrib.downloadermiddleware.useragent`
`scrapy.downloadermiddlewares.useragent`
`scrapy.contrib.exporter`
`scrapy.exporters`
PythonItemExporter
`scrapy.contrib.linkextractors`
`scrapy.contrib.linkextractors.htmlparser`
`scrapy.linkextractors.htmlparser`
`scrapy.contrib.linkextractors.lxmlhtml`
`scrapy.linkextractors.lxmlhtml`
`scrapy.contrib.linkextractors.regex`
`scrapy.linkextractors.regex`
`scrapy.contrib.linkextractors.sgml`
`scrapy.linkextractors.sgml`
`scrapy.contrib.loader`
`scrapy.loader`
`scrapy.contrib.loader.common`
`scrapy.loader.common`
scrapy.loader.common
`scrapy.contrib.loader.processor`
`scrapy.loader.processors`
`scrapy.contrib.pipeline`
`scrapy.pipelines`
scrapy.pipelines
`scrapy.contrib.pipeline.files`
`scrapy.pipelines.files`
`scrapy.contrib.pipeline.images`
`scrapy.pipelines.images`
`scrapy.contrib.pipeline.media`
`scrapy.pipelines.media`
`scrapy.contrib.spidermiddleware.depth`
`scrapy.spidermiddlewares.depth`
`scrapy.contrib.spidermiddleware.httperror`
`scrapy.spidermiddlewares.httperror`
`scrapy.contrib.spidermiddleware.offsite`
`scrapy.spidermiddlewares.offsite`
`scrapy.contrib.spidermiddleware.referer`
`scrapy.spidermiddlewares.referer`
`scrapy.contrib.spidermiddleware.urllength`
`scrapy.spidermiddlewares.urllength`
`scrapy.contrib.spiders`
`scrapy.contrib.spiders.crawl`
`scrapy.spiders.crawl`
`scrapy.contrib.spiders.feed`
`scrapy.spiders.feed`
`scrapy.contrib.spiders.init`
`scrapy.spiders.init`
`scrapy.contrib.spiders.sitemap`
`scrapy.spiders.sitemap`
`scrapy.contrib_exp.iterators`
`scrapy.utils.iterators`
`scrapy.contrib_exp.downloadermiddleware.decompression`
scrapy.core.scraper
Scraper
DontCloseSpider
CallLaterOnce
scheduler):
self.inprogress
progress
self.start_requests
iter(start_requests)
self.close_if_idle
close_if_idle
self.nextcall
self.scheduler
self.heartbeat
task.LoopingCall(nextcall.schedule)
add_request(self,
self.inprogress.add(request)
remove_request(self,
self.inprogress.remove(request)
_maybe_fire_closing(self):
self.inprogress:
self.nextcall:
self.nextcall.cancel()
self.heartbeat.stop()
self.closing.callback(None)
ExecutionEngine(object):
spider_closed_callback):
self.scheduler_cls
load_object(self.settings['SCHEDULER'])
downloader_cls
load_object(self.settings['DOWNLOADER'])
self.downloader
downloader_cls(crawler)
self.scraper
Scraper(crawler)
self._spider_closed_callback
spider_closed_callback
self.signals.send_catch_log_deferred(signal=signals.engine_started)
self._finish_stopping_engine())
self.open_spiders:
defer.succeed(self.downloader.close())
pause(self):
unpause(self):
_next_request(self,
slot:
self.paused:
self._next_request_from_scheduler(spider):
next(slot.start_requests)
obtaining
requests',
self.crawl(request,
self.spider_is_idle(spider)
slot.close_if_idle:
self._spider_idle(spider)
_needs_backout(self,
self.downloader.needs_backout()
self.scraper.slot.needs_backout()
_next_request_from_scheduler(self,
slot.scheduler.next_request()
d.addBoth(self._handle_downloader_output,
handling
slot.remove_request(request))
slot',
slot.nextcall.schedule())
scheduling
request',
_handle_downloader_output(self,
Failure)),
self.crawl(response,
self.scraper.enqueue_scrape(response,
enqueuing
spider_is_idle(self,
self.scraper.slot.is_idle():
self.downloader.active:
self.slot.start_requests
self.slot.scheduler.has_pending_requests():
open_spiders(self):
[self.spider]
has_capacity(self):
bool(self.slot)
self.open_spiders,
crawling:
(spider.name,
self.schedule(request,
self.slot.nextcall.schedule()
self.signals.send_catch_log(signal=signals.request_scheduled,
self.slot.scheduler.enqueue_request(request):
self.signals.send_catch_log(signal=signals.request_dropped,
d.addBoth(self._downloaded,
_downloaded(self,
slot.remove_request(request)
self.download(response,
_on_success(response):
Request))
tie
self.logformatter.crawled(request,
self.signals.send_catch_log(signal=signals.response_received,
_on_complete(_):
self.downloader.fetch(request,
dwld.addCallbacks(_on_success)
dwld.addBoth(_on_complete)
start_requests=(),
close_if_idle=True):
self.has_capacity(),
"No
opening
CallLaterOnce(self._next_request,
self.scheduler_cls.from_crawler(self.crawler)
self.scraper.spidermw.process_start_requests(start_requests,
Slot(start_requests,
scheduler)
scheduler.open(spider)
self.scraper.open_spider(spider)
self.crawler.stats.open_spider(spider)
self.signals.send_catch_log_deferred(signals.spider_opened,
slot.heartbeat.start(5)
_spider_idle(self,
self.signals.send_catch_log(signal=signals.spider_idle,
dont_log=DontCloseSpider)
any(isinstance(x,
isinstance(x.value,
DontCloseSpider)
self.spider_is_idle(spider):
self.close_spider(spider,
reason='finished')
slot.closing:
logger.info("Closing
log_failure(msg):
errback(failure):
self.downloader.close())
dfd.addErrback(log_failure('Downloader
self.scraper.close_spider(spider))
dfd.addErrback(log_failure('Scraper
slot.scheduler.close(reason))
dfd.addErrback(log_failure('Scheduler
signal=signals.spider_closed,
spider_close
signal'))
self.crawler.stats.close_spider(spider,
dfd.addErrback(log_failure('Stats
'slot',
slot'))
spider'))
self._spider_closed_callback(spider))
_close_all_spiders(self):
[self.close_spider(s,
reason='shutdown')
self.open_spiders]
defer.DeferredList(dfds)
_finish_stopping_engine(self):
self.signals.send_catch_log_deferred(signal=signals.engine_stopped)
self._closewait.callback(None)
Scheduler(object):
dupefilter,
jobdir=None,
dqclass=None,
mqclass=None,
logunser=False,
pqclass=None):
self.df
self._dqdir(jobdir)
self.pqclass
self.dqclass
self.mqclass
dupefilter_cls
load_object(settings['DUPEFILTER_CLASS'])
dupefilter_cls.from_settings(settings)
load_object(settings['SCHEDULER_PRIORITY_QUEUE'])
load_object(settings['SCHEDULER_DISK_QUEUE'])
load_object(settings['SCHEDULER_MEMORY_QUEUE'])
settings.getbool('LOG_UNSERIALIZABLE_REQUESTS',
settings.getbool('SCHEDULER_DEBUG'))
cls(dupefilter,
jobdir=job_dir(settings),
logunser=logunser,
stats=crawler.stats,
pqclass=pqclass,
dqclass=dqclass,
mqclass=mqclass)
has_pending_requests(self):
self.mqs
self.pqclass(self._newmq)
self._dq()
self.df.open()
self.dqs.close()
open(join(self.dqdir,
'active.json'),
json.dump(prios,
self.df.close(reason)
enqueue_request(self,
self.df.request_seen(request):
self.df.log(request,
dqok
self._dqpush(request)
dqok:
self.stats.inc_value('scheduler/enqueued/disk',
self._mqpush(request)
self.stats.inc_value('scheduler/enqueued/memory',
self.stats.inc_value('scheduler/enqueued',
next_request(self):
self.mqs.pop()
self.stats.inc_value('scheduler/dequeued/memory',
self._dqpop()
self.stats.inc_value('scheduler/dequeued/disk',
self.stats.inc_value('scheduler/dequeued',
len(self.dqs)
_dqpush(self,
reqd
self.dqs.push(reqd,
non
serializable
self.logunser:
("Unable
serialize
reason:"
%(reason)s
unserializable
be"
(stats
collected)")
logger.error(msg,
e},
self.stats.inc_value('scheduler/unserializable',
_mqpush(self,
self.mqs.push(request,
_dqpop(self):
self.dqs.pop()
_newmq(self,
self.mqclass()
_newdq(self,
self.dqclass(join(self.dqdir,
'p%s'
priority))
_dq(self):
activef
join(self.dqdir,
'active.json')
exists(activef):
open(activef)
self.pqclass(self._newdq,
startprios=prios)
q:
logger.info("Resuming
crawl
(%(queuesize)d
scheduled)",
{'queuesize':
len(q)},
_dqdir(self,
jobdir):
join(jobdir,
'requests.queue')
exists(dqdir):
os.makedirs(dqdir)
defer_result,
defer_succeed,
parallel,
CloseSpider,
DropItem,
scrapy.core.spidermw
SpiderMiddlewareManager
MIN_RESPONSE_SIZE
max_active_size=5000000):
max_active_size
self.itemproc_size
add_response_request(self,
self.queue.append((response,
next_response_request_deferred(self):
self.queue.popleft()
finish_response(self,
(self.queue
self.active)
Scraper(object):
self.spidermw
SpiderMiddlewareManager.from_crawler(crawler)
itemproc_cls
load_object(crawler.settings['ITEM_PROCESSOR'])
self.itemproc
itemproc_cls.from_crawler(crawler)
self.concurrent_items
crawler.settings.getint('CONCURRENT_ITEMS')
Slot()
self.itemproc.open_spider(spider)
slot.closing.addCallback(self.itemproc.close_spider)
_check_if_closing(self,
slot.is_idle():
slot.closing.callback(spider)
enqueue_scrape(self,
slot.add_response_request(response,
finish_scraping(_):
slot.finish_response(response,
dfd.addBoth(finish_scraping)
logger.error('Scraper
_scrape_next(self,
slot.queue:
slot.next_response_request_deferred()
self._scrape(response,
spider).chainDeferred(deferred)
_scrape(self,
self._scrape2(response,
processed
dfd.addErrback(self.handle_spider_error,
dfd.addCallback(self.handle_spider_output,
_scrape2(self,
isinstance(request_result,
self.spidermw.scrape_response(
self.call_spider,
self.call_spider(request_result,
self._log_download_errors,
call_spider(self,
result.request
dfd.addCallbacks(request.callback
spider.parse,
dfd.addCallback(iterate_spider_output)
handle_spider_error(self,
_failure,
CloseSpider):
exc.reason
%(referer)s)",
referer_str(request)},
exc_info=failure_to_exc_info(_failure),
self.signals.send_catch_log(
signal=signals.spider_error,
failure=_failure,
self.crawler.stats.inc_value(
"spider_exceptions/%s"
_failure.value.__class__.__name__,
handle_spider_output(self,
defer_succeed(None)
iter_errback(result,
self.handle_spider_error,
parallel(it,
self.concurrent_items,
self._process_spidermw_output,
_process_spidermw_output(self,
self.crawler.engine.crawl(request=output,
self.itemproc.process_item(output,
dfd.addBoth(self._itemproc_finished,
typename
type(output).__name__
logger.error('Spider
'got
%(typename)r
'typename':
typename},
_log_download_errors(self,
spider_failure,
download_failure,
(isinstance(download_failure,
download_failure.check(IgnoreRequest)):
download_failure.frames:
exc_info=failure_to_exc_info(download_failure),
errmsg
download_failure.getErrorMessage()
errmsg:
%(errmsg)s',
'errmsg':
errmsg},
download_failure:
_itemproc_finished(self,
output.value
isinstance(ex,
DropItem):
self.logformatter.dropped(item,
ex,
signal=signals.item_dropped,
exception=output.value)
{'item':
exc_info=failure_to_exc_info(output),
self.logformatter.scraped(output,
signal=signals.item_scraped,
item=output,
_isiterable(possible_iterator):
hasattr(possible_iterator,
SpiderMiddlewareManager(MiddlewareManager):
'spider
build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))
super(SpiderMiddlewareManager,
'process_spider_input'):
self.methods['process_spider_input'].append(mw.process_spider_input)
'process_spider_output'):
self.methods['process_spider_output'].insert(0,
mw.process_spider_output)
'process_spider_exception'):
self.methods['process_spider_exception'].insert(0,
mw.process_spider_exception)
'process_start_requests'):
self.methods['process_start_requests'].insert(0,
mw.process_start_requests)
scrape_response(self,
scrape_func,
f:'%s.%s'
six.get_method_self(f).__class__.__name__,
six.get_method_function(f).__name__)
process_spider_input(response):
self.methods['process_spider_input']:
scrape_func(Failure(),
scrape_func(response,
process_spider_exception(_failure):
self.methods['process_spider_exception']:
_failure
process_spider_output(result):
self.methods['process_spider_output']:
mustbe_deferred(process_spider_input,
dfd.addErrback(process_spider_exception)
dfd.addCallback(process_spider_output)
process_start_requests(self,
self._process_chain('process_start_requests',
.middleware
.handlers
randomize_delay):
randomize_delay
self.transferring
self.lastseen
free_transfer_slots(self):
len(self.transferring)
download_delay(self):
self.randomize_delay:
random.uniform(0.5
1.5
self.delay)
self.latercall.active():
self.latercall.cancel()
"%s(concurrency=%r,
delay=%0.2f,
randomize_delay=%r)"
cls_name,
"<downloader.Slot
concurrency=%r
delay=%0.2f
randomize_delay=%r
"len(active)=%d
len(queue)=%d
len(transferring)=%d
lastseen=%s>"
self.randomize_delay,
len(self.active),
len(self.queue),
len(self.transferring),
datetime.fromtimestamp(self.lastseen).isoformat()
_get_concurrency_delay(concurrency,
settings.getfloat('DOWNLOAD_DELAY')
'DOWNLOAD_DELAY'):
warnings.warn("%s.DOWNLOAD_DELAY
%s.download_delay
(type(spider).__name__,
type(spider).__name__))
spider.DOWNLOAD_DELAY
'download_delay'):
'max_concurrent_requests'):
spider.max_concurrent_requests
Downloader(object):
self.slots
self.handlers
self.settings.getint('CONCURRENT_REQUESTS')
self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
self.settings.getint('CONCURRENT_REQUESTS_PER_IP')
self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')
self.middleware
DownloaderMiddlewareManager.from_crawler(crawler)
self._slot_gc_loop
task.LoopingCall(self._slot_gc)
self._slot_gc_loop.start(60)
self.middleware.download(self._enqueue_request,
dfd.addBoth(_deactivate)
len(self.active)
self._get_slot_key(request,
self.slots:
_get_concurrency_delay(conc,
Slot(conc,
_get_slot_key(self,
'download_slot'
self.ip_concurrency:
dnscache.get(key,
_enqueue_request(self,
slot.active.remove(request)
slot.active.add(request)
defer.Deferred().addBoth(_deactivate)
slot.queue.append((request,
_process_queue(self,
slot.latercall.active():
slot.download_delay()
reactor.callLater(penalty,
self._process_queue,
slot.queue
slot.free_transfer_slots()
slot.queue.popleft()
self._download(slot,
dfd.chainDeferred(deferred)
mustbe_deferred(self.handlers.download_request,
_downloaded(response):
self.signals.send_catch_log(signal=signals.response_downloaded,
dfd.addCallback(_downloaded)
slot.transferring.add(request)
finish_transferring(_):
slot.transferring.remove(request)
dfd.addBoth(finish_transferring)
self._slot_gc_loop.stop()
six.itervalues(self.slots):
_slot_gc(self,
age=60):
mintime
list(self.slots.items()):
slot.active
mintime:
self.slots.pop(key).close()
zope.interface.declarations
(optionsForClientTLS,
CertificateOptions,
platformTrust)
BrowserLikePolicyForHTTPS
IPolicyForHTTPS
ScrapyClientTLSOptions
ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):
method=SSL.SSLv23_METHOD,
super(ScrapyClientContextFactory,
self._ssl_method
getCertificateOptions(self):
CertificateOptions(verify=False,
method=getattr(self,
'_ssl_method',
self.getCertificateOptions().getContext()
ScrapyClientTLSOptions(hostname.decode("ascii"),
self.getContext())
BrowserLikeContextFactory(ScrapyClientContextFactory):
optionsForClientTLS(hostname.decode("ascii"),
trustRoot=platformTrust(),
extraCertificateOptions={
self._ssl_method,
ScrapyClientContextFactory(ClientContextFactory):
permissive
bugs."
method=SSL.SSLv23_METHOD):
ctx.set_options(SSL.OP_ALL)
DownloaderMiddlewareManager(MiddlewareManager):
'downloader
build_component_list(
settings.getwithbase('DOWNLOADER_MIDDLEWARES'))
'process_request'):
self.methods['process_request'].append(mw.process_request)
'process_response'):
self.methods['process_response'].insert(0,
mw.process_response)
'process_exception'):
self.methods['process_exception'].insert(0,
mw.process_exception)
download_func,
process_request(request):
self.methods['process_request']:
%s.process_request
defer.returnValue((yield
download_func(request=request,spider=spider)))
process_response(response):
process_response'
self.methods['process_response']:
%s.process_response
process_exception(_failure):
self.methods['process_exception']:
%s.process_exception
defer.returnValue(_failure)
mustbe_deferred(process_request,
deferred.addErrback(process_exception)
deferred.addCallback(process_response)
METHOD_SSLv3
'SSLv3'
METHOD_TLS
METHOD_TLSv10
'TLSv1.0'
METHOD_TLSv11
'TLSv1.1'
METHOD_TLSv12
'TLSv1.2'
METHOD_TLS:
SSL.SSLv23_METHOD,
negotiation
(recommended)
METHOD_SSLv3:
SSL.SSLv3_METHOD,
(NOT
recommended)
METHOD_TLSv10:
SSL.TLSv1_METHOD,
METHOD_TLSv11:
'TLSv1_1_METHOD',
METHOD_TLSv12:
'TLSv1_2_METHOD',
6),
1.2
OpenSSL.SSL
SSL_CB_HANDSHAKE_DONE,
0x10
SSL_CB_HANDSHAKE_DONE
0x20
twisted.internet._sslverify
(ClientTLSOptions,
_maybeSetHostNameIndication,
verifyHostname,
VerificationError)
ScrapyClientTLSOptions(ClientTLSOptions):
_identityVerifyingInfoCallback(self,
where,
SSL_CB_HANDSHAKE_START:
_maybeSetHostNameIndication(connection,
self._hostnameBytes)
SSL_CB_HANDSHAKE_DONE:
verifyHostname(connection,
self._hostnameASCII)
VerificationError
'Remote
"{}";
{}'.format(
verifying
'from
(exception:
{})'.format(
repr(e)))
HTTPClientFactory
HTTPClient
_parsed_url_args(parsed):
to_bytes(s,
b(path)
b(parsed.hostname)
parsed.port
b(parsed.scheme)
b(parsed.netloc)
_parse(url):
ScrapyHTTPPageGetter(HTTPClient):
b'\n'
self.sendCommand(self.factory.method,
self.factory.path)
self.factory.headers.items():
self.sendHeader(key,
self.endHeaders()
self.factory.body
self.transport.write(self.factory.body)
HTTPClient.lineReceived(self,
handleHeader(self,
self.headers.appendlist(key,
handleStatus(self,
self.factory.gotStatus(version,
handleEndHeaders(self):
self.factory.gotHeaders(self.headers)
self._connection_lost_reason
HTTPClient.connectionLost(self,
self.factory.noPage(reason)
handleResponse(self,
self.factory.method.upper()
b'HEAD':
self.factory.page(b'')
self.factory.noPage(self._connection_lost_reason)
self.factory.page(response)
timeout(self):
self.factory.url.startswith(b'https'):
self.transport.stopProducing()
self.factory.noPage(\
defer.TimeoutError("Getting
(self.factory.url,
self.factory.timeout)))
ScrapyHTTPClientFactory(HTTPClientFactory):
ScrapyHTTPPageGetter
noisy
followRedirect
afterFoundGet
to_bytes(self._url,
to_bytes(request.method,
Headers(request.headers)
defer.Deferred().addCallback(self._build_response,
self._disconnectedDeferred
self._set_connection_attributes(request)
self.headers.setdefault('Host',
self.netloc)
len(self.body)
self.headers.setdefault("Connection",
"close")
self.headers_time-self.start_time
int(self.status)
Headers(self.response_headers)
url=self._url)
respcls(url=self._url,
_set_connection_attributes(self,
self.netloc,
gotHeaders(self,
self.headers_time
NotSupported,
DownloadHandlers(object):
self._schemes
acceptable
instancing
self._handlers
instanced
self._notconfigured
remembers
crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))
six.iteritems(handlers):
crawler.signals.connect(self._close,
_get_handler(self,
self._handlers:
self._notconfigured:
self._schemes:
scheme'
dhcls
load_object(path)
dhcls(self._crawler.settings)
logger.error('Loading
"%(clspath)s"
"%(scheme)s"',
{"clspath":
"scheme":
scheme},
self._crawler})
self._get_handler(scheme)
NotSupported("Unsupported
self._notconfigured[scheme]))
handler.download_request(request,
_close(self,
*_a,
**_kw):
self._handlers.values():
hasattr(dh,
dh.close()
defers
FileDownloadHandler(object):
@defers
file_uri_to_path(request.url)
open(filepath,
fo:
fo.read()
responsetypes.from_args(filename=filepath,
CommandFailed
ClientCreator
ReceivedDataProtocol(Protocol):
filename=None):
self.body.write(data)
filename(self):
self.body.close()
self.body.seek(0)
_CODE_RE
re.compile("\d+")
FTPDownloadHandler(object):
CODE_MAPPING
"550":
setting):
urlparse(request.url)
creator
ClientCreator(reactor,
request.meta["ftp_user"],
request.meta["ftp_password"],
passive=request.meta.get("ftp_passive",
creator.connectTCP(parsed_url.hostname,
parsed_url.port
21).addCallback(self.gotClient,
unquote(parsed_url.path))
gotClient(self,
filepath):
self.client
ReceivedDataProtocol(request.meta.get("ftp_local_filename"))
client.retrieveFile(filepath,
protocol)\
.addCallbacks(callback=self._build_response,
protocol),
errback=self._failed,
errbackArgs=(request,))
self.result
responsetypes.from_args(url=request.url)
protocol.close()
protocol.body.read()
{"local
filename":
"size":
protocol.size}
_failed(self,
result.getErrorMessage()
result.type
CommandFailed:
_CODE_RE.search(message)
ftpcode
m.group()
httpcode
self.CODE_MAPPING.get(ftpcode,
self.CODE_MAPPING["default"])
Response(url=request.url,
status=httpcode,
body=message)
result.type(result.value)
.http10
.http11
HttpDownloadHandler(HTTP10DownloadHandler):
warnings.warn('HttpDownloadHandler
scrapy.core.downloader'
'.handlers.http10.HTTP10DownloadHandler
super(HttpDownloadHandler,
HTTP10DownloadHandler(object):
self.HTTPClientFactory
load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])
self.ClientContextFactory
self.HTTPClientFactory(request)
self._connect(factory)
factory.deferred
_connect(self,
to_unicode(factory.host),
factory.port
factory.scheme
self.ClientContextFactory())
TxHeaders
IBodyProducer,
TimeoutError
Agent,
ProxyAgent,
HTTPConnectionPool,
scrapy.core.downloader.webclient
_parse
HTTP11DownloadHandler(object):
persistent=True)
self._pool.maxPersistentPerHost
settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
self._pool._factory.noisy
self._sslMethod
openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]
self._contextFactoryClass
self._contextFactoryClass(method=self._sslMethod)
self._contextFactoryClass()
settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)
warnings.warn(msg)
self._default_maxsize
settings.getint('DOWNLOAD_MAXSIZE')
self._default_warnsize
settings.getint('DOWNLOAD_WARNSIZE')
self._disconnect_timeout
ScrapyAgent(contextFactory=self._contextFactory,
pool=self._pool,
maxsize=getattr(spider,
'download_maxsize',
self._default_maxsize),
warnsize=getattr(spider,
'download_warnsize',
self._default_warnsize))
agent.download_request(request)
self._pool.closeCachedConnections()
delayed_call
reactor.callLater(self._disconnect_timeout,
cancel_delayed_call(result):
delayed_call.active():
delayed_call.cancel()
d.addBoth(cancel_delayed_call)
TunnelError(Exception):
TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):
_responseMatcher
re.compile(b'HTTP/1\..
(?P<status>\d{3})(?P<reason>.{,32})')
self._proxyAuthHeader
bindAddress)
self._tunneledHost
self._tunneledPort
requestTunnel(self,
tunnelReq
tunnel_request_data(self._tunneledHost,
self._tunneledPort,
self._proxyAuthHeader)
protocol.transport.write(tunnelReq)
self.processProxyResponse
self._protocol
processProxyResponse(self,
rcvd_bytes):
self._protocol.dataReceived
TunnelingTCP4ClientEndpoint._responseMatcher.match(rcvd_bytes)
int(respm.group('status'))
self._contextFactory.creatorForNetloc(
self._tunneledHost,
self._tunneledPort)
self._protocol.transport.startTLS(sslOptions,
self._protocolFactory)
self._tunnelReadyDeferred.callback(self._protocol)
respm:
int(respm.group('status')),
respm.group('reason').strip()}
rcvd_bytes[:32]
self._tunnelReadyDeferred.errback(
TunnelError('Could
tunnel
[%r]'
extra)))
connectFailed(self,
self._tunnelReadyDeferred.errback(reason)
self._protocolFactory
protocolFactory
connectDeferred
self).connect(protocolFactory)
connectDeferred.addCallback(self.requestTunnel)
connectDeferred.addErrback(self.connectFailed)
tunnel_request_data(host,
proxy_auth_header=None):
to_bytes(host,
to_bytes(str(port))
b'CONNECT
b'Host:
proxy_auth_header:
b'Proxy-Authorization:
proxy_auth_header
TunnelingAgent(Agent):
connectTimeout,
bindAddress,
uri.host,
uri.port,
self._endpointFactory._connectTimeout,
self._endpointFactory._bindAddress)
self._connectTimeout,
self._bindAddress)
self)._requestWithEndpoint(key,
requestPath)
ScrapyAgent(object):
_Agent
_ProxyAgent
_TunnelingAgent
TunnelingAgent
connectTimeout=10,
pool=None,
maxsize=0,
warnsize=0):
_get_agent(self,
bindaddress
request.meta.get('bindaddress')
_parse(request.url)[0]
proxyHost
to_unicode(proxyHost)
omitConnectTunnel
b'noconnect'
omitConnectTunnel:
(proxyHost,
request.headers.get(b'Proxy-Authorization',
self._TunnelingAgent(reactor,
TCP4ClientEndpoint(reactor,
bindAddress=bindaddress)
self._ProxyAgent(endpoint)
self._Agent(reactor,
self._get_agent(request,
TxHeaders(request.headers)
isinstance(agent,
self._TunnelingAgent):
headers.removeHeader(b'Proxy-Authorization')
request.body:
_RequestBodyProducer(request.body)
headers.addRawHeader(b'Content-Length',
agent.request(
encoding='ascii'),
bodyproducer)
d.addCallback(self._cb_latency,
start_time)
d.addCallback(self._cb_bodyready,
d.addCallback(self._cb_bodydone,
self._timeout_cl
reactor.callLater(timeout,
d.cancel)
d.addBoth(self._cb_timeout,
_cb_timeout(self,
self._timeout_cl.active():
self._timeout_cl.cancel()
self._txresponse:
self._txresponse._transport.stopProducing()
TimeoutError("Getting
timeout))
_cb_latency(self,
start_time):
_cb_bodyready(self,
request.meta.get('download_maxsize',
self._maxsize)
request.meta.get('download_warnsize',
self._warnsize)
maxsize:
("Cancelling
{url}:
({size})
({maxsize})."
).format(url=request.url,
size=expected_size,
maxsize=maxsize)
logger.error(error_message)
defer.CancelledError(error_message)
warnsize:
logger.warning("Expected
(%(size)s)
(%(warnsize)s).",
{'size':
expected_size,
'warnsize':
warnsize})
_cancel(_):
defer.Deferred(_cancel)
txresponse.deliverBody(_ResponseReader(d,
warnsize))
_cb_bodydone(self,
int(txresponse.code)
Headers(txresponse.headers.getAllRawHeaders())
flags=flags)
_RequestBodyProducer(object):
len(body)
consumer.write(self.body)
defer.succeed(None)
_ResponseReader(protocol.Protocol):
warnsize):
self._request
self._bodybuf
bodyBytes):
self._bodybuf.write(bodyBytes)
len(bodyBytes)
self._maxsize:
logger.error("Received
(%(bytes)s)
"max
(%(maxsize)s).",
{'bytes':
self._bytes_received,
'maxsize':
self._maxsize})
self._finished.cancel()
self._reached_warnsize:
logger.warning("Received
"warn
(%(warnsize)s)
%(request)s.",
{'warnsize':
self._warnsize,
self._request})
self._finished.called:
self._bodybuf.getvalue()
['partial']))
self._finished.errback(reason)
.http
_get_boto_connection():
_v19_S3Connection(S3Connection):
_v20_S3Connection(S3Connection):
http_request.authorize(connection=self)
_v19_S3Connection
_v20_S3Connection
S3DownloadHandler(object):
httpdownloadhandler=HTTPDownloadHandler,
kw['anon']
botocore.credentials
kw.pop('anon',
TypeError('Unexpected
arguments:
SignerCls
botocore.auth.AUTH_TYPE_MAPS['s3']
SignerCls(botocore.credentials.Credentials(
aws_secret_access_key))
_get_boto_connection()
_S3Connection(
NotConfigured(str(ex))
self._download_http
httpdownloadhandler(settings).download_request
request.meta.get('is_secure')
p.hostname
'%s://%s.s3.amazonaws.com%s'
request.replace(url=url)
botocore.awsrequest
awsrequest
botocore.awsrequest.AWSRequest(
url='%s://s3.amazonaws.com/%s%s'
path),
headers=request.headers.to_unicode_dict(),
self._signer.add_auth(awsrequest)
request.replace(
headers=awsrequest.headers.items())
signed_headers
self.conn.make_request(
key=unquote(p.path),
query_args=unquote(p.query),
headers=request.headers,
request.replace(url=url,
headers=signed_headers)
self._download_http(request,
AjaxCrawlMiddleware(object):
settings.getbool('AJAXCRAWL_ENABLED'):
self.lookup_bytes
settings.getint('AJAXCRAWL_MAXSIZE',
32768)
'GET':
'ajax_crawlable'
prevent
loops
self._has_ajax_crawlable_variant(response):
request.replace(url=request.url+'#!')
logger.debug("Downloading
AJAX
crawlable
%(ajax_crawl_request)s
{'ajax_crawl_request':
ajax_crawl_request,
ajax_crawl_request.meta['ajax_crawlable']
_has_ajax_crawlable_variant(self,
response.text[:self.lookup_bytes]
_has_ajaxcrawlable_meta(body)
_ajax_crawlable_re
re.compile(six.u(r'<meta\s+name=["\']fragment["\']\s+content=["\']!["\']/?>'))
_has_ajaxcrawlable_meta(text):
'fragment'
'content'
html.remove_tags_with_content(text,
('script',
html.replace_entities(text)
html.remove_comments(text)
_ajax_crawlable_re.search(text)
ChunkedTransferMiddleware(object):
response.headers.get('Transfer-Encoding')
decode_chunked_transfer(response.body)
response.replace(body=body)
CookiesMiddleware(object):
self.jars
defaultdict(CookieJar)
crawler.settings.getbool('COOKIES_ENABLED'):
cls(crawler.settings.getbool('COOKIES_DEBUG'))
self._get_request_cookies(jar,
cookies:
jar.set_cookie_if_ok(cookie,
request.headers.pop('Cookie',
jar.add_cookie_header(request)
self._debug_cookie(request,
jar.extract_cookies(response,
self._debug_set_cookie(response,
_debug_cookie(self,
request.headers.getlist('Cookie')]
"\n".join("Cookie:
"Sending
{}\n{}".format(request,
_debug_set_cookie(self,
response.headers.getlist('Set-Cookie')]
"\n".join("Set-Cookie:
"Received
{}\n{}".format(response,
_format_cookie(self,
'%s=%s'
(cookie['name'],
cookie['value'])
cookie.get('path',
Path=%s'
cookie['path']
cookie.get('domain',
Domain=%s'
cookie['domain']
_get_request_cookies(self,
isinstance(request.cookies,
six.iteritems(request.cookies)]
request.cookies
[self._format_cookie(x)
cookie_list]
cookies}
jar.make_cookies(response,
bz2
zipfile
tarfile
mktemp
DecompressionMiddleware(object):
self._formats
'tar':
self._is_tar,
'zip':
self._is_zip,
'gz':
self._is_gzip,
'bz2':
self._is_bzip2
_is_tar(self,
tar_file
tarfile.open(name=mktemp(),
fileobj=archive)
tarfile.ReadError:
tar_file.extractfile(tar_file.members[0]).read()
responsetypes.from_args(filename=tar_file.members[0].name,
_is_zip(self,
zip_file
zipfile.ZipFile(archive)
zipfile.BadZipfile:
namelist
zip_file.namelist()
zip_file.read(namelist[0])
responsetypes.from_args(filename=namelist[0],
_is_gzip(self,
gzip.GzipFile(fileobj=archive).read()
_is_bzip2(self,
bz2.decompress(response.body)
six.iteritems(self._formats):
func(response)
new_response:
logger.debug('Decompressed
%(responsefmt)s',
{'responsefmt':
fmt},
DefaultHeadersMiddleware(object):
self._headers
without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])
cls(headers.items())
self._headers:
request.headers.setdefault(k,
DownloadTimeoutMiddleware(object):
cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))
'download_timeout',
self._timeout:
request.meta.setdefault('download_timeout',
HttpAuthMiddleware(object):
'http_user',
'http_pass',
pwd:
basic_auth_header(usr,
pwd)
'auth',
b'Authorization'
request.headers[b'Authorization']
HttpCacheMiddleware(object):
DOWNLOAD_EXCEPTIONS
settings.getbool('HTTPCACHE_ENABLED'):
load_object(settings['HTTPCACHE_POLICY'])(settings)
load_object(settings['HTTPCACHE_STORAGE'])(settings)
self.ignore_missing
settings.getbool('HTTPCACHE_IGNORE_MISSING')
cls(crawler.settings,
crawler.stats)
self.storage.open_spider(spider)
self.storage.close_spider(spider)
self.policy.should_cache_request(request):
request.meta['_dont_cache']
uncacheable
self.storage.retrieve_response(spider,
self.stats.inc_value('httpcache/miss',
self.ignore_missing:
self.stats.inc_value('httpcache/ignore',
IgnoreRequest("Ignored
cachedresponse.flags.append('cached')
self.policy.is_cached_response_fresh(cachedresponse,
self.stats.inc_value('httpcache/hit',
request.meta['cached_response']
'_dont_cache'
request.meta.pop('_dont_cache',
response.headers['Date']
formatdate(usegmt=1)
self.stats.inc_value('httpcache/firsthand',
self.policy.is_cached_response_valid(cachedresponse,
self.stats.inc_value('httpcache/revalidate',
self.stats.inc_value('httpcache/invalidate',
self.DOWNLOAD_EXCEPTIONS):
self.stats.inc_value('httpcache/errorrecovery',
_cache_response(self,
self.policy.should_cache_response(response,
self.stats.inc_value('httpcache/store',
self.storage.store_response(spider,
self.stats.inc_value('httpcache/uncacheable',
HttpCompressionMiddleware(object):
crawler.settings.getbool('COMPRESSION_ENABLED'):
request.headers.setdefault('Accept-Encoding',
'gzip,deflate')
response.headers.getlist('Content-Encoding')
content_encoding.pop()
decoded_body
self._decode(response.body,
encoding.lower())
responsetypes.from_args(headers=response.headers,
dict(cls=respcls,
body=decoded_body)
issubclass(respcls,
response.replace(**kwargs)
_decode(self,
b'gzip'
b'x-gzip':
gunzip(body)
b'deflate':
zlib.decompress(body)
zlib.decompress(body,
-15)
six.moves.urllib.request
getproxies,
proxy_bypass
urllib.request
HttpProxyMiddleware(object):
auth_encoding='latin-1'):
self.auth_encoding
self.proxies
getproxies().items():
self.proxies[type]
self._get_proxy(url,
crawler.settings.get('HTTPPROXY_AUTH_ENCODING')
cls(auth_encoding)
_get_proxy(self,
orig_type):
proxy_type,
hostport
_parse_proxy(url)
urlunparse((proxy_type
orig_type,
hostport,
user:
user_pass
to_bytes(
(unquote(user),
unquote(password)),
encoding=self.auth_encoding)
base64.b64encode(user_pass).strip()
parsed.scheme
proxy_bypass(parsed.hostname):
self._set_proxy(request,
_set_proxy(self,
self.proxies[scheme]
request.meta['proxy']
creds:
request.headers['Proxy-Authorization']
get_meta_refresh
BaseRedirectMiddleware(object):
'REDIRECT_ENABLED'
settings.getbool(self.enabled_setting):
self.max_redirect_times
settings.getint('REDIRECT_MAX_TIMES')
settings.getint('REDIRECT_PRIORITY_ADJUST')
_redirect(self,
request.meta.setdefault('redirect_ttl',
self.max_redirect_times)
request.meta.get('redirect_times',
self.max_redirect_times:
redirected.meta['redirect_times']
redirected.meta['redirect_ttl']
redirected.meta['redirect_urls']
request.meta.get('redirect_urls',
[request.url]
redirected.dont_filter
redirected.priority
logger.debug("Redirecting
(%(reason)s)
%(redirected)s
'redirected':
logger.debug("Discarding
reached",
IgnoreRequest("max
reached")
_redirect_request_using_get(self,
redirect_url):
request.replace(url=redirect_url,
redirected.headers.pop('Content-Type',
redirected.headers.pop('Content-Length',
RedirectMiddleware(BaseRedirectMiddleware):
(request.meta.get('dont_redirect',
request.meta.get('handle_httpstatus_list',
request.meta.get('handle_httpstatus_all',
False)):
allowed_status
303,
'Location'
allowed_status:
to_native_str(response.headers['location'].decode('latin1'))
redirected_url
urljoin(request.url,
request.replace(url=redirected_url)
redirected_url)
MetaRefreshMiddleware(BaseRedirectMiddleware):
'METAREFRESH_ENABLED'
super(MetaRefreshMiddleware,
self._maxdelay
settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',
settings.getint('METAREFRESH_MAXDELAY'))
request.meta.get('dont_redirect',
interval,
get_meta_refresh(response)
self._maxdelay:
'meta
refresh')
response_status_message
TunnelError
RetryMiddleware(object):
EXCEPTIONS_TO_RETRY
TunnelError)
settings.getbool('RETRY_ENABLED'):
self.max_retry_times
settings.getint('RETRY_TIMES')
self.retry_http_codes
set(int(x)
settings.getlist('RETRY_HTTP_CODES'))
settings.getint('RETRY_PRIORITY_ADJUST')
self.retry_http_codes:
response_status_message(response.status)
self.EXCEPTIONS_TO_RETRY)
_retry(self,
request.meta.get('retry_times',
self.max_retry_times:
logger.debug("Retrying
request.copy()
retryreq.meta['retry_times']
retryreq.dont_filter
retryreq.priority
logger.debug("Gave
retrying
RobotsTxtMiddleware(object):
DOWNLOAD_PRIORITY
crawler.settings.getbool('ROBOTSTXT_OBEY'):
self._useragent
crawler.settings.get('USER_AGENT')
self._parsers
request.meta.get('dont_obey_robotstxt'):
maybeDeferred(self.robot_parser,
d.addCallback(self.process_request_2,
process_request_2(self,
rp.can_fetch(self._useragent,
request.url):
logger.debug("Forbidden
robots.txt:
IgnoreRequest()
robot_parser(self,
url.netloc
self._parsers:
robotsurl
"%s://%s/robots.txt"
(url.scheme,
url.netloc)
robotsreq
robotsurl,
priority=self.DOWNLOAD_PRIORITY,
meta={'dont_obey_robotstxt':
self.crawler.engine.download(robotsreq,
dfd.addCallback(self._parse_robots,
dfd.addErrback(self._logerror,
robotsreq,
dfd.addErrback(self._robots_error,
isinstance(self._parsers[netloc],
cb(result):
d.callback(result)
self._parsers[netloc].addCallback(cb)
_logerror(self,
failure.type
%(f_exception)s",
'f_exception':
_parse_robots(self,
robotparser.RobotFileParser(response.url)
'text'):
effort
response.body.decode('utf-8')
rp.parse(body.splitlines())
rp_dfd.callback(rp)
_robots_error(self,
rp_dfd.callback(None)
response_httprepr
DownloaderStats(object):
crawler.settings.getbool('DOWNLOADER_STATS'):
self.stats.inc_value('downloader/request_count',
self.stats.inc_value('downloader/request_method_count/%s'
reqlen
len(request_httprepr(request))
self.stats.inc_value('downloader/request_bytes',
reqlen,
self.stats.inc_value('downloader/response_count',
self.stats.inc_value('downloader/response_status_count/%s'
reslen
len(response_httprepr(response))
self.stats.inc_value('downloader/response_bytes',
reslen,
ex_class
(exception.__class__.__module__,
exception.__class__.__name__)
self.stats.inc_value('downloader/exception_count',
self.stats.inc_value('downloader/exception_type_count/%s'
ex_class,
UserAgentMiddleware(object):
user_agent='Scrapy'):
user_agent
cls(crawler.settings['USER_AGENT'])
'user_agent',
self.user_agent:
request.headers.setdefault(b'User-Agent',
CloseSpider(object):
self.close_on
'timeout':
crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),
crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),
'pagecount':
crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),
'errorcount':
crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),
any(self.close_on.values()):
self.counter
defaultdict(int)
self.close_on.get('errorcount'):
crawler.signals.connect(self.error_count,
signal=signals.spider_error)
self.close_on.get('pagecount'):
crawler.signals.connect(self.page_count,
self.close_on.get('timeout'):
crawler.signals.connect(self.spider_opened,
self.close_on.get('itemcount'):
crawler.signals.connect(self.item_scraped,
crawler.signals.connect(self.spider_closed,
error_count(self,
self.close_on['errorcount']:
page_count(self,
self.close_on['pagecount']:
reactor.callLater(self.close_on['timeout'],
self.crawler.engine.close_spider,
reason='closespider_timeout')
self.close_on['itemcount']:
'task',
task.active():
task.cancel()
CoreStats(object):
crawler.signals.connect(o.item_dropped,
signal=signals.item_dropped)
crawler.signals.connect(o.response_received,
self.stats.set_value('start_time',
self.stats.set_value('finish_time',
self.stats.set_value('finish_reason',
self.stats.inc_value('item_scraped_count',
response_received(self,
self.stats.inc_value('response_received_count',
item_dropped(self,
exception.__class__.__name__
self.stats.inc_value('item_dropped_count',
self.stats.inc_value('item_dropped_reasons_count/%s'
Pdb
format_engine_status
format_live_refs
StackTraceDump(object):
signal.signal(signal.SIGQUIT,
dump_stacktrace(self,
'stackdumps':
self._thread_stacks(),
'enginestatus':
format_engine_status(self.crawler.engine),
'liverefs':
format_live_refs(),
trace
status\n"
"%(enginestatus)s\n%(liverefs)s\n%(stackdumps)s",
_thread_stacks(self):
id2name
dict((th.ident,
th.name)
threading.enumerate())
sys._current_frames().items():
id2name.get(id_,
dump
''.join(traceback.format_stack(frame))
Thread:
{0}({1})\n{2}\n".format(name,
dump)
Debugger(object):
self._enter_debugger)
_enter_debugger(self,
Pdb().set_trace(frame.f_back)
NamedTemporaryFile
scrapy.utils.ftp
ftp_makedirs_cwd
IFeedStorage(Interface):
__init__(uri):
open(spider):
store(file):
BlockingFeedStorage(object):
spider.crawler.settings['FEED_TEMPDIR']
OSError('Not
Directory:
str(path))
NamedTemporaryFile(prefix='feed-',
dir=path)
threads.deferToThread(self._store_in_thread,
StdoutFeedStorage(object):
_stdout=None):
_stdout:
FileFeedStorage(object):
file_uri_to_path(uri)
os.path.dirname(self.path)
open(self.path,
'ab')
S3FeedStorage(BlockingFeedStorage):
scrapy.conf
self.bucketname
self.keyname
u.path[1:]
aws_access_key_id=self.access_key,
aws_secret_access_key=self.secret_key)
self.connect_s3
boto.connect_s3
self.s3_client.put_object(
Bucket=self.bucketname,
Key=self.keyname,
Body=file)
self.connect_s3(self.access_key,
self.secret_key)
conn.get_bucket(self.bucketname,
bucket.new_key(self.keyname)
key.set_contents_from_file(file)
FTPFeedStorage(BlockingFeedStorage):
int(u.port
'21')
FTP()
ftp.connect(self.host,
ftp.login(self.username,
posixpath.split(self.path)
dirname)
ftp.storbinary('STOR
ftp.quit()
SpiderSlot(object):
self.exporter
self.itemcount
FeedExporter(object):
settings['FEED_URI']
self.urifmt:
self.format
settings['FEED_FORMAT'].lower()
self.export_encoding
settings['FEED_EXPORT_ENCODING']
self.storages
self._load_components('FEED_STORAGES')
self.exporters
self._load_components('FEED_EXPORTERS')
self._storage_supported(self.urifmt):
self._exporter_supported(self.format):
self.store_empty
settings.getbool('FEED_STORE_EMPTY')
self.export_fields
settings.getlist('FEED_EXPORT_FIELDS')
settings['FEED_URI_PARAMS']
self._uripar
load_object(uripar)
crawler.signals.connect(o.open_spider,
signals.spider_opened)
crawler.signals.connect(o.close_spider,
self._get_uri_params(spider)
self._get_exporter(file,
fields_to_export=self.export_fields,
encoding=self.export_encoding)
exporter.start_exporting()
SpiderSlot(file,
self.store_empty:
slot.exporter.finish_exporting()
logfmt
%%(format)s
(%%(itemcount)d
%%(uri)s"
self.format,
slot.itemcount,
'uri':
slot.uri}
defer.maybeDeferred(slot.storage.store,
slot.file)
logger.info(logfmt
"Stored",
logger.error(logfmt
storing",
slot.exporter.export_item(item)
_load_components(self,
setting_prefix):
without_none_values(self.settings.getwithbase(setting_prefix))
conf.items():
load_object(v)
_exporter_supported(self,
format):
self.exporters:
%(format)s",
format})
_storage_supported(self,
self.storages:
logger.error("Disabled
self.exporters[self.format](*args,
_get_storage(self,
self.storages[urlparse(uri).scheme](uri)
_get_uri_params(self,
dir(spider):
datetime.utcnow().replace(microsecond=0).isoformat().replace(':',
params['time']
self._uripar(params,
WeakKeyDictionary
mktime_tz,
parsedate_tz
headers_raw_to_dict,
data_path
DummyPolicy(object):
settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]
RFC2616Policy(object):
MAXAGE
365
year
self.always_store
settings.getbool('HTTPCACHE_ALWAYS_STORE')
self.ignore_response_cache_controls
[to_bytes(cc)
settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]
self._cc_parsed
WeakKeyDictionary()
_parse_cachecontrol(self,
r):
self._cc_parsed:
cch
r.headers.get(b'Cache-Control',
parse_cachecontrol(cch)
self.ignore_response_cache_controls:
parsed.pop(key,
self.ignore_schemes:
self.always_store:
b'max-age'
401):
ccreq:
self._compute_freshness_lifetime(cachedresponse,
self._compute_current_age(cachedresponse,
self._get_max_age(ccreq)
min(freshnesslifetime,
reqmaxage)
freshnesslifetime:
b'max-stale'
ccreq[b'max-stale']
int(staleage)):
self._set_conditional_validators(request,
500:
_set_conditional_validators(self,
request.headers[b'If-Modified-Since']
cachedresponse.headers[b'Last-Modified']
request.headers[b'If-None-Match']
cachedresponse.headers[b'ETag']
_get_max_age(self,
cc):
int(cc[b'max-age']))
(KeyError,
_compute_freshness_lifetime(self,
self._get_max_age(cc)
rfc1123_to_epoch(response.headers[b'Expires'])
rfc1123_to_epoch(response.headers.get(b'Last-Modified'))
(date
lastmodified)
self.MAXAGE
_compute_current_age(self,
b'Age'
int(response.headers[b'Age'])
max(currentage,
age)
DbmCacheStorage(object):
self.dbmodule
import_module(settings['HTTPCACHE_DBM_MODULE'])
'%s.db'
self.dbmodule.open(dbpath,
self.db.close()
self.db['%s_data'
self.db['%s_time'
str(time())
'%s_time'
db[tkey]
pickle.loads(db['%s_data'
key])
FilesystemCacheStorage(object):
data_path(settings['HTTPCACHE_DIR'])
settings.getbool('HTTPCACHE_GZIP')
self._open
gzip.open
self._read_meta(spider,
rawheaders
metadata.get('response_url')
metadata['status']
Headers(headers_raw_to_dict(rawheaders))
os.path.exists(rpath):
os.makedirs(rpath)
'response_url':
time(),
'meta'),
f.write(to_bytes(repr(metadata)))
'pickled_meta'),
pickle.dump(metadata,
f.write(headers_dict_to_raw(response.headers))
f.write(response.body)
'request_headers'),
f.write(headers_dict_to_raw(request.headers))
'request_body'),
f.write(request.body)
_get_request_path(self,
key[0:2],
_read_meta(self,
metapath
os.path.join(rpath,
'pickled_meta')
os.path.exists(metapath):
os.stat(metapath).st_mtime
mtime:
self._open(metapath,
LeveldbCacheStorage(object):
self._leveldb
'%s.leveldb'
self._leveldb.LevelDB(dbpath)
self.db.CompactRange()
self._leveldb.WriteBatch()
b'_data',
protocol=2))
b'_time',
to_bytes(str(time())))
self.db.Write(batch)
b'_time')
b'_data')
pickle.loads(data)
to_bytes(request_fingerprint(request))
parse_cachecontrol(header):
directive
header.split(b','):
sep,
directive.strip().partition(b'=')
directives[key.lower()]
rfc1123_to_epoch(date_str):
date_str
to_unicode(date_str,
mktime_tz(parsedate_tz(date_str))
LogStats(object):
interval=60.0):
crawler.settings.getfloat('LOGSTATS_INTERVAL')
interval:
interval)
self.pagesprev
task.LoopingCall(self.log,
self.task.start(self.interval)
self.stats.get_value('item_scraped_count',
self.stats.get_value('response_received_count',
irate
(items
self.itemsprev)
prate
(pages
self.pagesprev)
self.pagesprev,
("Crawled
%(pages)d
%(pagerate)d
pages/min),
"scraped
%(items)d
%(itemrate)d
items/min)")
{'pages':
'pagerate':
prate,
'itemrate':
irate}
logger.info(msg,
self.task.running:
self.task.stop()
MemoryDebugger(object):
crawler.settings.getbool('MEMDEBUG_ENABLED'):
self.stats.set_value('memdebug/gc_garbage_count',
len(gc.garbage),
self.stats.set_value('memdebug/live_refs/%s'
MemoryUsage(object):
crawler.settings.getbool('MEMUSAGE_ENABLED'):
import_module('resource')
self.notify_mails
crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')
crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024
self.warning
crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024
self.report
crawler.settings.getbool('MEMUSAGE_REPORT')
self.check_interval
crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')
crawler.signals.connect(self.engine_started,
signal=signals.engine_started)
crawler.signals.connect(self.engine_stopped,
signal=signals.engine_stopped)
get_virtual_size(self):
self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss
engine_started(self):
self.crawler.stats.set_value('memusage/startup',
self.tasks
task.LoopingCall(self.update)
task.LoopingCall(self._check_limit)
task.LoopingCall(self._check_warning)
engine_stopped(self):
self.tasks:
tsk.running:
tsk.stop()
self.crawler.stats.max_value('memusage/max',
_check_limit(self):
self.crawler.stats.set_value('memusage/limit_reached',
self.limit/1024/1024
logger.error("Memory
%(memusage)dM.
Scrapy...",
terminated:
self.crawler.stats.set_value('memusage/limit_notified',
open_spiders
self.crawler.engine.open_spiders
'memusage_exceeded')
self.crawler.stop()
_check_warning(self):
self.warned:
self.crawler.stats.set_value('memusage/warning_reached',
self.warning/1024/1024
logger.warning("Memory
%(memusage)dM",
warning:
self.crawler.stats.set_value('memusage/warning_notified',
_send_report(self,
rcpts,
self.crawler.stats
"Memory
(stats.get_value('memusage/startup')/1024/1024)
(stats.get_value('memusage/max')/1024/1024)
"Current
(self.get_virtual_size()/1024/1024)
"ENGINE
-------------------------------------------------------
\r\n"
pformat(get_engine_status(self.crawler.engine))
self.mail.send(rcpts,
SpiderState(object):
jobdir=None):
job_dir(crawler.settings)
cls(jobdir)
crawler.signals.connect(obj.spider_closed,
crawler.signals.connect(obj.spider_opened,
self.jobdir:
pickle.dump(spider.state,
os.path.exists(self.statefn):
statefn(self):
os.path.join(self.jobdir,
'spider.state')
StatsMailer(object):
mail):
self.recipients
crawler.settings.getlist("STATSMAILER_RCPTS")
recipients:
mail)
spider_stats
self.stats.get_stats(spider)
self.stats.get_stats().items())
"\n\n%s
spider_stats.items())
self.mail.send(self.recipients,
twisted.conch
manhole,
twisted.conch.insults
insults
print_live_refs
print_engine_status
listen_tcp
guppy
guppy.hpy()
update_telnet_vars
TelnetConsole(protocol.ServerFactory):
crawler.settings.getbool('TELNETCONSOLE_ENABLED'):
TWISTED_CONCH_AVAILABLE:
self.noisy
self.portrange
crawler.settings.getlist('TELNETCONSOLE_PORT')]
crawler.settings['TELNETCONSOLE_HOST']
self.crawler.signals.connect(self.start_listening,
signals.engine_started)
self.crawler.signals.connect(self.stop_listening,
start_listening(self):
listen_tcp(self.portrange,
self.port.getHost()
logger.debug("Telnet
%(host)s:%(port)d",
{'host':
h.host,
h.port},
stop_listening(self):
protocol(self):
self._get_telnet_vars()
telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,
insults.ServerProtocol,
manhole.Manhole,
telnet_vars)
_get_telnet_vars(self):
'engine':
self.crawler.engine,
self.crawler.engine.spider,
self.crawler.engine.slot,
'crawler':
self.crawler,
'extensions':
self.crawler.extensions,
'stats':
self.crawler.stats,
'settings':
self.crawler.settings,
'est':
print_engine_status(self.crawler.engine),
'p':
pprint.pprint,
'prefs':
print_live_refs,
'hpy':
hpy,
'help':
console.
see:
"http://doc.scrapy.org/en/latest/topics/telnetconsole.html",
self.crawler.signals.send_catch_log(update_telnet_vars,
telnet_vars=telnet_vars)
AutoThrottle(object):
crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):
crawler.settings.getbool("AUTOTHROTTLE_DEBUG")
crawler.settings.getfloat("AUTOTHROTTLE_TARGET_CONCURRENCY")
crawler.signals.connect(self._spider_opened,
crawler.signals.connect(self._response_downloaded,
signal=signals.response_downloaded)
_spider_opened(self,
self.mindelay
self._min_delay(spider)
self.maxdelay
self._max_delay(spider)
self._start_delay(spider)
_min_delay(self,
'download_delay',
s.getfloat('DOWNLOAD_DELAY'))
_max_delay(self,
self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')
_start_delay(self,
max(self.mindelay,
self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))
request.meta.get('download_latency')
self._adjust_delay(slot,
len(response.body)
len(slot.transferring)
logger.info(
"slot:
%(slot)s
conc:%(concurrency)2d
"delay:%(delay)5d
(%(delaydiff)+d)
"latency:%(latency)5d
size:%(size)6d
bytes",
'concurrency':
'delay':
'delaydiff':
'latency':
request.meta.get('download_slot')
self.crawler.engine.downloader.slots.get(key)
_adjust_delay(self,
target_delay
(slot.delay
target_delay)
max(target_delay,
new_delay)
min(max(self.mindelay,
new_delay),
self.maxdelay)
slot.delay:
scrapy.http.request.form
scrapy.http.request.rpc
scrapy.http.response.html
scrapy.http.response.xml
obsolete_setter(setter,
attrname):
newsetter(self,
"%s.%s
modifiable,
%s.replace()
(c,
attrname,
newsetter
six.moves.http_cookiejar
_CookieJar,
DefaultCookiePolicy,
IPV4_RE
CookieJar(object):
check_expired_frequency=10000):
DefaultCookiePolicy()
self.jar
_CookieJar(self.policy)
self.jar._cookies_lock
_DummyLock()
check_expired_frequency
extract_cookies(self,
self.jar.extract_cookies(wrsp,
add_cookie_header(self,
self.policy._now
self.jar._now
req_host
IPV4_RE.search(req_host):
potential_domain_matches(req_host)
[req_host
".local"]
[req_host]
self.jar._cookies:
self.jar._cookies_for_domain(host,
self.jar._cookie_attrs(cookies)
wreq.has_header("Cookie"):
wreq.add_unredirected_header("Cookie",
";
".join(attrs))
self.jar.clear_expired_cookies()
_cookies(self):
self.jar._cookies
clear_session_cookies(self,
self.jar.clear_session_cookies(*args,
clear(self):
self.jar.clear()
iter(self.jar)
len(self.jar)
pol):
self.jar.set_policy(pol)
make_cookies(self,
self.jar.make_cookies(wrsp,
set_cookie(self,
self.jar.set_cookie(cookie)
set_cookie_if_ok(self,
cookie,
self.jar.set_cookie_if_ok(cookie,
WrappedRequest(request))
potential_domain_matches(domain):
[domain]
domain.index('.')
domain.rindex('.')
end:
matches.append(domain[start:])
domain.index('.',
['.'
matches]
_DummyLock(object):
acquire(self):
release(self):
WrappedRequest(object):
self.request.url
urlparse_cached(self.request).netloc
urlparse_cached(self.request).scheme
self.request.meta.get('is_unverifiable',
get_origin_req_host(self):
urlparse_cached(self.request).hostname
full_url(self):
self.get_full_url()
host(self):
self.get_host()
type(self):
self.get_type()
unverifiable(self):
self.is_unverifiable()
origin_req_host(self):
self.get_origin_req_host()
self.request.headers
to_native_str(self.request.headers.get(name,
default),
header_items(self):
(to_native_str(k,
[to_native_str(x,
v])
self.request.headers.items()
self.request.headers.appendlist(name,
WrappedResponse(object):
get_all(self,
[to_native_str(v,
self.response.headers.getlist(name)]
getheaders
get_all
Headers(CaselessDict):
seq=None,
self).__init__(seq)
self._tobytes(key.title())
[self._tobytes(x)
_tobytes(self,
x.encode(self.encoding)
six.text_type(x).encode(self.encoding)
TypeError('Unsupported
{}'.format(type(x)))
self).__getitem__(key)[-1]
self).get(key,
def_val)[-1]
def_val
self.normvalue(def_val)
self.setdefault(key,
lst.extend(self.normvalue(value))
list(self.iteritems())
((k,
self.getlist(k))
self.keys())
[self[k]
to_string(self):
headers_dict_to_raw(self)
to_unicode_dict(self):
CaselessDict(
(to_unicode(key,
encoding=self.encoding),
to_unicode(b','.join(value),
encoding=self.encoding))
self.items())
escape_ajax
Request(object_ref):
cookies=None,
priority=0,
dont_filter=False,
str(method).upper()
int),
"Request
integer:
"Cannot
callback"
self.errback
self.cookies
self.dont_filter
dict(meta)
TypeError('Request
type(url).__name__)
safe_url_string(url,
escape_ajax(s)
self._url:
ValueError('Missing
self._url)
to_bytes(body,
'cookies',
'meta',
'priority',
'dont_filter',
'callback',
'errback']:
urljoin,
lxml.html
parsel.selector
create_root_node
FormRequest(Request):
kwargs.pop('formdata',
kwargs.get('method')
kwargs['method']
super(FormRequest,
formdata:
formdata.items()
isinstance(formdata,
querystr
_urlencode(items,
self.headers.setdefault(b'Content-Type',
self._set_body(querystr)
self._set_url(self.url
('&'
'?')
querystr)
from_response(cls,
formname=None,
formid=None,
formnumber=0,
formdata=None,
clickdata=None,
dont_click=False,
formxpath=None,
formcss=None,
formcss
HTMLTranslator().css_to_xpath(formcss)
formxpath)
kwargs.pop('url',
kwargs.pop('method',
form.method)
cls(url=url,
formdata=formdata,
form.action)
_urlencode(seq,
enc):
[(to_bytes(k,
enc),
to_bytes(v,
enc))
(vs
is_listlike(vs)
[vs])]
urlencode(values,
formxpath):
create_root_node(response.text,
lxml.html.HTMLParser,
base_url=get_base_url(response))
forms
root.xpath('//form')
forms:
formname
root.xpath('//form[@name="%s"]'
formname)
formid
root.xpath('//form[@id="%s"]'
formid)
root.xpath(formxpath)
nodes[0]
'form':
el.getparent()
formxpath.encode('unicode_escape')
formnumber
forms[formnumber]
IndexError("Form
(formnumber,
dict(formdata
(ValueError,
TypeError):
ValueError('formdata
tuples')
form.xpath('descendant::textarea'
'|descendant::select'
'|descendant::input[not(@type)
@type['
"^(?:submit|image|reset)$",
"i"))'
(../@checked
or'
"^(?:checkbox|radio)$",
"i")))]]',
namespaces={
"re":
(_value(e)
inputs)
formdata]
dont_click:
form)
values.append(clickable)
values.extend(formdata.items())
_value(ele):
ele.name
ele.value
ele.tag
ele.multiple
ele.value_options
o[0])
selected_options
ele.xpath('.//option[@selected]')
[(o.get('value')
o.text
u'').strip()
selected_options]
form):
clickables
form.xpath(
'descendant::*[(self::input
self::button)'
re:test(@type,
"^submit$",
"i")]'
'|descendant::button[not(@type)]',
namespaces={"re":
clickables:
clickdata
clickables[0]
clickdata.get('nr',
list(form.inputs)[nr]
u'.//*'
u''.join(u'[@%s="%s"]'
six.iteritems(clickdata))
form.xpath(xpath)
(el[0].get('name'),
el[0].get('value')
ValueError("Multiple
(%r)
criteria
clickdata))
(clickdata,))
get_func_args(xmlrpclib.dumps)
XmlRpcRequest(Request):
'body'
'params'
kwargs.pop(k))
kwargs['body']
xmlrpclib.dumps(**kw)
kwargs.setdefault('method',
kwargs.setdefault('dont_filter',
super(XmlRpcRequest,
self.headers.setdefault('Content-Type',
'text/xml')
Response(object_ref):
flags=None,
int(status)
self.flags
list(flags)
self.request.meta
"Response.meta
available,
"is
tied
type(url).__name__))
HtmlResponse.")
"<%d
(self.status,
'flags']:
urljoin(self.url,
HtmlResponse(TextResponse):
html_to_unicode,
resolve_encoding,
html_body_declared_encoding,
http_content_type_encoding
TextResponse(Response):
_DEFAULT_ENCODING
TypeError("Cannot
"has
encoding"
self)._set_url(url)
detection
TypeError('Cannot
encoding'
body.encode(self._encoding)
self)._set_body(body)
Response.replace(self,
self._declared_encoding()
self._body_inferred_encoding()
_declared_encoding(self):
self._headers_encoding()
self._body_declared_encoding()
body_as_unicode(self):
text(self):
'charset=%s'
html_to_unicode(charset,
self.body)[1]
urljoin(get_base_url(self),
_headers_encoding(self):
self.headers.get(b'Content-Type',
http_content_type_encoding(to_native_str(content_type))
_body_inferred_encoding(self):
to_native_str(self.headers.get(b'Content-Type',
benc,
html_to_unicode(content_type,
self.body,
auto_detect_fun=self._auto_detect_fun,
default_encoding=self._DEFAULT_ENCODING)
_auto_detect_fun(self,
(self._DEFAULT_ENCODING,
'cp1252'):
text.decode(enc)
resolve_encoding(enc)
_body_declared_encoding(self):
html_body_declared_encoding(self.body)
selector(self):
Selector(self)
xpath(self,
self.selector.xpath(query)
css(self,
self.selector.css(query)
XmlResponse(TextResponse):
url_is_from_any_domain,
url_has_any_extension,
'mng',
'pct',
'bmp',
'gif',
'jpg',
'jpeg',
'png',
'pst',
'psp',
'tif',
'tiff',
'ai',
'drw',
'dxf',
'eps',
'ps',
'svg',
'mp3',
'wma',
'ogg',
'wav',
'ra',
'aac',
'mid',
'au',
'aiff',
'3gp',
'asf',
'asx',
'avi',
'mov',
'mp4',
'mpg',
'qt',
'rm',
'swf',
'wmv',
'm4a',
'xls',
'xlsx',
'ppt',
'pptx',
'pps',
'docx',
'odt',
'ods',
'odg',
'odp',
'css',
'pdf',
'exe',
'bin',
'rss',
'zip',
'rar',
_re_type
type(re.compile("",
_matches
regexs:
any((r.search(url)
regexs))
_is_valid_url
url.split('://',
{'http',
'file'}
FilteringLinkExtractor(object):
_csstranslator
HTMLTranslator()
allow,
deny,
allow_domains,
deny_domains,
restrict_xpaths,
canonicalize,
deny_extensions,
restrict_css):
arg_to_iter(allow)]
arg_to_iter(deny)]
set(arg_to_iter(allow_domains))
set(arg_to_iter(deny_domains))
tuple(arg_to_iter(restrict_xpaths))
tuple(map(self._csstranslator.css_to_xpath,
arg_to_iter(restrict_css)))
self.canonicalize
canonicalize
{'.'
arg_to_iter(deny_extensions)}
_link_allowed(self,
link):
_is_valid_url(link.url):
self.allow_res):
self.deny_res):
urlparse(link.url)
url_has_any_extension(parsed_url,
self.deny_extensions):
self.allow_res]
[True]
self.deny_res]
any(allowed)
any(denied)
self._link_allowed(x)]
self.canonicalize:
canonicalize_url(urlparse(link.url))
self.link_extractor._process_links(links)
self.link_extractor._extract_links(*args,
.lxmlhtml
six.moves.html_parser
HTMLParser
unique_list
HtmlParserLinkExtractor(HTMLParser):
HTMLParser.__init__(self)
"HtmlParserLinkExtractor
"future
response_encoding):
unique_list(self.links,
link.text.decode(response_encoding)
HTMLParser.reset(self)
handle_starttag(self,
self.process_attr(value)
Link(url=url)
handle_endtag(self,
XHTML_NAMESPACE
"http://www.w3.org/1999/xhtml"
_collect_string_content
etree.XPath("string()")
_nons(tag):
isinstance(tag,
tag[0]
'{'
tag[1:len(XHTML_NAMESPACE)+1]
XHTML_NAMESPACE:
tag.split('}')[-1]
LxmlParserLinkExtractor(object):
_iter_links(self,
document):
document.iter(etree.Element):
self.scan_tag(_nons(el.tag)):
attribs
el.attrib
attrib
attribs:
self.scan_attr(attrib):
attrib,
attribs[attrib])
selector,
base_url):
el,
self._iter_links(selector.root):
attr_val)
bogus
self.process_attr(attr_val)
encoding=response_encoding)
Link(url,
_collect_string_content(el)
nofollow=rel_has_nofollow(el.get('rel')))
links.append(link)
self._extract_links(response.selector,
_deduplicate_if_needed(self,
self.unique:
LxmlLinkExtractor(FilteringLinkExtractor):
LxmlParserLinkExtractor(tag=tag_func,
process=process_value)
super(LxmlLinkExtractor,
[subdoc
subdoc
response.xpath(x)]
[response.selector]
all_links
docs:
self._extract_links(doc,
all_links.extend(self._process_links(links))
unique_list(all_links)
remove_tags,
replace_entities,
replace_escape_chars,
.sgml
linkre
"<a\s.*?href=(\"[.#]+?\"|\'[.#]+?\'|[^\s]+?)(>|\s.*?>)(.*?)<[/
]?a>",
re.DOTALL
re.IGNORECASE)
clean_link(link_text):
link_text.strip("\t\r\n
'\"")
RegexLinkExtractor(SgmlLinkExtractor):
clean_text(text):
replace_escape_chars(remove_tags(text.decode(response_encoding))).strip()
clean_url(url):
replace_entities(clean_link(url.decode(response_encoding))))
get_base_url(response_text,
links_text
linkre.findall(response_text)
[Link(clean_url(url).encode(response_encoding),
clean_text(text))
links_text]
sgmllib
SGMLParser
BaseSgmlLinkExtractor(SGMLParser):
process_value=None):
"BaseSgmlLinkExtractor
SGMLParser.__init__(self)
self.process_value
self.links:
to_unicode(link.text,
errors='replace').strip()
SGMLParser.reset(self)
unknown_starttag(self,
self.process_value(value)
Link(url=url,
nofollow=rel_has_nofollow(dict(attrs).get('rel')))
unknown_endtag(self,
SgmlLinkExtractor(FilteringLinkExtractor):
"SgmlLinkExtractor
BaseSgmlLinkExtractor(tag=tag_func,
process_value=process_value)
super(SgmlLinkExtractor,
u''.join(f
response.xpath(x).extract()
).encode(response.encoding,
errors='xmlcharrefreplace')
self._extract_links(body,
extract_regex
flatten
.processors
Identity
ItemLoader(object):
default_selector_class
item=None,
selector=None,
self.default_selector_class(response)
context.update(selector=selector,
self.default_item_class()
self.context
context['item']
_values(self):
self.parent._values
item(self):
self.parent.item
nested_xpath(self,
self.selector.xpath(xpath)
nested_css(self,
self.selector.css(css)
self._add_value(k,
replace_value(self,
self._replace_value(k,
self._replace_value(field_name,
_add_value(self,
processed_value
self._process_input_value(field_name,
processed_value:
arg_to_iter(processed_value)
_replace_value(self,
self._values.pop(field_name,
kw.get('re',
regex:
flatten(extract_regex(regex,
processors:
load_item(self):
self.item
tuple(self._values):
self.get_output_value(field_name)
item[field_name]
get_output_value(self,
self.get_output_processor(field_name)
proc(self._values[field_name])
processor:
field=%r
value=%r
error='%s:
%s'"
(field_name,
self._values[field_name],
type(e).__name__,
str(e)))
get_collected_values(self,
get_input_processor(self,
'%s_in'
'input_processor',
self.default_input_processor)
get_output_processor(self,
'%s_out'
'output_processor',
self.default_output_processor)
_process_input_value(self,
self.get_input_processor(field_name)
_get_item_field_attr(self,
isinstance(self.item,
self.item.fields[field_name].get(key,
_check_selector_method(self):
RuntimeError("To
XPath
CSS
selectors,
instantiated
response"
add_xpath(self,
replace_xpath(self,
get_xpath(self,
@deprecated(use_instead='._get_xpathvalues()')
_get_values(self,
self._get_xpathvalues(xpaths,
_get_xpathvalues(self,
xpaths
arg_to_iter(xpaths)
flatten(self.selector.xpath(xpath).extract()
xpaths)
add_css(self,
replace_css(self,
get_css(self,
_get_cssvalues(self,
csss,
csss
arg_to_iter(csss)
flatten(self.selector.css(css).extract()
css
csss)
XPathItemLoader
create_deprecated_class('XPathItemLoader',
ItemLoader)
wrap_loader_context(function,
'loader_context'
get_func_args(function):
partial(function,
loader_context=context)
MergeDict
MapCompose(object):
arg_to_iter(func(v))
Compose(object):
self.stop_on_none
default_loader_context.get('stop_on_none',
self.stop_on_none:
func(value)
TakeFirst(object):
Identity(object):
SelectJmes(object):
json_path):
self.json_path
json_path
jmespath
self.compiled_path
jmespath.compile(self.json_path)
self.compiled_path.search(value)
Join(object):
separator=u'
self.separator
separator
self.separator.join(values)
ItemPipelineManager(MiddlewareManager):
'item
pipeline'
build_component_list(settings.getwithbase('ITEM_PIPELINES'))
pipe):
super(ItemPipelineManager,
self)._add_middleware(pipe)
hasattr(pipe,
'process_item'):
self.methods['process_item'].append(pipe.process_item)
self._process_chain('process_item',
parsedate_tz,
mktime_tz
FileException(Exception):
FSFilesStore(object):
basedir):
basedir:
basedir.split('://',
self.basedir
self._mkdir(self.basedir)
self.created_directories
defaultdict(set)
self._mkdir(os.path.dirname(absolute_path),
f.write(buf.getvalue())
os.path.getmtime(absolute_path)
FIXME:
catching
everything!
md5sum(f)
{'last_modified':
last_modified,
_get_filesystem_path(self,
path_comps
os.path.join(self.basedir,
*path_comps)
_mkdir(self,
self.created_directories[domain]
seen.add(dirname)
S3FilesStore(object):
POLICY
Overriden
settings.FILES_STORE_S3_ACL
'max-age=172800',
aws_access_key_id=self.AWS_ACCESS_KEY_ID,
aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)
self.S3Connection
uri.startswith('s3://')
self.bucket,
uri[5:].split('/',
_onsuccess(boto_key):
boto_key['ETag'].strip('"')
boto_key['LastModified']
time.mktime(last_modified.timetuple())
boto_key.etag.strip('"')
boto_key.last_modified
parsedate_tz(last_modified)
int(mktime_tz(modified_tuple))
{'checksum':
checksum,
modified_stamp}
self._get_boto_key(path).addCallback(_onsuccess)
_get_boto_bucket(self):
self.S3Connection(self.AWS_ACCESS_KEY_ID,
c.get_bucket(self.bucket,
_get_boto_key(self,
self.s3_client.head_object,
Key=key_name)
threads.deferToThread(b.get_key,
self._headers_to_botocore_kwargs(self.HEADERS)
extra.update(self._headers_to_botocore_kwargs(headers))
self.s3_client.put_object,
Key=key_name,
Body=buf,
Metadata={k:
str(v)
six.iteritems(meta
{})},
ACL=self.POLICY,
b.new_key(key_name)
metakey,
metavalue
six.iteritems(meta):
k.set_metadata(metakey,
str(metavalue))
self.HEADERS.copy()
h.update(headers)
k.set_contents_from_string,
buf.getvalue(),
headers=h,
policy=self.POLICY)
_headers_to_botocore_kwargs(self,
CaselessDict({
'CacheControl',
'ContentDisposition',
'ContentEncoding',
'Content-Language':
'ContentLanguage',
'ContentLength',
'ContentMD5',
'Expires',
'X-Amz-Grant-Full-Control':
'GrantFullControl',
'X-Amz-Grant-Read':
'GrantRead',
'X-Amz-Grant-Read-ACP':
'GrantReadACP',
'X-Amz-Grant-Write-ACP':
'GrantWriteACP',
six.iteritems(headers):
mapping[key]
'Header
botocore'
extra[kwarg]
FilesPipeline(MediaPipeline):
"file"
STORE_SCHEMES
S3FilesStore,
'file_urls'
'files'
store_uri:
"FilesPipeline"
self.store
self._get_store(store_uri)
base_class_name=cls_name)
resolve('FILES_EXPIRES'),
"FILES_URLS_FIELD"):
self.DEFAULT_FILES_URLS_FIELD
"FILES_RESULT_FIELD"):
self.DEFAULT_FILES_RESULT_FIELD
self.files_urls_field
resolve('FILES_URLS_FIELD'),
resolve('FILES_RESULT_FIELD'),
super(FilesPipeline,
self).__init__(download_func=download_func)
settings['FILES_STORE_S3_ACL']
settings['FILES_STORE']
_get_store(self,
os.path.isabs(uri):
like:
C:\\some\dir
store_cls
self.STORE_SCHEMES[scheme]
store_cls(uri)
_onsuccess(result):
result.get('last_modified',
last_modified:
self.expires:
(uptodate):
'uptodate')
result.get('checksum',
defer.maybeDeferred(self.store.stat_file,
dfd.addCallbacks(_onsuccess,
logger.error(self.__class__.__name__
'.store.stat_file',
IgnoreRequest):
%(exception)s',
(code:
%(status)s):
FileException('download-error')
(empty-content):
Empty
no-content',
FileException('empty-content')
'downloaded'
(%(status)s):
'<%(referer)s>',
self.file_downloaded(response,
(error):
%(errormsg)s',
'errormsg':
str(exc)},
info.spider},
exc_info=True
FileException(str(exc))
inc_stats(self,
spider.crawler.stats.inc_value('file_count',
spider.crawler.stats.inc_value('file_status_count/%s'
item.get(self.files_urls_field,
self.store.persist_file(path,
item[self.files_result_field]
warnings.warn('FilesPipeline.file_key(url)
'file_path(request,
'full/%s%s'
DropItem
FileException,
FilesPipeline
NoimagesDrop(DropItem):
ImageException(FileException):
ImagesPipeline(FilesPipeline):
'image'
'image_urls'
'images'
super(ImagesPipeline,
self).__init__(store_uri,
download_func=download_func)
base_class_name="ImagesPipeline")
resolve("IMAGES_EXPIRES"),
"IMAGES_RESULT_FIELD"):
self.DEFAULT_IMAGES_RESULT_FIELD
"IMAGES_URLS_FIELD"):
self.DEFAULT_IMAGES_URLS_FIELD
self.images_urls_field
resolve('IMAGES_URLS_FIELD'),
resolve('IMAGES_RESULT_FIELD'),
resolve('IMAGES_MIN_WIDTH'),
self.MIN_WIDTH
self.min_height
resolve('IMAGES_MIN_HEIGHT'),
self.MIN_HEIGHT
self.thumbs
resolve('IMAGES_THUMBS'),
self.THUMBS
settings['IMAGES_STORE_S3_ACL']
settings['IMAGES_STORE']
self.image_downloaded(response,
image_downloaded(self,
self.get_images(response,
image.size
self.store.persist_file(
meta={'width':
'height':
height},
'image/jpeg'})
get_images(self,
orig_image
Image.open(BytesIO(response.body))
orig_image.size
self.min_height:
ImageException("Image
small
(%dx%d
%dx%d)"
(width,
self.min_width,
self.min_height))
self.convert_image(orig_image)
six.iteritems(self.thumbs):
self.thumb_path(request,
self.convert_image(image,
thumb_path,
convert_image(self,
image.format
'PNG'
'RGBA':
Image.new('RGBA',
image.size,
(255,
255))
background.paste(image,
image)
background.convert('RGB')
'RGB':
image.convert('RGB')
image.copy()
image.thumbnail(size,
Image.ANTIALIAS)
image.save(buf,
'JPEG')
item.get(self.images_urls_field,
item[self.images_result_field]
warnings.warn('ImagesPipeline.image_key(url)
file_path(request,
hasattr(self.image_key,
'full/%s.jpg'
thumb_path(self,
warnings.warn('ImagesPipeline.thumb_key(url)
'thumb_path(request,
hasattr(self.thumb_key,
self.thumb_key(url,
'thumbs/%s/%s.jpg'
image_key._base
self.thumb_path(url,
thumb_key._base
DeferredList
defer_result
MediaPipeline(object):
LOG_FAILED_RESULTS
SpiderInfo(object):
self.downloading
self.downloaded
self.waiting
download_func=None):
self.download_func
_key_for_pipe(self,
base_class_name=None):
base_class_name
base_class_name:
"{}_{}".format(class_name.upper(),
cls.from_settings(crawler.settings)
pipe.crawler
self.SpiderInfo(spider)
arg_to_iter(self.get_media_requests(item,
[self._process_request(r,
requests]
DeferredList(dlist,
dfd.addCallback(self.item_completed,
info.downloaded:
defer_result(info.downloaded[fp]).addCallbacks(cb,
Deferred().addCallbacks(cb,
info.waiting[fp].append(wad)
info.downloading:
info.downloading.add(fp)
mustbe_deferred(self.media_to_download,
dfd.addCallback(self._check_media_to_download,
dfd.addBoth(self._cache_result_and_execute_waiters,
dfd.addErrback(lambda
f.value,
wad)
_check_media_to_download(self,
self.download_func:
mustbe_deferred(self.download_func,
self.crawler.engine.download(request,
_cache_result_and_execute_waiters(self,
result.cleanFailure()
result.frames
result.stack
info.downloading.remove(fp)
info.downloaded[fp]
info.waiting.pop(fp):
defer_result(result).chainDeferred(wad)
self.LOG_FAILED_RESULTS:
ok:
'%(class)s
{'class':
exc_info=failure_to_exc_info(value),
scrapy.selector.unified
'ScrapyXPathExpr',
new_class_path='parsel.csstranslator.XPathExpr')
ScrapyGenericTranslator
'ScrapyGenericTranslator',
new_class_path='parsel.csstranslator.GenericTranslator')
ScrapyHTMLTranslator
'ScrapyHTMLTranslator',
HTMLTranslator,
new_class_path='parsel.csstranslator.HTMLTranslator')
.unified
['HtmlXPathSelector',
'XPathSelectorList']
_xpathselector_css(self,
RuntimeError('.css()
'instantiate
scrapy.Selector
'instead'
'css':
_xpathselector_css,
old_class_path='scrapy.selector.XPathSelector',
'xml',
old_class_path='scrapy.selector.XmlXPathSelector',
'HtmlXPathSelector',
old_class_path='scrapy.selector.HtmlXPathSelector',
XPathSelectorList
create_deprecated_class('XPathSelectorList',
SelectorList)
parsel
_ParselSelector
['Selector',
'SelectorList']
XmlResponse)
'html'
rt(url='about:blank',
body=to_bytes(text,
SelectorList(_ParselSelector.selectorlist_cls,
[x.extract_unquoted()
self]
x(self,
self.select(xpath)
Selector(_ParselSelector,
['response']
selectorlist_cls
text=None,
root=None,
_root=None,
self._default_type)
warnings.warn("Argument
warnings.warn("Ignoring
`root`")
kwargs.setdefault('base_url',
super(Selector,
self).__init__(text=text,
type=st,
root=root,
_root(self):
warnings.warn("Attribute
self.root
self.extract()
SETTINGS_PRIORITIES
'command':
'project':
'cmdline':
40,
get_settings_priority(priority):
SETTINGS_PRIORITIES[priority]
SettingsAttribute(object):
max(self.value.maxpriority(),
self.priority:
BaseSettings(value,
priority=priority)
value={self.value!r}
"priority={self.priority}>".format(self=self)
BaseSettings(MutableMapping):
self.attributes[opt_name].value
bool(int(self.get(name,
default)))
int(self.get(name,
float(self.get(name,
getdict(self,
dict(value)
getwithbase(self,
compbs.update(self[name
'_BASE'])
compbs.update(self[name])
getpriority(self,
self.attributes[name].priority
maxpriority(self):
max(self.getpriority(name)
get_settings_priority('default')
SettingsAttribute):
self.attributes[name].set(value,
setdict(self,
setmodule(self,
isinstance(module,
dir(module):
self.set(key,
key),
json.loads(values)
values.getpriority(name))
self.getpriority(name):
_assert_mutability(self):
self.frozen:
TypeError("Trying
copy.deepcopy(self)
frozencopy(self):
copy.freeze()
iter(self.attributes)
len(self.attributes)
_to_dict(self):
(v._to_dict()
six.iteritems(self)}
copy_to_dict(self):
settings._to_dict()
_repr_pretty_(self,
cycle):
cycle:
p.text(repr(self))
p.text(pformat(self.copy_to_dict()))
overrides(self):
warnings.warn("`Settings.overrides`
priority='cmdline')`
defaults(self):
warnings.warn("`Settings.defaults`
priority='default')`
_DictProxy(MutableMapping):
self.o
len(self.o)
self.settings.set(k,
priority=self.priority)
__iter__(self,
iter(self.o)
Settings(BaseSettings):
super(Settings,
self.setmodule(default_settings,
six.iteritems(self):
BaseSettings(val,
CrawlerSettings(Settings):
settings_module=None,
Settings.__init__(self,
self.overrides:
self.overrides[opt_name]
hasattr(self.settings_module,
getattr(self.settings_module,
self.defaults:
self.defaults[opt_name]
Settings.__getitem__(self,
"<CrawlerSettings
module=%r>"
CrawlerSettings
'CrawlerSettings',
new_class_path='scrapy.settings.Settings')
dir(default_settings):
name.isupper():
overridden_settings(settings):
defvalue
settings[name]
isinstance(defvalue,
defvalue:
AJAXCRAWL_ENABLED
AUTOTHROTTLE_ENABLED
AUTOTHROTTLE_DEBUG
AUTOTHROTTLE_MAX_DELAY
AUTOTHROTTLE_START_DELAY
AUTOTHROTTLE_TARGET_CONCURRENCY
BOT_NAME
'scrapybot'
CLOSESPIDER_TIMEOUT
CLOSESPIDER_PAGECOUNT
CLOSESPIDER_ITEMCOUNT
CLOSESPIDER_ERRORCOUNT
COMMANDS_MODULE
COMPRESSION_ENABLED
CONCURRENT_ITEMS
CONCURRENT_REQUESTS
CONCURRENT_REQUESTS_PER_IP
COOKIES_ENABLED
COOKIES_DEBUG
DEFAULT_ITEM_CLASS
'scrapy.item.Item'
DEFAULT_REQUEST_HEADERS
'Accept':
'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
'Accept-Language':
DEPTH_LIMIT
DEPTH_STATS
DEPTH_PRIORITY
DNSCACHE_ENABLED
DNSCACHE_SIZE
DNS_TIMEOUT
DOWNLOAD_HANDLERS
DOWNLOAD_HANDLERS_BASE
'scrapy.core.downloader.handlers.file.FileDownloadHandler',
'scrapy.core.downloader.handlers.s3.S3DownloadHandler',
'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',
DOWNLOAD_TIMEOUT
3mins
DOWNLOAD_MAXSIZE
1024*1024*1024
1024m
DOWNLOAD_WARNSIZE
32*1024*1024
32m
DOWNLOADER
'scrapy.core.downloader.Downloader'
DOWNLOADER_HTTPCLIENTFACTORY
'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'
DOWNLOADER_CLIENTCONTEXTFACTORY
'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'
DOWNLOADER_CLIENT_TLS_METHOD
highest
TLS/SSL
DOWNLOADER_MIDDLEWARES
DOWNLOADER_MIDDLEWARES_BASE
'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware':
'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware':
'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware':
350,
'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware':
'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':
'scrapy.downloadermiddlewares.retry.RetryMiddleware':
550,
'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware':
560,
'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware':
580,
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware':
590,
'scrapy.downloadermiddlewares.redirect.RedirectMiddleware':
600,
'scrapy.downloadermiddlewares.cookies.CookiesMiddleware':
'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware':
750,
'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware':
830,
'scrapy.downloadermiddlewares.stats.DownloaderStats':
850,
'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware':
DOWNLOADER_STATS
DUPEFILTER_CLASS
'scrapy.dupefilters.RFPDupeFilter'
os.environ['EDITOR']
'win32':
idlelib.idle'
EXTENSIONS_BASE
'scrapy.extensions.corestats.CoreStats':
'scrapy.extensions.telnet.TelnetConsole':
'scrapy.extensions.memusage.MemoryUsage':
'scrapy.extensions.memdebug.MemoryDebugger':
'scrapy.extensions.closespider.CloseSpider':
'scrapy.extensions.feedexport.FeedExporter':
'scrapy.extensions.logstats.LogStats':
'scrapy.extensions.spiderstate.SpiderState':
'scrapy.extensions.throttle.AutoThrottle':
FEED_TEMPDIR
FEED_URI
FEED_URI_PARAMS
extend
FEED_FORMAT
'jsonlines'
FEED_STORE_EMPTY
FEED_EXPORT_ENCODING
FEED_EXPORT_FIELDS
FEED_STORAGES
FEED_STORAGES_BASE
'stdout':
'scrapy.extensions.feedexport.StdoutFeedStorage',
'scrapy.extensions.feedexport.S3FeedStorage',
'scrapy.extensions.feedexport.FTPFeedStorage',
FEED_EXPORTERS
FEED_EXPORTERS_BASE
'scrapy.exporters.JsonItemExporter',
'jl':
'scrapy.exporters.CsvItemExporter',
'scrapy.exporters.XmlItemExporter',
'marshal':
'scrapy.exporters.MarshalItemExporter',
'pickle':
'scrapy.exporters.PickleItemExporter',
FILES_STORE_S3_ACL
HTTPCACHE_ENABLED
HTTPCACHE_DIR
'httpcache'
HTTPCACHE_IGNORE_MISSING
HTTPCACHE_STORAGE
HTTPCACHE_EXPIRATION_SECS
HTTPCACHE_ALWAYS_STORE
HTTPCACHE_IGNORE_HTTP_CODES
HTTPCACHE_IGNORE_SCHEMES
['file']
HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS
HTTPCACHE_DBM_MODULE
'anydbm'
'dbm'
HTTPCACHE_POLICY
HTTPCACHE_GZIP
HTTPPROXY_AUTH_ENCODING
'latin-1'
IMAGES_STORE_S3_ACL
ITEM_PROCESSOR
'scrapy.pipelines.ItemPipelineManager'
ITEM_PIPELINES
ITEM_PIPELINES_BASE
LOG_ENABLED
LOG_ENCODING
LOG_FORMATTER
'scrapy.logformatter.LogFormatter'
LOG_FORMAT
'%(asctime)s
[%(name)s]
%(levelname)s:
LOG_DATEFORMAT
'%Y-%m-%d
%H:%M:%S'
LOG_STDOUT
LOG_LEVEL
'DEBUG'
LOG_FILE
LOGSTATS_INTERVAL
MAIL_HOST
MAIL_PORT
MAIL_FROM
'scrapy@localhost'
MAIL_PASS
MAIL_USER
MEMDEBUG_ENABLED
MEMDEBUG_NOTIFY
shutdown
MEMUSAGE_CHECK_INTERVAL_SECONDS
MEMUSAGE_ENABLED
MEMUSAGE_LIMIT_MB
MEMUSAGE_NOTIFY_MAIL
MEMUSAGE_REPORT
MEMUSAGE_WARNING_MB
METAREFRESH_ENABLED
NEWSPIDER_MODULE
RANDOMIZE_DOWNLOAD_DELAY
REACTOR_THREADPOOL_MAXSIZE
REDIRECT_ENABLED
REDIRECT_MAX_TIMES
Firefox
REDIRECT_PRIORITY_ADJUST
+2
REFERER_ENABLED
RETRY_ENABLED
RETRY_TIMES
RETRY_HTTP_CODES
504,
408]
RETRY_PRIORITY_ADJUST
ROBOTSTXT_OBEY
SCHEDULER
'scrapy.core.scheduler.Scheduler'
SCHEDULER_DISK_QUEUE
'scrapy.squeues.PickleLifoDiskQueue'
SCHEDULER_MEMORY_QUEUE
'scrapy.squeues.LifoMemoryQueue'
SCHEDULER_PRIORITY_QUEUE
'queuelib.PriorityQueue'
SPIDER_LOADER_CLASS
'scrapy.spiderloader.SpiderLoader'
SPIDER_MIDDLEWARES
SPIDER_MIDDLEWARES_BASE
'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware':
'scrapy.spidermiddlewares.offsite.OffsiteMiddleware':
'scrapy.spidermiddlewares.referer.RefererMiddleware':
'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware':
800,
'scrapy.spidermiddlewares.depth.DepthMiddleware':
SPIDER_MODULES
'scrapy.statscollectors.MemoryStatsCollector'
STATS_DUMP
STATSMAILER_RCPTS
TEMPLATES_DIR
abspath(join(dirname(__file__),
'templates'))
URLLENGTH_LIMIT
2083
USER_AGENT
'Scrapy/%s
(+http://scrapy.org)'
import_module('scrapy').__version__
TELNETCONSOLE_ENABLED
TELNETCONSOLE_PORT
[6023,
6073]
TELNETCONSOLE_HOST
SPIDER_CONTRACTS
SPIDER_CONTRACTS_BASE
'scrapy.contracts.default.UrlContract':
'scrapy.contracts.default.ReturnsContract':
'scrapy.contracts.default.ScrapesContract':
('TRACK_REFS',
(trackref
always
enabled)'),
('RESPONSE_CLASSES',
('DEFAULT_RESPONSE_ENCODING',
('BOT_VERSION',
(user
('ENCODING_ALIASES',
(encoding
discovery
('STATS_ENABLED',
(change
instead)'),
('SQLITE_DB',
('SELECTORS_BACKEND',
SCRAPY_SELECTORS_BACKEND
('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY',
('REDIRECT_MAX_METAREFRESH_DELAY',
('LOG_UNSERIALIZABLE_REQUESTS',
check_deprecated_settings(settings):
settings[x[0]]
deprecated:
obsolete"
(ask
scrapy-users@googlegroups.com
alternatives):"
".join("%s:
deprecated)
DepthMiddleware(object):
maxdepth,
verbose_stats=False,
prio=1):
self.verbose_stats
verbose_stats
settings.getint('DEPTH_LIMIT')
settings.getbool('DEPTH_STATS_VERBOSE')
settings.getint('DEPTH_PRIORITY')
cls(maxdepth,
crawler.stats,
verbose,
prio)
request.meta['depth']
self.prio:
self.maxdepth:
(depth
%(maxdepth)d):
%(requrl)s
{'maxdepth':
self.maxdepth,
'requrl':
self.stats:
self.stats.inc_value('request_depth_count/%s'
self.stats.max_value('request_depth_max',
'depth'
self.stats.inc_value('request_depth_count/0',
HttpError(IgnoreRequest):
super(HttpError,
HttpErrorMiddleware(object):
self.handle_httpstatus_all
settings.getbool('HTTPERROR_ALLOW_ALL')
self.handle_httpstatus_list
settings.getlist('HTTPERROR_ALLOWED_CODES')
process_spider_input(self,
response.meta
'handle_httpstatus_all'
'handle_httpstatus_list'
meta['handle_httpstatus_list']
self.handle_httpstatus_all:
self.handle_httpstatus_list)
allowed_statuses:
HttpError(response,
non-200
process_spider_exception(self,
%(response)r:
handled
allowed",
spider},
OffsiteMiddleware(object):
x.dont_filter
self.should_follow(x,
urlparse_cached(x).hostname
self.domains_seen:
self.domains_seen.add(domain)
logger.debug("Filtered
offsite
%(domain)r:
{'domain':
x},
self.stats.inc_value('offsite/domains',
self.stats.inc_value('offsite/filtered',
should_follow(self,
bool(regex.search(host))
get_host_regex(self,
allowed_domains:
re.compile('')
r'^(.*\.)?(%s)$'
'|'.join(re.escape(d)
re.compile(regex)
self.get_host_regex(spider)
self.domains_seen
RefererMiddleware(object):
crawler.settings.getbool('REFERER_ENABLED'):
_set_referer(r):
r.headers.setdefault('Referer',
(_set_referer(r)
UrlLengthMiddleware(object):
maxlength):
self.maxlength
settings.getint('URLLENGTH_LIMIT')
maxlength:
cls(maxlength)
len(request.url)
self.maxlength:
logger.debug("Ignoring
(url
%(maxlength)d):
%(url)s
{'maxlength':
self.maxlength,
url_is_from_spider
Spider(object_ref):
self.__dict__.update(kwargs)
'start_urls'):
logging.getLogger(self.name)
logging.LoggerAdapter(logger,
self.logger.log(level,
spider._set_crawler(crawler)
warnings.warn("set_crawler
bound
"spider
"instead.",
'crawler'),
bounded
"crawler"
self._set_crawler(crawler)
_set_crawler(self,
crawler.signals.connect(self.close,
update_settings(cls,
settings.setdict(cls.custom_settings
priority='spider')
url_is_from_spider(request.url,
close(spider,
'closed',
callable(closed):
closed(reason)
0x%0x>"
BaseSpider
create_deprecated_class('BaseSpider',
ObsoleteClass(object):
AttributeError(self.message)
ObsoleteClass(
scrapy.spider
SpiderLoader"
'it
settings"'
identity(x):
cb_kwargs=None,
follow=None,
process_links=None,
process_request=identity):
self.cb_kwargs
cb_kwargs
self.process_links
process_links
self.process_request
process_request
CrawlSpider(Spider):
self._compile_rules()
self.parse_start_url,
cb_kwargs={},
follow=True)
parse_start_url(self,
_requests_to_follow(self,
enumerate(self._rules):
[lnk
rule.link_extractor.extract_links(response)
seen]
rule.process_links:
rule.process_links(links)
seen.add(link)
callback=self._response_downloaded)
r.meta.update(rule=n,
link_text=link.text)
rule.process_request(r)
self._rules[response.meta['rule']]
rule.callback,
rule.cb_kwargs,
rule.follow)
cb_kwargs,
follow=True):
callback(response,
**cb_kwargs)
cb_res)
iterate_spider_output(cb_res):
self._follow_links:
self._requests_to_follow(response):
_compile_rules(self):
get_method(method):
callable(method):
isinstance(method,
self._rules
[copy.copy(r)
self.rules]
self._rules:
get_method(rule.callback)
rule.process_links
get_method(rule.process_links)
rule.process_request
get_method(rule.process_request)
cls).from_crawler(crawler,
spider._follow_links
crawler.settings.getbool(
'CRAWLSPIDER_FOLLOW_LINKS',
self).set_crawler(crawler)
self._follow_links
crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS',
csviter
XMLFeedSpider(Spider):
'iternodes'
'parse_item'):
backward
self.parse_item(response,
selector)
parse_nodes(self,
nodes):
iterate_spider_output(self.parse_node(response,
selector))
'parse_node'):
parse_node
XML
'iternodes':
self._iternodes(response)
'html':
type='html')
NotSupported('Unsupported
iterator')
self.parse_nodes(response,
nodes)
_iternodes(self,
xmliter(response,
self.itertag):
self._register_namespaces(node)
_register_namespaces(self,
self.namespaces:
selector.register_namespace(prefix,
CSVFeedSpider(Spider):
parse_row(self,
row):
parse_rows(self,
self.delimiter,
self.quotechar):
iterate_spider_output(self.parse_row(response,
'parse_row'):
parse_row
CSV
self.parse_rows(response)
InitSpider(Spider):
self._postinit_reqs
super(InitSpider,
self).start_requests()
iterate_spider_output(self.init_request())
initialized(self,
self.__dict__.pop('_postinit_reqs')
init_request(self):
self.initialized()
SitemapSpider(Spider):
sitemap_urls
sitemap_rules
[('',
'parse')]
sitemap_follow
sitemap_alternate_links
super(SitemapSpider,
self._cbs
self.sitemap_rules:
isinstance(c,
self._cbs.append((regex(r),
c))
self._follow
[regex(x)
self.sitemap_follow]
self.sitemap_urls:
self._parse_sitemap)
_parse_sitemap(self,
response.url.endswith('/robots.txt'):
sitemap_urls_from_robots(response.text):
self._get_sitemap_body(response)
logger.warning("Ignoring
sitemap:
%(response)s",
Sitemap(body)
'sitemapindex':
iterloc(s,
self.sitemap_alternate_links):
any(x.search(loc)
self._follow):
'urlset':
iterloc(s):
self._cbs:
r.search(loc):
callback=c)
_get_sitemap_body(self,
XmlResponse):
response.url.endswith('.xml'):
response.url.endswith('.xml.gz'):
regex(x):
iterloc(it,
alt=False):
it:
d['loc']
alt
'alternate'
d['alternate']:
Site
b'total',
b'show',
request.write(b"<html><head></head><body>")
args['n']
request.write("<a
href='/follow?{0}'>follow
{1}</a><br>"
.format(argstr,
nl).encode('utf8'))
request.write(b"</body></html>")
type=str):
type(request.args[name][0])
request.args
Site(root))
_print_listening():
print("Bench
http://{}:{}".format(httpHost.host,
httpHost.port))
reactor.callWhenRunning(_print_listening)
six.moves.configparser
build_component_list(compdict,
custom=None,
convert=update_classpath):
_check_components(complist):
len({convert(c)
complist})
len(complist):
settings'.format(complist))
_map_keys(compdict):
isinstance(compdict,
six.iteritems(compdict):
compdict.getpriority(k)
compbs.getpriority(convert(k))
prio:
settings'
''.format(list(compdict.keys())))
compbs.set(convert(k),
priority=prio)
_check_components(compdict)
{convert(k):
six.iteritems(compdict)}
isinstance(custom,
_check_components(custom)
type(custom)(convert(c)
compdict.update(custom)
compdict
without_none_values(_map_keys(compdict))
[k
sorted(six.iteritems(compdict),
key=itemgetter(1))]
arglist_to_dict(arglist):
dict(x.split('=',
arglist)
closest_scrapy_cfg(path='.',
prevpath=None):
prevpath:
'scrapy.cfg')
os.path.exists(cfgfile):
closest_scrapy_cfg(os.path.dirname(path),
init_env(project='default',
set_syspath=True):
cfg.has_option('settings',
os.environ['SCRAPY_SETTINGS_MODULE']
cfg.get('settings',
closest
closest:
os.path.dirname(closest)
set_syspath
sys.path:
sys.path.append(projdir)
get_config(use_closest=True):
get_sources(use_closest)
SafeConfigParser()
cfg.read(sources)
get_sources(use_closest=True):
os.environ.get('XDG_CONFIG_HOME')
os.path.expanduser('~/.config')
['/etc/scrapy.cfg',
r'c:\scrapy\scrapy.cfg',
'/scrapy.cfg',
os.path.expanduser('~/.scrapy.cfg')]
use_closest:
sources.append(closest_scrapy_cfg())
_embed_ipython_shell(namespace={},
IPython.terminal.embed
IPython.terminal.ipapp
IPython.frontend.terminal.embed
IPython.frontend.terminal.ipapp
@wraps(_embed_ipython_shell)
load_default_config()
InteractiveShellEmbed(
banner1=banner,
user_ns=namespace,
config=config)
shell()
_embed_bpython_shell(namespace={},
@wraps(_embed_bpython_shell)
bpython.embed(locals_=namespace,
_embed_standard_shell(namespace={},
unix
systems
rlcompleter
readline.parse_and_bind("tab:complete")
@wraps(_embed_standard_shell)
local=namespace)
OrderedDict([
('ipython',
_embed_ipython_shell),
('bpython',
_embed_bpython_shell),
'python',
_embed_standard_shell),
get_shell_embed_func(shells=None,
known_shells=None):
list,
preference
embeddable
DEFAULT_PYTHON_SHELLS.copy()
shells:
known_shells:
known_shells[shell]()
start_python_console(namespace=None,
banner='',
shells=None):
get_shell_embed_func(shells)
shell(namespace=namespace,
SystemExit:
exit()
code.interact
MultiValueDictKeyError(KeyError):
"scrapy.utils.datatypes.MultiValueDictKeyError
super(MultiValueDictKeyError,
MultiValueDict(dict):
key_to_list_mapping=()):
warnings.warn("scrapy.utils.datatypes.MultiValueDict
key_to_list_mapping)
"<%s:
MultiValueDictKeyError("Key
self))
list_[-1]
self.__class__(dict.items(self))
__deepcopy__(self,
memo=None):
self.__class__()
memo[id(self)]
dict.items(self):
dict.__setitem__(result,
copy.deepcopy(key,
memo),
copy.deepcopy(value,
memo))
list_)
self.setlist(key,
"Appends
internal
key"
[(key,
self[key])
lists(self):
pairs."
dict.items(self)
every
list."
[self[key]
object."
self.__deepcopy__()
"update()
extends
rather
replaces
existing
lists.
Also
accepts
args."
TypeError("update
arguments,
len(args))
other_dict
isinstance(other_dict,
MultiValueDict):
value_list
other_dict.lists():
[]).extend(value_list)
other_dict.items():
ValueError("MultiValueDict.update()
takes
MultiValueDict
dictionary")
six.iteritems(kwargs):
SiteNode(object):
"scrapy.utils.datatypes.SiteNode
self.itemnames
self.children
add_child(self,
node):
self.children.append(node)
node.parent
to_string(self,
level=0):
"%s%s\n"
'*level,
"%sScraped:
'*(level+1),
self.children:
node.to_string(level+1)
CaselessDict(dict):
seq=None):
seq:
self.update(seq)
self.normvalue(value))
dict.__contains__(self,
dict.get(self,
dict.setdefault(self,
seq):
seq.items()
isinstance(seq,
iseq
((self.normkey(k),
self.normvalue(v))
seq)
self).update(iseq)
fromkeys(cls,
cls((k,
keys)
dict.pop(self,
MergeDict(object):
*dicts):
self.dicts
dicts
dict_[key]
self.__class__(*self.dicts)
dict_.keys():
dict_.getlist(key)
item_list.extend(dict_.items())
dict_:
self.__copy__()
LocalCache(OrderedDict):
self.popitem(last=False)
self).__setitem__(key,
`scrapy.utils.decorator`
`scrapy.utils.decorators`
deprecated(use_instead=None):
deco(func):
wrapped(*args,
"Call
use_instead:
warnings.warn(message,
callable(use_instead):
deco(use_instead)
defers(func):
defer.maybeDeferred(func,
inthread(func):
threads.deferToThread(func,
defer_fail(_failure):
d.errback,
_failure)
defer_succeed(result):
defer_result(result):
defer_fail(result)
defer_succeed(result)
mustbe_deferred(f,
defer_fail(failure.Failure(e))
defer_fail(failure.Failure())
parallel(iterable,
coop
task.Cooperator()
(callable(elem,
iterable)
defer.DeferredList([coop.coiterate(work)
range(count)])
process_chain(callbacks,
callbacks:
d.addCallback(x,
process_chain_both(callbacks,
errbacks,
zip(callbacks,
errbacks):
d.addCallbacks(cb,
callbackArgs=a,
callbackKeywords=kw,
errbackArgs=a,
errbackKeywords=kw)
isinstance(input,
d.errback(input)
process_parallel(callbacks,
[defer.succeed(input).addCallback(x,
callbacks]
defer.DeferredList(dfds,
fireOnOneErrback=1,
r],
f.value.subFailure)
iter_errback(iterable,
iter(iterable)
next(it)
errback(failure.Failure(),
attribute(obj,
newattr,
version='0.12'):
obj.__class__.__name__
warnings.warn("%s.%s
(cname,
cname,
newattr),
stacklevel=3)
create_deprecated_class(name,
new_class,
clsdict=None,
warn_category=ScrapyDeprecationWarning,
warn_once=True,
old_class_path=None,
new_class_path=None,
subclass_warn_message="{cls}
{old},
"from
{new}.",
instance_warn_message="{cls}
{new}
instead."):
DeprecatedClass(new_class.__class__):
deprecated_class
warned_on_subclass
__new__(metacls,
metacls).__new__(metacls,
cls.__class__
meta.deprecated_class
(warn_once
meta.warned_on_subclass):
meta.warned_on_subclass
subclass_warn_message.format(cls=_clspath(cls),
old=_clspath(old,
warn_once:
others)'
__instancecheck__(cls,
inst):
any(cls.__subclasscheck__(c)
{type(inst),
inst.__class__})
__subclasscheck__(cls,
sub):
DeprecatedClass.deprecated_class:
cls).__subclasscheck__(sub)
inspect.isclass(sub):
TypeError("issubclass()
class")
mro
getattr(sub,
'__mro__',
any(c
{cls,
new_class}
mro)
__call__(cls,
DeprecatedClass.deprecated_class
old:
instance_warn_message.format(cls=_clspath(cls,
cls).__call__(*args,
DeprecatedClass(name,
(new_class,),
clsdict
frm
inspect.stack()[1]
inspect.getmodule(frm[0])
deprecated_cls.__module__
parent_module.__name__
warnings.warn("Error
_clspath(cls,
forced=None):
'{}.{}'.format(cls.__module__,
DEPRECATION_RULES
('scrapy.contrib_exp.downloadermiddleware.decompression.',
'scrapy.downloadermiddlewares.decompression.'),
('scrapy.contrib_exp.iterators.',
'scrapy.utils.iterators.'),
('scrapy.contrib.downloadermiddleware.',
'scrapy.downloadermiddlewares.'),
('scrapy.contrib.exporter.',
'scrapy.exporters.'),
('scrapy.contrib.linkextractors.',
('scrapy.contrib.loader.processor.',
'scrapy.loader.processors.'),
('scrapy.contrib.loader.',
'scrapy.loader.'),
('scrapy.contrib.pipeline.',
('scrapy.contrib.spidermiddleware.',
'scrapy.spidermiddlewares.'),
('scrapy.contrib.spiders.',
'scrapy.extensions.'),
('scrapy.command.',
'scrapy.commands.'),
('scrapy.dupefilter.',
'scrapy.dupefilters.'),
('scrapy.linkextractor.',
('scrapy.telnet.',
'scrapy.extensions.telnet.'),
('scrapy.spider.',
('scrapy.squeue.',
'scrapy.squeues.'),
('scrapy.statscol.',
'scrapy.statscollectors.'),
('scrapy.utils.decorator.',
'scrapy.utils.decorators.'),
('scrapy.spidermanager.SpiderManager',
'scrapy.spiderloader.SpiderLoader'),
update_classpath(path):
DEPRECATION_RULES:
path.startswith(prefix):
path.replace(prefix,
replacement,
warnings.warn("`{}`
`{}`
instead".format(path,
new_path),
pformat_
_colorize(text,
colorize=True):
sys.stdout.isatty():
pygments.formatters
TerminalFormatter
pygments.lexers
PythonLexer
highlight(text,
PythonLexer(),
TerminalFormatter())
pformat(obj,
_colorize(pformat_(obj),
kwargs.pop('colorize',
pprint(obj,
print(pformat(obj,
get_engine_status(engine):
"time()-engine.start_time",
"engine.has_capacity()",
"len(engine.downloader.active)",
"engine.scraper.is_idle()",
"engine.spider.name",
"engine.spider_is_idle(engine.spider)",
"engine.slot.closing",
"len(engine.slot.inprogress)",
"len(engine.slot.scheduler.dqs
[])",
"len(engine.slot.scheduler.mqs)",
"len(engine.scraper.slot.queue)",
"len(engine.scraper.slot.active)",
"engine.scraper.slot.active_size",
"engine.scraper.slot.itemproc_size",
"engine.scraper.slot.needs_backout()",
eval(test))]
(exception)"
type(e).__name__)]
format_engine_status(engine=None):
get_engine_status(engine)
"Execution
status\n\n"
checks:
"%-47s
(test,
print_engine_status(engine):
print(format_engine_status(engine))
error_perm
first_call=True):
error_perm:
dirname(path),
ftp.mkd(path)
first_call:
gzf.read(size)
gzf.read1(size)
gunzip(data):
GzipFile(fileobj=BytesIO(data))
b'.'
read1(f,
8196)
(IOError,
EOFError,
struct.error):
getattr(f,
'extrabuf',
f.extrabuf
_is_gzipped
re.compile(br'^application/(x-)?gzip\b',
_is_octetstream
re.compile(br'^(application|binary)/octet-stream\b',
ctype
response.headers.get('Content-Type',
response.headers.get('Content-Encoding',
b'').lower()
(_is_gzipped(ctype)
(_is_octetstream(ctype)
(b'gzip',
b'x-gzip')))
decode_chunked_transfer(chunked_body):
t.split('\r\n',
'0':
int(h,
16)
t[:size]
t[size+2:]
_urlparse_cache
urlparse_cached(request_or_response):
request_or_response
_urlparse_cache:
urlparse(request_or_response.url)
re_rsearch,
xmliter(obj,
nodename):
nodename_patt
re.escape(nodename)
HEADER_START_RE
re.compile(r'^(.*?)<\s*%s(?:\s|>)'
HEADER_END_RE
re.compile(r'<\s*/%s\s*>'
re.search(HEADER_START_RE,
header_start.group(1).strip()
re_rsearch(HEADER_END_RE,
text[header_end[1]:].strip()
re.compile(r'<%(np)s[\s>].*?</%(np)s>'
{'np':
nodename_patt},
re.DOTALL)
r.finditer(text):
match.group()
type='xml').xpath('//'
nodename)[0]
xmliter_lxml(obj,
nodename,
prefix='x'):
_StreamReader(obj)
'{%s}%s'
(namespace,
etree.iterparse(reader,
tag=tag,
encoding=reader.encoding)
selxpath
('%s:%s'
iterable:
etree.tostring(node,
encoding='unicode')
node.clear()
xs
xs.register_namespace(prefix,
namespace)
xs.xpath(selxpath)[0]
_StreamReader(object):
obj.body,
isinstance(self._text,
self.read
self._read_unicode
self._read_string
self.read(n).lstrip()
_read_string(self,
self._text[s:e]
_read_unicode(self,
self._text[s:e].encode('utf-8')
csviter(obj,
delimiter=None,
quotechar=None):
_getrow(csv_r):
[to_unicode(field,
next(csv_r)]
StringIO(_body_or_str(obj,
unicode=True))
BytesIO(_body_or_str(obj,
unicode=False))
kwargs["delimiter"]
quotechar:
kwargs["quotechar"]
csv_r
csv.reader(lines,
len(row)
len(headers):
logger.warning("ignoring
%(csvlnum)d
(length:
%(csvrow)d,
"should
be:
%(csvheader)d)",
{'csvlnum':
csv_r.line_num,
'csvrow':
len(row),
'csvheader':
len(headers)})
dict(zip(headers,
unicode=True):
expected_types
"obj
".join(t.__name__
type(obj).__name__)
unicode:
obj.body
obj.text
obj.body.decode('utf-8')
obj.encode('utf-8')
obj.decode('utf-8')
job_dir(settings):
settings['JOBDIR']
dictConfig
twisted_log
overridden_settings,
failure_to_exc_info(failure):
isinstance(failure,
(failure.type,
failure.getTracebackObject())
TopLevelFormatter(logging.Filter):
loggers=None):
self.loggers
loggers
any(record.name.startswith(l
self.loggers):
record.name
record.name.split('.',
DEFAULT_LOGGING
'disable_existing_loggers':
'loggers':
'scrapy':
'twisted':
configure_logging(settings=None,
install_root_handler=True):
sys.warnoptions:
logging.captureWarnings(True)
observer
twisted_log.PythonLoggingObserver('twisted')
observer.start()
dictConfig(DEFAULT_LOGGING)
settings.getbool('LOG_STDOUT'):
StreamLogger(logging.getLogger('stdout'))
install_root_handler:
logging.root.setLevel(logging.NOTSET)
_get_handler(settings)
_get_handler(settings):
settings.get('LOG_FILE')
settings.get('LOG_ENCODING')
logging.FileHandler(filename,
settings.getbool('LOG_ENABLED'):
logging.NullHandler()
logging.Formatter(
fmt=settings.get('LOG_FORMAT'),
datefmt=settings.get('LOG_DATEFORMAT')
handler.setFormatter(formatter)
handler.setLevel(settings.get('LOG_LEVEL'))
handler.addFilter(TopLevelFormatter(['scrapy']))
log_scrapy_info(settings):
logger.info("Scrapy
%(version)s
(bot:
%(bot)s)",
{'version':
'bot':
settings['BOT_NAME']})
dict(overridden_settings(settings))
logger.info("Overridden
settings:
%(settings)r",
{'settings':
StreamLogger(object):
logger,
log_level=logging.INFO):
self.log_level
log_level
self.linebuf
buf.rstrip().splitlines():
self.logger.log(self.log_level,
LogCounterHandler(logging.Handler):
super(LogCounterHandler,
sname
'log_count/{}'.format(record.levelname)
self.crawler.stats.inc_value(sname)
logformatter_adapter(logkws):
{'level',
'msg',
'args'}
set(logkws):
warnings.warn('Missing
method',
'format'
logkws:
warnings.warn('`format`
'deprecated,
`msg`
logkws.get('level',
logging.INFO)
logkws.get('format',
logkws.get('msg'))
logkws.get('args')
iter_modules
replace_entities
flatten,
_ITERABLE_SINGLE_VALUES
dict,
arg_to_iter(arg):
_ITERABLE_SINGLE_VALUES)
hasattr(arg,
[arg]
load_object(path):
path.rindex('.')
loading
path"
path[:dot],
path[dot+1:]
NameError("Module
(module,
walk_modules(path):
import_module(path)
mods.append(mod)
'__path__'):
subpath,
ispkg
iter_modules(mod.__path__):
subpath
ispkg:
walk_modules(fullpath)
submod
import_module(fullpath)
mods.append(submod)
extract_regex(regex,
isinstance(regex,
re.compile(regex,
re.UNICODE)
[regex.search(text).group('extract')]
regex.findall(text)
numbered
flatten(strings)
[replace_entities(s,
[replace_entities(to_unicode(s,
md5sum(file):
hashlib.md5()
file.read(8096)
m.update(d)
rel_has_nofollow(rel):
rel
'nofollow'
rel.split()
w3lib.form
dir(signal):
signame.startswith("SIG"):
signum
getattr(signal,
signame)
isinstance(signum,
install_shutdown_handlers(function,
override_sigint=True):
reactor._handleSignals()
signal.signal(signal.SIGTERM,
signal.getsignal(signal.SIGINT)
signal.default_int_handler
override_sigint:
hasattr(signal,
"SIGBREAK"):
signal.signal(signal.SIGBREAK,
isabs,
closest_scrapy_cfg,
get_config,
init_env
'SCRAPY_SETTINGS_MODULE'
DATADIR_CFG_SECTION
'datadir'
os.environ.get('SCRAPY_SETTINGS_MODULE')
import_module(scrapy_module)
warnings.warn("Cannot
(scrapy_module,
exc))
bool(closest_scrapy_cfg())
project_data_dir(project='default'):
NotConfigured("Not
cfg.has_option(DATADIR_CFG_SECTION,
cfg.get(DATADIR_CFG_SECTION,
scrapy_cfg
scrapy_cfg:
NotConfigured("Unable
infer
dir")
abspath(join(dirname(scrapy_cfg),
'.scrapy'))
exists(d):
os.makedirs(d)
data_path(path,
createdir=False):
isabs(path):
join(project_data_dir(),
createdir
exists(path):
get_project_settings():
os.environ.get('SCRAPY_PROJECT',
init_env(project)
settings_module_path
os.environ.get(ENVVAR)
settings_module_path:
settings.setmodule(settings_module_path,
pickled_settings
os.environ.get("SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE")
pickled_settings:
settings.setdict(pickle.loads(pickled_settings),
env_overrides
{k[7:]:
os.environ.items()
k.startswith('SCRAPY_')}
env_overrides:
settings.setdict(env_overrides,
partial,
flatten(x):
list(iflatten(x))
iflatten(x):
is_listlike(el):
iflatten(el):
is_listlike(x):
"__iter__")
unique(list_,
list_:
key(item)
seen.add(seenkey)
result.append(item)
@deprecated("scrapy.utils.python.to_unicode")
str_to_unicode(text,
@deprecated("scrapy.utils.python.to_bytes")
unicode_to_str(text,
TypeError('to_unicode
text.decode(encoding,
TypeError('to_bytes
text.encode(encoding,
to_native_str(text,
re_rsearch(pattern,
chunk_size=1024):
len(text)
(chunk_size
(text[offset:],
offset)
(text,
isinstance(pattern,
re.compile(pattern)
chunk,
[match
pattern.finditer(chunk)]
matches:
matches[-1].span()
memoizemethod_noargs(method):
@wraps(method)
new_method(self,
new_method
{six.b(chr(i))
range(32)}
{b"\0",
b"\t",
b"\n",
b"\r"}
{ord(ch)
_BINARYCHARS}
@deprecated("scrapy.utils.python.binary_is_text")
isbinarytext(text):
binary_is_text(text)
binary_is_text(data):
TypeError("data
type(data).__name__)
all(c
get_func_args(func,
stripself=False):
inspect.isfunction(func):
func_args,
inspect.isclass(func):
get_func_args(func.__init__,
get_func_args(func.__func__,
inspect.ismethoddescriptor(func):
isinstance(func,
partial):
get_func_args(func.func)[len(func.args):]
(func.keywords
func.keywords)]
inspect.isroutine(func):
'__call__':
get_func_args(func.__call__,
stripself:
func_args.pop(0)
func_args
get_spec(func):
inspect.isfunction(func)
inspect.getargspec(func.__call__)
spec.defaults
firstdefault
len(spec.args)
len(defaults)
spec.args[:firstdefault]
dict(zip(spec.args[firstdefault:],
defaults))
equal_attributes(obj1,
obj2,
attributes):
temp1,
temp2
callable(attr):
attr(obj1)
attr(obj2):
getattr(obj1,
temp1)
getattr(obj2,
temp2):
WeakKeyCache(object):
default_factory):
self.default_factory
default_factory
self._weakdict
self._weakdict:
self.default_factory(key)
stringify_dict(dct_or_tuples,
keys_only=True):
six.iteritems(dict(dct_or_tuples)):
k.encode(encoding)
keys_only:
v.encode(encoding)
is_writable(path):
os.access(path,
os.access(os.path.dirname(path),
setattr_default(obj,
retry_on_eintr(function,
function(*args,
errno.EINTR:
without_none_values(iterable):
six.iteritems(iterable)
type(iterable)((v
listen_tcp(portrange,
"invalid
portrange
hasattr(portrange,
reactor.listenTCP(portrange,
reactor.listenTCP(portrange[0],
range(portrange[0],
portrange[1]+1):
reactor.listenTCP(x,
error.CannotListenError:
portrange[1]:
CallLaterOnce(object):
self._func
self._a
self._kw
delay=0):
reactor.callLater(delay,
cancel(self):
self._call:
self._call.cancel()
__call__(self):
self._func(*self._a,
**self._kw)
callable(eb):
to_unicode(request.url),
(safe_string_url)
'callback':
'errback':
dict(request.headers),
'cookies':
request.cookies,
'meta':
request.meta,
'_encoding':
request._encoding,
'priority':
request.priority,
'dont_filter':
request.dont_filter,
d['callback']
d['errback']
url=to_native_str(d['url']),
errback=eb,
method=d['method'],
headers=d['headers'],
body=d['body'],
cookies=d['cookies'],
meta=d['meta'],
encoding=d['_encoding'],
priority=d['priority'],
dont_filter=d['dont_filter'])
_find_method(obj,
six.get_method_self(func)
__self__
six.get_method_function(func).__name__
ValueError("Function
(func,
_get_method(obj,
str(name)
ValueError("Method
canonicalize_url
_fingerprint_cache
request_fingerprint(request,
include_headers=None):
tuple(to_bytes(h.lower())
sorted(include_headers))
_fingerprint_cache.setdefault(request,
fp.update(to_bytes(request.method))
fp.update(to_bytes(canonicalize_url(request.url)))
fp.update(request.body
fp.update(hdr)
request.headers.getlist(hdr):
fp.update(v)
request_authenticate(request,
request.headers['Authorization']
basic_auth_header(username,
request_httprepr(request):
to_bytes(path)
HTTP/1.1\r\n"
to_bytes(parsed.hostname
request.headers.to_string()
referer_str(request):
request.headers.get('Referer')
to_native_str(referrer,
body_or_str(*a,
_body_or_str
_body_or_str(*a,
_baseurl_cache
get_base_url(response):
_baseurl_cache:
html.get_base_url(text,
_metaref_cache
get_meta_refresh(response):
_metaref_cache:
html.get_meta_refresh(text,
ignore_tags=('script',
response_status_message(status):
to_native_str(http.RESPONSES.get(int(status),
"Unknown
Status")))
response_httprepr(response):
b"HTTP/1.1
to_bytes(str(response.status))
to_bytes(http.RESPONSES.get(response.status,
response.headers.to_string()
_openfunc=webbrowser.open):
b'<base'
repl
'<head><base
href="%s">'
body.replace(b'<head>',
to_bytes(repl))
'.html'
'.txt'
TypeError("Unsupported
tempfile.mkstemp(ext)
os.write(fd,
os.close(fd)
_openfunc("file://%s"
ScrapyJSONEncoder(json.JSONEncoder):
DATE_FORMAT
"%Y-%m-%d"
TIME_FORMAT
"%H:%M:%S"
o.strftime("%s
(self.DATE_FORMAT,
self.TIME_FORMAT))
o.strftime(self.DATE_FORMAT)
datetime.time):
o.strftime(self.TIME_FORMAT)
decimal.Decimal):
dict(o)
o.method,
o.status,
super(ScrapyJSONEncoder,
self).default(o)
ScrapyJSONDecoder(json.JSONDecoder):
maybeDeferred,
Deferred
pydispatch.dispatcher
Any,
Anonymous,
liveReceivers,
getAllReceivers,
pydispatch.robustapply
robustApply
_IgnoredException(Exception):
send_catch_log(signal=Any,
_IgnoredException)
robustApply(receiver,
logger.error("Cannot
dont_log:
responses.append((receiver,
send_catch_log_deferred(signal=Any,
logerror(failure,
recv):
dont_log):
recv},
maybeDeferred(robustApply,
d.addErrback(logerror,
receiver)
(receiver,
dfds.append(d)
DeferredList(dfds)
out])
disconnect_all(signal=Any,
sender=Any):
disconnect(receiver,
sender=sender)
Sitemap(object):
xmltext):
xmlp
lxml.etree.XMLParser(recover=True,
remove_comments=True,
resolve_entities=False)
self._root
lxml.etree.fromstring(xmltext,
parser=xmlp)
self._root.tag
self._root.tag.split('}',
self._root.getchildren():
elem.getchildren():
tag.split('}',
'link':
'href'
el.attrib:
d.setdefault('alternate',
[]).append(el.get('href'))
el.text.strip()
el.text
'loc'
sitemap_urls_from_robots(robots_text):
robots_text.splitlines():
line.lstrip().lower().startswith('sitemap:'):
line.split(':',
1)[1].strip()
iterate_spider_output(result):
arg_to_iter(result)
six.itervalues(vars(module)):
module.__name__
default_spidercls=None,
log_none=False,
log_multiple=False):
snames
spider_loader.find_by_request(request)
spider_loader.load(snames[0])
log_multiple:
logger.error('More
handle:
%(snames)s',
'snames':
'.join(snames)})
log_none:
handles:
request})
default_spidercls
DefaultSpider(Spider):
render_templatefile(path,
fp.read().decode('utf8')
string.Template(raw).substitute(**kwargs)
path[:-len('.tmpl')]
path.endswith('.tmpl')
fp.write(content.encode('utf8'))
path.endswith('.tmpl'):
os.remove(path)
CAMELCASE_INVALID_CHARS
re.compile('[^a-zA-Z\d]')
string_camelcase(string):
CAMELCASE_INVALID_CHARS.sub('',
string.title())
assert_aws_environ():
'AWS_ACCESS_KEY_ID'
SkipTest("AWS
found")
skip_if_no_boto():
SkipTest(e)
get_s3_content_and_delete(bucket,
with_key=False):
session.create_client('s3')
client.get_object(Bucket=bucket,
key['Body'].read()
client.delete_object(Bucket=bucket,
boto.connect_s3().get_bucket(bucket,
bucket.get_key(path)
bucket.delete_key(path)
with_key
get_crawler(spidercls=None,
settings_dict=None):
CrawlerRunner(settings_dict)
runner.create_crawler(spidercls
get_pythonpath():
scrapy_path
import_module('scrapy').__path__[0]
os.path.dirname(scrapy_path)
os.pathsep
os.environ.get('PYTHONPATH',
get_testenv():
env['PYTHONPATH']
get_pythonpath()
assert_samelines(testcase,
text1,
text2,
testcase.assertEqual(text1.splitlines(),
text2.splitlines(),
ProcessTest(object):
'scrapy.cmdline']
cwd
trial
chdirs
dir
execute(self,
check_code=True,
env['SCRAPY_SETTINGS_MODULE']
[self.command]
list(args)
pp
TestProcessProtocol()
pp.deferred.addBoth(self._process_finished,
check_code)
reactor.spawnProcess(pp,
cmd[0],
env=env,
path=self.cwd)
pp.deferred
_process_finished(self,
pp,
check_code):
pp.exitcode
check_code:
"process
(cmd,
pp.exitcode)
stdout
pp.out
RuntimeError(msg)
pp.exitcode,
pp.out,
TestProcessProtocol(protocol.ProcessProtocol):
outReceived(self,
errReceived(self,
processEnded(self,
status.value.exitCode
self.deferred.callback(self)
SiteTest(object):
self.baseurl
"http://localhost:%d/"
self.site.getHost().port
self.site.stopListening()
urljoin(self.baseurl,
test_site():
resource.Resource()
r.putChild(b"text",
static.Data(b"Works",
r.putChild(b"html",
static.Data(b"<body><p
class='one'>Works</p><p
class='two'>World</p></body>",
"text/html"))
r.putChild(b"enc-gb18030",
static.Data(b"<p>gb18030
encoding</p>",
"text/html;
charset=gb18030"))
server.Site(r)
print("http://localhost:%d/"
NoneType
type(None)
defaultdict(weakref.WeakKeyDictionary)
object_ref(object):
object.__new__(cls)
live_refs[cls][obj]
format_live_refs(ignore=NoneType):
"Live
References\n\n"
sorted(six.iteritems(live_refs),
x[0].__name__):
issubclass(cls,
ignore):
min(six.itervalues(wdict))
"%-30s
%6d
%ds
ago\n"
print_live_refs(*a,
print(format_live_refs(*a,
get_oldest(class_name):
min(six.iteritems(wdict),
key=itemgetter(1))[0]
iter_all(class_name):
six.iterkeys(wdict)
(ParseResult,
urldefrag,
parse_qsl,
urlencode,
unquote)
_safe_chars
domains):
parse_url(url).netloc.lower()
[d.lower()
domains]
any((host
(host.endswith('.%s'
domains)
url_is_from_spider(url,
[spider.name]
list(getattr(spider,
url_has_any_extension(url,
extensions):
posixpath.splitext(parse_url(url).path)[1].lower()
_safe_ParseResult(parts,
encoding='utf8',
path_encoding='utf8'):
parts.netloc.encode('idna')
to_native_str(parts.scheme),
to_native_str(netloc),
quote(to_bytes(parts.path,
quote(to_bytes(parts.params,
quote(to_bytes(parts.query,
quote(to_bytes(parts.fragment,
keep_blank_values=True,
keep_fragments=False,
UnicodeEncodeError
parse_qsl(query,
parse_qsl_to_bytes(query,
keyvals.sort()
urlencode(keyvals)
uqp
_unquotepath(path)
quote(uqp,
keep_fragments
urlunparse((scheme,
netloc.lower(),
fragment))
_unquotepath(path):
reserved
('2f',
'2F',
'3f',
'3F'):
path.replace('%'
reserved,
'%25'
reserved.upper())
unquote(path)
unquote_to_bytes(path)
parse_url(url,
ParseResult):
urlparse(to_unicode(url,
_coerce_args,
parse_qsl_to_bytes(qs,
strict_parsing=False):
_coerce_result
_coerce_args(qs)
[s2
qs.split('&')
s1.split(';')]
pairs:
name_value.split('=',
ValueError("bad
(name_value,))
nv.append('')
len(nv[1])
nv[0].replace('+',
unquote_to_bytes(name)
_coerce_result(name)
nv[1].replace('+',
unquote_to_bytes(value)
_coerce_result(value)
r.append((name,
escape_ajax(url):
defrag,
frag
urldefrag(url)
frag.startswith('!'):
add_or_replace_parameter(defrag,
'_escaped_fragment_',
frag[1:])
add_http_if_no_scheme(url):
re.match(r"^\w+://",
flags=re.I)
"http:"
"http://"
guess_scheme(url):
parts.scheme:
re.match(r'''^
with...
...a
dot,
[^/\.]+
optionally
characters
".",
".."
".blabla"
''',
parts.path,
flags=re.VERBOSE):
any_to_uri(url)
add_http_if_no_scheme(url)
dispatcher,
errors,
robust,
robustapply,
saferef,
warnings.warn("Importing
will"
versions."
method,"
otherwise
needed."
See:
https://github.com/scrapy/scrapy/issues/1762",
(13,
_Mocked(object):
NotSupported('HTTP1.1
_Mock(object):
_Mocked
_Mock()
client.Agent
client.ProxyAgent
ResponseDone
client.ResponseDone
client.ResponseFailed
HTTPConnectionPool
client.HTTPConnectionPool
endpoints.TCP4ClientEndpoint
implements
twisted.python.reflect
fullyQualifiedName
IConsumer,
ConnectionDone
succeed,
CancelledError
Protocol
twisted.protocols.basic
LineReceiver
NO_CONTENT,
NOT_MODIFIED
_DataLoss,
_IdentityTransferDecoder,
_ChunkedTransferDecoder
'STATUS'
'HEADER'
'BODY'
'DONE'
BadHeaders(Exception):
ExcessWrite(Exception):
ParseError(Exception):
BadResponseVersion(ParseError):
_WrapperException(Exception):
reasons):
self.reasons
reasons
RequestGenerationFailed(_WrapperException):
RequestTransmissionFailed(_WrapperException):
ConnectionAborted(Exception):
WrongBodyLength(Exception):
ResponseDone(Exception):
ResponseFailed(_WrapperException):
_WrapperException.__init__(self,
ResponseNeverReceived(ResponseFailed):
RequestNotSent(Exception):
_callAppFunction(function):
function()
log.err(None,
fullyQualifiedName(function),))
HTTPParser(LineReceiver):
CONNECTION_CONTROL_HEADERS
'content-length',
'connection',
'keep-alive',
'te',
'trailers',
'transfer-encoding',
'upgrade',
'proxy-connection'])
switchToBodyMode(self,
decoder):
BODY:
RuntimeError("already
mode")
self.setRawMode()
line[-1:]
'\r':
line[:-1]
STATUS:
self.statusReceived(line)
HEADER:
\t':
''.join(self._partialHeader)
header.split(':',
self.headerReceived(name,
self.allHeadersReceived()
[line]
self._partialHeader.append(line)
rawDataReceived(self,
self.bodyDecoder.dataReceived(data)
self.CONNECTION_CONTROL_HEADERS
headerReceived(self,
self.isConnectionControlHeader(name):
headers.addRawHeader(name,
self.switchToBodyMode(None)
HTTPClientParser(HTTPParser):
NO_BODY_CODES
set([NO_CONTENT,
NOT_MODIFIED])
_transferDecoders
_ChunkedTransferDecoder,
bodyDecoder
finisher):
self.finisher
finisher
HTTPParser.dataReceived(self,
parseVersion(self,
strversion):
proto,
strnumber
strversion.split('/')
strnumber.split('.')
int(major),
int(minor)
BadResponseVersion(str(e),
major
BadResponseVersion("version
negative",
(proto,
minor)
status.split('
ParseError("wrong
parts",
statusCode
int(parts[1])
ParseError("non-integer
code",
Response(
self.parseVersion(parts[0]),
statusCode,
parts[2],
_finished(self,
self.finisher(rest)
HTTPParser.isConnectionControlHeader(self,
(self.response.code
self.NO_BODY_CODES
transferEncodingHeaders
self.connHeaders.getRawHeaders(
'transfer-encoding')
transferEncodingHeaders:
self._transferDecoders[transferEncodingHeaders[0].lower()]
self.connHeaders.getRawHeaders('content-length')
len(contentLengthHeaders)
int(contentLengthHeaders[0])
"Too
headers;
invalid")
_IdentityTransferDecoder(
contentLength,
y)
self.transport.pauseProducing()
self.switchToBodyMode(transferDecoder(
self.response._bodyDataReceived,
self._finished))
self._responseDeferred.callback(self.response)
self.bodyDecoder.noMoreData()
PotentialDataLoss:
self.response._bodyDataFinished(Failure())
_DataLoss:
self.response._bodyDataFinished(
Failure(ResponseFailed([reason,
Failure()],
self.response)))
DONE:
self._everReceivedData:
ResponseNeverReceived
self._responseDeferred.errback(Failure(exceptionClass([reason])))
Request:
persistent=False):
_writeHeaders(self,
TEorCL):
self.headers.getRawHeaders('host',
len(hosts)
BadHeaders("Exactly
Host
requestLines
requestLines.append(
self.persistent:
requestLines.append('Connection:
close\r\n')
TEorCL
requestLines.append(TEorCL)
self.headers.getAllRawHeaders():
requestLines.extend(['%s:
%s\r\n'
values])
requestLines.append('\r\n')
transport.writeSequence(requestLines)
_writeToChunked(self,
'Transfer-Encoding:
chunked\r\n')
ChunkedEncoder(transport)
encoder.registerProducer(self.bodyProducer,
cbProduced(ignored):
encoder.unregisterProducer()
ebProduced(err):
d.addCallbacks(cbProduced,
ebProduced)
_writeToContentLength(self,
self._writeHeaders(
'Content-Length:
%d\r\n'
(self.bodyProducer.length,))
finishedConsuming
LengthEnforcingConsumer(
self.bodyProducer,
finishedConsuming)
transport.registerProducer(self.bodyProducer,
finishedProducing
combine(consuming,
producing):
cancelConsuming(ign):
finishedProducing.cancel()
Deferred(cancelConsuming)
ebConsuming(err):
log.err(
err,
"Buggy
machine
%r/[%d]:
"ebConsuming
state[0]))
cbProducing(result):
encoder._noMoreWritesExpected()
ultimate.errback()
ultimate.callback(None)
ebProducing(err):
"Producer
buggy")
consuming.addErrback(ebConsuming)
producing.addCallbacks(cbProducing,
ebProducing)
combine(finishedConsuming,
finishedProducing)
f(passthrough):
d.addBoth(f)
writeTo(self,
self.bodyProducer.length
UNKNOWN_LENGTH:
self._writeToChunked(transport)
self._writeToContentLength(transport)
stopWriting(self):
_callAppFunction(self.bodyProducer.stopProducing)
LengthEnforcingConsumer:
consumer,
finished):
producer.length
self._consumer
consumer
self._consumer.write(bytes)
_callAppFunction(self._producer.stopProducing)
self._finished.errback(WrongBodyLength("too
written"))
_noMoreWritesExpected(self):
WrongBodyLength("too
few
written")
makeStatefulDispatcher(name,
dispatcher(self,
self._state,
"%r
self._state))
dispatcher.__doc__
template.__doc__
implements(IResponse)
_bodyProtocol
_bodyFinished
phrase,
_transport):
self.phrase
self._transport
_transport
'INITIAL'
makeStatefulDispatcher('deliverBody',
deliverBody)
_deliverBody_INITIAL(self,
self._transport.resumeProducing()
'CONNECTED'
_deliverBody_CONNECTED(self,
"again"
(self._bodyProtocol,))
_deliverBody_DEFERRED_CLOSE(self,
protocol.dataReceived(data)
protocol.connectionLost(self._reason)
_deliverBody_FINISHED(self,
now.")
_bodyDataReceived(self,
_bodyDataReceived
makeStatefulDispatcher('bodyDataReceived',
_bodyDataReceived)
_bodyDataReceived_INITIAL(self,
self._bodyBuffer.append(data)
_bodyDataReceived_CONNECTED(self,
_bodyDataReceived_DEFERRED_CLOSE(self,
_bodyDataFinished")
_bodyDataReceived_FINISHED(self,
_bodyDataFinished(self,
_bodyDataFinished
makeStatefulDispatcher('bodyDataFinished',
_bodyDataFinished)
_bodyDataFinished_INITIAL(self,
'DEFERRED_CLOSE'
self._reason
_bodyDataFinished_CONNECTED(self,
self._bodyProtocol.connectionLost(reason)
_bodyDataFinished_DEFERRED_CLOSE(self):
_bodyDataFinished_FINISHED(self):
ChunkedEncoder:
implements(IConsumer)
registerProducer(self,
self.transport.registerProducer(producer,
streaming)
self.transport.writeSequence(("%x\r\n"
len(data),
"\r\n"))
unregisterProducer(self):
self.write('')
self.transport.unregisterProducer()
TransportProxyProducer:
implements(IPushProducer)
disconnecting
producer):
_stopProxying(self):
self._producer.resumeProducing()
self._producer.pauseProducing()
HTTP11ClientProtocol(Protocol):
_state
_parser
_finishedRequest
_currentRequest
_transportProxy
_responseDeferred
quiescentCallback=lambda
'QUIESCENT':
fail(RequestNotSent())
'TRANSMITTING'
_requestDeferred
maybeDeferred(request.writeTo,
cancelRequest(ign):
'TRANSMITTING',
'TRANSMITTING_AFTER_RECEIVING_RESPONSE'):
_requestDeferred.cancel()
self._disconnectParser(Failure(CancelledError()))
Deferred(cancelRequest)
TransportProxyProducer(self.transport)
HTTPClientParser(request,
self._finishResponse)
self._parser.makeConnection(self._transportProxy)
self._parser._responseDeferred
cbRequestWrotten(ignored):
ebRequestWriting(err):
'GENERATION_FAILED'
Failure(RequestGenerationFailed([err])))
finalize
self._state)
_requestDeferred.addCallbacks(cbRequestWrotten,
ebRequestWriting)
_finishResponse(self,
_finishResponse
makeStatefulDispatcher('finishResponse',
_finishResponse)
_finishResponse_WAITING(self,
'WAITING':
'TRANSMITTING_AFTER_RECEIVING_RESPONSE'
ConnectionDone("synthetic!")
connHeaders
self._parser.connHeaders.getRawHeaders('connection',
(('close'
connHeaders)
"QUIESCENT"
self._currentRequest.persistent):
self._giveUp(Failure(reason))
self._quiescentCallback(self)
_finishResponse_TRANSMITTING
_finishResponse_WAITING
_disconnectParser(self,
self._transportProxy._stopProxying()
parser.connectionLost(reason)
_giveUp(self,
self._parser.dataReceived(bytes)
self._giveUp(Failure())
connectionLost
makeStatefulDispatcher('connectionLost',
connectionLost)
_connectionLost_QUIESCENT(self,
_connectionLost_GENERATION_FAILED(self,
_connectionLost_TRANSMITTING(self,
Failure(RequestTransmissionFailed([reason])))
self._currentRequest.stopWriting()
_connectionLost_TRANSMITTING_AFTER_RECEIVING_RESPONSE(self,
_connectionLost_WAITING(self,
_connectionLost_ABORTING(self,
self._disconnectParser(Failure(ConnectionAborted()))
self._abortDeferreds:
d.callback(None)
abort(self):
"CONNECTION_LOST":
'ABORTING'
self._abortDeferreds.append(d)
_urlunparse
urlunparse(parts):
_urlunparse(tuple([p.decode("charmap")
parts]))
result.encode("charmap")
IProtocol
.endpoints
SSL4ClientEndpoint
UNKNOWN_LENGTH,
IBodyProducer
PartialDownloadError(error.Error):
_URL(tuple):
__new__(self,
tuple.__new__(_URL,
path))
_parse(url,
defaultPort=None):
http.urlparse(url)
parsed[0]
urlunparse((b'',
parsed[2:])
parsed[1],
host.split(b':')
b'/'
_URL(scheme,
_makeGetterFactory(url,
factoryFactory,
_parse(url)
factoryFactory(url,
ssl.ClientContextFactory()
twisted.web.error
SchemeNotSupported
HTTP11ClientProtocol
RequestNotSent,
RequestTransmissionFailed
ResponseNeverReceived,
PotentialDataLoss,
_WrapperException)
WebClientContextFactory(object):
NotImplementedError("SSL
unavailable")
WebClientContextFactory(ClientContextFactory):
_WebToNormalContextFactory(object):
webContext,
self._webContext
webContext
self._hostname
getContext(self):
self._webContext.getContext(self._hostname,
FileBodyProducer(object):
_SEEK_SET
'SEEK_SET',
_SEEK_END
'SEEK_END',
inputFile,
cooperator=task,
readSize=2
16):
self._inputFile
inputFile
self._cooperate
cooperator.cooperate
self._readSize
readSize
self._determineLength(inputFile)
_determineLength(self,
fObj):
seek
fObj.seek
fObj.tell
seek(0,
self._SEEK_END)
seek(originalPosition,
self._SEEK_SET)
self._task.stop()
self._task
self._cooperate(self._writeloop(consumer))
self._task.whenDone()
maybeStopped(reason):
reason.trap(task.TaskStopped)
maybeStopped)
_writeloop(self,
self._inputFile.read(self._readSize)
bytes:
consumer.write(bytes)
self._task.pause()
self._task.resume()
_HTTP11ClientFactory(protocol.Factory):
quiescentCallback):
HTTP11ClientProtocol(self._quiescentCallback)
_RetryingHTTP11ClientProtocol(object):
clientProtocol,
newConnection):
self._clientProtocol
clientProtocol
self._newConnection
_shouldRetry(self,
bodyProducer):
"HEAD",
"OPTIONS",
"DELETE",
"TRACE"):
(RequestNotSent,
RequestTransmissionFailed,
ResponseNeverReceived)):
_WrapperException):
exception.reasons:
failure.check(defer.CancelledError):
self._clientProtocol.request(request)
failed(reason):
self._shouldRetry(request.method,
reason.value,
request.bodyProducer):
self._newConnection().addCallback(
connection.request(request))
d.addErrback(failed)
HTTPConnectionPool(object):
_factory
_HTTP11ClientFactory
maxPersistentPerHost
cachedConnectionTimeout
retryAutomatically
persistent=True):
getConnection(self,
self._connections.get(key)
connections:
self._timeouts[connection].cancel()
self.retryAutomatically:
_RetryingHTTP11ClientProtocol(
newConnection)
defer.succeed(connection)
_newConnection(self,
quiescentCallback(protocol):
self._putConnection(key,
protocol)
self._factory(quiescentCallback)
endpoint.connect(factory)
_removeConnection(self,
connection.transport.loseConnection()
self._connections[key].remove(connection)
_putConnection(self,
"BUG:
Non-quiescent
pool.")
self._connections.setdefault(key,
len(connections)
self.maxPersistentPerHost:
dropped
dropped.transport.loseConnection()
self._timeouts[dropped].cancel()
self._timeouts[dropped]
connections.append(connection)
self._reactor.callLater(self.cachedConnectionTimeout,
self._removeConnection,
closeCachedConnections(self):
protocols
self._connections.itervalues():
protocols:
results.append(p.abort())
dc
self._timeouts.values():
dc.cancel()
defer.gatherResults(results).addCallback(lambda
ign:
_AgentBase(object):
pool):
_computeHostValue(self,
(('http',
80),
443)):
headers.hasHeader('host'):
headers.addRawHeader(
self._computeHostValue(parsedURI.scheme,
parsedURI.port))
self._pool.getConnection(key,
cbConnected(proto):
proto.request(
Request(method,
requestPath,
persistent=self._pool.persistent))
d.addCallback(cbConnected)
Agent(_AgentBase):
contextFactory=WebClientContextFactory(),
_wrapContextFactory(self,
_WebToNormalContextFactory(self._contextFactory,
kwargs['bindAddress']
TCP4ClientEndpoint(self._reactor,
SSL4ClientEndpoint(self._reactor,
self._wrapContextFactory(host,
port),
SchemeNotSupported("Unsupported
(scheme,))
parsedURI
_parse(uri)
self._getEndpoint(parsedURI.scheme,
SchemeNotSupported:
defer.fail(Failure())
(parsedURI.scheme,
parsedURI.path)
ProxyAgent(_AgentBase):
reactor=None,
self._proxyEndpoint
("http-proxy",
self._proxyEndpoint)
self._proxyEndpoint,
_parse(uri),
_FakeUrllib2Request(object):
splittype(self.uri)
splithost(rest)
header):
self.headers.hasHeader(header)
self.headers.addRawHeader(name,
self.headers.getRawHeaders(name,
headers[0]
_FakeUrllib2Response(object):
_Meta(object):
getheaders(zelf,
self.response.headers.getRawHeaders(name,
_Meta()
CookieAgent(object):
cookieJar):
self.cookieJar
cookieJar
lastRequest
_FakeUrllib2Request(uri)
headers.hasHeader('cookie'):
self.cookieJar.add_cookie_header(lastRequest)
lastRequest.get_header('Cookie',
headers.addRawHeader('cookie',
cookieHeader)
d.addCallback(self._extractCookies,
lastRequest)
_extractCookies(self,
_FakeUrllib2Response(response)
self.cookieJar.extract_cookies(resp,
GzipDecoder(proxyForInterface(IResponse)):
self.original.deliverBody(_GzipProtocol(protocol,
self.original))
_GzipProtocol(proxyForInterface(IProtocol)):
self._zlibDecompress
zlib.decompressobj(16
zlib.MAX_WBITS)
self._zlibDecompress.decompress(data)
ResponseFailed([failure.Failure()],
self._zlibDecompress.flush()
ResponseFailed([reason,
failure.Failure()],
self.original.connectionLost(reason)
ContentDecoderAgent(object):
decoders):
self._decoders
dict(decoders)
self._supported
','.join([decoder[0]
decoders])
headers.addRawHeader('accept-encoding',
self._supported)
deferred.addCallback(self._handleResponse)
response.headers.getRawHeaders(
','.join(contentEncodingHeaders).split(',')
contentEncodingHeaders.pop().strip()
self._decoders.get(name)
decoder(response)
contentEncodingHeaders.append(name)
response.headers.setRawHeaders(
[','.join(contentEncodingHeaders)])
response.headers.removeHeader('content-encoding')
RedirectAgent(object):
redirectLimit=20):
self._redirectLimit
redirectLimit
_handleRedirect(self,
self._redirectLimit:
error.InfiniteRedirection(
'Infinite
redirection
detected',
locationHeaders
response.headers.getRawHeaders('location',
locationHeaders:
error.RedirectWithNoLocation(
field',
locationHeaders[0]
(http.MOVED_PERMANENTLY,
http.FOUND,
http.TEMPORARY_REDIRECT):
error.PageRedirect(response.code,
http.SEE_OTHER:
_ReadBodyProtocol(protocol.Protocol):
self.dataBuffer
self.dataBuffer.append(data)
self.deferred.callback(b''.join(self.dataBuffer))
self.deferred.errback(
PartialDownloadError(self.status,
b''.join(self.dataBuffer)))
self.deferred.errback(reason)
readBody(response):
response.deliverBody(_ReadBodyProtocol(response.code,
response.phrase,
'PartialDownloadError',
'HTTPPageGetter',
'HTTPPageDownloader',
'HTTPClientFactory',
'HTTPDownloader',
'getPage',
'downloadPage',
'ResponseDone',
'ResponseFailed',
'Agent',
'CookieAgent',
'ProxyAgent',
'ContentDecoderAgent',
'GzipDecoder',
'RedirectAgent',
'HTTPConnectionPool',
'readBody']
implementer,
directlyProvides
interfaces,
fdesc,
ClientFactory,
ProcessProtocol,
Factory)
IStreamServerEndpointStringParser
IStreamClientEndpointStringParser
twisted.plugin
IPlugin,
getPlugins
stdio
.interfaces
IFileDescriptorReceiver
["TCP4ClientEndpoint",
"SSL4ServerEndpoint"]
_WrappingProtocol(Protocol):
connectedDeferred,
wrappedProtocol):
self._connectedDeferred
connectedDeferred
self._wrappedProtocol
wrappedProtocol
iface
[interfaces.IHalfCloseableProtocol,
IFileDescriptorReceiver]:
iface.providedBy(self._wrappedProtocol):
directlyProvides(self,
iface)
logPrefix(self):
interfaces.ILoggingContext.providedBy(self._wrappedProtocol):
self._wrappedProtocol.logPrefix()
self._wrappedProtocol.__class__.__name__
self._wrappedProtocol.makeConnection(self.transport)
self._connectedDeferred.callback(self._wrappedProtocol)
self._wrappedProtocol.dataReceived(data)
fileDescriptorReceived(self,
descriptor):
self._wrappedProtocol.fileDescriptorReceived(descriptor)
self._wrappedProtocol.connectionLost(reason)
readConnectionLost(self):
self._wrappedProtocol.readConnectionLost()
writeConnectionLost(self):
self._wrappedProtocol.writeConnectionLost()
_WrappingFactory(ClientFactory):
_WrappingProtocol
wrappedFactory):
self._wrappedFactory
wrappedFactory
self._onConnection
defer.Deferred(canceller=self._canceller)
startedConnecting(self,
connector):
self._connector
connector
_canceller(self,
deferred.errback(
error.ConnectingCancelledError(
self._connector.getDestination()))
self._connector.stopConnecting()
doStart(self):
self._wrappedFactory.doStart()
doStop(self):
self._wrappedFactory.doStop()
proto
self._wrappedFactory.buildProtocol(addr)
self._onConnection.errback()
self.protocol(self._onConnection,
proto)
clientConnectionFailed(self,
connector,
self._onConnection.called:
self._onConnection.errback(reason)
@implementer(interfaces.ITransport)
_ProcessEndpointTransport(proxyForInterface(
interfaces.IProcessTransport,
'_process')):
writeSequence(self,
chunk)
_TCPServerEndpoint(object):
interface):
defer.execute(self._reactor.listenTCP,
TCP4ServerEndpoint(_TCPServerEndpoint):
TCP6ServerEndpoint(_TCPServerEndpoint):
TCP4ClientEndpoint(object):
self._reactor.connectTCP(
SSL4ServerEndpoint(object):
defer.execute(self._reactor.listenSSL,
contextFactory=self._sslContextFactory,
SSL4ClientEndpoint(object):
self._reactor.connectSSL(
self._sslContextFactory,
UNIXServerEndpoint(object):
self._mode
self._wantPID
wantPID
defer.execute(self._reactor.listenUNIX,
self._address,
mode=self._mode,
wantPID=self._wantPID)
UNIXClientEndpoint(object):
self._path
self._checkPID
checkPID
self._reactor.connectUNIX(
self._path,
checkPID=self._checkPID)
AdoptedStreamServerEndpoint(object):
_close
os.close
_setNonBlocking
staticmethod(fdesc.setNonBlocking)
fileno,
addressFamily):
self.reactor
self.fileno
fileno
self.addressFamily
addressFamily
self._used:
defer.fail(error.AlreadyListened())
self._setNonBlocking(self.fileno)
self.reactor.adoptStreamPort(
self.fileno,
self.addressFamily,
self._close(self.fileno)
defer.succeed(port)
_parseTCP(factory,
interface="",
(int(port),
int(backlog)}
_parseUNIX(factory,
mode='666',
lockfile=True):
(address,
{'mode':
int(mode,
8),
int(backlog),
'wantPID':
bool(int(lockfile))})
_parseSSL(factory,
privateKey="server.pem",
certKey=None,
sslmethod=None,
sslmethod
getattr(ssl.SSL,
sslmethod)
ssl.SSL.SSLv23_METHOD
certPEM
FilePath(certKey).getContent()
keyPEM
FilePath(privateKey).getContent()
privateCertificate
ssl.PrivateCertificate.loadPEM(certPEM
keyPEM)
privateKey=privateCertificate.privateKey.original,
certificate=privateCertificate.original,
**kw
((int(port),
cf),
int(backlog)})
_StandardIOParser(object):
"stdio"
reactor):
StandardIOEndpoint(reactor)
self._parseServer(reactor)
_TCP6ServerParser(object):
"tcp6"
Used
_parseServer
identify
int(backlog)
TCP6ServerEndpoint(reactor,
self._parseServer(reactor,
_serverParsers
{"tcp":
_parseTCP,
"unix":
_parseUNIX,
"ssl":
_parseSSL,
_STRING
range(2)
':='
nextOps
{':':
':=',
'=':
':'}
iter(description)
ops:
nextOps[n]
next(description)
_parse(description):
add(sofar):
len(sofar)
args.append(sofar[0])
kw[sofar[0]]
sofar[1]
_STRING:
(value,)
':':
_endpointServerFactories
TCP4ServerEndpoint,
SSL4ServerEndpoint,
UNIXServerEndpoint,
_endpointClientFactories
SSL4ClientEndpoint,
UNIXClientEndpoint,
_NO_DEFAULT
(len(args)
deprecationMessage
"Unqualified
strport
'service'."
qualified
descriptions;
'tcp:%s'."
(description,))
'tcp'
deprecationMessage,
stacklevel=4)
_NO_DEFAULT:
ValueError(deprecationMessage)
args[0:0]
[default]
endpointType
_serverParsers.get(endpointType)
getPlugins(IStreamServerEndpointStringParser):
plugin.prefix
endpointType:
(plugin,
args[1:],
(endpointType,))
(endpointType.upper(),)
parser(factory,
*args[1:],
default):
nameOrPlugin,
type(nameOrPlugin)
plugin.parseStreamServer(reactor,
args[:1]
args[2:]
_endpointServerFactories[name](reactor,
serverFromString(reactor,
_NO_DEFAULT)
quoteStringArgument(argument):
argument.replace('\\',
'\\\\').replace(':',
'\\:')
int(args[1])
int(args[0])
int(kwargs['port'])
_loadCAsFromDir(directoryPath):
directoryPath.children():
child.basename().split('.')[-1].lower()
'pem':
child.getContent()
theCert
ssl.Certificate.loadPEM(data)
ssl.SSL.Error:
caCerts[theCert.digest()]
theCert.original
caCerts.values()
_parseClientSSL(*args,
kwargs.pop('certKey',
kwargs.pop('privateKey',
kwargs.pop('caCertsDir',
ssl.Certificate.loadPEM(
FilePath(certKey).getContent()).original
ssl.PrivateCertificate.loadPEM(
FilePath(privateKey).getContent()).privateKey.original
_loadCAsFromDir(FilePath(caCertsDir))
kwargs['sslContextFactory']
method=ssl.SSL.SSLv23_METHOD,
certificate=certx509,
privateKey=privateKey,
verify=verify,
caCerts=caCerts
_parseClientUNIX(*args,
kwargs['checkPID']
bool(int(kwargs.pop('lockfile')))
_clientParsers
_parseClientTCP,
_parseClientSSL,
_parseClientUNIX,
clientFromString(reactor,
aname
args.pop(0)
aname.upper()
getPlugins(IStreamClientEndpointStringParser):
plugin.prefix.upper()
plugin.parseStreamClient(*args,
_clientParsers:
(aname,))
_clientParsers[name](*args,
_endpointClientFactories[name](reactor,
connectProtocol(endpoint,
OneShotFactory(Factory):
endpoint.connect(OneShotFactory())
IAddress(Interface):
IConnector(Interface):
stopConnecting():
disconnect():
connect():
getDestination():
IResolverSimple(Interface):
getHostByName(name,
45)):
IResolver(IResolverSimple):
query(query,
lookupAddress(name,
lookupAddress6(name,
lookupIPV6Address(name,
lookupMailExchange(name,
lookupNameservers(name,
lookupCanonicalName(name,
lookupMailBox(name,
lookupMailGroup(name,
lookupMailRename(name,
lookupPointer(name,
lookupAuthority(name,
lookupNull(name,
lookupWellKnownServices(name,
lookupHostInfo(name,
lookupMailboxInfo(name,
lookupText(name,
lookupResponsibility(name,
lookupAFSDatabase(name,
lookupService(name,
lookupAllRecords(name,
lookupSenderPolicy(name,
timeout=
10):
lookupNamingAuthorityPointer(name,
lookupZone(name,
IReactorTCP(Interface):
listenTCP(port,
connectTCP(host,
IReactorSSL(Interface):
connectSSL(host,
listenSSL(port,
IReactorUNIX(Interface):
connectUNIX(address,
listenUNIX(address,
IReactorUNIXDatagram(Interface):
connectUNIXDatagram(address,
listenUNIXDatagram(address,
IReactorWin32Events(Interface):
addEvent(event,
action):
removeEvent(event):
IReactorUDP(Interface):
listenUDP(port,
maxPacketSize=8192):
IReactorMulticast(Interface):
listenMulticast(port,
listenMultiple=False):
IReactorSocket(Interface):
adoptStreamPort(fileDescriptor,
adoptStreamConnection(fileDescriptor,
IReactorProcess(Interface):
spawnProcess(processProtocol,
executable,
env={},
uid=None,
gid=None,
usePTY=0,
childFDs=None):
IReactorTime(Interface):
seconds():
callLater(delay,
getDelayedCalls():
IDelayedCall(Interface):
getTime():
cancel():
delay(secondsLater):
reset(secondsFromNow):
active():
IReactorThreads(Interface):
getThreadPool():
callInThread(callable,
callFromThread(callable,
suggestThreadPoolSize(size):
IReactorCore(Interface):
C{bool}
C{True}
I{during
startup}
"I{during
shutdown}
C{False}
time.")
resolve(name,
timeout=10):
run():
stop():
crash():
iterate(delay=0):
fireSystemEvent(eventType):
addSystemEventTrigger(phase,
eventType,
removeSystemEventTrigger(triggerID):
callWhenRunning(callable,
IReactorPluggableResolver(Interface):
installResolver(resolver):
IReactorDaemonize(Interface):
beforeDaemonize():
afterDaemonize():
IReactorFDSet(Interface):
addReader(reader):
addWriter(writer):
removeReader(reader):
removeWriter(writer):
removeAll():
getReaders():
getWriters():
IListeningPort(Interface):
startListening():
ILoggingContext(Interface):
logPrefix():
IFileDescriptor(ILoggingContext):
fileno():
IReadDescriptor(IFileDescriptor):
doRead():
IWriteDescriptor(IFileDescriptor):
doWrite():
IReadWriteDescriptor(IReadDescriptor,
IWriteDescriptor):
IHalfCloseableDescriptor(Interface):
writeConnectionLost(reason):
readConnectionLost(reason):
ISystemHandle(Interface):
getHandle():
IConsumer(Interface):
registerProducer(producer,
unregisterProducer():
IProducer(Interface):
IPushProducer(IProducer):
pauseProducing():
IPullProducer(IProducer):
IProtocol(Interface):
dataReceived(data):
makeConnection(transport):
connectionMade():
IProcessProtocol(Interface):
makeConnection(process):
childDataReceived(childFD,
childConnectionLost(childFD):
processExited(reason):
processEnded(reason):
IHalfCloseableProtocol(Interface):
readConnectionLost():
writeConnectionLost():
IFileDescriptorReceiver(Interface):
fileDescriptorReceived(descriptor):
IProtocolFactory(Interface):
buildProtocol(addr):
doStart():
doStop():
ITransport(Interface):
writeSequence(data):
ITCPTransport(ITransport):
loseWriteConnection():
abortConnection():
getTcpNoDelay():
setTcpNoDelay(enabled):
getTcpKeepAlive():
setTcpKeepAlive(enabled):
IUNIXTransport(ITransport):
sendFileDescriptor(descriptor):
ITLSTransport(ITCPTransport):
startTLS(contextFactory):
ISSLTransport(ITCPTransport):
getPeerCertificate():
IProcessTransport(ITransport):
pid
"From
L{IProcessProtocol.makeConnection}
"L{IProcessProtocol.processEnded}
called,
L{int}
"giving
process.
L{None}
"at
times.")
closeStdin():
closeStdout():
closeStderr():
closeChildFD(descriptor):
writeToChild(childFD,
signalProcess(signalID):
IServiceCollection(Interface):
getServiceNamed(serviceName):
addService(service):
removeService(service):
IUDPTransport(Interface):
addr=None):
connect(host,
IUNIXDatagramTransport(Interface):
address):
IUNIXDatagramConnectedTransport(Interface):
write(packet):
IMulticastTransport(Interface):
getOutgoingInterface():
setOutgoingInterface(addr):
getLoopbackMode():
setLoopbackMode(mode):
getTTL():
setTTL(ttl):
joinGroup(addr,
leaveGroup(addr,
IStreamClientEndpoint(Interface):
connect(protocolFactory):
IStreamServerEndpoint(Interface):
listen(protocolFactory):
IStreamServerEndpointStringParser(Interface):
parseStreamServer(reactor,
IStreamClientEndpointStringParser(Interface):
parseStreamClient(*args,
IRequest(Interface):
Attribute("A
used.")
"query
arguments).")
URI.")
decoded
"corresponding
C{list}s
C{str}.
C{'foo=bar&foo=baz&quux=spam'}
"for
part,
C{args}
C{{'foo':
"'quux':
['spam']}}.")
received_headers
C{requestHeaders}.
"C{requestHeaders}
C{received_headers}
"like
C{dict}
values.")
requestHeaders
"headers.")
file-like
body.
"disk,
C{StringIO},
type.
decide
per-request
basis.")
C{responseHeaders}.
Use"
"C{responseHeaders}
C{headers}
"C{dict}
nor
"does
responseHeaders
holding
sent.")
getHeader(key):
getCookie(key):
getAllHeaders():
getRequestHostname():
getClientIP():
getClient():
getUser():
getPassword():
isSecure():
getSession(sessionInterface=None):
URLPath():
prePathURL():
rememberRootURL():
getRootURL():
addCookie(k,
max_age=None,
secure=None):
setResponseCode(code,
setHeader(k,
redirect(url):
setLastModified(when):
setETag(etag):
setHost(host,
ssl=0):
ICredentialFactory(Interface):
"this
associated.
C{'basic'}
C{'digest'}.")
getChallenge(request):
decode(response,
IBodyProducer(IPushProducer):
startProducing(consumer):
IRenderable(Interface):
lookupRenderMethod(name):
render(request):
ITemplateLoader(Interface):
load():
IResponse(Interface):
three-tuple
describing
C{str},
third
C{int}.
C{('HTTP',
1)}.")
C{int}.")
C{str}.")
L{Headers}
response.")
C{int}
"response
L{UNKNOWN_LENGTH}
indicate
"many
expect.
I{HEAD}
responses,
includes
I{Content-Length}
"available
C{headers}.")
deliverBody(protocol):
_IRequestEncoder(Interface):
encode(data):
_IRequestEncoderFactory(Interface):
encoderForRequest(request):
u"twisted.web.iweb.UNKNOWN_LENGTH"
"ICredentialFactory",
"IRequest",
"IBodyProducer",
"IRenderable",
"IResponse",
"_IRequestEncoder",
"_IRequestEncoderFactory",
"UNKNOWN_LENGTH"]
os.environ['ftp_proxy']
_sourceroot
os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
'COV_CORE_CONFIG'
os.environ['COVERAGE_FILE']
'.coverage')
os.environ['COV_CORE_CONFIG']
os.environ['COV_CORE_CONFIG'])
unittest.mock
'sample_data')
get_testdata(*paths):
*paths)
'rb').read()
time,
random,
deferLater(clock,
_cancel_method():
_cancel_cb(None)
d.errback(Exception())
_cancel_cb(result):
cl.active():
cl.cancel()
d.cancel
_cancel_method
d.addBoth(_cancel_cb)
clock.callLater(delay,
twisted.internet.task
deferLater
request.args[name][0]
LeafResource(Resource):
deferRequest(self,
_cancelrequest(_):
d.cancel()
deferLater(reactor,
request.notifyFinish().addErrback(_cancelrequest)
Follow(LeafResource):
b"total",
b"show",
b"order",
b"desc")
b"maxlatency",
b"rand":
range(n,
max(n
lag
lag,
self.renderRequest,
nlist)
renderRequest(self,
nlist):
args[b"n"]
[to_bytes(str(nl))]
"<a
href='/follow?%s'>follow
%d</a><br>"
(argstr,
nl)
request.write(to_bytes(s))
Delay(LeafResource):
b"b",
request.write('')
request.write(to_bytes("Response
delayed
%0.3f
seconds\n"
n))
Status(LeafResource):
request.setResponseCode(n)
b""
Raw(LeafResource):
render_POST
render_GET
b'raw',
b'HTTP
OK\n')
request.write(raw)
request.channel.transport.loseConnection()
Echo(LeafResource):
(to_unicode(k),
[to_unicode(v)
vs])
request.requestHeaders.getAllRawHeaders()),
to_unicode(request.content.read()),
to_bytes(json.dumps(output))
Partial(LeafResource):
request.setHeader(b"Content-Length",
b"1024")
request.write(b"partial
content\n")
Drop(Partial):
b"abort",
request.write(b"this
dropped\n")
tr
request.channel.transport
hasattr(tr,
'abortConnection'):
tr.abortConnection()
tr.loseConnection()
self.putChild(b"status",
Status())
self.putChild(b"follow",
Follow())
self.putChild(b"delay",
Delay())
self.putChild(b"partial",
Partial())
self.putChild(b"drop",
Drop())
self.putChild(b"raw",
Raw())
self.putChild(b"echo",
Echo())
GzipEncoderFactory
EncodingResourceWrapper
self.putChild(b"payload",
self.putChild(b"xpayload",
EncodingResourceWrapper(PayloadResource(),
[GzipEncoderFactory()]))
b'Scrapy
server\n'
MockServer():
'tests.mockserver'],
ssl_context_factory(keyfile='keys/cert.pem',
certfile='keys/cert.pem'):
ssl.DefaultOpenSSLContextFactory(
keyfile),
certfile),
ssl_context_factory()
httpsPort
reactor.listenSSL(8999,
print_listening():
httpsHost
httpsPort.getHost()
print("Mock
http://%s:%d
https://%s:%d"
httpHost.host,
httpHost.port,
httpsHost.host,
httpsHost.port))
reactor.callWhenRunning(print_listening)
MetaSpider(Spider):
'meta'
super(MetaSpider,
self.meta
self.meta['close_reason']
FollowAllSpider(MetaSpider):
total=10,
show=20,
order="rand",
maxlatency=0.0,
super(FollowAllSpider,
self.urls_visited
self.times
'order':
order,
'maxlatency':
maxlatency}
self.urls_visited.append(response.url)
self.times.append(time.time())
Request(link.url,
DelaySpider(MetaSpider):
'delay'
n=1,
b=0,
super(DelaySpider,
self.n
self.b
"http://localhost:8998/delay?n=%s&b=%s"
(self.n,
self.b)
errback=self.errback)
errback(self,
SimpleSpider(MetaSpider):
super(SimpleSpider,
self.logger.info("Got
ItemSpider(FollowAllSpider):
super(ItemSpider,
DefaultError(Exception):
ErrorSpider(FollowAllSpider):
exception_cls
DefaultError
raise_exception(self):
self.exception_cls('Expected
exception')
super(ErrorSpider,
self.raise_exception()
BrokenStartRequestsSpider(FollowAllSpider):
fail_before_yield
fail_yielding
self.seedsseen
self.fail_before_yield:
'seed':
s}
meta={'seed':
s})
self.fail_yielding:
self.seedsseen,
'All
consumed
happened'
self.seedsseen.append(response.meta.get('seed'))
SingleRequestSpider(MetaSpider):
seed
callback_func
errback_func
isinstance(self.seed,
self.seed.replace(callback=self.parse,
Request(self.seed,
self.meta.setdefault('responses',
[]).append(response)
callable(self.callback_func):
self.callback_func(response)
'next'
response.meta['next']
self.meta['failure']
callable(self.errback_func):
self.errback_func(failure)
DuplicateStartRequestsSpider(Spider):
'duplicatestartrequests'
distinct_urls
dupe_factor
self.distinct_urls):
self.dupe_factor):
"http://localhost:8998/echo?headers=1&body=test%d"
dont_filter=self.dont_filter)
super(DuplicateStartRequestsSpider,
ItemSpider,
ErrorSpider
TestCloseSpider(TestCase):
test_closespider_itemcount(self):
get_crawler(ItemSpider,
{'CLOSESPIDER_ITEMCOUNT':
itemcount
crawler.stats.get_value('item_scraped_count')
self.assertTrue(itemcount
test_closespider_pagecount(self):
{'CLOSESPIDER_PAGECOUNT':
pagecount
crawler.stats.get_value('response_received_count')
self.assertTrue(pagecount
test_closespider_errorcount(self):
get_crawler(ErrorSpider,
{'CLOSESPIDER_ERRORCOUNT':
'spider_exceptions/{name}'\
.format(name=crawler.spider.exception_cls.__name__)
errorcount
crawler.stats.get_value(key)
self.assertTrue(errorcount
test_closespider_timeout(self):
{'CLOSESPIDER_TIMEOUT':
'closespider_timeout')
crawler.stats
stats.get_value('start_time')
stats.get_value('finish_time')
diff.seconds
diff.microseconds
self.assertTrue(total_seconds
FetchTest(ProcessTest,
'fetch'
self.execute([self.url('/text')])
'--headers'])
out.replace(b'\r',
b'Server:
TwistedWeb'
b'Content-Type:
text/plain'
ShellTest(ProcessTest,
test_empty(self):
test_response_body(self):
'response.body'])
b'Works'
test_response_type_text(self):
b'TextResponse'
test_response_type_html(self):
b'HtmlResponse'
test_response_selector_html(self):
'response.xpath("//p[@class=\'one\']/text()").extract()[0]'
xpath])
test_response_encoding_gb18030(self):
self.execute([self.url('/enc-gb18030'),
'response.encoding'])
b'gb18030')
self.execute([self.url('/redirect'),
'response.url'])
out.strip().endswith(b'/redirected')
test_request_replace(self):
self.url('/text')
"fetch('{0}')
fetch(response.request.replace(method='POST'))"
code.format(url)])
test_local_file(self):
'test_site/index.html')
test_local_nofile(self):
'file:///tests/sample_data/test_site/nothinghere.html'
self.assertIn(b'No
directory',
test_dns_failures(self):
'www.somedomainthatdoesntexi.st'
self.execute([url,
self.assertIn(b'DNS
failed',
VersionTest(ProcessTest,
'version'
self.execute([])
out.strip().decode(encoding),
test_verbose_output(self):
self.execute(['-v'])
[l.partition(":")[0].strip()
out.strip().decode(encoding).splitlines()]
['Scrapy',
'lxml',
'libxml2',
'Twisted',
'Python',
'pyOpenSSL',
'Platform'])
sleep
rmtree,
copytree
retry_on_eintr
ProjectTest(unittest.TestCase):
'testproject'
self.proj_path
self.proj_mod_path
join(self.proj_path,
rmtree(self.temp_path)
subprocess.call(args,
stdout=out,
stderr=out,
proc(self,
subprocess.Popen(args,
p.poll()
sleep(interval)
15:
p.kill()
'Command
much
complete'
StartprojectTest(ProjectTest):
test_startproject(self):
test_startproject_with_project_dir(self):
'2'))
self.call('startproject'))
project_dir,
'another_params'))
StartprojectTemplatesTest(ProjectTest):
super(StartprojectTemplatesTest,
self.tmpl
self.tmpl_proj
join(self.tmpl,
test_startproject_template_override(self):
copytree(join(scrapy.__path__[0],
'templates'),
self.tmpl)
open(join(self.tmpl_proj,
'root_template'),
'w'):
exists(join(self.tmpl_proj,
['--set',
'TEMPLATES_DIR=%s'
self.tmpl]
self.proc('startproject',
self.assertIn("New
directory"
self.assertIn(self.tmpl_proj,
CommandTest(ProjectTest):
super(CommandTest,
'%s.settings'
GenspiderCommandTest(CommandTest):
test_arguments(self):
'test_name'))
'test_name',
'test.com'))
test_template(self,
tplname='crawl'):
['--template=%s'
tplname]
tplname
self.assertIn("Created
(spname,
tplname),
self.assertTrue(exists(join(self.proj_mod_path,
'test_spider.py')))
self.assertIn("Spider
test_template_basic(self):
self.test_template('basic')
test_template_csvfeed(self):
self.test_template('csvfeed')
test_template_xmlfeed(self):
self.test_template('xmlfeed')
'--list'))
test_dump(self):
'--dump=basic'))
'-d',
'basic'))
test_same_name_as_project(self):
'%s.py'
GenspiderStandaloneCommandTest(ProjectTest):
test_generate_standalone_spider(self):
'example',
exists(join(self.temp_path,
'example.py'))
MiscCommandsTest(CommandTest):
self.call('list'))
RunSpiderCommandTest(CommandTest):
_create_file(self,
os.mkdir(tmpdir)
abspath(join(tmpdir,
f.write(content)
rmtree(tmpdir)
runspider(self,
name='myspider.py'):
self._create_file(code,
fname:
test_runspider(self):
self.runspider(spider)
Closing
test_runspider_no_spider_found(self):
self.runspider("from
Spider\n")
self.assertIn("No
file",
test_runspider_file_not_found(self):
'some_non_existent_file')
self.assertIn("File
some_non_existent_file",
test_runspider_unable_to_load(self):
self.runspider('',
'myspider.txt')
self.assertIn('Unable
load',
test_start_requests_errors(self):
self.runspider(,
name="badspider.py")
print(log)
self.assertIn("start_requests",
self.assertIn("badspider.py",
ParseCommandTest(ProcessTest,
CommandTest):
super(ParseCommandTest,
self.spider_name
'parse_spider'
'myspider.py'))
f.write(.format(self.spider_name))
f.write()
f.write(
test_spider_arguments(self):
'-a',
'test_arg=1',
test_pipelines(self):
'--pipelines',
test_parse_items(self):
self.execute(
['--spider',
self.url('/html')]
self.assertIn(,
to_native_str(out))
BenchCommandTest(CommandTest):
test_run(self):
self.proc('bench',
'LOGSTATS_INTERVAL=0.001',
'CLOSESPIDER_TIMEOUT=0.01')
self.assertIn('INFO:
Crawled',
self.assertNotIn('Unhandled
scrapy.contracts.default
UrlContract,
ScrapesContract,
ResponseMock(object):
'http://scrapy.org'
'demo_spider'
returns_request(self,
Request('http://scrapy.org',
callback=self.returns_item)
returns_item(self,
returns_dict_item(self,
{"url":
returns_fail(self,
returns_dict_fail(self,
scrapes_item_ok(self,
TestItem(name='test',
scrapes_dict_item_ok(self,
scrapes_item_fail(self,
scrapes_dict_item_fail(self,
parse_no_url(self,
ContractsManagerTest(unittest.TestCase):
[UrlContract,
ScrapesContract]
self.conman
ContractsManager(self.contracts)
TextTestResult(stream=None,
descriptions=False,
verbosity=0)
should_succeed(self):
self.assertFalse(self.results.failures)
should_fail(self):
self.assertTrue(self.results.failures)
test_contracts(self):
self.conman.extract_contracts(spider.returns_request)
self.assertEqual(len(contracts),
self.assertEqual(frozenset(type(x)
contracts),
frozenset([UrlContract,
ReturnsContract]))
self.assertNotEqual(request,
self.conman.from_method(spider.parse_no_url,
self.assertEqual(request,
test_returns(self):
self.conman.from_method(spider.returns_item,
self.conman.from_method(spider.returns_dict_item,
self.conman.from_method(spider.returns_fail,
self.conman.from_method(spider.returns_dict_fail,
test_scrapes(self):
self.conman.from_method(spider.scrapes_item_ok,
self.conman.from_method(spider.scrapes_dict_item_ok,
self.conman.from_method(spider.scrapes_item_fail,
self.conman.from_method(spider.scrapes_dict_item_fail,
DelaySpider,
BrokenStartRequestsSpider,
SingleRequestSpider,
DuplicateStartRequestsSpider
CrawlTestCase(TestCase):
self.runner
test_follow_all(self):
self.runner.create_crawler(FollowAllSpider)
self.assertEqual(len(crawler.spider.urls_visited),
start_url
test_delay(self):
_test_delay(self,
randomize):
{"DOWNLOAD_DELAY":
'RANDOMIZE_DOWNLOAD_DELAY':
randomize}
CrawlerRunner(settings).create_crawler(FollowAllSpider)
crawler.crawl(maxlatency=delay
crawler.spider.times
t[-1]
avgd
(len(t)
tolerance
randomize
self.assertTrue(avgd
tolerance),
small:
avgd)
test_timeout_success(self):
self.runner.create_crawler(DelaySpider)
test_timeout_failure(self):
CrawlerRunner({"DOWNLOAD_TIMEOUT":
0.35}).create_crawler(DelaySpider)
crawler.crawl(n=0.5,
b=1)
test_retry_503(self):
crawler.crawl("http://localhost:8998/status?n=503")
test_retry_conn_failed(self):
crawler.crawl("http://localhost:65432/status?n=503")
test_retry_dns_error(self):
mock.patch('socket.gethostbyname',
side_effect=socket.gaierror(-5,
hostname')):
crawler.crawl("http://example.com/")
test_start_requests_bug_before_yield(self):
crawler.crawl(fail_before_yield=1)
test_start_requests_bug_yielding(self):
crawler.crawl(fail_yielding=1)
test_start_requests_lazyness(self):
CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)
test_start_requests_dupes(self):
CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)
crawler.crawl(dont_filter=True,
distinct_urls=2,
dupe_factor=3)
crawler.crawl(dont_filter=False,
distinct_urls=3,
dupe_factor=4)
test_unbounded_response(self):
urlencode({'raw':
HTTP/1.1
Server:
Apache-Coyote/1.1
X-Powered-By:
Servlet
2.4;
JBoss-4.2.3.GA
(build:
SVNTag=JBoss_4_2_3_GA
date=200807181417)/JBossWeb-2.0
Set-Cookie:
JSESSIONID=08515F572832D0E659FD2B0D8031D75F;
Path=/
Pragma:
Expires:
Thu,
Jan
1970
no-store
Content-Type:
text/html;charset=UTF-8
Content-Language:
en
Date:
Tue,
Aug
2013
13:05:05
Connection:
multiples
'''})
crawler.crawl("http://localhost:8998/raw?{0}".format(query))
self.assertEqual(str(l).count("Got
200"),
test_retry_conn_lost(self):
crawler.crawl("http://localhost:8998/drop?abort=0")
test_retry_conn_aborted(self):
crawler.crawl("http://localhost:8998/drop?abort=1")
_assert_retried(self,
self.assertEqual(str(log).count("Retrying"),
self.assertEqual(str(log).count("Gave
retrying"),
test_referer_header(self):
Request('http://localhost:8998/echo?headers=1&body=0',
dont_filter=1)
req0.replace()
req0.meta['next']
req1.meta['next']
req2.meta['next']
crawler.crawl(seed=req0)
self.assertIn('responses',
self.assertNotIn('failures',
echo0
echo0['headers'])
echo1
json.loads(to_unicode(crawler.spider.meta['responses'][1].body))
self.assertEqual(echo1['headers'].get('Referer'),
[req0.url])
echo2
echo2['headers'])
echo3
json.loads(to_unicode(crawler.spider.meta['responses'][3].body))
self.assertEqual(echo3['headers'].get('Referer'),
['http://example.com'])
test_engine_status(self):
est
cb(response):
est.append(get_engine_status(crawler.engine))
crawler.crawl(seed='http://localhost:8998/',
callback_func=cb)
self.assertEqual(len(est),
est)
dict(est[0])
self.assertEqual(s['engine.spider.name'],
crawler.spider.name)
self.assertEqual(s['len(engine.scraper.slot.active)'],
test_graceful_crawl_error_handling(self):
TestError(Exception):
FaultySpider(SimpleSpider):
TestError
self.runner.create_crawler(FaultySpider)
self.assertFailure(crawler.crawl(),
TestError)
self.assertFalse(crawler.crawling)
test_crawlerrunner_accepts_crawler(self):
self.runner.crawl(crawler,
test_crawl_multiple(self):
"http://localhost:8998/status?n=503")
self.runner.join()
self._assert_retried(log)
Crawler,
CrawlerRunner,
AutoThrottle
BaseCrawlerTest(unittest.TestCase):
assertOptionIsDefault(self,
self.assertIsInstance(settings,
self.assertEqual(settings[key],
CrawlerTestCase(BaseCrawlerTest):
Settings())
self.assertIn("Crawler.spiders",
load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])
"Warn
test_populate_spidercls_settings(self):
CustomSettingsSpider(DefaultSpider):
settings.setdict(project_settings,
Crawler(CustomSettingsSpider,
self.assertEqual(crawler.settings.get('TEST1'),
self.assertEqual(crawler.settings.get('TEST2'),
self.assertEqual(crawler.settings.get('TEST3'),
self.assertFalse(settings.frozen)
self.assertTrue(crawler.settings.frozen)
test_crawler_accepts_dict(self):
self.assertEqual(crawler.settings['foo'],
test_crawler_accepts_None(self):
Crawler(DefaultSpider)
SpiderSettingsTestCase(unittest.TestCase):
test_spider_custom_settings(self):
MySpider(scrapy.Spider):
'spider'
'AUTOTHROTTLE_ENABLED':
Crawler(MySpider,
enabled_exts
[e.__class__
crawler.extensions.middlewares]
self.assertIn(AutoThrottle,
enabled_exts)
SpiderLoaderWithWrongInterface(object):
unneeded_method(self):
CustomSpiderLoader(SpiderLoader):
CrawlerRunnerTestCase(BaseCrawlerTest):
test_spider_manager_verify_interface(self):
Settings({
'SPIDER_LOADER_CLASS':
'tests.test_crawler.SpiderLoaderWithWrongInterface'
self.assertRaises(AttributeError):
CrawlerRunner(settings)
self.assertIn("SPIDER_LOADER_CLASS",
self.assertIn("scrapy.interfaces.ISpiderLoader",
test_crawler_runner_accepts_dict(self):
CrawlerRunner({'foo':
test_crawler_runner_accepts_None(self):
CrawlerRunner(Settings())
runner.spiders
self.assertIn("CrawlerRunner.spiders",
self.assertIn("CrawlerRunner.spider_loader",
load_object(runner.settings['SPIDER_LOADER_CLASS'])
test_spidermanager_deprecation(self):
CrawlerRunner({
'SPIDER_MANAGER_CLASS':
'tests.test_crawler.CustomSpiderLoader'
self.assertIsInstance(runner.spider_loader,
CustomSpiderLoader)
self.assertIn('Please
SPIDER_LOADER_CLASS',
CrawlerProcessTest(BaseCrawlerTest):
test_crawler_process_accepts_dict(self):
CrawlerProcess({'foo':
test_crawler_process_accepts_None(self):
CrawlerProcess()
ScrapyUtilsTest(unittest.TestCase):
test_required_openssl_version(self):
import_module('OpenSSL')
unittest.SkipTest("OpenSSL
available")
hasattr(module,
'__version__'):
module.__version__.split('.')[:2]]
6],
"OpenSSL
required"
twisted.cred
portal,
checkers,
scrapy.core.downloader.handlers
scrapy.core.downloader.handlers.file
FileDownloadHandler
scrapy.core.downloader.handlers.http
HTTPDownloadHandler,
scrapy.core.downloader.handlers.http10
scrapy.core.downloader.handlers.s3
get_crawler,
skip_if_no_boto
MockServer,
ssl_context_factory
DummyDH(object):
OffDH(object):
LoadTestCase(unittest.TestCase):
test_enabled_handler(self):
'tests.test_downloader_handlers.DummyDH'}
test_not_configured_handler(self):
'tests.test_downloader_handlers.OffDH'}
test_disabled_handler(self):
FileTestCase(unittest.TestCase):
self.tmpname
fd
open(self.tmpname
'^',
fd.write('0123456789')
fd.close()
FileDownloadHandler(Settings()).download_request
b'0123456789')
Request(path_to_file_uri(self.tmpname
'^'))
request.url.upper().endswith('%5E')
test_non_existent(self):
Request('file://%s'
self.mktemp())
ContentLengthHeaderResource(resource.Resource):
request.requestHeaders.getRawHeaders(b"content-length")[0]
EmptyContentTypeHeaderResource(resource.Resource):
request.setHeader("content-type",
request.content.read()
HttpTestCase(unittest.TestCase):
r.putChild(b"hang-after-headers",
ForeverTakingResource(write=True))
r.putChild(b"contentlength",
ContentLengthHeaderResource())
r.putChild(b"nocontenttype",
EmptyContentTypeHeaderResource())
reactor.listenSSL(
ssl_context_factory(self.keyfile,
self.certfile),
"%s://%s:%d/%s"
self.portno,
test_download_head(self):
test_redirect_status(self):
Request(self.getURL('redirect'))
test_redirect_status_head(self):
Request(self.getURL('redirect'),
test_timeout_download_from_spider_nodata_rcvd(self):
Request(self.getURL('wait'),
test_timeout_download_from_spider_server_hangs(self):
Request(self.getURL('hang-after-headers'),
test_host_header_not_in_request_headers(self):
to_bytes('%s:%d'
self.portno)))
self.assertEquals(request.headers,
Request(self.getURL('host'))
test_host_header_seted_in_request_headers(self):
self.assertEquals(request.headers.get('Host'),
Request(self.getURL('host'),
test_content_length_zero_bodyless_post_request_headers(self):
Request(self.getURL('contentlength'),
test_payload(self):
Request(self.getURL('payload'),
DeprecatedHttpTestCase(HttpTestCase):
Http10TestCase(HttpTestCase):
Https10TestCase(Http10TestCase):
Http11TestCase(HttpTestCase):
test_download_without_maxsize_limit(self):
test_response_class_choosing_request(self):
b'Some
bytes\0'
_test_type(response):
self.assertEquals(type(response),
Request(self.getURL('nocontenttype'),
d.addCallback(_test_type)
test_download_with_maxsize(self):
download_maxsize=10))
download_maxsize=9))
test_download_with_maxsize_per_req(self):
{'download_maxsize':
2}
test_download_with_small_maxsize_per_spider(self):
download_maxsize=2))
test_download_with_large_maxsize_per_spider(self):
download_maxsize=100))
Https11TestCase(Http11TestCase):
Https11WrongHostnameTestCase(Http11TestCase):
'keys/example-com.key.pem'
'keys/example-com.cert.pem'
Https11InvalidDNSId(Https11TestCase):
super(Https11InvalidDNSId,
Http11MockServerTestCase(unittest.TestCase):
test_download_with_content_length(self):
crawler.crawl(seed=Request(url='http://localhost:8998/partial',
1000}))
crawler.crawl(seed=Request(url='http://localhost:8998'))
test_download_gzip_response(self):
Request('http://localhost:8998/payload',
50})
request.headers.setdefault(b'Accept-Encoding',
request.replace(url='http://localhost:8998/xpayload')
PY2")
12.3.0")
UriResource(resource.Resource):
b'CONNECT':
request.uri
HttpProxyTestCase(unittest.TestCase):
site
server.Site(UriResource(),
WrappingFactory(site)
wrapper,
interface='127.0.0.1')
test_download_with_proxy(self):
b'http://example.com')
Request('http://example.com',
test_download_with_proxy_https_noconnect(self):
b'https://example.com')
'%s?noconnect'
Request('https://example.com',
test_download_without_proxy(self):
b'/path/to/resource')
Request(self.getURL('path/to/resource'))
DeprecatedHttpProxyTestCase(unittest.TestCase):
Http10ProxyTestCase(HttpProxyTestCase):
Http11ProxyTestCase(HttpProxyTestCase):
test_download_with_proxy_https_timeout(self):
'https://no-such-domain.nosuch'
http_proxy,
'download_timeout':
0.2})
self.assertIn(domain,
timeout.osError)
HttpDownloadHandlerMock(object):
S3AnonTestCase(unittest.TestCase):
self.s3reqh
httpdownloadhandler=HttpDownloadHandlerMock,
self.s3reqh.download_request
test_anon_request(self):
Request('s3://aws-publicdatasets/')
self.assertEqual(hasattr(self.s3reqh,
'anon'),
self.assertEqual(self.s3reqh.anon,
httpreq.url,
'http://aws-publicdatasets.s3.amazonaws.com/')
S3TestCase(unittest.TestCase):
'0PN5J17HBGZHT7JJ3X82'
'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'
s3reqh
self.AWS_ACCESS_KEY_ID,
httpdownloadhandler=HttpDownloadHandlerMock)
s3reqh.download_request
@contextlib.contextmanager
_mocked_date(self,
mock.patch('botocore.auth.formatdate')
mock_formatdate:
mock_formatdate.return_value
test_extra_kw(self):
extra_kw=True)
self.assertIsInstance(e,
NotConfigured))
test_request_signing1(self):
='Tue,
19:36:42
0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')
test_request_signing2(self):
21:15:45
'image/jpeg',
'94328',
0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')
test_request_signing3(self):
Request('s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy',
'Mozilla/5.0',
0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')
test_request_signing4(self):
19:44:46
Request('s3://johnsmith/?acl',
0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')
test_request_signing5(self):
'botocore
overriding
x-amz-date')
21:20:27
method='DELETE',
'x-amz-date':
21:20:26
+0000',
0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')
test_request_signing6(self):
21:06:08
Request('s3://static.johnsmith.net:8080/db-backup.dat.gz',
'curl/7.15.5',
'static.johnsmith.net:8080',
'x-amz-acl':
'application/x-download',
'4gJE4saaMU4BqNR0kLY+lw==',
'X-Amz-Meta-ReviewedBy':
'joe@johnsmith.net,jane@johnsmith.net',
'X-Amz-Meta-FileChecksum':
'0x02661779',
'X-Amz-Meta-ChecksumAlgorithm':
'crc32',
filename=database.dat',
'5913339',
0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')
test_request_signing7(self):
("s3://johnsmith/photos/my
puppy.jpg"
"?response-content-disposition=my
puppy.jpg"),
date},
httpreq.headers['Authorization'],
0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')
FTPTestCase(unittest.TestCase):
"passwd"
10.2.0
home
/home"
PY3"
FTPRealm,
FTPFactory
scrapy.core.downloader.handlers.ftp
FTPDownloadHandler
self.directory
os.mkdir(self.directory)
userdir
os.path.join(self.directory,
self.username)
os.mkdir(userdir)
FilePath(userdir)
fp.child('file.txt').setContent("I
fp.child('file
spaces.txt').setContent("Moooooooooo
realm
FTPRealm(anonymousRoot=self.directory,
userHome=self.directory)
portal.Portal(realm)
users_checker
checkers.InMemoryUsernamePasswordDatabaseDontUse()
users_checker.addUser(self.username,
p.registerChecker(users_checker,
credentials.IUsernamePassword)
self.factory
FTPFactory(portal=p)
self.factory,
self.portNum
FTPDownloadHandler(Settings())
self.addCleanup(self.port.stopListening)
_add_test_callbacks(self,
deferred,
_clean(data):
self.download_handler.client.transport.loseConnection()
deferred.addCallback(_clean)
deferred.addCallback(callback)
errback:
deferred.addErrback(errback)
test_ftp_download_success(self):
test_ftp_download_path_with_spaces(self):
url="ftp://127.0.0.1:%s/file
spaces.txt"
self.password}
'Moooooooooo
['18']})
test_ftp_download_notexist(self):
Request(url="ftp://127.0.0.1:%s/notexist.txt"
test_ftp_local_filename(self):
local_fname
"/tmp/file.txt"
self.password,
"ftp_local_filename":
local_fname})
local_fname)
['/tmp/file.txt'],
self.assertTrue(os.path.exists(local_fname))
open(local_fname)
self.assertEqual(f.read(),
"I
os.remove(local_fname)
test_invalid_credentials(self):
ConnectionLost
'invalid'})
self.assertEqual(r.type,
ConnectionLost)
errback=_test)
scrapy.core.downloader.middleware
ManagerTestCase(TestCase):
self.settings_dict)
self.mwman
DownloaderMiddlewareManager.from_crawler(self.crawler)
self.mwman.open_spider(self.spider)
self.mwman.close_spider(self.spider)
Response(request.url)
download_func(**kwargs):
isinstance(ret,
ret.raiseException()
DefaultsTest(ManagerTestCase):
test_request_response(self):
self._download(req,
"Non-response
returned")
test_3xx_and_invalid_gzipped_body_must_redirect(self):
status=302,
self._download(request=req,
redirected:
{0!r}".format(ret))
self.assertEqual(to_bytes(ret.url),
resp.headers['Location'],
header")
test_200_and_invalid_gzipped_body_must_fail(self):
self._download,
ResponseFromProcessRequestTest(ManagerTestCase):
test_download_func_not_called(self):
Response('http://example.com/index.html')
ResponseMiddleware(object):
self.mwman._add_middleware(ResponseMiddleware())
self.assertIs(results[0],
self.assertFalse(download_func.called)
AjaxCrawlMiddleware
['scrapy.downloadermiddlewares.ajaxcrawl']
AjaxCrawlMiddlewareTest(unittest.TestCase):
{'AJAXCRAWL_ENABLED':
AjaxCrawlMiddleware.from_crawler(crawler)
_ajaxcrawlable_body(self):
b'<html><head><meta
name="fragment"
content="!"/></head><body></body></html>'
_req_resp(self,
req_kwargs=None,
resp_kwargs=None):
**(req_kwargs
**(resp_kwargs
test_non_get(self):
{'method':
test_binary_response(self):
Request('http://example.com/')
Response('http://example.com/',
body=b'foobar\x00\x01\x02',
test_ajaxcrawl(self):
self._req_resp(
{'meta':
'bar'}},
self._ajaxcrawlable_body()}
self.assertEqual(req2.meta['foo'],
test_ajaxcrawl_loop(self):
self._ajaxcrawlable_body()})
body=resp.body,
request=req2)
resp2,
isinstance(resp3,
(resp3.__class__,
resp3)
self.assertEqual(resp3.request.url,
test_noncrawlable_body(self):
b'<html></html>'})
CookiesMiddlewareTest(TestCase):
assertCookieValEqual(self,
cookievaleq
cv:
re.split(';\s*',
cv.decode('latin1'))
sorted(cookievaleq(first)),
sorted(cookievaleq(second)),
CookiesMiddleware()
self.assertEquals(req2.headers.get('Cookie'),
b"C1=value1")
test_setting_false_cookies_enabled(self):
CookiesMiddleware.from_crawler,
test_setting_default_cookies_enabled(self):
CookiesMiddleware.from_crawler(get_crawler()),
test_setting_true_cookies_enabled(self):
CookiesMiddleware.from_crawler(
test_setting_enabled_cookies_debug(self):
l.check(
http://scrapytest.org/>\n'
'Set-Cookie:
C1=value1;
path=/\n'),
'Sending
http://scrapytest.org/sub1/>\n'
'Cookie:
C1=value1\n'),
test_setting_disabled_cookies_debug(self):
l.check()
test_do_not_break_on_non_utf8_header(self):
b'C1=in\xa3valid;
path=/',
'Other':
b'ignore\xa3me'}
req2.headers)
test_dont_merge_cookies(self):
Request('http://scrapytest.org/dontmerge',
Response('http://scrapytest.org/dontmerge',
'dont=mergeme;
Request('http://scrapytest.org/mergeme')
Request('http://scrapytest.org/mergeme',
0})
test_complex_cookies(self):
'C1',
'C2',
'/bar',
'C3',
'value3',
'C4',
'value4',
'scrapy.org'}]
cookies=cookies)
Request('http://scrapytest.org/foo')
req.headers.get('Cookie')
(b'C1=value1;
C3=value3',
b'C3=value3;
C1=value1')
Request('http://scrapytest.org/bar')
b'C2=value2')
Request('http://scrapytest.org/baz')
test_merge_request_cookies(self):
'salada'})
b"C1=value1;
galleta=salada")
test_cookiejar_key(self):
'salada'},
"store1"})
meta=res.meta)
b'C1=value1;
galleta=salada')
'dulce'},
"store2"})
self.mw.process_request(req3,
self.assertEquals(req3.headers.get('Cookie'),
b'galleta=dulce')
'C2=value2;
request=req3)
self.mw.process_response(req3,
res2,
req4
meta=res2.meta)
self.mw.process_request(req4,
self.assertCookieValEqual(req4.headers.get('Cookie'),
b'C2=value2;
galleta=dulce')
req5_1
Request('http://scrapytest.org:1104/')
self.mw.process_request(req5_1,
Response('http://scrapytest.org:1104/',
request=req5_1)
self.mw.process_response(req5_1,
res5_1,
req5_2
Request('http://scrapytest.org:1104/some-redirected-path')
self.mw.process_request(req5_2,
self.assertEquals(req5_2.headers.get('Cookie'),
req5_3
Request('http://scrapytest.org/some-redirected-path')
self.mw.process_request(req5_3,
self.assertEquals(req5_3.headers.get('Cookie'),
req6
Request('file:///scrapy/sometempfile')
self.mw.process_request(req6,
self.assertEquals(req6.headers.get('Cookie'),
test_local_domain(self):
Request("http://example-host/",
cookies={'currencyCookie':
'USD'})
self.assertEqual(b'currencyCookie=USD',
request.headers['Cookie'])
assert_samelines
_test_data(formats):
uncompressed_body
'feed-sample1.xml')
'feed-sample1.'
test_responses[format]
Response('http://foo.com/bar',
DecompressionMiddlewareTest(TestCase):
test_formats
['tar',
'xml.bz2',
'xml.gz',
'zip']
_test_data(test_formats)
DecompressionMiddleware()
test_known_compression_formats(self):
self.test_formats:
self.test_responses[fmt]
isinstance(new,
'Failed
(fmt,
type(new).__name__)
self.uncompressed_body,
fmt)
test_plain_response(self):
body=self.uncompressed_body)
rsp.body)
test_empty_response(self):
rsp.body
new.body
DefaultHeadersMiddleware
get_defaults_spider_mw(self):
to_bytes(k):
[to_bytes(v)]
crawler.settings.get('DEFAULT_REQUEST_HEADERS').items()
DefaultHeadersMiddleware.from_crawler(crawler)
Request('http://www.scrapytest.org')
test_update_headers(self):
{'Accept-Language':
['es'],
'Test-Header':
['test']}
bytes_headers
{b'Accept-Language':
[b'es'],
b'Test-Header':
[b'test']}
Request('http://www.scrapytest.org',
bytes_headers)
defaults.update(bytes_headers)
DownloadTimeoutMiddleware
DownloadTimeoutMiddlewareTest(unittest.TestCase):
get_request_spider_mw(self,
DownloadTimeoutMiddleware.from_crawler(crawler)
test_default_download_timeout(self):
test_string_download_timeout(self):
self.get_request_spider_mw({'DOWNLOAD_TIMEOUT':
'20.1'})
20.1)
test_spider_has_download_timeout(self):
test_request_has_download_timeout(self):
req.meta['download_timeout']
HttpAuthMiddleware
http_user
http_pass
HttpAuthMiddlewareTest(unittest.TestCase):
HttpAuthMiddleware()
TestSpider('foo')
test_auth(self):
Zm9vOmJhcg==')
test_auth_already_set(self):
headers=dict(Authorization='Digest
123'))
b'Digest
123')
HttpCacheMiddleware
_BaseTest(unittest.TestCase):
self.yesterday
self.today
email.utils.formatdate()
self.tomorrow
self.crawler._create_spider('example.com')
Request('http://www.example.com',
'test'})
Response('http://www.example.com',
'text/html'},
body=b'test
body',
status=202)
shutil.rmtree(self.tmpdir)
'HTTPCACHE_ENABLED':
'HTTPCACHE_DIR':
self.tmpdir,
'HTTPCACHE_EXPIRATION_SECS':
'HTTPCACHE_IGNORE_HTTP_CODES':
'HTTPCACHE_POLICY':
self.policy_class,
'HTTPCACHE_STORAGE':
self.storage_class,
settings.update(new_settings)
_storage(self,
mw.storage
_policy(self,
mw.policy
_middleware(self,
self._get_settings(**new_settings)
HttpCacheMiddleware(settings,
self.crawler.stats)
mw.spider_opened(self.spider)
mw.spider_closed(self.spider)
assertEqualResponse(self,
response1,
response2):
self.assertEqual(response1.url,
response2.url)
self.assertEqual(response1.status,
response2.status)
self.assertEqual(response1.headers,
response2.headers)
self.assertEqual(response1.body,
response2.body)
assertEqualRequest(self,
self.assertEqual(request1.headers,
request2.headers)
assertEqualRequestButWithCacheValidators(self,
b'If-None-Match'
b'If-Modified-Since'
any(h
request2.headers
(b'If-None-Match',
b'If-Modified-Since'))
test_dont_cache(self):
self.assertEqual(mw.storage.retrieve_response(self.spider,
mw.policy.should_cache_response(self.response,
self.request):
self.assertIsInstance(mw.storage.retrieve_response(self.spider,
self.response.__class__)
DefaultStorageTest(_BaseTest):
test_storage(self):
self.request.copy()
isinstance(response2,
content-type
response2)
test_storage_never_expire(self):
self._storage(HTTPCACHE_EXPIRATION_SECS=0)
time.sleep(0.5)
give
chance
DbmStorageTest(DefaultStorageTest):
DbmStorageWithCustomDbmModuleTest(DbmStorageTest):
dbm_module
'tests.mocks.dummydbm'
new_settings.setdefault('HTTPCACHE_DBM_MODULE',
super(DbmStorageWithCustomDbmModuleTest,
test_custom_dbm_module_loaded(self):
self.assertEqual(storage.dbmodule.__name__,
FilesystemStorageTest(DefaultStorageTest):
FilesystemStorageGzipTest(FilesystemStorageTest):
new_settings.setdefault('HTTPCACHE_GZIP',
super(FilesystemStorageTest,
LeveldbStorageTest(DefaultStorageTest):
pytest.importorskip('leveldb')
'scrapy.extensions.httpcache.LeveldbCacheStorage'
DummyPolicyTest(_BaseTest):
test_middleware(self):
test_different_request_response_urls(self):
Request('http://host.com/path')
Response('http://host2.net/test.html')
test_middleware_ignore_missing(self):
self._middleware(HTTPCACHE_IGNORE_MISSING=True)
mw.process_request,
test_middleware_ignore_schemes(self):
Request('http://test.com/'),
Response('http://test.com/')
Request('file:///tmp/t.txt'),
Response('file:///tmp/t.txt')
Request('s3://bucket/key'),
Response('http://bucket/key')
Request('s3://bucket/key2'),
Response('http://bucket/key2')
self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3'])
test_middleware_ignore_http_codes(self):
self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202])
self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203])
RFC2616PolicyTest(DefaultStorageTest):
_process_requestresponse(self,
mw,
mw.process_request(request,
Response))
mw.process_response(request,
print('Request',
print('Response',
print('Result',
test_request_cacheability(self):
self.tomorrow})
'no-store'})
'no-cache'})
req1)
res0.replace(body=b'foo')
req2,
test_response_cacheability(self):
'bar'}),
'max-age=3600'}),
'no-store,
max-age=300'}),
'no-store',
307,
401,
'public,
max-age=600'}),
(shouldcache,
self._middleware(HTTPCACHE_ALWAYS_STORE=True)
'no-store'
Request('http://example2-%d.com'
test_cached_and_fresh(self):
'max-age=86405'}),
'max-age=300'}),
'86405',
'max-age='
3),
'86400',
'public',
1)}),
(308,
'max-age=0'})
mw.process_request(req1,
res304)
test_cached_and_stale(self):
'max-age=86400',
'no-cache,must-revalidate',
'must-revalidate',
'max-age=86400,must-revalidate',
res0a
res0a.replace(body=b'bar')
'ETag'
'Last-Modified'
res0c
res0b.replace(status=304)
res0c)
self.assertEqualResponse(res3,
res0d
res0b.replace(status=500)
'max-stale'})
'no-cache'
self.yesterday})
Request(self.request.url)
mw.DOWNLOAD_EXCEPTIONS:
e('foo'),
self.assertEqualResponse(res0,
Exception('foo'),
test_ignore_response_cache_controls(self):
'no-store,max-age=86405'}),
'max-age=300,no-cache'}),
self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache',
'no-store'])
HttpCompressionMiddleware
FORMAT
'gzip':
'x-gzip':
'rawdeflate':
('html-rawdeflate.bin',
'zlibdeflate':
('html-zlibdeflate.bin',
HttpCompressionTest(TestCase):
HttpCompressionMiddleware()
_getresponse(self,
coding):
coding
FORMAT:
ValueError()
samplefile,
contentencoding
FORMAT[coding]
samplefile),
sample:
sample.read()
'Server':
'Yaws/1.49
Yet
Another
Server',
'Sun,
08
2009
00:41:03
GMT',
len(body),
contentencoding,
headers={'Accept-Encoding':
'gzip,deflate'})
'Accept-Encoding'
request.headers
self.assertEqual(request.headers.get('Accept-Encoding'),
test_process_response_gzip(self):
test_process_response_rawdeflate(self):
self._getresponse('rawdeflate')
test_process_response_zlibdelate(self):
self._getresponse('zlibdeflate')
test_process_response_plain(self):
Response('http://scrapytest.org',
body=b'<!DOCTYPE...')
response.headers.get('Content-Encoding')
test_multipleencodings(self):
['uuencode',
'gzip']
self.assertEqual(newresponse.headers.getlist('Content-Encoding'),
[b'uuencode'])
test_process_response_encoding_inside_body(self):
Response("http;//www.example.com/",
test_process_response_force_recalculate_encoding(self):
HtmlResponse("http;//www.example.com/page.html",
test_process_response_gzipped_contenttype(self):
b'application/gzip')
test_process_response_gzip_app_octetstream_contenttype(self):
b'application/octet-stream')
test_process_response_gzip_binary_octetstream_contenttype(self):
self._getresponse('x-gzip')
'binary/octet-stream'
b'binary/octet-stream')
test_process_response_head_request_no_decode_required(self):
response.replace(body
HttpProxyMiddleware
failureException
test_no_proxies(self):
HttpProxyMiddleware)
test_no_enviroment_proxies(self):
{'dummy_proxy':
'reset_env_and_do_not_raise'}
('http://e.com',
'https://e.com',
'file:///tmp/a'):
test_enviroment_proxies(self):
https_proxy
'http://proxy.for.https:8080'
os.environ.pop('file_proxy',
[('http://e.com',
http_proxy),
('https://e.com',
https_proxy),
('file://tmp/a',
None)]:
self.assertEquals(req.meta.get('proxy'),
proxy)
test_proxy_auth(self):
'https://user:pass@proxy:3128'
dXNlcjpwYXNz')
test_proxy_auth_empty_passwd(self):
'https://user:@proxy:3128'
dXNlcjo=')
test_proxy_auth_encoding(self):
u'https://m\u00E1n:pass@proxy:3128'
HttpProxyMiddleware(auth_encoding='utf-8')
bcOhbjpwYXNz')
HttpProxyMiddleware(auth_encoding='latin-1')
beFuOnBhc3M=')
test_proxy_already_seted(self):
Request('http://noproxy.com',
req.meta['proxy']
test_no_proxy(self):
'other.com'
'other.com,noproxy.com'
RedirectMiddleware,
MetaRefreshMiddleware
RedirectMiddlewareTest(unittest.TestCase):
RedirectMiddleware.from_crawler(self.crawler)
Response('http://a.com',
'http://a.com/redirected'},
test_redirect_301(self):
_test(method):
_test('GET')
_test('POST')
_test('HEAD')
test_dont_redirect(self):
Response(url2,
test_redirect_302(self):
test_redirect_302_head(self):
Request('http://scrapytest.org/302')
Response('http://scrapytest.org/302',
Response('http://www.scrapytest.org/302',
Response('http://scrapytest.org/redirected',
'/redirected2'},
test_spider_handling(self):
smartspider
self.crawler._create_spider('smarty')
smartspider.handle_httpstatus_list
302]
smartspider)
test_request_meta_handling(self):
_test_passthrough(req):
status=301,
302]}))
meta={'handle_httpstatus_all':
test_latin1_location(self):
latin1_location
u'/ação'.encode('latin1')
historically
latin1
latin1_location},
'http://scrapytest.org/a%C3%A7%C3%A3o'
test_location_with_wrong_encoding(self):
utf8_location
u'/ação'
(utf-8)
utf8_location},
'http://scrapytest.org/a%C3%83%C2%A7%C3%83%C2%A3o'
MetaRefreshMiddlewareTest(unittest.TestCase):
MetaRefreshMiddleware.from_crawler(crawler)
_body(self,
interval=5,
url='http://example.org/newpage'):
html.format(interval,
url).encode('utf-8')
test_meta_refresh(self):
test_meta_refresh_with_high_interval(self):
HtmlResponse(url='http://example.org',
body=self._body(interval=1000),
test_meta_refresh_trough_posted_request(self):
Request(url='http://example.org',
Request('http://scrapytest.org/max')
HtmlResponse(req1.url,
body=self._body(url='/redirected'))
body=self._body(url='/redirected2'))
isinstance(req3,
RetryMiddleware
RetryTest(unittest.TestCase):
RetryMiddleware.from_crawler(crawler)
self.mw.max_retry_times
test_404(self):
Request('http://www.scrapytest.org/404')
Response('http://www.scrapytest.org/404',
status=404)
test_dont_retry(self):
Response('http://www.scrapytest.org/503')
test_dont_retry_exc(self):
DNSLookupError(),
test_503(self):
test_twistederrors(self):
[defer.TimeoutError,
ConnectionLost]
http11
exceptions.append(ResponseFailed)
exceptions:
Request('http://www.scrapytest.org/%s'
exc.__name__)
self._test_retry_exception(req,
exc('foo'))
_test_retry_exception(self,
self.assertEqual(req,
(RobotsTxtMiddleware,
mw_module_logger)
RobotsTxtMiddlewareTest(unittest.TestCase):
self.crawler.engine.download
test_robotstxt_settings(self):
self.crawler.settings.set('USER_AGENT',
'CustomAgent')
RobotsTxtMiddleware,
self.crawler)
_get_successful_crawler(self):
ROBOTS
re.sub(b'^\s+(?m)',
User-Agent:
/admin/
/static/
TextResponse('http://site.local/robots.txt',
body=ROBOTS)
test_robotstxt(self):
self.assertIgnored(Request('http://site.local/admin/main'),
self.assertIgnored(Request('http://site.local/static/'),
test_robotstxt_ready_parser(self):
middleware))
test_robotstxt_meta(self):
{'dont_obey_robotstxt':
self.assertNotIgnored(Request('http://site.local/allowed',
self.assertNotIgnored(Request('http://site.local/admin/main',
self.assertNotIgnored(Request('http://site.local/static/',
_get_garbage_crawler(self):
Response('http://site.local/robots.txt',
body=b'GIF89a\xd3\x00\xfe\x00\xa2')
test_robotstxt_garbage(self):
RobotsTxtMiddleware(self._get_garbage_crawler())
_get_emptybody_crawler(self):
Response('http://site.local/robots.txt')
test_robotstxt_empty_response(self):
RobotsTxtMiddleware(self._get_emptybody_crawler())
test_robotstxt_error(self):
return_failure(request,
failure.Failure(err))
return_failure
middleware._logerror
mock.MagicMock(side_effect=middleware._logerror)
middleware.process_request(Request('http://site.local'),
deferred.addCallback(lambda
self.assertTrue(middleware._logerror.called))
test_robotstxt_immediate_error(self):
immediate_failure(request,
deferred.errback(failure.Failure(err))
immediate_failure
test_ignore_robotstxt_request(self):
ignore_request(request,
failure.Failure(IgnoreRequest()))
ignore_request
mw_module_logger.error
self.assertFalse(mw_module_logger.error.called))
assertNotIgnored(self,
maybeDeferred(middleware.process_request,
dfd.addCallback(self.assertIsNone)
assertIgnored(self,
self.assertFailure(maybeDeferred(middleware.process_request,
spider),
IgnoreRequest)
DownloaderStats
TestDownloaderStats(TestCase):
self.crawler._create_spider('scrapytest.org')
DownloaderStats(self.crawler.stats)
self.res
Response('scrapytest.org',
assertStatsEqual(self,
self.crawler.stats.get_value(key,
spider=self.spider),
str(self.crawler.stats.get_stats(self.spider))
self.mw.process_request(self.req,
self.assertStatsEqual('downloader/request_count',
test_process_response(self):
self.mw.process_response(self.req,
self.res,
self.assertStatsEqual('downloader/response_count',
self.mw.process_exception(self.req,
MyException(),
self.assertStatsEqual('downloader/exception_count',
self.assertStatsEqual(
'downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException',
UserAgentMiddleware
UserAgentMiddlewareTest(TestCase):
get_spider_and_mw(self,
default_useragent):
{'USER_AGENT':
default_useragent})
UserAgentMiddleware.from_crawler(crawler)
test_default_agent(self):
b'default_useragent')
test_remove_agent(self):
req.headers.get('User-Agent')
test_spider_agent(self):
b'spider_useragent')
test_header_agent(self):
'header_useragent'})
b'header_useragent')
test_no_agent(self):
self.get_spider_and_mw(None)
'User-Agent'
RFPDupeFilter
RFPDupeFilterTest(unittest.TestCase):
test_filter(self):
dupefilter.request_seen(r3)
test_dupefilter_path(self):
df
df.open()
df.close('finished')
df2
df2.open()
df2.request_seen(r1)
df2.close('finished')
Request('http://scrapytest.org/index.html')
Request('http://scrapytest.org/INDEX.html')
CaseInsensitiveRFPDupeFilter(RFPDupeFilter):
fp.update(to_bytes(request.url.lower()))
case_insensitive_dupefilter
CaseInsensitiveRFPDupeFilter()
case_insensitive_dupefilter.open()
case_insensitive_dupefilter.request_seen(r1)
case_insensitive_dupefilter.request_seen(r2)
case_insensitive_dupefilter.close('finished')
"scrapytest.org"
["scrapytest.org",
"localhost"]
itemurl_re
re.compile("item\d+.html")
name_re
re.compile("<h1>(.*?)</h1>",
price_re
re.compile(">Price:
\$(.*?)<",
xlink
itemre
re.compile(self.itemurl_re)
xlink.extract_links(response):
itemre.search(link.url):
callback=self.parse_item)
self.item_cls()
self.name_re.search(response.text)
item['url']
self.price_re.search(response.text)
item['price']
TestDupeFilterSpider(TestSpider):
dont_filter=False
DictItemsSpider(TestSpider):
start_test_site(debug=False):
root_dir
"test_site")
static.File(root_dir)
server.Site(r),
debug:
print("Test
http://localhost:%d/
Ctrl-C
finish."
CrawlerRun(object):
spider_class):
self.respplug
self.reqplug
self.reqdropped
self.itemresp
self.signals_catched
self.spider_class
start_test_site()
[self.geturl("/"),
self.geturl("/redirect"),
self.geturl("/redirect")]
dispatcher.connect(self.record_signal,
signal)
get_crawler(self.spider_class)
self.crawler.signals.connect(self.item_scraped,
self.crawler.signals.connect(self.request_scheduled,
signals.request_scheduled)
self.crawler.signals.connect(self.request_dropped,
signals.request_dropped)
self.crawler.signals.connect(self.response_downloaded,
signals.response_downloaded)
self.crawler.crawl(start_urls=start_urls)
dispatcher.connect(self.stop,
self.deferred.callback(None)
geturl(self,
"http://localhost:%s%s"
getpath(self,
self.itemresp.append((item,
request_scheduled(self,
self.reqplug.append((request,
request_dropped(self,
self.reqdropped.append((request,
response_downloaded(self,
self.respplug.append((response,
record_signal(self,
kwargs.copy()
signalargs.pop('signal')
signalargs.pop('sender',
self.signals_catched[sig]
EngineTest(unittest.TestCase):
test_crawler(self):
TestSpider,
DictItemsSpider:
CrawlerRun(spider)
self._assert_visited_urls()
self._assert_scheduled_requests(urls_to_visit=8)
self._assert_downloaded_responses()
self._assert_scraped_items()
self._assert_signals_catched()
CrawlerRun(TestDupeFilterSpider)
self._assert_scheduled_requests(urls_to_visit=7)
self._assert_dropped_requests()
_assert_visited_urls(self):
must_be_visited
["/",
"/redirect",
"/redirected",
"/item1.html",
"/item2.html",
"/item999.html"]
urls_visited
set([rp[0].url
self.run.respplug])
must_be_visited])
urls_visited,
"URLs
visited:
list(urls_expected
urls_visited)
_assert_scheduled_requests(self,
urls_to_visit=None):
self.assertEqual(urls_to_visit,
len(self.run.reqplug))
paths_expected
['/item999.html',
'/item2.html',
'/item1.html']
set([rq[0].url
rq
self.run.reqplug])
paths_expected])
scheduled_requests_count
len(self.run.reqplug)
len(self.run.reqdropped)
responses_count
len(self.run.respplug)
self.assertEqual(scheduled_requests_count,
responses_count)
_assert_dropped_requests(self):
self.assertEqual(len(self.run.reqdropped),
_assert_downloaded_responses(self):
self.assertEqual(8,
len(self.run.respplug))
self.run.respplug:
'/item999.html':
self.assertEqual(404,
'/redirect':
self.assertEqual(302,
_assert_scraped_items(self):
len(self.run.itemresp))
self.run.itemresp:
'item1.html'
self.assertEqual('100',
'item2.html'
self.assertEqual('200',
_assert_signals_catched(self):
signals.engine_started
signals.engine_stopped
signals.spider_opened
signals.spider_idle
signals.spider_closed
self.run.signals_catched[signals.spider_opened])
self.run.signals_catched[signals.spider_idle])
self.run.signals_catched[signals.spider_closed].pop('spider_stats',
XXX:
0.17
self.run.spider,
'finished'},
self.run.signals_catched[signals.spider_closed])
test_close_downloader(self):
test_close_spiders_downloader(self):
test_close_engine_spiders_downloader(self):
e.start()
self.assertTrue(e.running)
self.assertFalse(e.running)
len(sys.argv)
sys.argv[1]
'runserver':
start_test_site(debug=True)
BaseItemExporter,
PprintItemExporter,
PickleItemExporter,
CsvItemExporter,
XmlItemExporter,
JsonLinesItemExporter,
JsonItemExporter,
PythonItemExporter,
MarshalItemExporter
BaseItemExporterTest(unittest.TestCase):
self.i
BaseItemExporter(**kwargs)
_assert_expected_item(self,
exported_dict):
exported_dict.items():
exported_dict[k]
to_unicode(v)
self.assertEqual(self.i,
exported_dict)
_get_nonstring_types_item(self):
'time':
datetime(2015,
'float':
assertItemExportWorks(self,
self.ie.__class__
BaseItemExporter:
test_export_item(self):
self.assertItemExportWorks(self.i)
test_export_dict_item(self):
self.assertItemExportWorks(dict(self.i))
test_serialize_field(self):
self.ie.serialize_field(self.i.fields['name'],
self.i['name'])
self.ie.serialize_field(self.i.fields['age'],
self.i['age'])
u'22')
test_fields_to_export(self):
self._get_exporter(fields_to_export=['name'])
self.assertEqual(list(ie._get_serialized_fields(self.i)),
[('name',
u'John\xa3')])
self._get_exporter(fields_to_export=['name'],
list(ie._get_serialized_fields(self.i))[0]
test_field_custom_serializer(self):
custom_serializer(value):
CustomFieldItem(Item):
Field(serializer=custom_serializer)
CustomFieldItem(name=u'John\xa3',
'24')
PythonItemExporterTest(BaseItemExporterTest):
PythonItemExporter(binary=False,
test_invalid_option(self):
invalid_option"):
PythonItemExporter(invalid_option='something')
self.assertEqual(type(exported),
u'Joseph'},
u'Maria'},
self.assertEqual(type(exported['age']),
self.assertEqual(type(exported['age']['age']),
test_export_list(self):
test_export_item_dict_list(self):
test_export_binary(self):
PythonItemExporter(binary=True)
{b'name':
b'John\xc2\xa3',
b'age':
b'22'}
exporter.export_item(value))
PprintItemExporterTest(BaseItemExporterTest):
PprintItemExporter(self.output,
self._assert_expected_item(eval(self.output.getvalue()))
PickleItemExporterTest(BaseItemExporterTest):
PickleItemExporter(self.output,
self._assert_expected_item(pickle.loads(self.output.getvalue()))
test_export_multiple_items(self):
TestItem(name='hello',
TestItem(name='bye',
PickleItemExporter(f)
ie.export_item(i1)
ie.export_item(i2)
i1)
PickleItemExporter(fp)
self.assertEqual(pickle.loads(fp.getvalue()),
MarshalItemExporterTest(BaseItemExporterTest):
MarshalItemExporter(self.output,
self.output.seek(0)
self._assert_expected_item(marshal.load(self.output))
item.pop('time')
marshallable
MarshalItemExporter(fp)
self.assertEqual(marshal.load(fp),
CsvItemExporterTest(BaseItemExporterTest):
CsvItemExporter(self.output,
assertCsvEqual(self,
to_unicode(first)
to_unicode(second)
csvsplit
csv:
[sorted(re.split(r'(,|\s+)',
csv.splitlines(True)]
self.assertEqual(csvsplit(first),
csvsplit(second),
self.assertCsvEqual(to_unicode(self.output.getvalue()),
u'age,name\r\n22,John\xa3\r\n')
CsvItemExporter(fp,
self.assertCsvEqual(fp.getvalue(),
test_header_export_all(self):
item=self.i,
fields_to_export=self.i.fields.keys(),
test_header_export_all_dict(self):
item=dict(self.i),
test_header_export_single_field(self):
fields_to_export=['age'],
expected=b'age\r\n22\r\n',
test_header_export_two_items(self):
CsvItemExporter(output)
self.assertCsvEqual(output.getvalue(),
b'age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n')
test_header_no_header_line(self):
expected=b'22,John\xc2\xa3\r\n',
test_join_multivalue(self):
TestItem2(Item):
friends
TestItem2,
item=cls(name='John',
friends=['Mary',
'Paul']),
expected='"Mary,Paul",John\r\n',
test_join_multivalue_not_strings(self):
item=dict(name='John',
friends=[4,
8]),
expected='"[4,
8]",John\r\n',
item=self._get_nonstring_types_item(),
expected='22,False,3.14,2015-01-01
01:01:01\r\n'
XmlItemExporterTest(BaseItemExporterTest):
XmlItemExporter(self.output,
assertXmlEquivalent(self,
xmltuple(elem):
children
list(elem.iterchildren())
children:
[(child.tag,
sorted(xmltuple(child)))
children]
[(elem.tag,
[(elem.text,
())])]
xmlsplit(xmlcontent):
lxml.etree.fromstring(xmlcontent)
xmltuple(doc)
self.assertEqual(xmlsplit(first),
xmlsplit(second),
XmlItemExporter(fp)
self.assertXmlEquivalent(fp.getvalue(),
encoding="utf-8"?>\n<items><item><age>22</age><name>John\xc2\xa3</name></item></items>'
self.assertXmlEquivalent(self.output.getvalue(),
test_multivalued_fields(self):
TestItem(name=[u'John\xa3',
u'Doe']),
encoding="utf-8"?>\n<items><item><name><value>John\xc2\xa3</value><value>Doe</value></name></item></items>'
TestItem(name=u'foo\xa3hoo',
b'<age>22</age>'
b'<name>foo\xc2\xa3hoo</name>'
b'<name>bar</name>'
test_nested_list_item(self):
TestItem(name=u'foo')
v2={"egg":
["spam"]})
age=[i1,
i2])
b'<value><name>foo</name></value>'
b'<value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>'
self.assertExportResult(item,
b'<float>3.14</float>'
b'<boolean>False</boolean>'
b'<number>22</number>'
b'<time>2015-01-01
01:01:01</time>'
JsonLinesItemExporterTest(BaseItemExporterTest):
'Joseph',
'22'}}}
JsonLinesItemExporter(self.output,
dict(self.i))
self._expected_nested)
test_extra_keywords(self):
self._get_exporter(sort_keys=True)
self.test_export_item()
self._get_exporter,
foo_unknown_keyword_bar=True)
JsonItemExporterTest(JsonLinesItemExporterTest):
[JsonLinesItemExporterTest._expected_nested]
JsonItemExporter(self.output,
[dict(self.i)])
assertTwoItemsExported(self,
[dict(item),
dict(item)])
test_two_items(self):
self.assertTwoItemsExported(self.i)
test_two_dict_items(self):
self.assertTwoItemsExported(dict(self.i))
TestItem(name=u'Joseph\xa3',
dict(i1)}}
test_nested_dict_item(self):
dict(name=u'Joseph\xa3',
dict(name=u'Jesus',
i1}}
[item])
CustomItemExporterTest(unittest.TestCase):
test_exporter_custom_serializer(self):
CustomItemExporter(BaseItemExporter):
super(CustomItemExporter,
self).serialize_field(field,
TestItem(name=u'John',
CustomItemExporter()
u'John',
'22'}
i2['name']),
i2['age']),
IFeedStorage,
FileFeedStorage,
FTPFeedStorage,
S3FeedStorage,
StdoutFeedStorage,
BlockingFeedStorage)
get_s3_content_and_delete,
FileFeedStorageTest(unittest.TestCase):
test_store_file_uri(self):
test_store_file_uri_makedirs(self):
'paths',
'file.txt')
test_store_direct_path(self):
test_store_direct_path_relative(self):
FileFeedStorage(path)
FTPFeedStorageTest(unittest.TestCase):
os.environ.get('FEEDTEST_FTP_URI')
os.environ.get('FEEDTEST_FTP_PATH')
(uri
FTPFeedStorage(uri)
self._assert_stores(st,
storage.store(BytesIO(b"new
content"))
b"new
content")
BlockingFeedStorageTest(unittest.TestCase):
get_test_spider(self,
get_crawler(settings_dict=settings)
TestSpider.from_crawler(crawler)
test_default_temp_dir(self):
b.open(self.get_test_spider())
tempfile.gettempdir())
test_temp_file(self):
tests_path})
b.open(spider)
tests_path)
test_invalid_folder(self):
invalid_path
os.path.join(tests_path,
'invalid_path')
invalid_path})
self.assertRaises(OSError,
b.open,
S3FeedStorageTest(unittest.TestCase):
S3FeedStorage(uri)
storage)
expected_content
b"content:
file.write(expected_content)
get_s3_content_and_delete(u.hostname,
u.path[1:])
expected_content)
StdoutFeedStorageTest(unittest.TestCase):
StdoutFeedStorage('stdout:',
_stdout=out)
self.assertEqual(out.getvalue(),
FeedExportTest(unittest.TestCase):
MyItem(scrapy.Item):
baz
run_and_export(self,
spider_cls,
res_name
'/res'
'FEED_URI':
res_name,
'FEED_FORMAT':
'csv',
defaults.update(settings
CrawlerRunner(Settings(defaults))
runner.crawl(spider_cls)
open(res_name,
defer.returnValue(f.read())
shutil.rmtree(tmpdir)
exported_data(self,
'testspider'
['http://localhost:8998/']
self.run_and_export(TestSpider,
defer.returnValue(data)
assertExportedCsv(self,
'csv'})
csv.DictReader(to_native_str(data).splitlines())
list(reader)
ordered:
self.assertEqual(reader.fieldnames,
header)
self.assertEqual(set(reader.fieldnames),
set(header))
assertExportedJsonLines(self,
'jl'})
[json.loads(to_native_str(line))
data.splitlines()]
parsed)
assertExportedXml(self,
'xml'})
lxml.etree.fromstring(data)
[{e.tag:
e.text
it}
root.findall('item')]
_load_until_eof(self,
load_func):
bytes_output
result.append(load_func(bytes_output))
assertExportedPickle(self,
'pickle'})
load_func=pickle.load)
assertExportedMarshal(self,
'marshal'})
load_func=marshal.load)
assertExported(self,
ordered)
self.assertExportedXml(items,
self.assertExportedPickle(items,
test_export_items(self):
test_export_multiple_item_classes(self):
MyItem2(scrapy.Item):
MyItem2({'hello':
'world2',
'bar2'}),
'quux3'}),
{'hello':
'world4',
'spam4'},
'quux3'},
'spam4',
[dict(row)
items]
[]}
rows_jl,
"baz",
"hello"]
header}
'world2'},
'quux3',
'world4'},
test_export_dicts(self):
'spam'},
'quux'},
'foo'],
test_export_feed_export_fields(self):
[self.MyItem,
dict]:
'foo,baz,egg'}
'egg'],
'egg,baz'}
test_export_encoding(self):
[dict({'foo':
u'Test\xd6'})]
"Test\\u00d6"}\n]'.encode('utf-8'),
"Test\\u00d6"}\n'.encode('utf-8'),
encoding="utf-8"?>\n<items><item><foo>Test\xd6</foo></item></items>'.encode('utf-8'),
u'foo\r\nTest\xd6\r\n'.encode('utf-8'),
format}
"Test\xd6"}\n]'.encode('latin-1'),
"Test\xd6"}\n'.encode('latin-1'),
encoding="latin-1"?>\n<items><item><foo>Test\xd6</foo></item></items>'.encode('latin-1'),
u'foo\r\nTest\xd6\r\n'.encode('latin-1'),
format,
'FEED_EXPORT_ENCODING':
'latin-1'}
WrappedRequest,
WrappedResponse
WrappedRequestTest(TestCase):
Request("http://www.example.com/page.html",
headers={"Content-Type":
WrappedRequest(self.request)
test_get_full_url(self):
self.assertEqual(self.wrapped.get_full_url(),
self.assertEqual(self.wrapped.full_url,
test_get_host(self):
self.assertEqual(self.wrapped.get_host(),
self.assertEqual(self.wrapped.host,
test_get_type(self):
self.assertEqual(self.wrapped.get_type(),
self.assertEqual(self.wrapped.type,
test_is_unverifiable(self):
self.assertFalse(self.wrapped.is_unverifiable())
self.assertFalse(self.wrapped.unverifiable)
test_is_unverifiable2(self):
self.request.meta['is_unverifiable']
self.assertTrue(self.wrapped.is_unverifiable())
self.assertTrue(self.wrapped.unverifiable)
test_get_origin_req_host(self):
self.assertEqual(self.wrapped.get_origin_req_host(),
self.assertEqual(self.wrapped.origin_req_host,
test_has_header(self):
self.assertTrue(self.wrapped.has_header('content-type'))
self.assertFalse(self.wrapped.has_header('xxxxx'))
test_get_header(self):
self.assertEqual(self.wrapped.get_header('content-type'),
self.assertEqual(self.wrapped.get_header('xxxxx',
'def'),
'def')
test_header_items(self):
self.assertEqual(self.wrapped.header_items(),
['text/html'])])
test_add_unredirected_header(self):
self.wrapped.add_unredirected_header('hello',
'world')
self.assertEqual(self.request.headers['hello'],
b'world')
WrappedResponseTest(TestCase):
Response("http://www.example.com/page.html",
headers={"Content-TYpe":
WrappedResponse(self.response)
test_info(self):
self.assert_(self.wrapped.info()
self.wrapped)
test_getheaders(self):
self.assertEqual(self.wrapped.getheaders('content-type'),
test_get_all(self):
self.assertEqual(self.wrapped.get_all('content-type'),
HeadersTest(unittest.TestCase):
test_basics(self):
1234})
h['Content-Length']
h.__getitem__,
'Accept')
self.assertEqual(h.get('Accept'),
self.assertEqual(h.getlist('Accept'),
self.assertEqual(h.get('Accept',
b'*/*')
[b'*/*'])
['text/html',
'images/jpeg']),
[b'text/html',
b'images/jpeg'])
test_single_value(self):
self.assertEqual(h['Content-Type'],
self.assertEqual(h.get('Content-Type'),
test_multivalue(self):
h['X-Forwarded-For']
self.assertEqual(h['X-Forwarded-For'],
self.assertEqual(h.get('X-Forwarded-For'),
test_encode_utf8(self):
isinstance(val[0],
test_encode_latin1(self):
b'\xa3')
test_encode_multiple(self):
[u'\xa3']},
test_delete_and_contains(self):
hlist)
'ip1')
[b'ip1'])
test_iterables(self):
idict
'ip2']}
Headers(idict)
self.assertDictEqual(dict(h),
[b'text/html'],
b'X-Forwarded-For':
b'ip2']})
self.assertSortedEqual(h.keys(),
[b'X-Forwarded-For',
b'Content-Type'])
self.assertSortedEqual(h.items(),
self.assertSortedEqual(h.iteritems(),
self.assertSortedEqual(h.values(),
[b'ip2',
b'text/html'])
h.update({'Content-Type':
'ip2']})
'value2']})
h2.getlist('header1'))
h1.getlist('header1')
h2.getlist('header1')
test_appendlist(self):
'value1')
test_setlist(self):
h1.setlist('header1',
test_setlistdefault(self):
h1.setlistdefault('header1',
h1.setlistdefault('header2',
self.assertEqual(h1.getlist('header2'),
test_none_value(self):
h1.setdefault('foo',
self.assertEqual(h1.get('foo'),
test_int_value(self):
Headers({'hey':
h1.setdefault('bar',
h1.setlist('buz',
'dos',
self.assertEqual(h1.getlist('bar'),
self.assertEqual(h1.getlist('buz'),
[b'1',
b'dos',
b'3'])
self.assertEqual(h1.getlist('hey'),
test_invalid_value(self):
object()})
Headers().__setitem__,
Headers().setdefault,
Headers().setlist,
[object()])
FormRequest,
XmlRpcRequest,
RequestTest(unittest.TestCase):
default_meta
self.request_class)
self.request_class('http://www.example.com')
self.request_class("http://www.example.com")
self.default_method)
self.default_headers)
self.default_meta)
{"lala":
"lolo"}
{b"caca":
b"coco"}
body="a
r.meta
meta)
self.assertEqual(r.headers[b"caca"],
b"coco")
test_url_no_scheme(self):
{b'Accept':'gzip',
b'Custom-Header':'nothing
you'}
headers=r.headers)
p.headers)
self.assertFalse(r.headers
self.assertFalse(p.headers
r.headers)
Headers({'key1':
u'val1',
u'key2':
h[u'newkey']
u'newval'
h.iteritems():
self.assert_(isinstance(k,
self.assert_(isinstance(s,
test_eq(self):
self.assertNotEqual(r1,
r2)
set_
set_.add(r1)
set_.add(r2)
self.assertEqual(len(set_),
test_url(self):
self.request_class(url="http://www.scrapy.org/path")
"http://www.scrapy.org/path")
test_url_quoting(self):
self.request_class(url="http://www.scrapy.org/blank%20space")
self.request_class(url="http://www.scrapy.org/blank
space")
test_url_encoding(self):
self.request_class(url=u"http://www.scrapy.org/price/£")
test_url_encoding_other(self):
test_url_encoding_query(self):
self.request_class(url=u"http://www.scrapy.org/price/£?unit=µ")
self.request_class(url=u"http://www.scrapy.org/price/£?unit=µ",
test_url_encoding_query_latin1(self):
self.request_class(url=u"http://www.scrapy.org/price/µ?currency=£",
"http://www.scrapy.org/price/%C2%B5?currency=%A3")
test_url_encoding_nonutf8_untouched(self):
self.request_class(url=u"http://www.scrapy.org/price/%a3")
"http://www.scrapy.org/price/%a3")
self.request_class(url=u"http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3")
self.request_class(url=u"http://www.scrapy.org/résumé/%a3")
self.request_class(url=u"http://www.example.org/r%E9sum%E9.html")
"http://www.example.org/r%E9sum%E9.html")
test_body(self):
self.request_class(url="http://www.example.com/")
body=b"")
isinstance(r2.body,
isinstance(r3.body,
\xc2\xa3100")
isinstance(r4.body,
\xa3100")
test_ajax_url(self):
self.request_class(url="http://www.example.com/ajax.html#!key=value")
self.request_class(url=u"http://www.example.com/ajax.html#!key=value")
somecallback():
callback=somecallback,
errback=somecallback)
r1.meta['foo']
r1.errback
r2.callback
r2.meta,
"meta
self.assertEqual(r1.encoding,
r2.encoding)
CustomRequest(self.request_class):
CustomRequest('http://www.example.com')
CustomRequest
method='GET')
Headers(r1.headers)
hdrs[b'key']
r1.replace(method="POST",
body="New
self.assertEqual((r1.method,
r2.method),
"POST"))
(self.default_headers,
r3.replace(url="http://www.example.com/2",
meta={},
dont_filter=False)
"http://www.example.com/2")
self.assertEqual(r4.meta,
r4.dont_filter
test_method_always_str(self):
method=u"POST")
isinstance(r.method,
self.request_class("http://example.com")
FormRequestTest(RequestTest):
assertQueryEqual(self,
to_native_str(first).split("&")
to_native_str(second).split("&")
test_empty_formdata(self):
formdata={})
test_default_encoding_bytes(self):
b'price=%C2%A3+100&one=two')
test_default_encoding_textual_data(self):
{u'µ
u'two',
u'price':
b'price=%C2%A3+100&%C2%B5+one=two')
test_default_encoding_mixed_data(self):
{u'\u00b5one':
b'price\xc2\xa3':
u'\u00a3
b'%C2%B5one=two&price%C2%A3=%C2%A3+100')
test_custom_encoding_bytes(self):
{b'\xb5
b'price=%A3+100&%B5+one=two')
test_custom_encoding_textual_data(self):
self.assertEqual(r3.encoding,
b'price=%A3+100')
test_multi_key_values(self):
u'\xa3
100',
'colours':
['red',
'blue',
'green']}
self.assertQueryEqual(r3.body,
b'colours=red&colours=blue&colours=green&price=%C2%A3+100')
test_from_response_post(self):
{b'val1',
b'val2'})
{b'two',
b'three'})
test_from_response_post_nonascii_bytes_utf8(self):
test_from_response_post_nonascii_bytes_latin1(self):
url="http://www.example.com/this/list.html",
encoding='latin1',
test_from_response_post_nonascii_unicode(self):
test_from_response_extra_headers(self):
'seven'},
headers={"Accept-Encoding":
"gzip,deflate"})
self.assertEqual(req.headers['Accept-Encoding'],
test_from_response_get(self):
self.assertEqual(urlparse(r1.url).hostname,
"www.example.com")
self.assertEqual(urlparse(r1.url).path,
"/this/get.php")
set([b'val1',
b'val2']))
set([b'two',
b'three']))
test_from_response_override_params(self):
test_from_response_override_method(self):
test_from_response_override_url(self):
'http://example.com/app')
url='http://foo.bar/absolute')
'http://foo.bar/absolute')
url='/relative')
'http://example.com/relative')
test_from_response_case_insensitive(self):
self.assertFalse(b'i1'
_get_inputs()
_get_clickable()
test_from_response_submit_first_clickable(self):
test_from_response_submit_not_first_clickable(self):
'clickable2'})
self.assertEqual(fs[b'clickable2'],
test_from_response_dont_submit_image_as_input(self):
[b'i1v']})
test_from_response_dont_submit_reset_as_input(self):
[b'i2v']})
test_from_response_multiple_clickdata(self):
u'clickable',
u'clicked2'})
test_from_response_unicode_clickdata(self):
u)
\u00a3'})
\u00a3'])
test_from_response_unicode_clickdata_latin1(self):
\u00a5'})
\u00a5'])
test_from_response_multiple_forms_clickdata(self):
formname='form2',
u'clickable'})
self.assertEqual(fs[b'field2'],
[b'value2'])
self.assertFalse(b'field1'
test_from_response_override_clickable(self):
_buildresponse('''<form><input
type="submit"
name="clickme"
value="one">
formdata={'clickme':
'clickme'})
self.assertEqual(fs[b'clickme'],
[b'two'])
test_from_response_dont_click(self):
test_from_response_ambiguous_clickdata(self):
clickdata={'type':
'submit'})
test_from_response_non_matching_clickdata(self):
clickdata={'nonexistent':
'notme'})
test_from_response_nr_index_clickdata(self):
self.assertIn(b'clickable2',
self.assertNotIn(b'clickable1',
test_from_response_invalid_nr_index_clickdata(self):
test_from_response_errors_noform(self):
_buildresponse()
test_from_response_invalid_html5(self):
formdata={'bar':
'buz'})
{b'foo':
[b'xxx'],
b'bar':
[b'buz']})
test_from_response_errors_formnumber(self):
formnumber=1)
test_from_response_noformname(self):
formdata={'two':'3'})
self.assertEqual(r1.headers['Content-type'],
[b'1'],
b'two':
test_from_response_formname_exists(self):
formname="form2")
test_from_response_formname_notexist(self):
formname="form3")
test_from_response_formname_errors_formnumber(self):
test_from_response_formid_exists(self):
test_from_response_formname_notexists_fallback_formid(self):
test_from_response_formid_notexist(self):
formid="form3")
test_from_response_formid_errors_formnumber(self):
formid="form3",
test_from_response_select(self):
value="i1v1">option
value="i1v2"
value="i2v1">option
value="i2v2">option
<select>
value="i3v1">option
value="i3v2">option
value="i4v1">option
value="i4v3"
3</option>
name="i5"
value="i5v1">option
value="i5v2">option
name="i6"></select>
name="i7"/>
{'i1':
['i1v2'],
'i2':
['i2v1'],
'i4':
['i4v2',
'i4v3']})
test_from_response_radio(self):
type="radio">
test_from_response_checkbox(self):
type="checkbox">
test_from_response_input_text(self):
type="text">
b'i4':
[b'i4v1']})
test_from_response_input_hidden(self):
type="hidden">
test_from_response_input_textarea(self):
name="i1">i1v</textarea>
name="i2"></textarea>
name="i3"/>
<textarea>i4v</textarea>
b'i3':
test_from_response_descendants(self):
<div>
<fieldset>
value="v1"
selected>
</fieldset>
value="i3v2"
name="i5"></textarea>
name="h1"
value="h1v">
</div>
name="h2"
value="h2v">
self.assertEqual(set(fs),
set([b'h2',
b'i2',
b'i1',
b'i3',
b'h1',
b'i5',
b'i4']))
test_from_response_xpath(self):
formxpath="//form[@action='post.php']")
formxpath="//form/input[@name='four']")
formxpath="//form/input[@name='abc']")
test_from_response_unicode_xpath(self):
_buildresponse(b'<form
name="\xd1\x8a"></form>')
formxpath=u"//form[@name='\u044a']")
_qs(r)
u"//form[@name='\u03b1']"
xpath.encode('unicode_escape')
re.escape(encoded),
formxpath=xpath)
test_from_response_button_submit(self):
test_from_response_button_notype(self):
test_from_response_submit_novalue(self):
test_from_response_button_novalue(self):
test_html_base_form_action(self):
url='http://a.com/'
'http://b.com/test_form')
test_from_response_css(self):
formcss="form[action='post.php']")
formcss="input[name='four']")
formcss="input[name='abc']")
_buildresponse(body,
kwargs.setdefault('body',
kwargs.setdefault('url',
HtmlResponse(**kwargs)
to_unicode=False):
req.url.partition('?')[2]
unquote(to_native_str(qs,
unquote_to_bytes(qs)
to_unicode:
uqs.decode(encoding)
parse_qs(uqs,
XmlRpcRequestTest(RequestTest):
[b'text/xml']}
_test_request(self,
self.request_class('http://scrapytest.org/rpc2',
self.assertEqual(r.headers[b'Content-Type'],
b'text/xml')
to_bytes(xmlrpclib.dumps(**kwargs),
encoding=kwargs.get('encoding',
'utf-8')))
self.assertEqual(r.encoding,
self.assertTrue(r.dont_filter,
test_xmlrpc_dumps(self):
self._test_request(params=('value',))
self._test_request(params=('username',
'password'),
methodname='login')
self._test_request(params=('response',
methodresponse='login')
self._test_request(params=(None,),
allow_none=1)
self._test_request)
self._test_request,
params=(None,))
test_latin1(self):
BaseResponseTest(unittest.TestCase):
self.assertTrue(isinstance(self.response_class('http://example.com/'),
b"http://example.com")
body=b''),
body=b'body'),
status=200),
"bar"}
b"a
self.assertEqual(r.headers[b"foo"],
b"bar")
status='301')
"http://example.com",
status='lala200')
r1.flags.append('cached')
self.assertEqual(r1.status,
r2.status)
r1.flags
r2.flags,
"flags
self.assertEqual(r1.flags,
r2.flags)
test_copy_meta(self):
req.meta['foo']
CustomResponse(self.response_class):
CustomResponse('http://www.example.com')
CustomResponse
Headers({"key":
"value"})
r1.replace(status=301,
body=b"New
self.assertEqual((r1.status,
r2.status),
301))
({},
r3.replace(body=b'',
flags=[])
self.assertEqual(r4.flags,
_assert_response_values(self,
body.encode(encoding)
body.decode(encoding)
isinstance(response.body,
isinstance(response.text,
self._assert_response_encoding(response,
self.assertEqual(response.body,
body_bytes)
self.assertEqual(response.body_as_unicode(),
_assert_response_encoding(self,
resolve_encoding(encoding))
self.response_class("http://example.com")
test_urljoin(self):
self.response_class('http://www.example.com').urljoin('/test')
TextResponseTest(BaseResponseTest):
super(TextResponseTest,
self).test_replace()
body="hello",
encoding="cp852")
r1.replace(url="http://www.example.com/other",
"cp852")
"latin1")
test_unicode_url(self):
self.response_class(u"http://www.example.com/")
self._assert_response_encoding(resp,
self.response_class._DEFAULT_ENCODING)
self.response_class(url=u"http://www.example.com/",
isinstance(resp.url,
charset=utf-8"]})
charset=iso-8859-1"]})
test_unicode_body(self):
unicode_string
u'\u043a\u0438\u0440\u0438\u043b\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439
\u0442\u0435\u043a\u0441\u0442'
body=u'unicode
original_string
unicode_string.encode('cp1251')
body=original_string,
encoding='cp1251')
self.assertTrue(isinstance(r1.body_as_unicode(),
self.assertEqual(r1.body_as_unicode(),
self.assertTrue(isinstance(r1.text,
self.assertEqual(r1.text,
test_encoding(self):
body=b"\xa2\xa3")
charset=None"]},
charset=gb2312"]},
charset=gbk"]},
self.assertEqual(r1._headers_encoding(),
self.assertEqual(r2._headers_encoding(),
self.assertEqual(r2._declared_encoding(),
self.assertEqual(r3._headers_encoding(),
self.assertEqual(r4._headers_encoding(),
self.assertEqual(r5._headers_encoding(),
self._assert_response_encoding(r5,
"http://www.example.com",
test_declared_encoding_invalid(self):
charset=UKNOWN"]},
self.assertEqual(r._declared_encoding(),
test_utf16(self):
body=b'\xff\xfeh\x00i\x00',
encoding='utf-16')
'utf-16',
u"hi")
test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):
body=b"\xef\xbb\xbfWORD\xe3\xab")
self.assertEqual(r6.encoding,
self.assertEqual(r6.text,
u'WORD\ufffd\ufffd')
test_bom_is_removed_from_body(self):
'http://example.com'
b"\xef\xbb\xbfWORD"
{"Content-type":
charset=utf-8"]}
test_replace_wrong_encoding(self):
body=b'PREFIX\xe3\xabSUFFIX')
u'\ufffd'
u'PREFIX'
u'SUFFIX'
self.response_class("http://example.com",
body=b'\xf0<span>value</span>')
u'<span>value</span>'
response.selector.re("Some
(.*)</title>"),
[u'page']
response.xpath("//title/text()").extract(),
response.css("title::text").extract(),
test_urljoin_with_base_url(self):
href="https://example.net"></body></html>'
body=body).urljoin('/test')
'https://example.net/test'
href="/elsewhere"></body></html>'
href="/elsewhere/"></body></html>'
'http://www.example.com/elsewhere/test'
HtmlResponseTest(TextResponseTest):
test_html_encoding(self):
\xa3"
r3.replace(body=body)
test_html5_meta_charset(self):
'gb2312',
XmlResponseTest(TextResponseTest):
test_xml_encoding(self):
b"<xml></xml>"
self.response_class._DEFAULT_ENCODING,
r3.replace(body=body2)
test_replace_encoding(self):
r5.replace(body=body2)
r5.replace(body=body2,
self._assert_response_values(r5,
[u'value']
response.xpath("//elem/text()").extract(),
ItemTest(unittest.TestCase):
test_simple(self):
u'name'
self.assertEqual(i['name'],
u'name')
TestItem(name=u'john
TestItem({'name':
doe'})
self.assertEqual(i3['name'],
i4
TestItem(i3)
self.assertEqual(i4['name'],
doe',
u'foo'})
test_invalid_field(self):
i.__setitem__,
'field',
'text')
'field')
Doe'
i['number']
itemrepr
repr(i)
eval(itemrepr)
Doe')
self.assertEqual(i2['number'],
test_private_attr(self):
i._private
self.assertEqual(i._private,
test_raise_getattr(self):
getattr,
test_raise_setattr(self):
'john')
test_custom_methods(self):
change_name(self,
i.get_name)
i.change_name(u'other')
'other')
test_metaclass(self):
['name'])
u'Keys'
i['values']
u'Values'
self.assertSortedEqual(list(i.keys()),
['keys',
'values',
'name'])
self.assertSortedEqual(list(i.values()),
[u'Keys',
u'Values',
u'John'])
test_metaclass_with_fields_attribute(self):
{'new':
Field(default='X')}
TestItem(new=u'New')
self.assertSortedEqual(list(item.keys()),
['new'])
self.assertSortedEqual(list(item.values()),
[u'New'])
test_metaclass_inheritance(self):
BaseItem(Item):
TestItem(BaseItem):
['keys'])
[3])
test_metaclass_multiple_inheritance_simple(self):
C(Item):
D(save='X',
load='Y')
self.assertEqual(item['save'],
self.assertEqual(item['load'],
'Y')
test_metaclass_multiple_inheritance_diamond(self):
C(A):
Field(default='D')}
Field(default='D')
self.assertEqual(D(load='X')['load'],
'D'},
'D'}})
Field(default='E')
'E'},
test_metaclass_multiple_inheritance_without_metaclass(self):
C(object):
not_allowed
Field(default='not_allowed')
D,
E,
self.assertEqual(dict(i),
u'John'})
TestItem({'name':'lower'})
copied_item
item.copy()
self.assertNotEqual(id(item),
id(copied_item))
copied_item['name']
copied_item['name'].upper()
self.assertNotEqual(item['name'],
copied_item['name'])
LinkTest(unittest.TestCase):
_assert_same_links(self,
self.assertEqual(link1,
self.assertEqual(hash(link1),
_assert_different_links(self,
self.assertNotEqual(link1,
self.assertNotEqual(hash(link1),
test_eq_and_hash(self):
Link("http://www.example.com/other")
l3
l1)
self._assert_different_links(l1,
l3)
l4
l5
text="test2")
l6
l4)
self._assert_different_links(l4,
l5)
l6)
l7
l8
l9
l10
fragment='other',
self._assert_same_links(l7,
l8)
l9)
l10)
eval(repr(l1))
test_non_str_url_py2(self):
Link(u"http://www.example.com/\xa3")
self.assertIsInstance(link.url,
self.assertEqual(link.url,
b'http://www.example.com/\xc2\xa3')
len(w)
"warning
issued"
Link(b"http://www.example.com/\xc2\xa3")
Base:
LinkExtractorTestCase(unittest.TestCase):
test_urls_type(self):
resulting
self.assertTrue(all(isinstance(link.url,
lx.extract_links(self.response)))
test_extract_all_links(self):
test_extract_filter_allow(self):
test_extract_filter_allow_with_duplicates(self):
unique=False)
test_extract_filter_allow_and_deny(self):
deny=('3',
test_extract_filter_allowed_domains(self):
self.extractor_cls(allow_domains=('google.com',
test_extraction_using_single_values(self):
among
situations'''
self.extractor_cls(allow='sample')
self.extractor_cls(allow='sample',
deny='3')
self.extractor_cls(allow_domains='google.com')
self.extractor_cls(deny_domains='example.com')
test_nofollow(self):
rel="nofollow"'''
Link(url='http://example.org/follow.html',
link'),
Link(url='http://example.org/nofollow.html',
Link(url='http://example.org/nofollow2.html',
not'),
self.extractor_cls(allow=(r'stuff1',
self.extractor_cls(deny=(r'uglystuff',
self.extractor_cls(allow_domains=('evenmorestuff.com',
self.extractor_cls(deny_domains=('lotsofstuff.com',
self.extractor_cls(allow=('blah1',),
deny=('blah2',),
allow_domains=('blah1.com',),
deny_domains=('blah2.com',))
self.assertEqual(lx.matches('http://blah1.com/blah1'),
self.assertEqual(lx.matches('http://blah1.com/blah2'),
self.assertEqual(lx.matches('http://blah2.com/blah1'),
self.assertEqual(lx.matches('http://blah2.com/blah2'),
test_restrict_xpaths(self):
test_restrict_xpaths_encoding(self):
self.extractor_cls(restrict_xpaths="//div[@class='links']")
[Link(url='http://example.org/about.html',
us\xa3')])
b'<html><body><p><a
href="/&hearts;/you?c=&euro;">text</a></p></body></html>'
encoding='iso8859-15')
self.extractor_cls(restrict_xpaths='//p').extract_links(response)
self.assertEqual(links,
[Link(url='http://example.org/%E2%99%A5/you?c=%E2%82%AC',
text=u'text')])
test_restrict_xpaths_concat_in_handle_data(self):
encoding='gb18030')
text=u'>\u4eac<\u4e1c',
test_restrict_css(self):
self.extractor_cls(restrict_css=('#subwrapper
a',))
2')
test_restrict_css_and_restrict_xpaths_together(self):
restrict_css=('#subwrapper
a',
test_area_tag_with_unicode_present(self):
test_encoded_url(self):
test_encoded_url_in_restricted_xpath(self):
test_ignored_extensions(self):
self.extractor_cls(deny_extensions=['html'])
Link(url='http://example.org/photo.jpg'),
test_process_value(self):
process_value(value):
re.search("javascript:goToPage\('(.*?)'",
self.extractor_cls(process_value=process_value)
[Link(url='http://example.org/other/page.html',
text='Link
text')])
test_base_url_with_restrict_xpaths(self):
self.extractor_cls(restrict_xpaths="//p")
test_attrs(self):
self.extractor_cls(attrs="href")
self.extractor_cls(attrs=("href","src"),
tags=("a","area","img"),
self.extractor_cls(attrs=None)
test_tags(self):
self.extractor_cls(tags=None)
self.extractor_cls(tags="area")
self.extractor_cls(tags="a")
self.extractor_cls(tags=("a","img"),
attrs=("href",
"src"),
test_tags_attrs(self):
self.extractor_cls(tags='div',
attrs='data-url')
self.extractor_cls(tags=('div',),
attrs=('data-url',))
test_xhtml(self):
xhtml
HtmlResponse("http://example.com/index.xhtml",
XmlResponse("http://example.com/index.xhtml",
LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):
@pytest.mark.xfail
super(LxmlLinkExtractorTestCase,
self).test_restrict_xpaths_with_html_entities()
RegexLinkExtractor
HtmlParserLinkExtractor
SgmlLinkExtractor,
BaseSgmlLinkExtractor
tests.test_linkextractors
Base
BaseSgmlLinkExtractorTestCase(unittest.TestCase):
[Link(url='http://example.org/somepage/item/12.html',
12'),
text='About
Link(url='http://example.org/othercat.html',
text='Other
category'),
text='>>'),
text='')])
test_base_url(self):
[Link(url='https://example.org/item/12.html',
[Link(url='https://noschemedomain.com/path/to/item/12.html',
test_link_text_wrong_encoding(self):
Link(url='http://www.example.com/item/12.html',
text=u'Wrong:
\ufffd'),
test_extraction_encoding(self):
'linkextractor_noenc.html')
response_utf8
HtmlResponse(url='http://example.com/utf8',
charset=utf-8']})
response_noenc
HtmlResponse(url='http://example.com/noenc',
'linkextractor_latin1.html')
response_latin1
HtmlResponse(url='http://example.com/latin1',
self.assertEqual(lx.extract_links(response_utf8),
self.assertEqual(lx.extract_links(response_noenc),
self.assertEqual(lx.extract_links(response_latin1),
Link(url='http://example.com/sample_%C3%A1.html',
\xe1
text'.decode('latin1')),
Link(url='http://example.com/sample_%C3%B6.html?price=%A332&%B5=unit',
HtmlParserLinkExtractorTestCase(unittest.TestCase):
SgmlLinkExtractorTestCase(Base.LinkExtractorTestCase):
test_deny_extensions(self):
SgmlLinkExtractor(deny_extensions="jpg")
test_attrs_sgml(self):
SgmlLinkExtractor(attrs="href")
test_link_nofollow(self):
HtmlResponse("http://example.org/page.html",
SgmlLinkExtractor()
Link(url='http://example.org/page.html?action=print',
text=u'Printer-friendly
page',
text=u'Something',
RegexLinkExtractorTestCase(unittest.TestCase):
test_html_base_href(self):
HtmlResponse("http://a.com/",
Link(url='http://b.com/test.html',
Join,
Identity,
TakeFirst,
Compose,
MapCompose,
SelectJmes
NameItem(Item):
TestItem(NameItem):
summary
TestNestedItem(Item):
name_div
NameItemLoader(ItemLoader):
NestedItemLoader(ItemLoader):
TestNestedItem
v.title())
DefaultedItemLoader(NameItemLoader):
processor_with_args(value,
other=None,
'key'
loader_context['key']
BasicItemLoaderTest(unittest.TestCase):
test_load_item_using_default_loader(self):
i['summary']
ItemLoader(item=i)
test_load_item_using_custom_loader(self):
test_load_item_ignore_none_field_values(self):
validate_sku(value):
value.isdigit():
vs:
vs[0])
allows
price_out
sku_out
validate_sku)
valid_fragment
1234'
invalid_fragment
available'
sku_re
'SKU:
(.+)'
il.add_value('sku',
[invalid_fragment],
u'')
il.add_value('price',
[u'0'])
il.replace_value('sku',
[valid_fragment],
self.assertEqual(il.load_item()['sku'],
u'1234')
test_self_referencing_loader(self):
img_url_out(self,
(self.get_output_value('url')
values[0]
'http://example.com/1234.png',
'1234.png',
test_add_value(self):
self.assertEqual(il.get_collected_values('summary'),
il.add_value(None,
u'Pepe',
u'Jim'])
test_add_zero(self):
[0])
test_replace_value(self):
il.replace_value(None,
[u'Jim'])
test_get_value(self):
self.assertEqual(u'FOO',
il.get_value([u'foo',
six.text_type.upper))
self.assertEqual([u'foo',
self.assertEqual(u'foo',
[u'name:foo',
self.assertEqual([u'foo'],
u'name:bar',
self.assertEqual([u'bar'],
test_iter_on_input_processor_input(self):
NameFirstItemLoader(NameItemLoader):
[u'jose',
u'pedro'])
test_map_compose_filter(self):
filter_world(x):
'world',
'this',
'is',
'scrapy']),
['HELLO',
'THIS',
'IS',
'SCRAPY'])
test_map_compose_filter_multil(self):
test_default_input_processor(self):
DefaultedItemLoader()
test_inherited_default_input_processor(self):
InheritDefaultedItemLoader(DefaultedItemLoader):
InheritDefaultedItemLoader()
test_input_processor_inheritance(self):
v.lower())
u'HTTP://scrapy.ORG')
[u'http://scrapy.org'])
ChildChildItemLoader(ChildItemLoader):
v.upper())
summary_in
ChildChildItemLoader()
u'http://scrapy.org')
[u'HTTP://SCRAPY.ORG'])
test_empty_map_compose(self):
MapCompose()
test_identity_input_processor(self):
test_extend_custom_input_processors(self):
MapCompose(TestItemLoader.name_in,
[u'mARTA'])
test_extend_default_input_processors(self):
ChildDefaultedItemLoader(DefaultedItemLoader):
MapCompose(DefaultedItemLoader.default_input_processor,
ChildDefaultedItemLoader()
[u'MART'])
test_output_processor_using_function(self):
u"
".join
test_output_processor_error(self):
TestItemLoader(ItemLoader):
MapCompose(float)
[u'$10'])
float('$10')
str(exc)
'$10'
'ValueError'
test_output_processor_using_classes(self):
Join("<br>")
u'Mar<br>Ta')
test_default_output_processor(self):
LalaItemLoader(TestItemLoader):
LalaItemLoader()
test_loader_context_on_declaration(self):
MapCompose(processor_with_args,
key=u'val')
test_loader_context_on_instantiation(self):
ChildItemLoader(key=u'val')
test_loader_context_on_assign(self):
il.context['key']
u'val'
test_item_passed_to_input_processor_functions(self):
processor(value,
loader_context):
loader_context['item']['name']
MapCompose(processor)
TestItem(name='marta')
ChildItemLoader(item=it)
test_add_value_on_unknown_field(self):
il.add_value,
'wrong_field',
[u'lala',
u'lolo'])
test_compose_processor(self):
u'other'])
test_partial_processor(self):
join(values,
loader_context=None,
ignored=None):
sep.join(values)
loader_context
'sep'
loader_context['sep'].join(values)
''.join(values)
sep='+'))
loader_context={'sep':
'.'}))
summary_out
ignored='foo'))
u'rabbit+hole')
u'rabbit.hole')
u'rabbithole')
ProcessorsTest(unittest.TestCase):
test_take_first(self):
test_identity(self):
test_join(self):
self.assertEqual(proc(['',
u'
u'hello
self.assert_(isinstance(proc(['hello',
test_compose(self):
'HELLO')
Compose(str.upper)
self.assertEqual(proc(None),
Compose(str.upper,
stop_on_none=False)
test_mapcompose(self):
filter_world
six.text_type.upper)
self.assertEqual(proc([u'hello',
u'world',
u'this',
u'is',
u'scrapy']),
[u'HELLO',
u'THIS',
u'IS',
u'SCRAPY'])
SelectortemLoaderTest(unittest.TestCase):
self.assertEqual(l.selector,
test_constructor_errors(self):
l.add_xpath,
l.replace_xpath,
l.get_xpath,
l.add_css,
l.replace_css,
l.get_css,
test_constructor_with_selector(self):
test_constructor_with_selector_css(self):
test_constructor_with_response(self):
test_constructor_with_response_css(self):
u'Marta'])
'//img/@src')
test_add_xpath_re(self):
test_replace_xpath(self):
'//p/text()')
['//p/text()',
'//div/text()'])
test_get_xpath(self):
self.assertEqual(l.get_xpath('//p/text()'),
self.assertEqual(l.get_xpath(['//p/text()',
'//div/text()']),
test_replace_xpath_multi_fields(self):
l.add_xpath(None,
l.replace_xpath(None,
'//p/text()',
test_replace_xpath_re(self):
test_add_css_re(self):
test_replace_css(self):
'p::text')
['p::text',
'div::text'])
'img::attr(src)')
test_get_css(self):
self.assertEqual(l.get_css('p::text'),
self.assertEqual(l.get_css(['p::text',
'div::text']),
self.assertEqual(l.get_css(['a::attr(href)',
'img::attr(src)']),
test_replace_css_multi_fields(self):
'p::text',
'img::attr(src)',
test_replace_css_re(self):
re='http://www\.(.+)')
[u'scrapy.org'])
SubselectorLoaderTest(unittest.TestCase):
test_nested_xpath(self):
l.nested_xpath("//header")
test_nested_css(self):
l.nested_css("header")
test_nested_replace(self):
nl1.replace_xpath('url',
nl2.replace_xpath('url',
'@href')
test_nested_ordering(self):
nl2.add_xpath('url',
'text()')
u'/images/logo.png',
u'homepage',
test_nested_load_item(self):
nl1.nested_xpath('img')
'//header/div/text()')
'a/@href')
nl2.add_xpath('image',
'@src')
l.load_item()
l.item
nl1.item
nl2.item
self.assertEqual(item['image'],
SelectJmesTestCase(unittest.TestCase):
test_list_equals
'simple':
'invalid':
('foo.bar.baz',
'top_level':
('foo',
"baz"}),
'double_vs_single_quote_string':
'foo.bar[*].name',
[{"name":
"one"},
"two"}]}},
'two']
'list':
('[1]',
self.test_list_equals:
expr,
test_list,
self.test_list_equals[l]
SelectJmes(expr)(test_list)
msg='test
{}'.format(l,
scrapy.logformatter
CustomItem(Item):
"name:
LoggingContribTest(unittest.TestCase):
self.formatter
LogFormatter()
Spider('default')
test_crawled(self):
None)")
headers={'referer':
http://example.com)
['cached']")
test_dropped(self):
Exception(u"\u2018")
self.formatter.dropped(item,
[u"Dropped:
\u2018",
'{}'])
test_scraped(self):
CustomItem()
u'\xa3'
self.formatter.scraped(item,
[u"Scraped
http://www.example.com>",
u'name:
\xa3'])
email.charset
Charset
MailSenderTest(unittest.TestCase):
test_send(self):
test_send_html(self):
body='<p>body</p>',
mimetype='text/html',
'<p>body</p>')
test_send_attach(self):
attach.write(b'content')
self.assertEqual(text.get_payload(decode=True),
b'body')
Charset('us-ascii'))
self.assertEqual(attach.get_payload(decode=True),
b'content')
_catch_mail_sent(self,
dict(**kwargs)
test_send_utf8(self):
test_send_attach_utf8(self):
attach.write(body.encode('utf-8'))
'multipart/mixed;
self.assertEqual(text.get_payload(decode=True).decode('utf-8'),
self.assertEqual(attach.get_payload(decode=True).decode('utf-8'),
M1(object):
M2(object):
M3(object):
MOff(object):
TestMiddlewareManager(MiddlewareManager):
['tests.test_middleware.%s'
['M1',
'MOff',
'M3']]
super(TestMiddlewareManager,
'process'):
self.methods['process'].append(mw.process)
MiddlewareManagerTest(unittest.TestCase):
TestMiddlewareManager(m1,
self.assertEqual(mwman.methods['open_spider'],
[m1.open_spider,
m2.open_spider])
self.assertEqual(mwman.methods['close_spider'],
[m2.close_spider,
m1.close_spider])
self.assertEqual(mwman.methods['process'],
[m1.process,
m3.process])
test_methods(self):
TestMiddlewareManager(M1(),
M3())
test_enabled(self):
MiddlewareManager(m1,
self.assertEqual(mwman.middlewares,
(m1,
m3))
test_enabled_from_settings(self):
TestMiddlewareManager.from_settings(settings)
[x.__class__
mwman.middlewares]
self.assertEqual(classes,
FilesPipeline,
S3FilesStore
get_s3_content_and_delete
FilesPipelineTestCase(unittest.TestCase):
self.assertEqual(file_path(Request("https://dev.mydeco.com/mydeco.pdf")),
self.assertEqual(file_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt")),
'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')
self.assertEqual(file_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")),
'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')
'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')
test_fs_store(self):
isinstance(self.pipeline.store,
FSFilesStore)
self.assertEqual(self.pipeline.store.basedir,
self.tempdir)
'some/image/key.jpg'
os.path.join(self.tempdir,
'some',
'key.jpg')
self.assertEqual(self.pipeline.store._get_filesystem_path(path),
test_file_not_expired(self):
"http://example.com/file.pdf"
return_value=True),
time.time()}),
return_value=[_prepare_request_object(item_url)])
self.assertEqual(result['files'][0]['checksum'],
test_file_expired(self):
"http://example.com/file2.pdf"
(self.pipeline.expires
2)}),
return_value=[_prepare_request_object(item_url)]),
self.assertNotEqual(result['files'][0]['checksum'],
DeprecatedFilesPipeline(FilesPipeline):
'empty/%s%s'
DeprecatedFilesPipelineTestCase(unittest.TestCase):
pipeline_class.from_settings(Settings({'FILES_STORE':
self.init_pipeline(FilesPipeline)
self.assertEqual(self.pipeline.file_key("https://dev.mydeco.com/mydeco.pdf"),
self.init_pipeline(DeprecatedFilesPipeline)
self.assertEqual(self.pipeline.file_path(Request("https://dev.mydeco.com/mydeco.pdf")),
'empty/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')
FilesPipelineTestCaseFields(unittest.TestCase):
'file_urls':
's3://example/files/'}))
self.assertEqual(item['files'],
stored_file
'files':
FilesPipeline.from_settings(Settings({
'FILES_STORE':
's3://example/files/',
'FILES_URLS_FIELD':
'files',
'FILES_RESULT_FIELD':
'stored_file'
self.assertEqual(item['stored_file'],
FilesPipelineTestCaseCustomSettings(unittest.TestCase):
default_cls_settings
"EXPIRES":
"file_urls",
"files"
file_cls_attr_settings_map
"FILES_EXPIRES",
"expires"),
("FILES_URLS_FIELD",
"FILES_URLS_FIELD",
"files_urls_field"),
("FILES_RESULT_FIELD",
"FILES_RESULT_FIELD",
"files_result_field")
"FILES_EXPIRES":
"FILES_STORE":
"FILES_STORE"
_generate_fake_pipeline(self):
UserDefinedFilePipeline(FilesPipeline):
FILES_URLS_FIELD
"alfa"
FILES_RESULT_FIELD
"beta"
UserDefinedFilePipeline
another_pipeline
FilesPipeline.from_settings(Settings(custom_settings))
one_pipeline
FilesPipeline(self.tempdir)
self.default_cls_settings[pipe_attr]
self.assertEqual(getattr(one_pipeline,
pipe_attr),
default_value)
custom_settings[settings_attr]
self.assertNotEqual(default_value,
self.assertEqual(getattr(another_pipeline,
test_subclass_attributes_preserved_if_no_settings(self):
pipe_cls
pipe_cls.from_settings(Settings({"FILES_STORE":
self.assertEqual(getattr(pipe,
self.default_cls_settings.get(pipe_attr.upper())
UserDefinedFilesPipeline.__name__.upper()
UserDefinedFilesPipeline.from_settings(Settings(settings))
pipe_cls_attr,
self.default_cls_settings[pipe_cls_attr])
"this"
"that"
self.assertEqual(pipeline.files_result_field,
"this")
self.assertEqual(pipeline.files_urls_field,
"that")
TestS3FilesStore(unittest.TestCase):
test_persist(self):
b"TestS3FilesStore:
S3FilesStore(uri)
store.persist_file(
info=None,
'image/png'})
store.stat_file(path,
self.assertIn('last_modified',
self.assertIn('checksum',
self.assertEqual(s['checksum'],
'3187896a9657a28163abb31667df64c8')
get_s3_content_and_delete(
u.hostname,
u.path[1:],
with_key=True)
self.assertEqual(key['Metadata'],
key['CacheControl'],
self.assertEqual(key['ContentType'],
self.assertEqual(key.metadata,
key.cache_control,
ItemWithFiles(Item):
_create_item_with_files(*files):
ItemWithFiles()
item['file_urls']
_prepare_request_object(item_url):
item_url,
meta={'response':
Response(item_url,
body=b'data')})
mkdtemp,
ImagesPipeline
Imaging
Library,
https://pypi.python.org/pypi/Pillow'
set(('jpeg_encoder',
'jpeg_decoder'))
encoders.issubset(set(Image.core.__dict__)):
JPEG
encoders'
ImagesPipelineTestCase(unittest.TestCase):
self.assertEqual(file_path(Request("https://dev.mydeco.com/mydeco.gif")),
self.assertEqual(file_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg")),
'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')
self.assertEqual(file_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif")),
'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')
'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')
test_thumbnail_name(self):
self.pipeline.thumb_path
'50'
self.assertEqual(thumb_path(Request("file:///tmp/foo.jpg"),
self.assertEqual(thumb_path(Request("file://foo.png"),
'thumbs/50/e55b765eba0ec7348e50a1df496040449071b96a.jpg')
self.assertEqual(thumb_path(Request("file:///tmp/foo"),
'thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg')
response=Response("file:///tmp/some.name/foo"),
test_convert_image(self):
SIZE
(100,
255)
_create_image('JPEG',
'RGB',
COLOUR)])
thumbnail,
self.pipeline.convert_image(converted,
size=(10,
25))
self.assertEquals(thumbnail.mode,
self.assertEquals(thumbnail.size,
10))
_create_image('PNG',
'RGBA',
(205,
230,
255))])
DeprecatedImagesPipeline(ImagesPipeline):
'empty/%s.jpg'
'thumbsup/%s/%s.jpg'
DeprecatedImagesPipelineTestCase(unittest.TestCase):
pipeline_class(self.tempdir,
self.assertEqual(self.pipeline.file_key("https://dev.mydeco.com/mydeco.gif"),
test_default_image_key_method(self):
self.assertEqual(self.pipeline.image_key("https://dev.mydeco.com/mydeco.gif"),
self.assertEqual(self.pipeline.file_path(Request("https://dev.mydeco.com/mydeco.gif")),
'empty/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')
test_default_thumb_key_method(self):
self.assertEqual(self.pipeline.thumb_key("file:///tmp/foo.jpg",
test_overridden_thumb_key_method(self):
self.assertEqual(self.pipeline.thumb_path(Request("file:///tmp/foo.jpg"),
'thumbsup/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')
ImagesPipelineTestCaseFields(unittest.TestCase):
image_urls
'image_urls':
ImagesPipeline.from_settings(Settings({'IMAGES_STORE':
's3://example/images/'}))
self.assertEqual(item['images'],
stored_image
'image':
ImagesPipeline.from_settings(Settings({
'IMAGES_STORE':
's3://example/images/',
'IMAGES_URLS_FIELD':
'IMAGES_RESULT_FIELD':
'stored_image'
self.assertEqual(item['stored_image'],
ImagesPipelineTestCaseCustomSettings(unittest.TestCase):
img_cls_attribute_names
"IMAGES_EXPIRES"),
("MIN_WIDTH",
"IMAGES_MIN_WIDTH"),
("MIN_HEIGHT",
"IMAGES_MIN_HEIGHT"),
("IMAGES_URLS_FIELD",
"IMAGES_URLS_FIELD"),
("IMAGES_RESULT_FIELD",
"IMAGES_RESULT_FIELD"),
("THUMBS",
"IMAGES_THUMBS")
default_pipeline_settings
MIN_WIDTH=0,
MIN_HEIGHT=0,
EXPIRES=0,
THUMBS={},
IMAGES_URLS_FIELD='image_urls',
IMAGES_RESULT_FIELD='images'
"IMAGES_EXPIRES":
"IMAGES_STORE":
self.tempdir,
"IMAGES_RESULT_FIELD":
"IMAGES_URLS_FIELD":
"IMAGES_MIN_WIDTH":
"IMAGES_MIN_HEIGHT":
"IMAGES_THUMBS":
1000)),
1000))
"IMAGES_STORE"
_generate_fake_pipeline_subclass(self):
2000)),
2000))
IMAGES_URLS_FIELD
"field_one"
IMAGES_RESULT_FIELD
"field_two"
UserDefinedImagePipeline
default_sts_pipe
settings=default_settings)
user_sts_pipe
ImagesPipeline.from_settings(Settings(custom_settings))
expected_default_value
self.default_pipeline_settings.get(pipe_attr)
custom_settings.get(settings_attr)
self.assertNotEqual(expected_default_value,
self.assertEqual(getattr(default_sts_pipe,
expected_default_value)
self.assertEqual(getattr(user_sts_pipe,
test_subclass_attrs_preserved_default_settings(self):
pipeline_cls.from_settings(Settings({"IMAGES_STORE":
self.assertNotEqual(attr_value,
self.assertEqual(attr_value,
self.default_pipeline_settings.get(pipe_attr.upper())
UserDefinedImagePipeline.__name__.upper()
UserDefinedImagePipeline.from_settings(Settings(settings))
"something"
"something_else"
self.assertEqual(pipeline.images_result_field,
"something_else")
self.assertEqual(pipeline.images_urls_field,
"something")
_create_image(format,
Image.new(*a,
**kw).save(buf,
Image.open(buf)
inlineCallbacks
BaseMediaPipelineTestCase(unittest.TestCase):
Spider('media.com')
self.pipe
self.pipeline_class(download_func=_mocked_download_func)
self.pipe.open_spider(self.spider)
self.info
self.pipe.spiderinfo
test_default_media_to_download(self):
self.pipe.media_to_download(request,
test_default_get_media_requests(self):
self.pipe.get_media_requests(item,
test_default_media_downloaded(self):
Response('http://url',
self.pipe.media_downloaded(response,
test_default_media_failed(self):
self.pipe.media_failed(fail,
test_default_item_completed(self):
self.pipe.item_completed([],
fail)]
record.levelname
'ERROR'
self.assertTupleEqual(record.exc_info,
failure_to_exc_info(fail))
test_default_process_item(self):
MockedMediaPipeline(MediaPipeline):
self._mockcalled
self._mockcalled.append('download')
self).download(request,
self._mockcalled.append('media_to_download')
'result'
request.meta.get('result')
self).media_to_download(request,
self._mockcalled.append('get_media_requests')
item.get('requests')
self._mockcalled.append('media_downloaded')
self).media_downloaded(response,
self._mockcalled.append('media_failed')
self).media_failed(failure,
self._mockcalled.append('item_completed')
self).item_completed(results,
item['results']
MediaPipelineTestCase(BaseMediaPipelineTestCase):
MockedMediaPipeline
test_result_succeed(self):
meta=dict(response=rsp),
rsp)])
'media_downloaded',
'request_callback',
test_result_failure(self):
meta=dict(response=fail),
[(False,
'media_failed',
'request_errback',
test_mix_of_success_and_failure(self):
Request('http://url2',
meta=dict(response=fail))
self.pipe._mockcalled
self.assertEqual(m[0],
'get_media_requests')
self.assertEqual(m.count('get_media_requests'),
self.assertEqual(m.count('item_completed'),
self.assertEqual(m[-1],
'item_completed')
self.assertEqual(m.count('media_to_download'),
self.assertEqual(m.count('media_downloaded'),
self.assertEqual(m.count('media_failed'),
test_get_media_requests(self):
request_fingerprint(req)
Request('http://url1')
Request('http://url2')
dict(requests=iter([req1,
req2]))
request_fingerprint(req2)
test_results_are_cached_across_multiple_items(self):
dict(requests=req1)
dict(requests=req2)
self.assertEqual(request_fingerprint(req1),
request_fingerprint(req2))
test_results_are_cached_for_requests_of_single_item(self):
test_wait_if_request_is_downloading(self):
_check_downloading(response):
self.info.downloading)
self.info.waiting)
self.info.downloaded)
self.assertEqual(len(self.info.waiting[fp]),
Response('http://url')
rsp1_func():
Deferred().addCallback(_check_downloading)
reactor.callLater(.1,
rsp1)
rsp2_func():
self.fail('it
redownload')
meta=dict(response=rsp1_func))
meta=dict(response=rsp2_func))
test_use_media_to_download_result(self):
meta=dict(result='ITSME',
response=self.fail))
'ITSME')])
libmproxy
controller,
netlib
http_auth
HTTPSProxy(controller.Master,
Thread):
password_manager
http_auth.PassManSingleUser('scrapy',
'scrapy')
http_auth.BasicProxyAuth(password_manager,
"mitmproxy")
'keys',
'mitmproxy-ca.pem')
proxy.ProxyServer(proxy.ProxyConfig(
authenticator,
cacert
cert_path),
Thread.__init__(self)
controller.Master.__init__(self,
ProxyConnectTestCase(TestCase):
self._proxy
HTTPSProxy(8888)
self._proxy.start()
time.sleep(1.0)
self._proxy.shutdown()
test_https_connect_tunnel(self):
test_https_noconnect(self):
'http://scrapy:scrapy@localhost:8888?noconnect'
test_https_connect_tunnel_error(self):
crawler.crawl("https://localhost:99999/status?n=200")
test_https_tunnel_auth_error(self):
'http://wrong:wronger@localhost:8888'
test_https_tunnel_without_leak_proxy_authorization_header(self):
Request("https://localhost:8999/echo")
json.loads(crawler.spider.meta['responses'][0].body)
self.assertTrue('Proxy-Authorization'
echo['headers'])
test_https_noconnect_auth_error(self):
'http://wrong:wronger@localhost:8888?noconnect'
self._assert_got_response_code(407,
_assert_got_response_code(self,
self.assertEqual(str(log).count('Crawled
(%d)'
code),
_assert_got_tunnel_error(self,
self.assertEqual(str(log).count('TunnelError'),
DeprecatedPydispatchTest(unittest.TestCase):
test_import_xlib_pydispatch_show_warning(self):
scrapy.xlib
reload_module(pydispatch)
self.assertIn('Importing
deprecated',
ResponseTypesTest(unittest.TestCase):
test_from_filename(self):
('data.bin',
('file.txt',
('file.xml.gz',
('file.xml',
('file.html',
('file.unknownext',
responsetypes.from_filename(source)
test_from_content_disposition(self):
filename="data.xml"',
filename=data.xml',
(u'attachment;filename=data£.tar.gz'.encode('utf-8'),
(u'attachment;filename=dataµ.tar.gz'.encode('latin-1'),
(u'attachment;filename=data高.doc'.encode('gbk'),
(u'attachment;filename=دورهdata.html'.encode('cp720'),
(u'attachment;filename=日本語版Wikipedia.xml'.encode('iso2022_jp'),
responsetypes.from_content_disposition(source)
test_from_content_type(self):
('text/html;
('text/xml;
('application/xhtml+xml;
('application/vnd.wap.xhtml+xml;
charset=utf-8',
('application/xml;
('application/octet-stream',
('application/x-json;
encoding=UTF8;charset=UTF-8',
responsetypes.from_content_type(source)
test_from_body(self):
(b'\x03\x02\xdf\xdd\x23',
(b'Some
bytes\0',
(b'<html><head><title>Hello</title></head>',
(b'<?xml
encoding="utf-8"',
responsetypes.from_body(source)
test_from_headers(self):
charset=utf-8']},
['application/octet-stream'],
filename=data.txt']},
charset=utf-8'],
['gzip']},
Headers(source)
responsetypes.from_headers(source)
test_from_args(self):
({'url':
'http://www.example.com/data.csv'},
charset=utf-8']}),
'http://www.example.com/item/'},
Headers({'Content-Disposition':
filename="data.xml.gz"']}),
'http://www.example.com/page/'},
responsetypes.from_args(**source)
test_custom_mime_types_loaded(self):
self.assertEqual(responsetypes.mimetypes.guess_type('x.scrapytest')[0],
'x-scrapy/test')
XmlXPathSelector,
HtmlXPathSelector,
SelectorTestCase(unittest.TestCase):
test_simple_selection(self):
b"<p><input
name='a'value='1'/><input
name='b'value='2'/></p>"
TextResponse(url="http://example.com",
xl
sel.xpath('//input')
len(xl))
xl:
self.assertEqual(sel.xpath('//input').extract(),
[x.extract()
sel.xpath('//input')])
sel.xpath("//input[@name='a']/@name")],
[u'a'])
sel.xpath("number(concat(//input[@name='a']/@value,
//input[@name='b']/@value))")],
[u'12.0'])
self.assertEqual(sel.xpath("concat('xpath',
'rules')").extract(),
[u'xpathrules'])
sel.xpath("concat(//input[@name='a']/@value,
//input[@name='b']/@value)")],
[u'12'])
test_root_base_url(self):
b'<html><form
action="/path"><input
name="a"
/></form></html>'
"http://example.com"
TextResponse(url=url,
sel.root.base)
test_deprecated_root_argument(self):
Selector(_root=root)
self.assertEqual(str(w[-1].message),
'Argument
instead')
test_deprecated_root_argument_ambiguous(self):
etree.fromstring(u'<xml/>')
Selector(_root=_root,
root=root)
argument',
test_flavor_detection(self):
b'<div><img
Selector(XmlResponse('http://example.com',
src="a.jpg"><p>Hello</p></img></div>'])
Selector(HtmlResponse('http://example.com',
src="a.jpg"><p>Hello</p></div>'])
test_http_header_encoding_precedence(self):
u'<meta
http-equiv="Content-Type"
content="text/html;
charset=iso-8859-1">'
u'<head>'
u'</head>'
u'<span
id="blank">\xa3</span>'
u'<body>'
u'</body>'
u'<html>'
u'</html>'
html_utf8
html.encode(encoding)
charset=utf-8']}
HtmlResponse(url="http://example.com",
body=html_utf8)
self.assertEquals(x.xpath("//span[@id='blank']/text()").extract(),
[u'\xa3'])
test_badly_encoded_body(self):
TextResponse('http://www.example.com',
body=b'<html><p>an
Jos\xe9
de</p><html>',
Selector(r1).xpath('//text()').extract()
test_weakref_slots(self):
Selector(text='')
weakref.ref(x)
'__dict__'),
__slots__"
x.__class__.__name__
test_deprecated_selector_methods(self):
sel.select('//p')
sel.extract_unquoted()
test_deprecated_selectorlist_methods(self):
sel.xpath('//p').select('.')
sel.xpath('//p').extract_unquoted()
DeprecatedXpathSelectorTest(unittest.TestCase):
'<div><img
test_warnings_xpathselector(self):
test_warnings_xmlxpathselector(self):
test_warnings_htmlxpathselector(self):
scrapy.selector.csstranslator
ScrapyHTMLTranslator,
DeprecatedClassesTest(unittest.TestCase):
test_deprecated_warnings(self):
[ScrapyHTMLTranslator,
ScrapyXPathExpr]:
self.assertIn('%s
str(w[-1].message),
deprecate
warning
BaseSpider,
CSVFeedSpider,
SpiderTest(unittest.TestCase):
warnings.simplefilter("always")
warnings.resetwarnings()
test_base_spider(self):
self.assertEqual(spider.name,
self.assertEqual(spider.start_urls,
test_start_requests(self):
spider.start_requests()
self.assertTrue(inspect.isgenerator(start_requests))
self.assertEqual(list(start_requests),
test_spider_args(self):
self.spider_class('example.com',
self.assertEqual(spider.foo,
test_spider_without_name(self):
self.spider_class)
self.spider_class,
somearg='foo')
test_deprecated_set_crawler_method(self):
spider.set_crawler(crawler)
self.assertIn("set_crawler",
test_from_crawler_crawler_and_settings_population(self):
test_from_crawler_init_call(self):
mock.patch.object(self.spider_class,
'__init__',
return_value=None)
mock_init:
self.spider_class.from_crawler(get_crawler(),
mock_init.assert_called_once_with('example.com',
test_closed_signal_call(self):
TestSpider(self.spider_class):
closed_called
self.closed_called
TestSpider.from_crawler(crawler,
crawler.signals.send_catch_log(signal=signals.spider_opened,
crawler.signals.send_catch_log(signal=signals.spider_closed,
reason=None)
self.assertTrue(spider.closed_called)
test_update_settings(self):
self.spider_class.custom_settings
Settings(project_settings,
self.spider_class.update_settings(settings)
self.assertEqual(settings.get('TEST1'),
self.assertEqual(settings.get('TEST2'),
self.assertEqual(settings.get('TEST3'),
test_logger(self):
spider.logger.info('test
l.check(('example.com',
self.assertIn('spider',
record.__dict__)
self.assertIs(record.spider,
test_log(self):
mock.patch('scrapy.spiders.Spider.logger')
mock_logger:
spider.log('test
msg',
'INFO')
mock_logger.log.assert_called_once_with('INFO',
InitSpiderTest(SpiderTest):
XMLFeedSpiderTest(SpiderTest):
XMLFeedSpider
test_register_namespace(self):
XmlResponse(url='http://example.com/sitemap.xml',
_XMLSpider(self.spider_class):
'http://www.google.com/schemas/sitemap/0.84'),
('b',
'http://www.example.com/schemas/extras/1.0'),
selector.xpath('a:loc/text()').extract(),
selector.xpath('b:updated/text()').extract(),
selector.xpath('other/@value').extract(),
selector.xpath('other/@b:custom').extract(),
('iternodes',
'xml'):
_XMLSpider('example',
iterator=iterator)
list(spider.parse(response))
[u'http://www.example.com/Special-Offers.html'],
[u'fuu'],
[u'bar']},
[u'foo'],
[]},
CSVFeedSpiderTest(SpiderTest):
CrawlSpiderTest(SpiderTest):
test_body
test_process_links(self):
test_process_links_filter(self):
process_links="filter_process_links"),
_test_regex
re.compile('nofollow')
filter_process_links(self,
[link
self._test_regex.search(link.url)]
'http://example.org/about.html'])
test_process_links_generator(self):
test_follow_links_attribute_population(self):
get_crawler(settings_dict=settings_dict)
test_follow_links_attribute_deprecated_population(self):
self.assertFalse(hasattr(spider,
spider.set_crawler(get_crawler())
spider.set_crawler(get_crawler(settings_dict=settings_dict))
SitemapSpiderTest(SpiderTest):
b"SITEMAP"
gzip.GzipFile(fileobj=f,
mode='w+b')
g.write(BODY)
g.close()
GZBODY
f.getvalue()
assertSitemapBody(self,
self.assertEqual(spider._get_sitemap_body(response),
test_get_sitemap_body(self):
XmlResponse(url="http://www.example.com/",
HtmlResponse(url="http://www.example.com/",
Response(url="http://www.example.com/favicon.ico",
test_get_sitemap_body_gzip_headers(self):
Response(url="http://www.example.com/sitemap",
body=self.GZBODY,
headers={"content-type":
test_get_sitemap_body_xml_url(self):
TextResponse(url="http://www.example.com/sitemap.xml",
test_get_sitemap_body_xml_url_compressed(self):
Response(url="http://www.example.com/sitemap.xml.gz",
body=self.GZBODY)
test_get_sitemap_urls_from_robotstxt(self):
TextResponse(url="http://www.example.com/robots.txt",
body=robots)
self.assertEqual([req.url
spider._parse_sitemap(r)],
BaseSpiderDeprecationTest(unittest.TestCase):
test_basespider_is_deprecated(self):
MySpider1(BaseSpider):
self.assertEqual(w[0].category,
inspect.getsourcelines(MySpider1)[1])
test_basespider_issubclass(self):
MySpider2(Spider):
MySpider2a(MySpider2):
issubclass(MySpider2,
issubclass(MySpider2a,
issubclass(Foo,
issubclass(Foo2,
test_basespider_isinstance(self):
MySpider3(Spider):
'myspider3'
MySpider3a(MySpider3):
isinstance(MySpider3(),
isinstance(MySpider3a(),
isinstance(Foo(),
isinstance(Foo2(),
test_crawl_spider(self):
DepthMiddleware
StatsCollector
TestDepthMiddleware(TestCase):
crawler._create_spider('scrapytest.org')
StatsCollector(crawler)
self.stats.open_spider(self.spider)
DepthMiddleware(1,
self.stats,
resp.request
[Request('http://scrapytest.org')]
rdc
self.stats.get_value('request_depth_count/1',
self.assertEquals(rdc,
req.meta['depth']
out2
self.assertEquals(out2,
rdm
self.stats.get_value('request_depth_max',
self.assertEquals(rdm,
self.stats.close_spider(self.spider,
TrialTestCase
HttpErrorMiddleware,
HttpError
_HttpErrorSpider(Spider):
'httperror'
"http://localhost:8998/status?n=200",
"http://localhost:8998/status?n=404",
"http://localhost:8998/status?n=402",
"http://localhost:8998/status?n=500",
bypass_status_codes
super(_HttpErrorSpider,
self.failed
self.skipped
self.parsed
self.parse,
self.parsed.add(response.url[-3:])
failure.value.response
self.bypass_status_codes:
self.skipped.add(response.url[-3:])
self.parse(response)
self.failed.add(failure.value.response.url[-3:])
_responses(request,
status_codes):
status_codes:
status=code)
responses.append(response)
TestHttpErrorMiddleware(TestCase):
HttpErrorMiddleware(Settings({}))
self.res404
404])
test_process_spider_exception(self):
self.assertEquals([],
HttpError(self.res404),
Exception(),
test_handle_httpstatus_list(self):
res.request
self.mw.process_spider_input(res,
TestHttpErrorMiddlewareSettings(TestCase):
HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES':
(402,)}))
self.mw.process_spider_input(self.res402,
test_spider_override_settings(self):
self.res402,
TestHttpErrorMiddlewareHandleAll(TestCase):
HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL':
TestHttpErrorMiddlewareIntegrational(TrialTestCase):
test_middleware_works(self):
crawler.spider.skipped,
crawler.spider.skipped
{'200'})
'402',
crawler.crawl(bypass_status_codes={402})
{'200',
'402'})
self.assertEqual(crawler.spider.skipped,
{'402'})
<404',
<500',
<200',
<402',
OffsiteMiddleware
TestOffsiteMiddleware(TestCase):
crawler._create_spider(**self._get_spiderargs())
OffsiteMiddleware.from_crawler(crawler)
'scrapy.org',
'scrapy.test.org'])
[Request('http://scrapytest.org/1'),
Request('http://scrapy.org/1'),
Request('http://sub.scrapy.org/1'),
Request('http://offsite.tld/letmepass',
dont_filter=True),
Request('http://scrapy.test.org/')]
[Request('http://scrapy2.org'),
Request('http://offsite.tld/'),
Request('http://offsite.tld/scrapytest.org'),
Request('http://offsite.tld/rogue.scrapytest.org'),
Request('http://rogue.scrapytest.org.haha.com'),
Request('http://roguescrapytest.org'),
Request('http://test.org/'),
Request('http://notscrapy.test.org/')]
onsite_reqs)
TestOffsiteMiddleware2(TestOffsiteMiddleware):
allowed_domains=None)
[Request('http://a.com/b.html'),
Request('http://b.com/1')]
TestOffsiteMiddleware3(TestOffsiteMiddleware2):
TestOffsiteMiddleware4(TestOffsiteMiddleware3):
bad_hostname
urlparse('http:////scrapytest.org').hostname
bad_hostname])
[Request('http://scrapytest.org/1')]
RefererMiddleware
TestRefererMiddleware(TestCase):
RefererMiddleware()
[Request('http://scrapytest.org/')]
self.assertEquals(out[0].headers.get('Referer'),
b'http://scrapytest.org')
UrlLengthMiddleware
TestUrlLengthMiddleware(TestCase):
short_url_req
long_url_req
Request('http://scrapytest.org/this_is_a_long_url')
[short_url_req,
long_url_req]
UrlLengthMiddleware(maxlength=25)
list(mw.process_spider_output(res,
[short_url_req])
SpiderState
SpiderStateTest(unittest.TestCase):
test_store_load(self):
os.mkdir(jobdir)
spider.state['one']
spider.state['dt']
spider2
ss2
ss2.spider_opened(spider2)
'dt':
dt})
ss2.spider_closed(spider2)
test_state_attribute(self):
SpiderState()
test_not_configured(self):
SpiderState.from_crawler,
queuelib.tests
test_queue
MarshalFifoDiskQueue,
MarshalLifoDiskQueue,
PickleFifoDiskQueue,
_test_procesor(x):
TestLoader(ItemLoader):
staticmethod(_test_procesor)
MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):
MarshalFifoDiskQueue(self.qpath,
ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
PickleFifoDiskQueueTest(MarshalFifoDiskQueueTest):
PickleFifoDiskQueue(self.qpath,
ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):
MarshalLifoDiskQueue(self.qpath)
PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):
PickleLifoDiskQueue(self.qpath)
StatsCollector,
DummyStatsCollector
StatsCollectorTest(unittest.TestCase):
test_collector(self):
StatsCollector(self.crawler)
stats.set_value('test2',
'test2':
stats.inc_value('test2')
stats.inc_value('test2',
stats.max_value('test3',
self.assertEqual(stats.get_value('test3'),
stats.min_value('test4',
self.assertEqual(stats.get_value('test4'),
test_dummy_collector(self):
DummyStatsCollector(self.crawler)
stats.inc_value('v1')
stats.max_value('v2',
stats.min_value('v3',
stats.open_spider('a')
self.assertEqual(stats.get_stats('a'),
ToplevelTestCase(TestCase):
test_version(self):
self.assertIs(type(scrapy.__version__),
test_version_info(self):
self.assertIs(type(scrapy.version_info),
test_request_shortcut(self):
self.assertIs(scrapy.Request,
self.assertIs(scrapy.FormRequest,
FormRequest)
test_spider_shortcut(self):
self.assertIs(scrapy.Spider,
test_selector_shortcut(self):
self.assertIs(scrapy.Selector,
test_item_shortcut(self):
self.assertIs(scrapy.Item,
Item)
self.assertIs(scrapy.Field,
Field)
UrlparseTestCase(unittest.TestCase):
test_s3_url(self):
urlparse('s3://bucket/key/name?param=value')
self.assertEquals(p.scheme,
self.assertEquals(p.hostname,
'bucket')
self.assertEquals(p.path,
'/key/name')
self.assertEquals(p.query,
'param=value')
BuildComponentListTest(unittest.TestCase):
test_build_dict(self):
self.assertEqual(build_component_list(d,
test_backwards_compatible_build_dict(self):
'five':
{'two':
self.assertEqual(build_component_list(base,
test_return_list(self):
test_map_dict(self):
self.assertEqual(build_component_list({},
['ONE',
'TWO',
'THREE'])
test_map_list(self):
'C'])
test_duplicate_components_in_dict(self):
duplicate_dict
'ONE':
duplicate_dict,
test_duplicate_components_in_list(self):
duplicate_list
'a']
duplicate_list,
test_duplicate_components_in_basesettings(self):
duplicate_bs
BaseSettings({'one':
priority=10)
'one'])
duplicate_bs.set('one',
duplicate_bs['one'],
duplicate_bs['ONE'],
duplicate_bs,
UtilsConfTestCase(unittest.TestCase):
test_arglist_to_dict(self):
self.assertEqual(arglist_to_dict(['arg1=val1',
'arg2=val2']),
{'arg1':
'arg2':
get_shell_embed_func
UtilsConsoleTestCase(unittest.TestCase):
test_get_shell_embed_func(self):
get_shell_embed_func(['invalid'])
self.assertEqual(shell,
get_shell_embed_func(['invalid','python'])
'_embed_standard_shell')
bpy,
'bpython
test_get_shell_embed_func2(self):
get_shell_embed_func(['bpython'])
'_embed_bpython_shell')
ipy,
'IPython
test_get_shell_embed_func3(self):
get_shell_embed_func()
'_embed_ipython_shell')
['scrapy.utils.datatypes']
CaselessDictTest(unittest.TestCase):
{'red':
'black':
(('red',
('black',
3))
test_caseless(self):
d['key_Lower']
self.assertEqual(d['KEy_loWer'],
self.assertEqual(d.get('KEy_loWer'),
d['KEY_LOWER']
self.assertEqual(d['key_Lower'],
self.assertEqual(d.get('key_Lower'),
CaselessDict({'key_lower':
d['key_LOWER']
'key_LOWER')
'key_lower')
test_getdefault(self):
d['c']
CaselessDict({'a':
2})
d.setdefault('A',
d.setdefault('c',
self.assertEqual(d['C'],
test_fromkeys(self):
CaselessDict.fromkeys(keys)
CaselessDict.fromkeys(keys,
instance.fromkeys(keys)
instance.fromkeys(keys,
test_pop(self):
self.assertEqual(d.pop('A'),
d.pop,
test_normkey(self):
key.title()
d['key-one']
self.assertEqual(list(d.keys()),
['Key-One'])
test_normvalue(self):
MyDict({'key':
d['key']
d.setdefault('key',
d.update({'key':
MyDict.fromkeys(('key',),
CaselessDict({'header1':
self.assertEqual(h1.get('header1'),
h2.get('header1'))
CaselessDict)
process_chain_both,
xrange
MustbeDeferredTest(unittest.TestCase):
test_success_function(self):
test_unfired_deferred(self):
steps)
cb1(value,
"(cb1
cb2(value,
defer.succeed("(cb2
arg2))
cb3(value,
cb_fail(value,
Failure(TypeError())
eb1(failure,
"(eb1
(failure.value.__class__.__name__,
DeferUtilsTest(unittest.TestCase):
test_process_chain(self):
(cb1
self.assertTrue(gotexc)
test_process_chain_both(self):
process_chain_both([cb_fail,
eb1,
Failure(ZeroDivisionError())
process_chain_both([eb1,
[eb1,
test_process_parallel(self):
['(cb1
'(cb2
'(cb3
v2)'])
test_process_parallel_failure(self):
self.failUnlessFailure(d,
TypeError)
IterErrbackTest(unittest.TestCase):
test_iter_errback_good(self):
itergood():
list(iter_errback(itergood(),
list(range(10)))
self.failIf(errors)
test_iter_errback_bad(self):
iterbad():
list(iter_errback(iterbad(),
self.assertEqual(len(errors),
self.assertIsInstance(errors[0].value,
create_deprecated_class,
MyWarning(UserWarning):
SomeBaseClass(object):
NewName(SomeBaseClass):
WarnWhenSubclassedTest(unittest.TestCase):
_mywarnings(self,
category=MyWarning):
x.category
MyWarning]
test_no_warning_on_definition(self):
self.assertEqual(w,
test_subclassing_warning_message(self):
"tests.test_utils_deprecate.UserClass
tests.test_utils_deprecate.Deprecated,
"please
tests.test_utils_deprecate.NewName."
others)"
inspect.getsourcelines(UserClass)[1])
test_custom_class_paths(self):
new_class_path='foo.NewClass',
old_class_path='bar.OldClass',
Deprecated()
test_subclassing_warns_only_on_direct_childs(self):
warn_once=False,
NoWarnOnMe(UserClass):
test_subclassing_warns_once_by_default(self):
FooClass(Deprecated):
BarClass(Deprecated):
test_warning_on_instance(self):
lineno
Deprecated(),
inspect.getlineno(inspect.currentframe())
UserClass()
"tests.test_utils_deprecate.Deprecated
tests.test_utils_deprecate.NewName
lineno)
test_warning_auto_message(self):
UserClass2(Deprecated):
str(w[0].message)
self.assertIn("tests.test_utils_deprecate.NewName",
self.assertIn("tests.test_utils_deprecate.Deprecated",
test_issubclass(self):
UpdatedUserClass1(NewName):
UpdatedUserClass1a(NewName):
OutdatedUserClass1(DeprecatedName):
OutdatedUserClass1a(DeprecatedName):
issubclass(UnrelatedClass,
OutdatedUserClass1a)
issubclass(OutdatedUserClass1a,
OutdatedUserClass1)
issubclass,
test_isinstance(self):
UpdatedUserClass2(NewName):
UpdatedUserClass2a(NewName):
OutdatedUserClass2(DeprecatedName):
OutdatedUserClass2a(DeprecatedName):
OutdatedUserClass2)
OutdatedUserClass2a)
isinstance(UnrelatedClass(),
isinstance(OldStyleClass(),
test_clsdict(self):
self.assertEqual(Deprecated.foo,
test_deprecate_a_class_with_custom_metaclass(self):
Meta1
type('Meta1',
(type,),
Meta1('New',
New)
test_deprecate_subclass_of_deprecated_class(self):
AlsoDeprecated
create_deprecated_class('AlsoDeprecated',
Deprecated,
new_class_path='foo.Bar',
str(map(str,
w)))
AlsoDeprecated()
UserClass(AlsoDeprecated):
test_inspect_stack(self):
mock.patch('inspect.stack',
side_effect=IndexError):
SubClass(DeprecatedName):
self.assertIn("Error
module",
@mock.patch('scrapy.utils.deprecate.DEPRECATION_RULES',
[('scrapy.contrib.pipeline.',
'scrapy.extensions.')])
UpdateClassPathTest(unittest.TestCase):
test_old_path_gets_fixed(self):
update_classpath('scrapy.contrib.debug.Debug')
'scrapy.extensions.debug.Debug')
self.assertIn("scrapy.contrib.debug.Debug",
self.assertIn("scrapy.extensions.debug.Debug",
test_sorted_replacement(self):
update_classpath('scrapy.contrib.pipeline.Pipeline')
'scrapy.pipelines.Pipeline')
test_unmatched_path_stays_the_same(self):
update_classpath('scrapy.unmatched.Path')
'scrapy.unmatched.Path')
GunzipTest(unittest.TestCase):
test_gunzip_basic(self):
'feed-sample1.xml.gz'),
self.assertEqual(len(text),
9950)
test_gunzip_truncated(self):
'truncated-crc-error.gz'),
text.endswith(b'</html')
test_gunzip_no_gzip_file_raises(self):
'feed-sample1.xml'),
test_gunzip_truncated_short(self):
'truncated-crc-error-short.gz'),
text.endswith(b'</html>')
test_is_x_gzipped_right(self):
"application/x-gzip"})
test_is_gzipped_right(self):
test_is_gzipped_not_quite(self):
"application/gzippppp"})
test_is_gzipped_case_insensitive(self):
"Application/X-Gzip"})
"application/X-GZIP
;
charset=utf-8"})
test_is_gzipped_empty(self):
test_is_gzipped_wrong(self):
"application/javascript"})
test_is_gzipped_with_charset(self):
"application/x-gzip;charset=utf-8"})
ChunkedTest(unittest.TestCase):
test_decode_chunked_transfer(self):
"25\r\n"
chunk\r\n\r\n"
"1C\r\n"
one\r\n\r\n"
"3\r\n"
"con\r\n"
"8\r\n"
"sequence\r\n"
"0\r\n\r\n"
decode_chunked_transfer(chunked_body)
self.assertEqual(body,
chunk\r\n"
one\r\n"
"consequence")
HttpobjUtilsTest(unittest.TestCase):
test_urlparse_cached(self):
"http://www.example.com/index.html"
request1
urlparse_cached(request2)
csviter,
_body_or_str,
FOOBAR_NL
u"foo"
u"bar"
XmliterTestCase(unittest.TestCase):
staticmethod(xmliter)
test_xmliter(self):
'product'):
attrs.append((x.xpath("@id").extract(),
x.xpath("name/text()").extract(),
x.xpath("./type/text()").extract()))
[(['001'],
1'],
1']),
(['002'],
2'],
2'])])
test_xmliter_unusual_node(self):
nodenames
[e.xpath('name()').extract()
'matchme...')]
self.assertEqual(nodenames,
[['matchme...']])
test_xmliter_unicode(self):
body=body.encode('utf-8')),
encoding='utf-8')):
self.xmliter(r,
u'þingflokkur'):
attrs.append((x.xpath('@id').extract(),
x.xpath(u'./skammstafanir/stuttskammstöfun/text()').extract(),
x.xpath(u'./tímabil/fyrstaþing/text()').extract()))
[([u'26'],
[u'-'],
[u'80']),
([u'21'],
[u'Ab'],
[u'76']),
([u'27'],
[u'A'],
[u'27'])])
test_xmliter_text(self):
self.assertEqual([x.xpath("text()").extract()
'product')],
[[u'one'],
[u'two']])
test_xmliter_namespaces(self):
node.register_namespace('g',
self.assertEqual(node.xpath('title/text()').extract(),
['Item
self.assertEqual(node.xpath('description/text()').extract(),
['This
self.assertEqual(node.xpath('link/text()').extract(),
['http://www.mydummycompany.com/items/1'])
self.assertEqual(node.xpath('g:image_link/text()').extract(),
self.assertEqual(node.xpath('g:id/text()').extract(),
['ITEM_1'])
self.assertEqual(node.xpath('g:price/text()').extract(),
['400'])
self.assertEqual(node.xpath('image_link/text()').extract(),
self.assertEqual(node.xpath('id/text()').extract(),
self.assertEqual(node.xpath('price/text()').extract(),
test_xmliter_exception(self):
self.assertRaises(AssertionError,
test_xmliter_encoding(self):
encoding="ISO-8859-9"?>\n<xml>\n
<item>Some
\xd6\xc7\xde\xdd\xd0\xdc
\xfc\xf0\xfd\xfe\xe7\xf6</item>\n</xml>\n\n'
XmlResponse('http://www.example.com',
next(self.xmliter(response,
'item')).extract(),
u'<item>Some
\xd6\xc7\u015e\u0130\u011e\xdc
\xfc\u011f\u0131\u015f\xe7\xf6</item>'
LxmlXmliterTestCase(XmliterTestCase):
staticmethod(xmliter_lxml)
test_xmliter_iterate_namespace(self):
no_namespace_iter
'image_link')
self.assertEqual(len(list(no_namespace_iter)),
namespace_iter
'image_link',
['http://www.mydummycompany.com/images/item2.jpg'])
test_xmliter_namespaces_prefix(self):
'http://www.w3.org/TR/html4/',
'h')
self.assertEqual(len(node.xpath('h:tr/h:td').extract()),
self.assertEqual(node.xpath('h:tr/h:td[1]/text()').extract(),
['Apples'])
self.assertEqual(node.xpath('h:tr/h:td[2]/text()').extract(),
['Bananas'])
'http://www.w3schools.com/furniture',
'f')
self.assertEqual(node.xpath('f:name/text()').extract(),
['African
Coffee
Table'])
UtilsCsvTestCase(unittest.TestCase):
sample_feeds_dir
'sample_data',
'feeds')
sample_feed_path
sample_feed2_path
sample_feed3_path
test_csviter_defaults(self):
[row
csv]
result_row
self.assert_(all((isinstance(k,
result_row.keys())))
self.assert_(all((isinstance(v,
result_row.values())))
test_csviter_delimiter(self):
test_csviter_quotechar(self):
'feed-sample6.csv').replace(b',',
b'|')
response1
body=body1)
csv1
csviter(response1,
csv1],
body=body2)
csv2
csviter(response2,
delimiter="|",
csv2],
test_csviter_wrong_quotechar(self):
[{u"'id'":
u"1",
u"'alpha'",
u"'foobar'"},
u"2",
u"'unicode'",
u"'\xfan\xedc\xf3d\xe9\u203d'"},
u"'3'",
u"'multi'",
u"'foo"},
u"4",
u"'empty'",
u""}])
test_csviter_delimiter_binary_response_assume_utf8_encoding(self):
Response(url="http://example.com/",
test_csviter_headers(self):
'feed-sample3.csv').splitlines()
sample[0].split(b','),
b'\n'.join(sample[1:])
headers=[h.decode('utf-8')
headers])
u'foo\nbar'},
test_csviter_falserow(self):
b'\n'.join((body,
b'a,b',
b'a,b,c,d'))
test_csviter_exception(self):
test_csviter_encoding(self):
body=body1,
u'latin1',
u'\xf1\xe1\xe9\xf3'}])
body=body2,
encoding='cp852')
u'cp852',
u'\u255a\u2569\u2569\u2569\u2550\u2550\u2557'}])
TestHelper(unittest.TestCase):
b'utf8-body'
bbody.decode('utf8')
txtresponse
body=bbody,
Response(url='http://example.org/',
body=bbody)
test_body_or_str(self):
(self.bbody,
self.txtresponse,
self.response):
self._assert_type_and_value(r1,
unicode=True)
self._assert_type_and_value(r2,
unicode=False)
self._assert_type_and_value(r3,
self.bbody,
type(r2))
type(r3))
_assert_type_and_value(self,
self.assertTrue(type(a)
{!r}'.format(type(a),
self.assertEqual(a,
(failure_to_exc_info,
TopLevelFormatter,
StreamLogger)
FailureToExcInfoTest(unittest.TestCase):
test_failure(self):
0/0
self.assertTupleEqual(exc_info,
failure_to_exc_info(failure))
test_non_failure(self):
self.assertIsNone(failure_to_exc_info('test'))
TopLevelFormatterTest(unittest.TestCase):
self.handler.addFilter(TopLevelFormatter(['test']))
test_top_level_logger(self):
test_children_logger(self):
logging.getLogger('test.test1')
test_overlapping_name_logger(self):
logging.getLogger('test2')
l.check(('test2',
test_different_name_logger(self):
logging.getLogger('different')
l.check(('different',
LogCounterHandlerTest(unittest.TestCase):
self.logger.setLevel(logging.NOTSET)
get_crawler(settings_dict={'LOG_LEVEL':
'WARNING'})
LogCounterHandler(self.crawler)
self.logger.addHandler(self.handler)
self.logger.removeHandler(self.handler)
self.assertIsNone(self.crawler.stats.get_value('log_count/DEBUG'))
self.assertIsNone(self.crawler.stats.get_value('log_count/WARNING'))
self.assertIsNone(self.crawler.stats.get_value('log_count/ERROR'))
self.assertIsNone(self.crawler.stats.get_value('log_count/CRITICAL'))
test_accepted_level(self):
self.logger.error('test
self.assertEqual(self.crawler.stats.get_value('log_count/ERROR'),
test_filtered_out_level(self):
self.logger.debug('test
StreamLoggerTest(unittest.TestCase):
logger.setLevel(logging.WARNING)
StreamLogger(logger,
print('test
equal_attributes,
WeakKeyCache,
stringify_dict,
get_func_args,
without_none_values)
['scrapy.utils.python']
ToUnicodeTest(unittest.TestCase):
test_converting_an_utf8_encoded_string_to_unicode(self):
self.assertEqual(to_unicode(b'lel\xc3\xb1e'),
test_converting_a_latin_1_encoded_string_to_unicode(self):
self.assertEqual(to_unicode(b'lel\xf1e',
test_converting_a_unicode_to_unicode_should_return_the_same_object(self):
self.assertEqual(to_unicode(u'\xf1e\xf1e\xf1e'),
u'\xf1e\xf1e\xf1e')
423)
to_unicode(b'a\xedb',
u'a\ufffdb'
ToBytesTest(unittest.TestCase):
test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):
49'),
test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):
49',
test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):
self.assertEqual(to_bytes(b'lel\xf1e'),
b'lel\xf1e')
unittest)
to_bytes(u'a\ufffdb',
'latin-1',
b'a?b'
MemoizedMethodTest(unittest.TestCase):
test_memoizemethod_noargs(self):
cached(self):
noncached(self):
A()
a.noncached()
BinaryIsTextTest(unittest.TestCase):
test_binaryistext(self):
binary_is_text(b"hello")
test_utf_16_strings_contain_null_bytes(self):
binary_is_text(u"hello".encode('utf-16'))
test_one_with_encoding(self):
binary_is_text(b"<div>Price
\xa3</div>")
test_real_binary_bytes(self):
binary_is_text(b"\x02\xa3")
UtilsPythonTestCase(unittest.TestCase):
test_equal_attributes(self):
Obj:
a.x
b.x
['x']))
b.y
a.meta
b.meta
['meta']))
b.meta['z']
get_z
operator.itemgetter('z')
get_meta
operator.attrgetter('meta')
compare_z
get_z(get_meta(obj))
test_weakkeycache(self):
_Weakme(object):
_values
count()
wk
WeakKeyCache(lambda
next(_values))
_Weakme()
wk[k]
self.assertNotEqual(v,
wk[_Weakme()])
self.assertFalse(len(wk._weakdict))
test_stringify_dict(self):
b'c',
stringify_dict(d,
test_stringify_dict_tuples(self):
tuples
[('a',
123),
(u'b',
'c'),
(u'd',
u'e'),
(object(),
u'e')]
dict(tuples)
stringify_dict(tuples,
d2.keys()),
d2.keys())
test_stringify_dict_keys_only(self):
stringify_dict(d)
test_get_func_args(self):
f1(a,
f2(a,
b=None,
c=None):
Callable(object):
A(1,
cal
Callable()
partial_f1
partial_f2
b=None)
partial_f3
functools.partial(partial_f2,
self.assertEqual(get_func_args(f1),
self.assertEqual(get_func_args(f2),
self.assertEqual(get_func_args(A),
self.assertEqual(get_func_args(a.method),
self.assertEqual(get_func_args(partial_f1),
['b',
self.assertEqual(get_func_args(partial_f2),
self.assertEqual(get_func_args(partial_f3),
['c'])
self.assertEqual(get_func_args(cal),
self.assertEqual(get_func_args(object),
self.assertEqual(get_func_args(six.text_type.split),
self.assertEqual(get_func_args("
".join),
self.assertEqual(get_func_args(operator.itemgetter(2)),
test_without_none_values(self):
self.assertEqual(without_none_values([1,
4]),
self.assertEqual(without_none_values((1,
4)),
4))
without_none_values({'one':
'none':
4}),
RequestSerializationTest(unittest.TestCase):
test_all_attributes(self):
callback='parse_item',
errback='handle_error',
method="POST",
body=b"some
headers={'content-encoding':
'text/html;
charset=latin-1'},
cookies={'currency':
u'руб'},
priority=20,
'b'})
test_latin1_body(self):
test_utf8_body(self):
_assert_serializes_ok(self,
self._assert_same_request(request,
_assert_same_request(self,
r1,
r2):
self.assertEqual(r1.callback,
r2.callback)
self.assertEqual(r1.errback,
r2.errback)
r2.method)
self.assertEqual(r1.cookies,
r2.cookies)
self.assertEqual(r1._encoding,
r2._encoding)
self.assertEqual(r1.priority,
r2.priority)
test_callback_serialization(self):
callback=self.spider.parse_item,
errback=self.spider.handle_error)
self._assert_serializes_ok(r,
test_unserializable_callback1(self):
callback=lambda
test_unserializable_callback2(self):
callback=self.spider.parse_item)
handle_error(self,
request_fingerprint,
_fingerprint_cache,
request_authenticate,
UtilsRequestTest(unittest.TestCase):
Request("http://www.example.com/query?id=111&cat=222")
Request("http://www.example.com/query?cat=222&id=111")
request_fingerprint(r1))
Request('http://www.example.com/hnnoticiaj1.aspx?78132,199')
Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')
_fingerprint_cache[r1][None])
r2.headers['SESSIONID']
r2.headers['Accept-Language']
r3.headers['Accept-Language']
r3.headers['SESSIONID']
request_fingerprint(r2),
request_fingerprint(r1,
request_fingerprint(r2,
self.assertEqual(request_fingerprint(r3,
include_headers=['accept-language',
'sessionid']),
request_fingerprint(r3,
include_headers=['SESSIONID',
'Accept-Language']))
body=b'request
self.assertNotEqual(request_fingerprint(r2),
fp1
request_fingerprint(r1)
request_fingerprint(r2)
self.assertNotEqual(fp1,
fp2)
test_request_authenticate(self):
request_authenticate(r,
'someuser',
'somepass')
self.assertEqual(r.headers['Authorization'],
c29tZXVzZXI6c29tZXBhc3M=')
test_request_httprepr(self):
Request("http://www.example.com/some/page.html?arg=1")
/some/page.html?arg=1
b"text/html"},
b'POST
www.example.com\r\nContent-Type:
test_request_httprepr_for_non_http_request(self):
request_httprepr(Request("file:///tmp/foo.txt"))
request_httprepr(Request("ftp://localhost/tmp/foo.txt"))
(response_httprepr,
get_meta_refresh,
get_base_url,
response_status_message)
['scrapy.utils.response']
ResponseUtilsTest(unittest.TestCase):
dummy_response
body=b'dummy_response')
test_response_httprepr(self):
OK\r\n\r\n')
status=404,
Found\r\nContent-Type:
status=6666,
6666
\r\nContent-Type:
test_open_in_browser(self):
"http:///www.example.com/some/page.html"
b"<html>
<head>
<title>test
page</title>
</head>
<body>test
body</body>
</html>"
browser_open(burl):
urlparse(burl).path
burl.replace('file://',
self.assertIn(b'<base
href="'
to_bytes(url)
b'">',
bbody)
_openfunc=browser_open),
"Browser
test_get_meta_refresh(self):
self.assertEqual(get_meta_refresh(r1),
(5.0,
'http://example.org/newpage'))
self.assertEqual(get_meta_refresh(r2),
self.assertEqual(get_meta_refresh(r3),
test_get_base_url(self):
self.assertEqual(get_base_url(resp),
"http://www.example.com/img/")
self.assertEqual(get_base_url(resp2),
test_response_status_message(self):
self.assertEqual(response_status_message(200),
'200
OK')
self.assertEqual(response_status_message(404),
'404
self.assertEqual(response_status_message(573),
"573
Status")
JsonEncoderTestCase(unittest.TestCase):
ScrapyJSONEncoder()
test_encode_decode(self):
dts
"2010-01-02
10:11:12"
datetime.date(2010,
ds
"2010-01-02"
datetime.time(10,
"10:11:12"
dec
Decimal("1000.12")
"1000.12"
'foo'),
(d,
ds),
ts),
(dt,
dts),
(dec,
decs),
(['foo',
d],
ds])]:
self.assertEqual(self.encoder.encode(input),
json.dumps(output))
test_encode_deferred(self):
self.assertIn('Deferred',
self.encoder.encode(defer.Deferred()))
test_encode_request(self):
Request("http://www.example.com/lala")
self.assertIn(r.method,
test_encode_response(self):
Response("http://www.example.com/lala")
self.assertIn(str(r.status),
send_catch_log,
send_catch_log_deferred
SendCatchLogTest(unittest.TestCase):
test_send_catch_log(self):
dispatcher.connect(self.error_handler,
dispatcher.connect(self.ok_handler,
defer.maybeDeferred(
self._get_result,
test_signal,
arg='test',
handlers_called=handlers_called
self.error_handler
self.ok_handler
self.assertIn('error_handler',
record.getMessage())
self.assertEqual(record.levelname,
'ERROR')
self.assertEqual(result[0][0],
self.error_handler)
self.assert_(isinstance(result[0][1],
self.assertEqual(result[1],
(self.ok_handler,
"OK"))
dispatcher.disconnect(self.error_handler,
dispatcher.disconnect(self.ok_handler,
send_catch_log(signal,
error_handler(self,
handlers_called.add(self.error_handler)
"OK"
SendCatchLogDeferredTest(SendCatchLogTest):
SendCatchLogDeferredTest2(SendCatchLogTest):
"OK")
SendCatchLogTest2(unittest.TestCase):
test_error_logged_if_deferred_not_supported(self):
test_handler
dispatcher.connect(test_handler,
send_catch_log(test_signal)
self.assertIn("Cannot
handler",
str(l))
dispatcher.disconnect(test_handler,
SitemapTest(unittest.TestCase):
test_sitemap(self):
{'priority':
'0.8',
'http://www.example.com/Special-Offers.html',
'weekly'}])
test_sitemap_index(self):
'sitemapindex'
'http://www.example.com/sitemap1.xml.gz',
'2004-10-01T18:23:17+00:00'},
'http://www.example.com/sitemap2.xml.gz',
'2005-01-01'}])
test_sitemap_strip(self):
test_sitemap_wrong_ns(self):
test_sitemap_wrong_ns2(self):
test_sitemap_urls_from_robots(self):
self.assertEqual(list(sitemap_urls_from_robots(robots)),
test_sitemap_blanklines(self):
'http://www.example.com/sitemap1.xml'},
'http://www.example.com/sitemap2.xml'},
'http://www.example.com/sitemap3.xml'},
test_comment(self):
'http://www.example.com/'}
test_alternate(self):
'http://www.example.com/english/',
'alternate':
['http://www.example.com/deutsch/',
'http://www.example.com/schweiz-deutsch/',
'http://www.example.com/english/']
test_xml_entity_expansion(self):
'http://127.0.0.1:8000/'}])
MyBaseSpider(CrawlSpider):
MySpider1(MyBaseSpider):
'myspider1'
MySpider2(MyBaseSpider):
'myspider2'
UtilsSpidersTestCase(unittest.TestCase):
test_iterate_spider_output(self):
BaseItem()
self.assertEqual(list(iterate_spider_output(i)),
[i])
self.assertEqual(list(iterate_spider_output(r)),
[r])
self.assertEqual(list(iterate_spider_output(o)),
[o])
self.assertEqual(list(iterate_spider_output([r,
o])),
[r,
o])
test_iter_spider_classes(self):
tests.test_utils_spider
iter_spider_classes(tests.test_utils_spider)
self.assertEqual(set(it),
{MySpider1,
MySpider2})
render_templatefile
['scrapy.utils.template']
UtilsRenderTemplateFileTestCase(unittest.TestCase):
self.tmp_path
rmtree(self.tmp_path)
test_simple_render(self):
dict(project_name='proj',
name='spi',
classname='TheSpider')
${project_name}.spiders.${name}
${classname}'
rendered
proj.spiders.spi
TheSpider'
template_path
'templ.py.tmpl')
'templ.py')
open(template_path,
tmpl_file:
tmpl_file.write(template.encode('utf8'))
os.path.isfile(template_path)
render_templatefile(template_path,
**context)
self.assertFalse(os.path.exists(template_path))
self.assertEqual(result.read().decode('utf8'),
rendered)
os.remove(render_path)
os.path.exists(render_path)
iself
__name__:
trackref
Foo(trackref.object_ref):
Bar(trackref.object_ref):
TrackrefTestCase(unittest.TestCase):
trackref.live_refs.clear()
test_format_live_refs(self):
trackref.format_live_refs(),
trackref.format_live_refs(ignore=Foo),
test_print_live_refs_empty(self,
'Live
References\n\n\n')
test_print_live_refs_with_objects(self,
ago\n\n''')
test_get_oldest(self):
self.assertIs(trackref.get_oldest('Foo'),
self.assertIs(trackref.get_oldest('Bar'),
self.assertIsNone(trackref.get_oldest('XXX'))
test_iter_all(self):
set(trackref.iter_all('Foo')),
{o1,
o3},
(url_is_from_any_domain,
url_is_from_spider,
add_http_if_no_scheme,
guess_scheme,
parse_url)
['scrapy.utils.url']
UrlUtilsTest(unittest.TestCase):
test_url_is_from_any_domain(self):
'http://www.wheele-bin-art.co.uk/get/product/123'
'http://wheele-bin-art.co.uk/get/product/123'
'http://www.Wheele-Bin-Art.co.uk/get/product/123'
['wheele-bin-art.CO.UK']))
['WHEELE-BIN-ART.CO.UK']))
'http://192.169.0.15:8080/mypage.html'
['192.169.0.15:8080']))
['192.169.0.15']))
'javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20javascript:%20document.orderform_2581_1190810811.submit%28%29'
self.assertFalse(url_is_from_any_domain(url+'.testdomain.com',
test_url_is_from_spider(self):
Spider(name='example.com')
test_url_is_from_spider_class_attributes(self):
test_url_is_from_spider_with_allowed_domains(self):
allowed_domains=['example.org',
'example.net'])
allowed_domains=set(('example.com',
'example.net')))
allowed_domains=('example.com',
'example.net'))
test_url_is_from_spider_with_allowed_domains_class_attributes(self):
('example.org',
'example.net')
CanonicalizeUrlTest(unittest.TestCase):
test_canonicalize_url(self):
self.assertEqual(canonicalize_url("http://www.example.com/"),
test_return_str(self):
isinstance(canonicalize_url(u"http://www.example.com"),
isinstance(canonicalize_url(b"http://www.example.com"),
test_append_missing_path(self):
self.assertEqual(canonicalize_url("http://www.example.com"),
test_typical_usage(self):
self.assertEqual(canonicalize_url("http://www.example.com/do?a=1&b=2&c=3"),
"http://www.example.com/do?a=1&b=2&c=3")
self.assertEqual(canonicalize_url("http://www.example.com/do?c=1&b=2&a=3"),
"http://www.example.com/do?a=3&b=2&c=1")
self.assertEqual(canonicalize_url("http://www.example.com/do?&a=1"),
"http://www.example.com/do?a=1")
test_sorting(self):
self.assertEqual(canonicalize_url("http://www.example.com/do?c=3&b=5&b=2&a=50"),
"http://www.example.com/do?a=50&b=2&b=5&c=3")
test_keep_blank_values(self):
self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2",
self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2"),
"http://www.example.com/do?a=2&b=")
self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2",
self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2"),
"http://www.example.com/do?a=2&b=&c=")
self.assertEqual(canonicalize_url(u'http://www.example.com/do?1750,4'),
'http://www.example.com/do?1750%2C4=')
test_spaces(self):
self.assertEqual(canonicalize_url("http://www.example.com/do?q=a
space&a=1"),
self.assertEqual(canonicalize_url("http://www.example.com/do?q=a+space&a=1"),
self.assertEqual(canonicalize_url("http://www.example.com/do?q=a%20space&a=1"),
test_canonicalize_url_unicode_path(self):
self.assertEqual(canonicalize_url(u"http://www.example.com/résumé"),
test_canonicalize_url_unicode_query_string(self):
self.assertEqual(canonicalize_url(u"http://www.example.com/résumé?q=résumé"),
self.assertEqual(canonicalize_url(u"http://www.example.com/résumé?q=résumé",
"http://www.example.com/r%C3%A9sum%C3%A9?q=r%E9sum%E9")
encoding='cp1251'),
"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%EE%F1%F1%E8%FF")
test_canonicalize_url_unicode_query_string_wrong_encoding(self):
self.assertEqual(canonicalize_url(u"http://www.example.com/résumé?currency=€",
"http://www.example.com/r%C3%A9sum%C3%A9?currency=%E2%82%AC")
"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F")
test_normalize_percent_encoding_in_paths(self):
self.assertEqual(canonicalize_url("http://www.example.com/r%c3%a9sum%c3%a9"),
self.assertEqual(canonicalize_url("http://www.example.com/a%a3do"),
"http://www.example.com/a%A3do")
self.assertEqual(canonicalize_url("http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9"),
self.assertEqual(canonicalize_url("http://www.example.com/a%a3do?q=r%e9sum%e9"),
"http://www.example.com/a%A3do?q=r%E9sum%E9")
test_normalize_percent_encoding_in_query_arguments(self):
self.assertEqual(canonicalize_url("http://www.example.com/do?k=b%a3"),
"http://www.example.com/do?k=b%A3")
self.assertEqual(canonicalize_url("http://www.example.com/do?k=r%c3%a9sum%c3%a9"),
"http://www.example.com/do?k=r%C3%A9sum%C3%A9")
test_non_ascii_percent_encoding_in_paths(self):
do?a=1"),
"http://www.example.com/a%20do?a=1"),
%20do?a=1"),
"http://www.example.com/a%20%20do?a=1"),
self.assertEqual(canonicalize_url(u"http://www.example.com/a
do£.html?a=1"),
self.assertEqual(canonicalize_url(b"http://www.example.com/a
do\xc2\xa3.html?a=1"),
test_non_ascii_percent_encoding_in_query_arguments(self):
self.assertEqual(canonicalize_url(u"http://www.example.com/do?price=£500&a=5&z=3"),
u"http://www.example.com/do?a=5&price=%C2%A3500&z=3")
self.assertEqual(canonicalize_url(b"http://www.example.com/do?price=\xc2\xa3500&a=5&z=3"),
"http://www.example.com/do?a=5&price=%C2%A3500&z=3")
self.assertEqual(canonicalize_url(b"http://www.example.com/do?price(\xc2\xa3)=500&a=1"),
"http://www.example.com/do?a=1&price%28%C2%A3%29=500")
test_urls_with_auth_and_ports(self):
self.assertEqual(canonicalize_url(u"http://user:pass@www.example.com:81/do?now=1"),
u"http://user:pass@www.example.com:81/do?now=1")
test_remove_fragments(self):
self.assertEqual(canonicalize_url(u"http://user:pass@www.example.com/do?a=1#frag"),
u"http://user:pass@www.example.com/do?a=1")
self.assertEqual(canonicalize_url(u"http://user:pass@www.example.com/do?a=1#frag",
keep_fragments=True),
u"http://user:pass@www.example.com/do?a=1#frag")
test_dont_convert_safe_characters(self):
self.assertEqual(canonicalize_url(
"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html"),
"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html")
test_safe_characters_unicode(self):
self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),
test_domains_are_case_insensitive(self):
self.assertEqual(canonicalize_url("http://www.EXAMPLE.com/"),
test_canonicalize_idns(self):
self.assertEqual(canonicalize_url(u'http://www.bücher.de?q=bücher'),
'http://www.xn--bcher-kva.de/?q=b%C3%BCcher')
self.assertEqual(canonicalize_url(u'http://はじめよう.みんな/?query=サ&maxResults=5'),
'http://xn--p8j9a0d9c9a.xn--q9jyb4c/?maxResults=5&query=%E3%82%B5')
test_quoted_slash_and_question_sign(self):
self.assertEqual(canonicalize_url("http://foo.com/AC%2FDC+rocks%3f/?yeah=1"),
"http://foo.com/AC%2FDC+rocks%3F/?yeah=1")
self.assertEqual(canonicalize_url("http://foo.com/AC%2FDC/"),
"http://foo.com/AC%2FDC/")
test_canonicalize_urlparsed(self):
self.assertEqual(canonicalize_url(urlparse(u"http://www.example.com/résumé?q=résumé")),
self.assertEqual(canonicalize_url(urlparse('http://www.example.com/caf%e9-con-leche.htm')),
self.assertEqual(canonicalize_url(urlparse("http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9")),
test_canonicalize_parse_url(self):
self.assertEqual(canonicalize_url(parse_url(u"http://www.example.com/résumé?q=résumé")),
self.assertEqual(canonicalize_url(parse_url('http://www.example.com/caf%e9-con-leche.htm')),
self.assertEqual(canonicalize_url(parse_url("http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9")),
test_canonicalize_url_idempotence(self):
[(u'http://www.bücher.de/résumé?q=résumé',
'utf8'),
(u'http://www.example.com/résumé?q=résumé',
'latin1'),
(u'http://www.example.com/résumé?country=Россия',
'cp1251'),
(u'http://はじめよう.みんな/?query=サ&maxResults=5',
'iso2022jp')]:
canonicalized
encoding=enc)
self.assertEqual(canonicalize_url(canonicalized,
encoding=enc),
self.assertEqual(canonicalize_url(canonicalized),
test_canonicalize_url_idna_exceptions(self):
canonicalize_url(u"http://.example.com/résumé?q=résumé"),
"http://.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9")
canonicalize_url(
u"http://www.{label}.com/résumé?q=résumé".format(
label=u"example"*11)),
"http://www.{label}.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9".format(
label=u"example"*11))
AddHttpIfNoScheme(unittest.TestCase):
test_add_scheme(self):
self.assertEqual(add_http_if_no_scheme('www.example.com'),
test_without_subdomain(self):
self.assertEqual(add_http_if_no_scheme('example.com'),
test_path(self):
self.assertEqual(add_http_if_no_scheme('www.example.com/some/page.html'),
test_port(self):
self.assertEqual(add_http_if_no_scheme('www.example.com:80'),
test_fragment(self):
self.assertEqual(add_http_if_no_scheme('www.example.com/some/page#frag'),
self.assertEqual(add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),
test_username_password(self):
self.assertEqual(add_http_if_no_scheme('username:password@www.example.com'),
test_complete_url(self):
self.assertEqual(add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
test_preserve_http(self):
self.assertEqual(add_http_if_no_scheme('http://www.example.com'),
test_preserve_http_without_subdomain(self):
self.assertEqual(add_http_if_no_scheme('http://example.com'),
test_preserve_http_path(self):
self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),
test_preserve_http_port(self):
self.assertEqual(add_http_if_no_scheme('http://www.example.com:80'),
test_preserve_http_fragment(self):
self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page#frag'),
test_preserve_http_query(self):
self.assertEqual(add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),
test_preserve_http_username_password(self):
self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com'),
test_preserve_http_complete_url(self):
self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
test_protocol_relative(self):
self.assertEqual(add_http_if_no_scheme('//www.example.com'),
test_protocol_relative_without_subdomain(self):
self.assertEqual(add_http_if_no_scheme('//example.com'),
test_protocol_relative_path(self):
self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),
test_protocol_relative_port(self):
self.assertEqual(add_http_if_no_scheme('//www.example.com:80'),
test_protocol_relative_fragment(self):
self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page#frag'),
test_protocol_relative_query(self):
self.assertEqual(add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),
test_protocol_relative_username_password(self):
self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com'),
test_protocol_relative_complete_url(self):
self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
test_preserve_https(self):
self.assertEqual(add_http_if_no_scheme('https://www.example.com'),
'https://www.example.com')
test_preserve_ftp(self):
self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'),
'ftp://www.example.com')
GuessSchemeTest(unittest.TestCase):
create_guess_scheme_t(args):
url.startswith(args[1]),
'Wrong
guessed:
`%s`,
`%s...`'
args[0],
args[1])
create_skipped_scheme_t(args):
unittest.SkipTest(args[2])
url.startswith(args[1])
('/index',
('/index.html',
('./index.html',
('../index.html',
('../../index.html',
('./data/index.html',
('.hidden/data/index.html',
('/home/user/www/index.html',
('//home/user/www/index.html',
('file:///home/user/www/index.html',
('index.html',
('example.com',
('www.example.com',
('www.example.com/index.html',
('http://example.com',
('http://example.com/index.html',
('localhost/index.html',
('/',
('.../test',
create_guess_scheme_t(args)
'test_uri_%03d'
('C:\absolute\path\to\a\file.html',
'file://',
'Windows
shell'),
create_skipped_scheme_t(args)
'test_uri_skipped_%03d'
twisted.test.proto_helpers
StringTransport
scrapy.core.downloader
webclient
getPage(url,
response_transform=None,
_clientfactory(url,
to_unicode(url)
kwargs.pop('timeout',
client.ScrapyHTTPClientFactory(
**kwargs),
f.deferred.addCallback(response_transform
r.body))
_makeGetterFactory
_makeGetterFactory(to_bytes(url),
_clientfactory,
contextFactory=contextFactory,
**kwargs).deferred
ParseUrlTestCase(unittest.TestCase):
_parse(self,
(f.scheme,
f.netloc,
f.host,
f.port,
f.path)
testParse(self):
lip
("http://127.0.0.1?c=v&c2=v2#fragment",
("http://127.0.0.1/?c=v&c2=v2#fragment",
("http://127.0.0.1/foo?c=v&c2=v2#frag",
("http://127.0.0.1:100?c=v&c2=v2#fragment",
("http://127.0.0.1:100/?c=v&c2=v2#frag",
("http://127.0.0.1:100/foo?c=v&c2=v2#frag",
("http://127.0.0.1",
("http://127.0.0.1/",
("http://127.0.0.1/foo",
("http://127.0.0.1?param=value",
("http://127.0.0.1/?param=value",
("http://127.0.0.1:12345/foo",
("http://spam:12345/foo",
'spam:12345',
("http://spam.test.org/foo",
("https://127.0.0.1/foo",
("https://127.0.0.1/?param=value",
("https://127.0.0.1:12345/",
("http://scrapytest.org/foo
("http://egg:7890
'egg:7890',
'egg',
7890,
tuple(
to_bytes(x)
self.assertEquals(client._parse(url),
test_externalUnicodeInterference(self):
"Applies
Py2,
ONLY
Py3")
u'http://example.com/path'
goodInput
badInput.encode('ascii')
self._parse(badInput)
self._parse(goodInput)
self.assertTrue(isinstance(scheme,
self.assertTrue(isinstance(netloc,
self.assertTrue(isinstance(host,
self.assertTrue(isinstance(path,
self.assertTrue(isinstance(port,
ScrapyHTTPPageGetterTests(unittest.TestCase):
test_earlyHeaders(self):
body="some
data",
'example.net',
'fooble',
'Cookie':
'blah
blah',
'12981',
'Useful':
'value'}))
9\r\n"
b"Useful:
value\r\n"
b"User-Agent:
fooble\r\n"
example.net\r\n"
b"Cookie:
blah
blah\r\n"
b"some
data")
client.ScrapyHTTPClientFactory(Request('http://foo/bar'))
body='name=value',
'application/x-www-form-urlencoded'}))
b"Content-Type:
application/x-www-form-urlencoded\r\n"
10\r\n"
b"name=value")
url='http://foo/bar'
0\r\n"
headers=Headers({
})))
_test(self,
testvalue):
StringTransport()
protocol.makeConnection(transport)
set(transport.value().splitlines()),
set(testvalue.splitlines()))
testvalue
test_non_standard_line_endings(self):
url='http://foo/bar'))
protocol.headers
protocol.dataReceived(b"HTTP/1.0
OK\n")
protocol.dataReceived(b"Hello:
World\n")
protocol.dataReceived(b"Foo:
Bar\n")
protocol.dataReceived(b"\n")
self.assertEqual(protocol.headers,
Headers({'Hello':
['World'],
'Foo':
['Bar']}))
ErrorResource,
EncodingResource(resource.Resource):
out_encoding
'cp1251'
to_unicode(request.content.read())
request.setHeader(b'content-encoding',
self.out_encoding)
body.encode(self.out_encoding)
WebClientTestCase(unittest.TestCase):
_listen(self,
site):
site,
r.putChild(b"error",
ErrorResource())
r.putChild(b"encoding",
EncodingResource())
self._listen(self.wrapper)
testPayload(self):
"0123456789"
getPage(self.getURL("payload"),
body=s).addCallback(
to_bytes(s))
testHostHeader(self):
getPage(self.getURL("host")).addCallback(
self.portno)),
headers={"Host":
"www.example.com"}).addCallback(
to_bytes("www.example.com"))])
test_getPage(self):
getPage(self.getURL("file"))
test_getPageHead(self):
_getPage(method):
getPage(self.getURL("file"),
_getPage("head").addCallback(self.assertEqual,
b""),
_getPage("HEAD").addCallback(self.assertEqual,
b"")])
test_timeoutNotTriggering(self):
timeout=100)
d.addCallback(
self.portno))
test_timeoutTriggering(self):
self.assertFailure(
getPage(self.getURL("wait"),
timeout=0.000001),
defer.TimeoutError)
cleanup(passthrough):
connected
list(six.iterkeys(self.wrapper.protocols))
connected:
connected[0].transport.loseConnection()
finished.addBoth(cleanup)
testNotFound(self):
getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)
_cbNoSuchFile(self,
self.assert_(b'404
Such
Resource'
pageData)
testFactoryInfo(self):
self.getURL('file')
client._parse(url)
reactor.connectTCP(to_unicode(host),
factory.deferred.addCallback(self._cbFactoryInfo,
_cbFactoryInfo(self,
ignoredResult,
self.assertEquals(factory.status,
b'200')
self.assert_(factory.version.startswith(b'HTTP/'))
self.assertEquals(factory.message,
b'OK')
self.assertEquals(factory.response_headers[b'content-length'],
b'10')
testRedirect(self):
getPage(self.getURL("redirect")).addCallback(self._cbRedirect)
_cbRedirect(self,
self.assertEquals(pageData,
b'\n<html>\n
<head>\n
<meta
http-equiv="refresh"
content="0;URL=/file">\n'
</head>\n
<body
bgcolor="#FFFFFF"
text="#000000">\n
b'<a
href="/file">click
here</a>\n
</body>\n</html>\n')
test_Encoding(self):
b'\xd0\x81\xd1\x8e\xd0\xaf'
getPage(
self.getURL('encoding'),
response_transform=lambda
r)\
.addCallback(self._check_Encoding,
_check_Encoding(self,
original_body):
to_unicode(response.headers[b'Content-Encoding'])
self.assertEquals(content_encoding,
EncodingResource.out_encoding)
response.body.decode(content_encoding),
to_unicode(original_body))
DummyDB(dict):
_DATABASES
collections.defaultdict(DummyDB)
open(file,
flag='r',
_DATABASES[file]
pstats
CmdlineTest(unittest.TestCase):
'tests.test_cmdline.settings'
_execute(self,
Popen(args,
stderr=PIPE,
comm
proc.communicate()[0].strip()
comm.decode(encoding)
test_default_settings(self):
test_override_settings_using_set_arg(self):
'TEST1',
'TEST1=override'),
test_override_settings_using_envvar(self):
self.env['SCRAPY_TEST1']
'override'
test_profiling(self):
'res.prof')
self._execute('version',
'--profile',
self.assertTrue(os.path.exists(filename))
pstats.Stats(filename,
stream=out)
stats.print_stats()
out.seek(0)
out.read()
self.assertIn('scrapy/commands/version.py',
self.assertIn('tottime',
test_override_dict_settings(self):
EXT_PATH
"tests.test_cmdline.extensions.DummyExtension"
{EXT_PATH:
200}
self._execute('settings',
'EXTENSIONS',
'EXTENSIONS='
json.dumps(EXTENSIONS))
self.assertNotIn("...",
settingsstr)
char
("'",
"<",
">",
'u"'):
settingsstr.replace(char,
'"')
settingsdict
json.loads(settingsstr)
settingsdict.keys(),
EXTENSIONS.keys())
self.assertEquals(200,
settingsdict[EXT_PATH])
TestExtension(object):
settings.set('TEST1',
(settings['TEST1'],
'started'))
DummyExtension(object):
'tests.test_cmdline.extensions.TestExtension':
TEST1
(BaseSettings,
SettingsAttribute,
SETTINGS_PRIORITIES,
get_settings_priority)
SettingsGlobalFuncsTest(unittest.TestCase):
test_get_settings_priority(self):
prio_str,
prio_num
six.iteritems(SETTINGS_PRIORITIES):
self.assertEqual(get_settings_priority(prio_str),
prio_num)
self.assertEqual(get_settings_priority(99),
SettingsAttributeTest(unittest.TestCase):
test_set_greater_priority(self):
test_set_equal_priority(self):
test_set_less_priority(self):
test_overwrite_basesettings(self):
original_dict
20}
original_settings
BaseSettings(original_dict,
SettingsAttribute(original_settings,
new_dict
{'three':
21}
attribute.set(new_dict,
self.assertIsInstance(attribute.value,
original_settings,
original_dict)
new_settings
BaseSettings({'five':
12},
Insufficient
new_settings)
self.assertEqual(repr(self.attribute),
value='value'
priority=10>")
BaseSettingsTest(unittest.TestCase):
test_set_new_attribute(self):
self.settings.attributes['TEST_OPTION']
test_set_settingsattribute(self):
myattr
SettingsAttribute(0,
self.settings.set('TEST_ATTR',
myattr,
self.assertEqual(self.settings.get('TEST_ATTR'),
self.assertEqual(self.settings.getpriority('TEST_ATTR'),
test_set_instance_identity_on_update(self):
self.assertIs(attr,
self.settings.attributes['TEST_OPTION'])
test_set_calls_settings_attributes_methods_on_update(self):
'__setattr__')
mock_setattr,
20):
mock_set.assert_called_once_with('othervalue',
self.assertFalse(mock_setattr.called)
mock_set.reset_mock()
mock_setattr.reset_mock()
test_setitem(self):
settings.set('key',
'a',
settings['key2']
self.assertIn('key2',
self.assertEqual(settings['key2'],
self.assertEqual(settings.getpriority('key2'),
test_setdict_alias(self):
self.settings.setdict({'TEST_1':
'TEST_2':
self.assertEqual(mock_set.call_count,
[mock.call('TEST_1',
10),
mock.call('TEST_2',
10)]
mock_set.assert_has_calls(calls,
test_setmodule_only_load_uppercase_vars(self):
ModuleMock():
UPPERCASE_VAR
MIXEDcase_VAR
'othervalue'
lowercase_var
'anothervalue'
self.settings.setmodule(ModuleMock(),
self.assertIn('UPPERCASE_VAR',
self.assertNotIn('MIXEDcase_VAR',
self.assertNotIn('lowercase_var',
self.assertEqual(len(self.settings.attributes),
test_setmodule_alias(self):
mock_set.assert_any_call('TEST_DEFAULT',
'defvalue',
mock_set.assert_any_call('TEST_DICT',
'val'},
test_setmodule_by_path(self):
ctrl_attributes
self.settings.attributes.copy()
self.settings.setmodule(
'tests.test_settings.default_settings',
self.assertItemsEqual(six.iterkeys(self.settings.attributes),
six.iterkeys(ctrl_attributes))
six.iterkeys(ctrl_attributes):
self.settings.attributes[key]
ctrl_attr
ctrl_attributes[key]
ctrl_attr.value)
ctrl_attr.priority)
11},
priority=30)
custom_settings.set('newkey_one',
custom_dict
{'key_lowprio':
'newkey_two':
settings.update(custom_dict,
self.assertIn('newkey_two',
self.assertEqual(settings.getpriority('newkey_two'),
settings.update(custom_settings)
self.assertIn('newkey_one',
self.assertEqual(settings.getpriority('newkey_one'),
settings.update({'key_lowprio':
test_update_jsonstring(self):
BaseSettings({'number':
'val'})})
settings.update('{"number":
"newnumber":
2}')
self.assertEqual(settings['number'],
self.assertEqual(settings['newnumber'],
settings.set("dict",
'{"key":
"newval",
"newkey":
"newval2"}')
self.assertEqual(settings['dict']['key'],
"newval")
self.assertEqual(settings['dict']['newkey'],
"newval2")
settings.delete('key')
settings.delete('key_highprio')
self.assertNotIn('key',
self.assertIn('key_highprio',
settings['key_highprio']
self.assertNotIn('key_highprio',
test_configuration
'TEST_ENABLED1':
'TEST_ENABLED2':
'TEST_ENABLED3':
'TEST_DISABLED1':
'TEST_DISABLED2':
'TEST_DISABLED3':
'TEST_INT1':
'TEST_INT2':
'TEST_FLOAT1':
'TEST_FLOAT2':
'123.45',
'TEST_LIST1':
'TEST_LIST2':
'one,two',
'TEST_STR':
'TEST_DICT1':
'TEST_DICT2':
'{"key1":
"val1",
"ke2":
3}',
settings.attributes
{key:
six.iteritems(test_configuration)}
self.assertTrue(settings.getbool('TEST_ENABLED1'))
self.assertTrue(settings.getbool('TEST_ENABLED2'))
self.assertTrue(settings.getbool('TEST_ENABLED3'))
self.assertFalse(settings.getbool('TEST_ENABLEDx'))
self.assertTrue(settings.getbool('TEST_ENABLEDx',
self.assertFalse(settings.getbool('TEST_DISABLED1'))
self.assertFalse(settings.getbool('TEST_DISABLED2'))
self.assertFalse(settings.getbool('TEST_DISABLED3'))
self.assertEqual(settings.getint('TEST_INT1'),
self.assertEqual(settings.getint('TEST_INT2'),
self.assertEqual(settings.getint('TEST_INTx'),
self.assertEqual(settings.getint('TEST_INTx',
self.assertEqual(settings.getfloat('TEST_FLOAT1'),
self.assertEqual(settings.getfloat('TEST_FLOAT2'),
self.assertEqual(settings.getfloat('TEST_FLOATx'),
self.assertEqual(settings.getfloat('TEST_FLOATx',
55.0),
55.0)
self.assertEqual(settings.getlist('TEST_LIST1'),
self.assertEqual(settings.getlist('TEST_LIST2'),
self.assertEqual(settings.getlist('TEST_LISTx'),
self.assertEqual(settings.getlist('TEST_LISTx',
['default']),
['default'])
self.assertEqual(settings['TEST_STR'],
self.assertEqual(settings.get('TEST_STR'),
self.assertEqual(settings['TEST_STRx'],
self.assertEqual(settings.get('TEST_STRx'),
self.assertEqual(settings.get('TEST_STRx',
self.assertEqual(settings.getdict('TEST_DICT1'),
self.assertEqual(settings.getdict('TEST_DICT2'),
self.assertEqual(settings.getdict('TEST_DICT3'),
self.assertEqual(settings.getdict('TEST_DICT3',
5}),
settings.getdict,
'TEST_LIST1')
test_getpriority(self):
priority=99)
self.assertEqual(settings.getpriority('nonexistentkey'),
test_getwithbase(self):
BaseSettings({'TEST_BASE':
s['TEST'].set(2,
s.getwithbase('TEST'),
30})
s.getwithbase('HASNOBASE'),
s['HASNOBASE'])
self.assertEqual(s.getwithbase('NONEXISTENT'),
test_maxpriority(self):
self.settings.set('A',
self.settings.set('B',
'TEST_BOOL':
'TEST_LIST_OF_LISTS':
[['first_one',
'first_two'],
['second_one',
'second_two']]
self.settings.setdict(values)
self.settings.copy()
self.assertTrue(copy.get('TEST_BOOL'))
test_list
self.settings.get('TEST_LIST')
test_list.append('three')
self.assertListEqual(copy.get('TEST_LIST'),
test_list_of_lists
self.settings.get('TEST_LIST_OF_LISTS')
test_list_of_lists[0].append('first_three')
self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],
['first_one',
'first_two'])
test_copy_to_dict(self):
BaseSettings({'TEST_STRING':
string',
self.assertDictEqual(s.copy_to_dict(),
{'HASNOBASE':
{3:
'TEST_STRING':
string'})
test_freeze(self):
self.assertRaises(TypeError)
self.assertEqual(str(cm.exception),
"Trying
test_frozencopy(self):
frozencopy
self.settings.frozencopy()
self.assertTrue(frozencopy.frozen)
self.assertIsNot(frozencopy,
test_deprecated_attribute_overrides(self):
self.settings.overrides['BAR']
self.assertIn("Settings.overrides",
self.settings.overrides)
self.settings.overrides.update(BAR='bus')
self.settings.overrides.setdefault('BAR',
self.settings.overrides.setdefault('FOO',
self.assertEqual(self.settings.get('FOO'),
self.assertEqual(self.settings.overrides.get('FOO'),
test_deprecated_attribute_defaults(self):
priority='default')
self.settings.defaults['BAR']
self.assertIn("Settings.defaults",
self.assertEqual(self.settings.defaults.get('BAR'),
self.settings.defaults)
SettingsTest(unittest.TestCase):
test_initial_defaults(self):
self.assertIn('TEST_DEFAULT',
settings.attributes['TEST_DEFAULT']
'defvalue')
test_initial_values(self):
Settings({'TEST_OPTION':
settings.attributes['TEST_OPTION']
test_autopromote_dicts(self):
settings.get('TEST_DICT')
self.assertEqual(mydict.getpriority('key'),
test_getdict_autodegrade_basesettings(self):
settings.getdict('TEST_DICT')
self.assertEqual(len(mydict),
CrawlerSettingsTest(unittest.TestCase):
test_deprecated_crawlersettings(self):
_get_settings(settings_dict=None):
type('SettingsModuleMock',
(object,),
CrawlerSettings(settings_module)
_get_settings()
self.assertIn("CrawlerSettings
deprecated",
settings.defaults['DOWNLOAD_TIMEOUT']
'99'
settings.overrides['DOWNLOAD_TIMEOUT']
'15'
TEST_DEFAULT
'defvalue'
TEST_DICT
module_dir
SpiderLoaderTest(unittest.TestCase):
orig_spiders_dir
os.path.join(module_dir,
'test_spiders')
os.mkdir(self.tmpdir)
self.spiders_dir
os.path.join(self.tmpdir,
'test_spiders_xxx')
shutil.copytree(orig_spiders_dir,
self.spiders_dir)
sys.path.append(self.tmpdir)
['test_spiders_xxx']})
sys.modules['test_spiders_xxx']
sys.path.remove(self.tmpdir)
verifyObject(ISpiderLoader,
self.spider_loader)
self.assertEqual(set(self.spider_loader.list()),
'spider2',
'spider3']))
spider1
self.spider_loader.load("spider1")
self.assertEqual(spider1.__name__,
'Spider1')
test_find_by_request(self):
self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),
['spider1'])
self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),
['spider2'])
self.assertEqual(set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),
'spider2']))
self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),
self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com')),
self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),
['spider3'])
'tests.test_spiderloader.test_spiders.'
','.join(prefix
('spider1',
'spider2'))
module})
test_load_base_spider(self):
'tests.test_spiderloader.test_spiders.spider0'
test_crawler_runner_loading(self):
CrawlerRunner({'SPIDER_MODULES':
self.assertRaisesRegexp(KeyError,
'Spider
runner.create_crawler,
'spider2')
runner.create_crawler('spider1')
self.assertTrue(issubclass(crawler.spidercls,
scrapy.Spider))
self.assertEqual(crawler.spidercls.name,
'spider1')
Spider0(Spider):
Spider1(Spider):
"spider1"
Spider2(Spider):
"spider2"
["scrapy2.org",
Spider3(Spider):
"spider3"
['spider3.com']
'http://spider3.com/onlythis'
['scrapy.utils.misc']
UtilsMiscTestCase(unittest.TestCase):
test_load_object(self):
load_object('scrapy.utils.misc.load_object')
'nomodule999.mod.function')
self.assertRaises(NameError,
'scrapy.utils.misc.load_object999')
test_walk_modules(self):
walk_modules('tests.test_utils_misc.test_walk_modules')
'tests.test_utils_misc.test_walk_modules',
walk_modules('tests.test_utils_misc.test_walk_modules.mod')
walk_modules('tests.test_utils_misc.test_walk_modules.mod1')
walk_modules,
'nomodule999')
test_walk_modules_egg(self):
'test.egg')
sys.path.append(egg)
walk_modules('testegg')
'testegg.spiders',
'testegg.spiders.a',
'testegg.spiders.b',
'testegg'
sys.path.remove(egg)
test_arg_to_iter(self):
hasattr(arg_to_iter(None),
hasattr(arg_to_iter(100),
hasattr(arg_to_iter('lala'),
hasattr(arg_to_iter([1,
3]),
hasattr(arg_to_iter(l
'abcd'),
self.assertEqual(list(arg_to_iter(None)),
self.assertEqual(list(arg_to_iter('lala')),
['lala'])
self.assertEqual(list(arg_to_iter(100)),
[100])
self.assertEqual(list(arg_to_iter(l
'abc')),
self.assertEqual(list(arg_to_iter([1,
self.assertEqual(list(arg_to_iter({'a':1})),
[{'a':
self.assertEqual(list(arg_to_iter(TestItem(name="john"))),
[TestItem(name="john")])
_version_re
re.compile(r'__version__\s+=\s+(.*)')
open('flask/__init__.py',
str(ast.literal_eval(_version_re.search(
f.read().decode('utf-8')).group(1)))
name='Flask',
version=version,
url='http://github.com/pallets/flask/',
license='BSD',
author='Armin
author_email='armin.ronacher@active-4.com',
description='A
microframework
based
Werkzeug,
Jinja2
good
intentions',
long_description=__doc__,
packages=['flask',
'flask.ext'],
zip_safe=False,
platforms='any',
'Werkzeug>=0.7',
'Jinja2>=2.4',
'itsdangerous>=0.21',
'click>=2.0',
classifiers=[
'Development
Beta',
'Environment
Environment',
'Intended
Developers',
'License
BSD
License',
'Operating
Independent',
Python',
2.6',
2.7',
3.3',
3.4',
3.5',
Internet
WWW/HTTP
Dynamic
Content',
Software
Development
Libraries
Modules'
entry_points='''
[console_scripts]
flask=flask.cli:main
'_themes'))
sys.path.append(os.path.dirname(__file__))
'sphinx.ext.autodoc',
'flaskdocext'
u'Flask'
u'2010
{0},
Armin
Ronacher'.format(datetime.utcnow().year)
pkg_resources.get_distribution('Flask').version
pkg_resources.DistributionNotFound:
print('Flask
documentation.')
print('Install
`pip
-e
.`
virtualenv.')
'dev'
release:
''.join(release.partition('dev')[:2])
'.'.join(release.split('.')[:2])
exclude_patterns
['_build']
['_themes']
html_favicon
'_static/flask-favicon.ico'
html_sidebars
'sidebarintro.html',
'**':
'sidebarlogo.html',
'localtoc.html',
'relations.html',
html_use_modindex
html_show_sphinx
'Flaskdoc'
('latexindex',
'Flask.tex',
u'Flask
u'Armin
latex_use_modindex
latex_elements
'fontpkg':
r'\usepackage{mathpazo}',
'papersize':
'a4paper',
'pointsize':
'12pt',
'preamble':
r'\usepackage{flaskstyle}'
latex_use_parts
latex_additional_files
['flaskstyle.sty',
'logo.pdf']
('https://docs.python.org/3/',
'werkzeug':
('http://werkzeug.pocoo.org/docs/',
'click':
('http://click.pocoo.org/',
'jinja':
('http://jinja.pocoo.org/docs/',
'sqlalchemy':
('http://docs.sqlalchemy.org/en/latest/',
'wtforms':
('https://wtforms.readthedocs.io/en/latest/',
'blinker':
('https://pythonhosted.org/blinker/',
__import__('flask_theme_support')
'flask_theme_support.FlaskyStyle'
'flask'
html_theme_options
'touch_icon':
'touch-icon.png'
themes
unavailable.
Building
theme')
print('If
themes,
again:')
git
--init')
unwrap_decorators():
sphinx.util.inspect
old_getargspec
getargspec(x):
old_getargspec(getattr(x,
'_original_function',
getargspec
old_update_wrapper
old_update_wrapper(wrapper,
rv._original_function
unwrap_decorators()
unwrap_decorators
_internal_mark_re
re.compile(r'^\s*:internal:\s*$(?m)')
skip_member(app,
what,
skip,
docstring
inspect.getdoc(obj)
skip:
_internal_mark_re.search(docstring
app.connect('autodoc-skip-member',
skip_member)
pygments.style
Style
pygments.token
Keyword,
Name,
Comment,
Number,
Operator,
Generic,
Whitespace,
Punctuation,
Other,
Literal
FlaskyStyle(Style):
background_color
"#f8f8f8"
default_style
styles
Whitespace:
"underline
#f8f8f8",
"#a40000
border:#ef2929",
'err'
Other:
Comment:
Comment.Preproc:
"noitalic",
'cp'
Keyword:
'k'
Keyword.Constant:
'kc'
Keyword.Declaration:
'kd'
Keyword.Namespace:
'kn'
Keyword.Pseudo:
'kp'
Keyword.Reserved:
'kr'
Keyword.Type:
'kt'
Operator:
"#582800",
'o'
Operator.Word:
'ow'
Punctuation:
'p'
'n'
Name.Attribute:
"#c4a000",
'na'
Name.Builtin:
"#004461",
'nb'
Name.Builtin.Pseudo:
"#3465a4",
'bp'
Name.Class:
'nc'
Name.Constant:
'no'
Name.Decorator:
'nd'
Name.Entity:
"#ce5c00",
'ni'
Name.Exception:
#cc0000",
'ne'
Name.Function:
'nf'
Name.Property:
'py'
Name.Label:
"#f57900",
'nl'
Name.Namespace:
'nn'
Name.Other:
'nx'
Name.Tag:
'nt'
Name.Variable:
'nv'
Name.Variable.Class:
'vc'
Name.Variable.Global:
'vg'
Name.Variable.Instance:
Number:
"#990000",
'm'
Literal:
'l'
Literal.Date:
'ld'
String:
's'
String.Backtick:
'sb'
String.Char:
'sc'
String.Doc:
'sd'
String.Double:
's2'
String.Escape:
'se'
String.Heredoc:
'sh'
String.Interpol:
'si'
String.Other:
'sx'
String.Regex:
'sr'
String.Single:
's1'
String.Symbol:
'ss'
Generic:
'g'
Generic.Deleted:
"#a40000",
'gd'
Generic.Emph:
'ge'
Generic.Error:
"#ef2929",
'gr'
Generic.Heading:
#000080",
'gh'
Generic.Inserted:
"#00A000",
'gi'
Generic.Output:
'go'
Generic.Prompt:
"#745334",
'gp'
Generic.Strong:
Generic.Subheading:
#800080",
'gu'
Generic.Traceback:
#a40000",
'gt'
simple_page.simple_page
app.register_blueprint(simple_page)
app.register_blueprint(simple_page,
url_prefix='/pages')
__name__=='__main__':
blueprintexample
client():
blueprintexample.app.test_client()
test_urls(client):
client.get('/hello')
client.get('/world')
client.get('/pages/hello')
client.get('/pages/world')
Blueprint('simple_page',
@simple_page.route('/',
'index'})
@simple_page.route('/<page>')
show(page):
render_template('pages/%s.html'
name='flaskr',
packages=['flaskr'],
'flask',
setup_requires=[
'pytest-runner',
tests_require=[
'pytest',
app.config.update(dict(
DATABASE=os.path.join(app.root_path,
'flaskr.db'),
DEBUG=True,
SECRET_KEY='development
USERNAME='admin',
PASSWORD='default'
app.config.from_envvar('FLASKR_SETTINGS',
connect_db():
rv.row_factory
connect_db()
close_db(error):
g.sqlite_db.close()
show_entries():
db.execute('select
desc')
render_template('show_entries.html',
entries=entries)
add_entry():
session.get('logged_in'):
(title,
[request.form['title'],
request.form['text']])
flash('New
posted')
request.form['username']
app.config['USERNAME']:
app.config['PASSWORD']:
session['logged_in']
session.pop('logged_in',
'/../')
flaskr.app.config['DATABASE']
flaskr.app.config['TESTING']
flaskr.app.test_client()
flaskr.app.app_context():
flaskr.init_db()
os.unlink(flaskr.app.config['DATABASE'])
username=username,
password=password
test_empty_db(client):
flaskr.app.config['USERNAME']
flaskr.app.config['PASSWORD']
test_messages(client):
client.post('/add',
title='<Hello>',
text='<strong>HTML</strong>
b'&lt;Hello&gt;'
b'<strong>HTML</strong>
@app.route('/_add_numbers')
add_numbers():
request.args.get('a',
request.args.get('b',
jsonify(result=a
render_template('index.html')
werkzeug
check_password_hash,
generate_password_hash
DATABASE
'/tmp/minitwit.db'
PER_PAGE
app.config.from_envvar('MINITWIT_SETTINGS',
top.sqlite_db.row_factory
close_database(exception):
top.sqlite_db.close()
query_db(query,
one=False):
get_db().execute(query,
(rv[0]
get_user_id(username):
rv[0]
format_datetime(timestamp):
datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d
%H:%M')
gravatar_url(email,
size=80):
'http://www.gravatar.com/avatar/%s?d=identicon&s=%d'
(md5(email.strip().lower().encode('utf-8')).hexdigest(),
[session['user_id']],
timeline():
(select
who_id
?))
session['user_id'],
PER_PAGE]))
@app.route('/public')
public_timeline():
[PER_PAGE]))
@app.route('/<username>')
user_timeline(username):
follower.who_id
follower.whom_id
profile_user['user_id']],
[profile_user['user_id'],
PER_PAGE]),
followed=followed,
profile_user=profile_user)
@app.route('/<username>/follow')
follow_user(username):
(who_id,
whom_id)
@app.route('/<username>/unfollow')
unfollow_user(username):
db.execute('delete
who_id=?
whom_id=?',
@app.route('/add_message',
add_message():
request.form['text']:
(author_id,
pub_date)
(session['user_id'],
request.form['text'],
int(time.time())))
flash('Your
recorded')
[request.form['username']],
check_password_hash(user['pw_hash'],
request.form['password']):
session['user_id']
user['user_id']
@app.route('/register',
register():
request.form['username']:
request.form['email']
'@'
request.form['email']:
request.form['password']:
request.form['password2']:
get_user_id(request.form['username'])
pw_hash)
[request.form['username'],
request.form['email'],
generate_password_hash(request.form['password'])])
now')
redirect(url_for('login'))
render_template('register.html',
session.pop('user_id',
app.jinja_env.filters['datetimeformat']
format_datetime
app.jinja_env.filters['gravatar']
gravatar_url
minitwit
minitwit.app.config['DATABASE']
minitwit.app.test_client()
minitwit.app.app_context():
minitwit.init_db()
os.unlink(minitwit.app.config['DATABASE'])
password2=None,
email=None):
'@example.com'
client.post('/register',
'password2':
password2,
client.post('/add_message',
data={'text':
text},
b'Your
recorded'
test_register(client):
b'and
now'
'y')
email='broken')
'user2',
test_message_recording(client):
'<test
2>')
b'&lt;test
2&gt;'
test_timelines(client):
foo')
bar')
client.get('/public')
client.get('/foo/follow',
client.get('/bar')
client.get('/foo')
client.get('/foo/unfollow',
'0.11.2-dev'
escape
send_file,
send_from_directory,
get_template_attribute,
make_response,
safe_join,
stream_with_context
has_request_context,
has_app_context,
after_this_request,
copy_current_request_context
render_template_string
signals_available,
appcontext_tearing_down,
appcontext_popped,
message_flashed,
jsonify
json.jsonify
json_available
.cli
(str,)
(int,)
iter(d.keys())
iter(d.values())
iter(d.items())
implements_to_string
d.iterkeys()
d.itervalues()
d.iteritems()
exec('def
tb=None):\n
tp,
tb')
implements_to_string(cls):
cls.__unicode__
x.__unicode__().encode('utf-8')
metaclass(type):
'pypy_version_info'):
_Mgr(object):
_Mgr():
AssertionError()
chain
ImmutableDict
Map,
RequestRedirect,
HTTPException,
InternalServerError,
MethodNotAllowed,
default_exceptions
locked_cached_property,
_endpoint_from_view_func,
find_package,
.wrappers
ConfigAttribute,
RequestContext,
AppContext,
SecureCookieSessionInterface
DispatchingJinjaLoader,
Environment,
_default_template_ctx_processor
reraise,
_logger_lock
_make_timedelta(value):
timedelta):
timedelta(seconds=value)
setupmethod(f):
wrapper_func(self,
AssertionError('A
'first
handled.
usually
late.\n'
'To
modules,
'database
models
related
central
'before
starts
serving
requests.')
f(self,
update_wrapper(wrapper_func,
Flask(_PackageBoundObject):
app_ctx_globals_class
_get_request_globals_class(self):
_set_request_globals_class(self,
warn(DeprecationWarning('request_globals_class
'called
app_ctx_globals_class'))
request_globals_class
property(_get_request_globals_class,
_set_request_globals_class)
_get_request_globals_class,
_set_request_globals_class
ConfigAttribute('DEBUG')
ConfigAttribute('TESTING')
ConfigAttribute('SECRET_KEY')
session_cookie_name
ConfigAttribute('SESSION_COOKIE_NAME')
permanent_session_lifetime
ConfigAttribute('PERMANENT_SESSION_LIFETIME',
send_file_max_age_default
ConfigAttribute('SEND_FILE_MAX_AGE_DEFAULT',
use_x_sendfile
ConfigAttribute('USE_X_SENDFILE')
logger_name
ConfigAttribute('LOGGER_NAME')
json_encoder
json.JSONEncoder
json_decoder
json.JSONDecoder
jinja_options
ImmutableDict(
extensions=['jinja2.ext.autoescape',
'jinja2.ext.with_']
default_config
ImmutableDict({
'DEBUG':
get_debug_flag(default=False),
'TESTING':
'PROPAGATE_EXCEPTIONS':
'PRESERVE_CONTEXT_ON_EXCEPTION':
'PERMANENT_SESSION_LIFETIME':
timedelta(days=31),
'USE_X_SENDFILE':
'LOGGER_NAME':
'LOGGER_HANDLER_POLICY':
'always',
'SERVER_NAME':
'APPLICATION_ROOT':
'SESSION_COOKIE_NAME':
'session',
'SESSION_COOKIE_DOMAIN':
'SESSION_COOKIE_PATH':
'SESSION_COOKIE_HTTPONLY':
'SESSION_COOKIE_SECURE':
'SESSION_REFRESH_EACH_REQUEST':
'MAX_CONTENT_LENGTH':
'SEND_FILE_MAX_AGE_DEFAULT':
timedelta(hours=12),
'TRAP_BAD_REQUEST_ERRORS':
'TRAP_HTTP_EXCEPTIONS':
'EXPLAIN_TEMPLATE_LOADING':
'PREFERRED_URL_SCHEME':
'JSON_AS_ASCII':
'JSON_SORT_KEYS':
'JSONIFY_PRETTYPRINT_REGULAR':
'JSONIFY_MIMETYPE':
'application/json',
'TEMPLATES_AUTO_RELOAD':
url_rule_class
test_client_class
session_interface
SecureCookieSessionInterface()
static_path=None,
static_folder='static',
instance_path=None,
instance_relative_config=False,
template_folder=template_folder,
warn(DeprecationWarning('static_path
'static_url_path'),
self.auto_find_instance_path()
os.path.isabs(instance_path):
ValueError('If
'absolute.
instead.')
self.make_config(instance_relative_config)
self.logger_name
self.view_functions
self.error_handler_spec
{None:
self._error_handlers}
self.url_build_error_handlers
self.before_request_funcs
self.before_first_request_funcs
self.after_request_funcs
self.teardown_request_funcs
self.teardown_appcontext_funcs
self.url_value_preprocessors
self.url_default_functions
self.template_context_processors
[_default_template_ctx_processor]
self.shell_context_processors
self.blueprints
self._blueprint_order
self.url_map
Map()
self._before_request_lock
self.add_url_rule(self.static_url_path
endpoint='static',
view_func=self.send_static_file)
self.cli
cli.AppGroup(self.name)
_get_error_handlers(self):
warn(DeprecationWarning('error_handlers
error_handler_spec
_set_error_handlers(self,
self.error_handler_spec[None]
error_handlers
property(_get_error_handlers,
_set_error_handlers)
_get_error_handlers,
_set_error_handlers
getattr(sys.modules['__main__'],
os.path.splitext(os.path.basename(fn))[0]
propagate_exceptions(self):
self.config['PROPAGATE_EXCEPTIONS']
self.testing
preserve_context_on_exception(self):
self.config['PRESERVE_CONTEXT_ON_EXCEPTION']
_logger_lock:
flask.logging
create_logger
create_logger(self)
jinja_env(self):
self.create_jinja_environment()
got_first_request(self):
make_config(self,
instance_relative=False):
instance_relative:
self.config_class(root_path,
self.default_config)
auto_find_instance_path(self):
find_package(self.import_name)
os.path.join(package_path,
os.path.join(prefix,
'var',
'-instance')
open_instance_resource(self,
open(os.path.join(self.instance_path,
create_jinja_environment(self):
dict(self.jinja_options)
'autoescape'
options['autoescape']
self.select_jinja_autoescape
'auto_reload'
self.jinja_environment(self,
rv.globals.update(
url_for=url_for,
get_flashed_messages=get_flashed_messages,
config=self.config,
session=session,
g=g
rv.filters['tojson']
json.tojson_filter
DispatchingJinjaLoader(self)
init_jinja_globals(self):
select_jinja_autoescape(self,
filename.endswith(('.html',
'.htm',
'.xml',
'.xhtml'))
update_template_context(self,
self.template_context_processors[None]
self.template_context_processors:
self.template_context_processors[bp])
orig_ctx
context.copy()
context.update(func())
context.update(orig_ctx)
make_shell_context(self):
{'app':
'g':
g}
processor
self.shell_context_processors:
rv.update(processor())
debug=None,
server_name:
int(server_name.rsplit(':',
1)[1])
5000
options.setdefault('use_reloader',
options.setdefault('use_debugger',
options.setdefault('passthrough_errors',
test_client(self,
use_cookies=True,
self.test_client_class
FlaskClient
cls(self,
use_cookies=use_cookies,
self.session_interface.open_session(self,
self.session_interface.save_session(self,
make_null_session(self):
self.session_interface.make_null_session(self)
register_blueprint(self,
blueprint.name
self.blueprints:
blueprint\'s
collision
'%r.
Both
share
Blueprints
'are
fly
names.'
(blueprint,
self.blueprints[blueprint.name],
blueprint.name)
self._blueprint_order.append(blueprint)
blueprint.register(self,
iter_blueprints(self):
iter(self._blueprint_order)
options['endpoint']
options.pop('methods',
'methods',
('GET',)
isinstance(methods,
TypeError('Allowed
iterables
strings,
example:
@app.route(...,
methods=["POST"])')
set(item.upper()
methods)
set(getattr(view_func,
'required_methods',
'OPTIONS'
required_methods.add('OPTIONS')
self.url_rule_class(rule,
methods=methods,
rule.provide_automatic_options
self.url_map.add(rule)
self.view_functions.get(endpoint)
view_func:
AssertionError('View
overwriting
'existing
function:
options.pop('endpoint',
_get_exc_class_and_code(exc_class_or_code):
isinstance(exc_class_or_code,
integer_types):
default_exceptions[exc_class_or_code]
exc_class_or_code
Exception)
exc_class.code
_register_error_handler(self,
isinstance(code_or_exception,
broken
'Tried
register
{0!r}.
'Handlers
codes.'
.format(code_or_exception))
self._get_exc_class_and_code(code_or_exception)
self.error_handler_spec.setdefault(key,
{}).setdefault(code,
handlers[exc_class]
template_filter(self,
self.add_template_filter(f,
add_template_filter(self,
self.jinja_env.filters[name
template_test(self,
self.add_template_test(f,
add_template_test(self,
self.jinja_env.tests[name
template_global(self,
self.add_template_global(f,
add_template_global(self,
self.jinja_env.globals[name
self.before_request_funcs.setdefault(None,
before_first_request(self,
self.before_first_request_funcs.append(f)
self.after_request_funcs.setdefault(None,
self.teardown_request_funcs.setdefault(None,
teardown_appcontext(self,
self.teardown_appcontext_funcs.append(f)
self.template_context_processors[None].append(f)
shell_context_processor(self,
self.shell_context_processors.append(f)
self.url_value_preprocessors.setdefault(None,
self.url_default_functions.setdefault(None,
_find_error_handler(self,
self._get_exc_class_and_code(type(e))
find_handler(handler_map):
handler_map:
deque(exc_class.__mro__)
queue.popleft()
done.add(cls)
handler_map.get(cls)
handler_map[exc_class]
queue.extend(cls.__mro__)
find_handler(self.error_handler_spec
.get(request.blueprint,
.get(code))
find_handler(self.error_handler_spec[None].get(code))
handle_http_exception(self,
trap_http_exception(self,
self.config['TRAP_HTTP_EXCEPTIONS']:
self.config['TRAP_BAD_REQUEST_ERRORS']:
BadRequest)
handle_user_exception(self,
HTTPException)
self.trap_http_exception(e):
self.handle_http_exception(e)
handle_exception(self,
got_request_exception.send(self,
exception=e)
self._find_error_handler(InternalServerError())
self.propagate_exceptions:
self.log_exception((exc_type,
tb))
self.logger.error('Exception
exc_info=exc_info)
raise_routing_exception(self,
isinstance(request.routing_exception,
RequestRedirect)
'OPTIONS'):
FormDataRoutingRedirect
FormDataRoutingRedirect(request)
_request_ctx_stack.top.request
req.routing_exception
self.raise_routing_exception(req)
req.url_rule
getattr(rule,
'OPTIONS':
self.make_default_options_response()
self.view_functions[rule.endpoint](**req.view_args)
full_dispatch_request(self):
self.try_trigger_before_first_request_functions()
request_started.send(self)
self.preprocess_request()
self.dispatch_request()
self.handle_user_exception(e)
self.make_response(rv)
self.process_response(response)
request_finished.send(self,
try_trigger_before_first_request_functions(self):
self._before_request_lock:
self.before_first_request_funcs:
make_default_options_response(self):
_request_ctx_stack.top.url_adapter
hasattr(adapter,
'allowed_methods'):
adapter.allowed_methods()
adapter.match(method='--')
MethodNotAllowed
e.valid_methods
self.response_class()
rv.allow.update(methods)
should_ignore_error(self,
make_response(self,
rv):
(None,)
(3
len(rv))
ValueError('View
(dict,
self.response_class):
(text_type,
bytearray)):
self.response_class(rv,
status=status_or_headers)
self.response_class.force_type(rv,
request.environ)
rv.status
rv.headers.extend(headers)
create_url_adapter(self,
self.url_map.bind_to_environ(request.environ,
server_name=self.config['SERVER_NAME'])
self.url_map.bind(
self.config['SERVER_NAME'],
script_name=self.config['APPLICATION_ROOT']
url_scheme=self.config['PREFERRED_URL_SCHEME'])
inject_url_defaults(self,
self.url_default_functions.get(None,
endpoint.rsplit('.',
self.url_default_functions.get(bp,
func(endpoint,
handle_url_build_error(self,
self.url_build_error_handlers:
exc_value:
preprocess_request(self):
self.url_value_preprocessors.get(None,
self.url_value_preprocessors:
self.url_value_preprocessors[bp])
func(request.endpoint,
request.view_args)
self.before_request_funcs.get(None,
self.before_request_funcs:
self.before_request_funcs[bp])
ctx.request.blueprint
ctx._after_request_functions
reversed(self.after_request_funcs[bp]))
reversed(self.after_request_funcs[None]))
handler(response)
self.session_interface.is_null_session(ctx.session):
self.save_session(ctx.session,
do_teardown_request(self,
reversed(self.teardown_request_funcs.get(None,
self.teardown_request_funcs:
reversed(self.teardown_request_funcs[bp]))
request_tearing_down.send(self,
do_teardown_appcontext(self,
reversed(self.teardown_appcontext_funcs):
appcontext_tearing_down.send(self,
app_context(self):
AppContext(self)
request_context(self,
environ):
RequestContext(self,
environ)
test_request_context(self,
make_test_environ_builder
make_test_environ_builder(self,
self.request_context(builder.get_environ())
builder.close()
wsgi_app(self,
self.request_context(environ)
self.full_dispatch_request()
self.make_response(self.handle_exception(e))
response(environ,
self.should_ignore_error(error):
ctx.auto_pop(error)
self.wsgi_app(environ,
_endpoint_from_view_func
BlueprintSetupState(object):
first_registration):
self.first_registration
self.options.get('subdomain')
self.blueprint.subdomain
self.options.get('url_prefix')
self.blueprint.url_prefix
dict(self.blueprint.url_values_defaults)
self.url_defaults.update(self.options.get('url_defaults',
self.url_prefix:
options.setdefault('subdomain',
self.subdomain)
'defaults'
dict(defaults,
**options.pop('defaults'))
self.app.add_url_rule(rule,
(self.blueprint.name,
endpoint),
defaults=defaults,
Blueprint(_PackageBoundObject):
warn_on_modifications
_got_registered_once
static_folder=None,
url_prefix=None,
subdomain=None,
url_defaults=None,
template_folder,
self.deferred_functions
self.url_values_defaults
record(self,
self.warn_on_modifications:
warn(Warning('The
getting
now.
These
'will
up.'))
self.deferred_functions.append(func)
record_once(self,
wrapper(state):
state.first_registration:
func(state)
self.record(update_wrapper(wrapper,
func))
make_setup_state(self,
BlueprintSetupState(self,
self.make_setup_state(app,
state.add_url_rule(self.static_url_path
view_func=self.send_static_file,
endpoint='static')
self.deferred_functions:
deferred(state)
options.pop("endpoint",
f.__name__)
"Blueprint
dots"
self.record(lambda
s.add_url_rule(rule,
**options))
register_endpoint(state):
state.app.view_functions[endpoint]
self.record_once(register_endpoint)
app_template_filter(self,
self.add_app_template_filter(f,
add_app_template_filter(self,
state.app.jinja_env.filters[name
app_template_test(self,
self.add_app_template_test(f,
add_app_template_test(self,
state.app.jinja_env.tests[name
app_template_global(self,
self.add_app_template_global(f,
add_app_template_global(self,
state.app.jinja_env.globals[name
before_app_request(self,
before_app_first_request(self,
s.app.before_first_request_funcs.append(f))
after_app_request(self,
teardown_app_request(self,
app_context_processor(self,
app_errorhandler(self,
code):
s.app.errorhandler(code)(f))
app_url_value_preprocessor(self,
app_url_defaults(self,
Lock,
NoAppException(click.UsageError):
find_best_app(module):
'app',
'application':
isinstance(app,
[v
iteritems(module.__dict__)
Flask)]
len(matches)
matches[0]
NoAppException('Failed
Are
application?
'using
function.'
module.__name__)
prepare_exec_for_file(filename):
os.path.split(filename)[1]
os.path.dirname(filename)
filename[:-3]
NoAppException('The
'valid
'be
'extension
.py'
dirpath
os.path.split(dirpath)
module.append(extra)
os.path.isfile(os.path.join(dirpath,
'__init__.py')):
dirpath)
'.'.join(module[::-1])
locate_app(app_id):
app_id:
app_id.split(':',
__import__(module)
sys.modules[module]
find_best_app(mod)
app_obj,
RuntimeError('Failed
find_default_import_path():
os.environ.get('FLASK_APP')
os.path.isfile(app):
prepare_exec_for_file(app)
get_version(ctx,
ctx.resilient_parsing:
%(version)s\nPython
%(python_version)s'
click.echo(message
'python_version':
color=ctx.color)
ctx.exit()
version_option
click.Option(['--version'],
help='Show
version',
expose_value=False,
callback=get_version,
is_flag=True,
is_eager=True)
DispatchingApp(object):
use_eager_loading=False):
self._lock
use_eager_loading:
self._load_in_background()
_load_in_background(self):
_load_app():
Thread(target=_load_app,
args=())
_flush_bg_loading_exception(self):
reraise(*exc_info)
_load_unlocked(self):
self.loader()
self._app(environ,
self._flush_bg_loading_exception()
rv(environ,
ScriptInfo(object):
app_import_path=None,
create_app=None):
load_app(self):
self.create_app(self)
self.app_import_path:
NoAppException(
'Could
locate
FLASK_APP
variable.\n\nFor
'information
'http://flask.pocoo.org/docs/latest/quickstart/')
locate_app(self.app_import_path)
rv.debug
pass_script_info
click.make_pass_decorator(ScriptInfo,
ensure=True)
with_appcontext(f):
@click.pass_context
decorator(__ctx,
__ctx.ensure_object(ScriptInfo).load_app().app_context():
__ctx.invoke(f,
AppGroup(click.Group):
command(self,
wrap_for_ctx
kwargs.pop('with_appcontext',
wrap_for_ctx:
with_appcontext(f)
click.Group.command(self,
**kwargs)(f)
group(self,
AppGroup)
click.Group.group(self,
FlaskGroup(AppGroup):
add_default_commands=True,
create_app=None,
add_version_option=True,
**extra):
list(extra.pop('params',
add_version_option:
params.append(version_option)
AppGroup.__init__(self,
add_default_commands:
self.add_command(run_command)
self.add_command(shell_command)
_load_plugin_commands(self):
self._loaded_plugin_commands:
ep
pkg_resources.iter_entry_points('flask.commands'):
self.add_command(ep.load(),
ep.name)
get_command(self,
AppGroup.get_command(self,
info.load_app().cli.get_command(ctx,
NoAppException:
list_commands(self,
ctx):
set(click.Group.list_commands(self,
ctx))
rv.update(info.load_app().cli.list_commands(ctx))
sorted(rv)
kwargs.get('obj')
ScriptInfo(create_app=self.create_app)
kwargs['obj']
kwargs.setdefault('auto_envvar_prefix',
'FLASK')
AppGroup.main(self,
@click.command('run',
development
server.')
@click.option('--host',
'-h',
default='127.0.0.1',
@click.option('--port',
'-p',
default=5000,
@click.option('--reload/--no-reload',
reloader.
@click.option('--debugger/--no-debugger',
debugger.
@click.option('--eager-loading/--lazy-loader',
loading.
'loading
@click.option('--with-threads/--without-threads',
multithreading.')
@pass_script_info
run_command(info,
reload,
debugger,
eager_loading,
with_threads):
DispatchingApp(info.load_app,
use_eager_loading=eager_loading)
os.environ.get('WERKZEUG_RUN_MAIN')
info.app_import_path
Serving
info.app_import_path)
Forcing
(debug
'on'
'off'))
use_reloader=reload,
use_debugger=debugger,
threaded=with_threads,
passthrough_errors=True)
@click.command('shell',
context.')
shell_command():
flask.globals
_app_ctx_stack.top.app
banner
%s\nApp:
%s%s\nInstance:
sys.platform,
app.import_name,
[debug]'
app.instance_path,
os.environ.get('PYTHONSTARTUP')
os.path.isfile(startup):
open(startup,
eval(compile(f.read(),
startup,
ctx)
ctx.update(app.make_shell_context())
local=ctx)
FlaskGroup(help=
'cmd':
'export'
'set',
'$
main(as_module=False):
'.cli'
as_module:
this_module.rsplit('.',
['-m',
this_module]
cli.main(args=args,
prog_name=name)
import_string
ConfigAttribute(object):
get_converter=None):
get_converter
self.get_converter(rv)
Config(dict):
root_path,
defaults=None):
from_envvar(self,
variable_name,
os.environ.get(variable_name)
rv:
'loaded.
'point
variable_name)
self.from_pyfile(rv,
silent=silent)
from_pyfile(self,
types.ModuleType('config')
d.__file__
exec(compile(config_file.read(),
d.__dict__)
self.from_object(d)
from_object(self,
import_string(obj)
dir(obj):
from_json(self,
json_file:
json.loads(json_file.read())
self.from_mapping(obj)
from_mapping(self,
*mapping,
hasattr(mapping[0],
mappings.append(mapping[0].items())
mappings.append(mapping[0])
positional
mappings.append(kwargs.items())
mapping:
get_namespace(self,
lowercase=True,
trim_namespace=True):
k.startswith(namespace):
trim_namespace:
k[len(namespace):]
lowercase:
rv[key]
BROKEN_PYPY_CTXMGR_EXIT,
_AppCtxGlobals(object):
self.__dict__.get(name,
default=_sentinel):
self.__dict__.pop(name)
self.__dict__.pop(name,
self.__dict__.setdefault(name,
iter(self.__dict__)
'<flask.g
top.app.name
object.__repr__(self)
after_this_request(f):
_request_ctx_stack.top._after_request_functions.append(f)
copy_current_request_context(f):
RuntimeError('This
'when
stack.
'view
functions.')
top.copy()
has_request_context():
has_app_context():
AppContext(object):
app.create_url_adapter(None)
self.g
app.app_ctx_globals_class()
_app_ctx_stack.push(self)
appcontext_pushed.send(self.app)
self.app.do_teardown_appcontext(exc)
_app_ctx_stack.pop()
(%r
appcontext_popped.send(self.app)
self.pop(exc_value)
RequestContext(object):
app.request_class(environ)
app.create_url_adapter(self.request)
self.flashes
self._implicit_app_ctx_stack
self._after_request_functions
self.match_request()
_get_g(self):
_set_g(self,
property(_get_g,
_set_g)
_get_g,
_set_g
self.__class__(self.app,
environ=self.request.environ,
request=self.request
match_request(self):
url_rule,
self.request.view_args
self.url_adapter.match(return_rule=True)
self.request.url_rule
self.request.routing_exception
top.pop(top._preserved_exc)
app_ctx.app
self.app:
self.app.app_context()
app_ctx.push()
self._implicit_app_ctx_stack.append(app_ctx)
self._implicit_app_ctx_stack.append(None)
_request_ctx_stack.push(self)
self.app.open_session(self.request)
self.app.make_null_session()
self._implicit_app_ctx_stack.pop()
self._implicit_app_ctx_stack:
self.app.do_teardown_request(exc)
getattr(self.request,
'close',
request_close()
clear_request:
rv.request.environ['werkzeug.request']
app_ctx.pop(exc)
'(%r
auto_pop(self,
exc):
self.request.environ.get('flask._preserve_context')
(exc
self.app.preserve_context_on_exception):
self.pop(exc)
self.auto_pop(exc_value)
\'%s\'
[%s]
self.request.url,
self.request.method,
self.app.name,
implements_to_string,
UnexpectedUnicodeError(AssertionError,
UnicodeError):
@implements_to_string
DebugFilesKeyError(KeyError,
AssertionError):
form_matches
request.form.getlist(key)
['You
tried
request.files
'dictionary
exist.
"multipart/form-data"
transmitted.
'provide
enctype="multipart/form-data"
form.'
request.mimetype)]
form_matches:
buf.append('\n\nThe
transmitted
names.
'.join('"%s"'
form_matches))
FormDataRoutingRedirect(AssertionError):
['A
'issued
routing
(request.url,
exc.new_url)]
request.base_url
exc.new_url.split('?')[0]:
accessed
'without
one.')
Make
%s-request
'since
can\'t
browsers
clients
reliably
interaction.'
buf.append('\n\nNote:
mode')
AssertionError.__init__(self,
''.join(buf).encode('utf-8'))
attach_enctype_error_multidict(request):
oldcls
newcls(oldcls):
oldcls.__getitem__(self,
DebugFilesKeyError(request,
newcls.__name__
oldcls.__name__
newcls.__module__
oldcls.__module__
newcls
'class:
(type(loader).__module__,
type(loader).__name__)
sorted(loader.__dict__.items()):
key.startswith('_'):
(tuple,
text_type))
'%s:'
bool)):
explain_template_loading_attempts(app,
attempts):
['Locating
"%s":'
template]
(loader,
triple)
enumerate(attempts):
srcobj.import_name
Blueprint):
'blueprint
(srcobj.name,
srcobj.import_name)
repr(srcobj)
info.append('%
5d:
src_info))
triple
'found
(%r)'
(triple[1]
'<string>')
->
detail)
info.append('Error:
found.')
info.append('Warning:
loaders
template.')
seems_fishy:
looked
'belongs
blueprint)
right
folder?')
http://flask.pocoo.org/docs/blueprints/#templates')
app.logger.info('\n'.join(info))
ExtDeprecationWarning(DeprecationWarning):
warnings.simplefilter('always',
ExtDeprecationWarning)
fullname.startswith(self.prefix)
'flask.ext.ExtDeprecationWarning':
"Importing
flask.ext.{x}
flask_{x}
ExtDeprecationWarning,
tb.tb_next)
realname.startswith('flaskext.'):
"Detected
flaskext.{x},
rename
flask_{x}.
deprecated."
LocalStack,
_request_ctx_err_msg
Consult
problem.\
_app_ctx_err_msg
way.
solve
app.app_context().
information.\
_lookup_req_object(name):
RuntimeError(_request_ctx_err_msg)
_lookup_app_object(name):
_find_app():
top.app
LocalProxy(_find_app)
'request'))
'session'))
LocalProxy(partial(_lookup_app_object,
'g'))
adler32
RLock
quote
werkzeug.wsgi
FileSystemLoader
_app_ctx_stack,
_os_alt_seps
list(sep
[os.path.sep,
os.path.altsep]
'/'))
get_debug_flag(default=None):
os.environ.get('FLASK_DEBUG')
('0',
'no')
_endpoint_from_view_func(view_func):
provided.'
view_func.__name__
stream_with_context(generator_or_function):
iter(generator_or_function)
decorator(*args,
generator_or_function(*args,
stream_with_context(gen)
generator_or_function)
generator():
stream
'there
keep
around.')
ctx:
gen:
hasattr(gen,
gen.close()
generator()
next(wrapped_g)
make_response(*args):
current_app.response_class()
current_app.make_response(args)
url_for(endpoint,
**values):
pushed.
'executed
available.')
reqctx.url_adapter
reqctx.request._is_old_module:
endpoint[:1]
appctx.url_adapter
RuntimeError('Application
'adapter
independent
generation.
SERVER_NAME
variable.')
values.pop('_anchor',
values.pop('_method',
values.pop('_scheme',
appctx.app.inject_url_defaults(endpoint,
external:
ValueError('When
_scheme,
_external
True')
url_adapter.build(endpoint,
force_external=external)
values['_external']
values['_anchor']
values['_method']
appctx.app.handle_url_build_error(error,
'#'
url_quote(anchor)
get_template_attribute(template_name,
getattr(current_app.jinja_env.get_template(template_name).module,
flash(message,
category='message'):
session.get('_flashes',
flashes.append((category,
message))
session['_flashes']
message_flashed.send(current_app._get_current_object(),
message=message,
category=category)
get_flashed_messages(with_categories=False,
category_filter=[]):
session.pop('_flashes')
'_flashes'
category_filter:
category_filter,
flashes))
with_categories:
flashes]
send_file(filename_or_fp,
mimetype=None,
as_attachment=False,
attachment_filename=None,
add_etags=True,
cache_timeout=None,
conditional=False,
last_modified=None):
isinstance(filename_or_fp,
getattr(file,
(filename
attachment_filename):
mimetypes.guess_type(filename
attachment_filename)[0]
as_attachment:
TypeError('filename
unavailable,
'sending
attachment')
os.path.basename(filename)
headers.add('Content-Disposition',
filename=attachment_filename)
current_app.use_x_sendfile
headers['X-Sendfile']
os.path.getmtime(filename)
wrap_file(request.environ,
current_app.response_class(data,
mimetype=mimetype,
direct_passthrough=True)
rv.cache_control.public
current_app.get_send_file_max_age(filename)
rv.cache_control.max_age
rv.expires
int(time()
cache_timeout)
add_etags
rv.set_etag('%s-%s-%s'
os.path.getmtime(filename),
os.path.getsize(filename),
adler32(
filename.encode('utf-8')
isinstance(filename,
text_type)
warn('Access
maybe
'headers'
conditional:
rv.make_conditional(request)
rv.headers.pop('x-sendfile',
*pathnames):
pathnames:
posixpath.normpath(filename)
_os_alt_seps:
os.path.isabs(filename)
'..'
filename.startswith('../'):
os.path.join(directory,
send_from_directory(directory,
os.path.isfile(filename):
options.setdefault('conditional',
send_file(filename,
get_root_path(import_name):
sys.modules.get(import_name)
'__file__'):
os.path.dirname(os.path.abspath(mod.__file__))
pkgutil.get_loader(import_name)
loader.get_filename(import_name)
sys.modules[import_name]
came
'it\'s
package.
In
explicitly
'provided.'
import_name)
os.path.dirname(os.path.abspath(filepath))
_matching_loader_thinks_module_is_package(loader,
mod_name):
'is_package'):
loader.is_package(mod_name)
(loader.__class__.__module__
'_frozen_importlib'
loader.__class__.__name__
'NamespaceLoader'):
('%s.is_package()
'PEP
hooks.
hooks
encounter
Flask.')
loader.__class__.__name__)
find_package(import_name):
root_mod_name
import_name.split('.')[0]
pkgutil.get_loader(root_mod_name)
loader.get_filename(root_mod_name)
'archive'):
loader.archive
sys.modules[import_name].__file__
os.path.abspath(os.path.dirname(filename))
_matching_loader_thinks_module_is_package(
root_mod_name):
os.path.dirname(package_path)
site_parent,
site_folder
os.path.split(package_path)
py_prefix
os.path.abspath(sys.prefix)
package_path.startswith(py_prefix):
py_prefix,
site_folder.lower()
'site-packages':
os.path.split(site_parent)
folder.lower()
os.path.basename(parent).lower()
os.path.dirname(parent)
site_parent
base_dir,
locked_cached_property(object):
self.__module__
func.__module__
self.func
self.lock
RLock()
self.lock:
obj.__dict__.get(self.__name__,
self.func(obj)
obj.__dict__[self.__name__]
_PackageBoundObject(object):
template_folder
get_root_path(self.import_name)
_get_static_folder(self):
self._static_folder)
_set_static_folder(self,
property(_get_static_folder,
_set_static_folder,
doc='''
folder.
_get_static_folder,
_set_static_folder
_get_static_url_path(self):
os.path.basename(self.static_folder)
_set_static_url_path(self,
property(_get_static_url_path,
_set_static_url_path)
_get_static_url_path,
_set_static_url_path
has_static_folder(self):
jinja_loader(self):
FileSystemLoader(os.path.join(self.root_path,
self.template_folder))
total_seconds(current_app.send_file_max_age_default)
send_static_file(self,
self.get_send_file_max_age(filename)
send_from_directory(self.static_folder,
cache_timeout=cache_timeout)
open_resource(self,
('r',
'rb'):
ValueError('Resources
reading')
open(os.path.join(self.root_path,
total_seconds(td):
td.seconds
Markup
_json
_slash_escape
'\\/'
_json.dumps('/')
['dump',
'dumps',
'load',
'htmlsafe_dump',
'htmlsafe_dumps',
'JSONDecoder',
'JSONEncoder',
'jsonify']
isinstance(fp.read(0),
io.TextIOWrapper(io.BufferedReader(fp),
fp.write('')
io.TextIOWrapper(fp,
JSONEncoder(_json.JSONEncoder):
http_date(o.timetuple())
hasattr(o,
'__html__'):
text_type(o.__html__())
_json.JSONEncoder.default(self,
JSONDecoder(_json.JSONDecoder):
_dump_arg_defaults(kwargs):
current_app.json_encoder)
current_app.config['JSON_AS_ASCII']:
current_app.config['JSON_SORT_KEYS'])
JSONEncoder)
_load_arg_defaults(kwargs):
current_app.json_decoder)
JSONDecoder)
_json.dumps(obj,
text_type):
rv.encode(encoding)
dump(obj,
_json.dump(obj,
loads(s,
s.decode(kwargs.pop('encoding',
_json.loads(s,
load(fp,
_json.load(fp,
htmlsafe_dumps(obj,
.replace(u'<',
u'\\u003c')
.replace(u'>',
u'\\u003e')
.replace(u'&',
u'\\u0026')
.replace(u"'",
u'\\u0027')
_slash_escape:
rv.replace('\\/',
htmlsafe_dump(obj,
fp.write(text_type(htmlsafe_dumps(obj,
**kwargs)))
jsonify(*args,
(',',
':')
current_app.config['JSONIFY_PRETTYPRINT_REGULAR']
request.is_xhr:
(',
TypeError('jsonify()
undefined
kwargs')
dumps()
current_app.response_class(
(dumps(data,
indent=indent,
separators=separators),
'\n'),
mimetype=current_app.config['JSONIFY_MIMETYPE']
tojson_filter(obj,
Markup(htmlsafe_dumps(obj,
getLogger,
StreamHandler,
Formatter,
getLoggerClass,
DEBUG,
PROD_LOG_FORMAT
'[%(asctime)s]
%(levelname)s
%(module)s:
DEBUG_LOG_FORMAT
'%(levelname)s
%(module)s
[%(pathname)s:%(lineno)d]:\n'
'%(message)s\n'
@LocalProxy
_proxy_stream():
ctx.request.environ['wsgi.errors']
sys.stderr
mode):
'always':
create_logger(app):
Logger
getLoggerClass()
DebugLogger(Logger):
getEffectiveLevel(self):
self.level
app.debug:
Logger.getEffectiveLevel(self)
DebugHandler(StreamHandler):
'debug'):
ProductionHandler(StreamHandler):
'production'):
debug_handler
DebugHandler()
debug_handler.setLevel(DEBUG)
debug_handler.setFormatter(Formatter(DEBUG_LOG_FORMAT))
prod_handler
ProductionHandler(_proxy_stream)
prod_handler.setLevel(ERROR)
prod_handler.setFormatter(Formatter(PROD_LOG_FORMAT))
getLogger(app.logger_name)
logger.handlers[:]
logger.__class__
DebugLogger
logger.addHandler(debug_handler)
logger.addHandler(prod_handler)
b64encode,
http_date,
CallbackDict
URLSafeTimedSerializer,
BadSignature
SessionMixin(object):
_get_permanent(self):
self.get('_permanent',
_set_permanent(self,
self['_permanent']
property(_get_permanent,
_set_permanent)
_get_permanent,
_set_permanent
_tag(value):
value]}
value.hex}
b64encode(value).decode('ascii')}
callable(getattr(value,
'__html__',
text_type(value.__html__())}
http_date(value)}
_tag(v))
iteritems(value))
text_type(value)
UnexpectedUnicodeError
UnexpectedUnicodeError(u'A
u'non-ASCII
u'which
strings.
Consider
u'base64
(String
TaggedJSONSerializer(object):
dumps(self,
json.dumps(_tag(value),
separators=(',',
':'))
loads(self,
object_hook(obj):
the_key,
the_value
next(iteritems(obj))
tuple(the_value)
uuid.UUID(the_value)
b64decode(the_value)
Markup(the_value)
parse_date(the_value)
json.loads(value,
TaggedJSONSerializer()
SecureCookieSession(CallbackDict,
SessionMixin):
initial=None):
on_update(self):
CallbackDict.__init__(self,
initial,
on_update)
NullSession(SecureCookieSession):
'key
set.
secret.')
__setitem__
__delitem__
clear
pop
popitem
setdefault
SessionInterface(object):
null_session_class
NullSession
pickle_based
make_null_session(self,
self.null_session_class()
is_null_session(self,
self.null_session_class)
get_cookie_domain(self,
app.config['SERVER_NAME'].rsplit(':',
'.localhost':
rv.lstrip('.')
get_cookie_path(self,
app.config['SESSION_COOKIE_PATH']
get_cookie_httponly(self,
app.config['SESSION_COOKIE_HTTPONLY']
get_cookie_secure(self,
app.config['SESSION_COOKIE_SECURE']
get_expiration_time(self,
session.permanent:
should_set_cookie(self,
session.permanent
SecureCookieSessionInterface(SessionInterface):
salt
'cookie-session'
digest_method
staticmethod(hashlib.sha1)
key_derivation
'hmac'
session_class
get_signing_serializer(self,
app.secret_key:
signer_kwargs
key_derivation=self.key_derivation,
digest_method=self.digest_method
URLSafeTimedSerializer(app.secret_key,
salt=self.salt,
serializer=self.serializer,
signer_kwargs=signer_kwargs)
self.get_signing_serializer(app)
request.cookies.get(app.session_cookie_name)
max_age
total_seconds(app.permanent_session_lifetime)
s.loads(val,
max_age=max_age)
self.session_class(data)
BadSignature:
self.get_cookie_domain(app)
response.delete_cookie(app.session_cookie_name,
path=path)
self.should_set_cookie(app,
httponly
self.get_cookie_httponly(app)
self.get_cookie_secure(app)
self.get_expiration_time(app,
session)
self.get_signing_serializer(app).dumps(dict(session))
response.set_cookie(app.session_cookie_name,
httponly=httponly,
secure=secure)
Namespace
Namespace(object):
_FakeSignal(name,
_FakeSignal(object):
RuntimeError('signalling
'because
installed.')
**kw:
has_receivers_for
receivers_for
temporarily_connected_to
connected_to
_signals
Namespace()
template_rendered
_signals.signal('template-rendered')
_signals.signal('before-render-template')
request_started
_signals.signal('request-started')
request_finished
_signals.signal('request-finished')
request_tearing_down
_signals.signal('request-tearing-down')
got_request_exception
_signals.signal('got-request-exception')
_signals.signal('appcontext-tearing-down')
appcontext_pushed
_signals.signal('appcontext-pushed')
_signals.signal('appcontext-popped')
_signals.signal('message-flashed')
BaseLoader,
BaseEnvironment,
_default_template_ctx_processor():
rv['g']
appctx.g
rv['request']
reqctx.request
rv['session']
reqctx.session
Environment(BaseEnvironment):
'loader'
options['loader']
app.create_global_jinja_loader()
BaseEnvironment.__init__(self,
DispatchingJinjaLoader(BaseLoader):
get_source(self,
self.app.config['EXPLAIN_TEMPLATE_LOADING']:
self._get_source_explained(environment,
self._get_source_fast(environment,
_get_source_explained(self,
attempts.append((loader,
rv))
explain_template_loading_attempts
explain_template_loading_attempts(self.app,
attempts)
_get_source_fast(self,
_iter_loaders(self,
self.app,
list_templates(self):
result.update(loader.list_templates())
loader.list_templates():
result.add(template)
list(result)
_render(template,
before_render_template.send(app,
template.render(context)
template_rendered.send(app,
render_template(template_name_or_list,
_render(ctx.app.jinja_env.get_or_select_template(template_name_or_list),
render_template_string(source,
_render(ctx.app.jinja_env.from_string(source),
werkzeug.test
Client,
EnvironBuilder
make_test_environ_builder(app,
base_url=None,
app.config.get('SERVER_NAME')
app_root
app.config.get('APPLICATION_ROOT')
url_parse(path)
(url.netloc
app_root:
app_root.lstrip('/')
url.netloc:
url.path
url.query:
url.query
EnvironBuilder(path,
base_url,
FlaskClient(Client):
preserve_context
session_transaction(self,
self.cookie_jar
sense
environ_overrides
self.cookie_jar.inject_wsgi(environ_overrides)
outer_reqctx
app.test_request_context(*args,
app.open_session(c.request)
session.
configuration')
_request_ctx_stack.push(outer_reqctx)
app.response_class()
app.session_interface.is_null_session(sess):
app.save_session(sess,
resp.get_wsgi_headers(c.request.environ)
self.cookie_jar.extract_wsgi(c.request.environ,
['flask._preserve_context']
as_tuple
kwargs.pop('as_tuple',
buffered
kwargs.pop('buffered',
follow_redirects
kwargs.pop('follow_redirects',
make_test_environ_builder(self.application,
Client.open(self,
builder,
as_tuple=as_tuple,
buffered=buffered,
follow_redirects=follow_redirects)
self.preserve_context:
RuntimeError('Cannot
invocations')
top.pop()
with_metaclass
http_method_funcs
frozenset(['get',
'post',
'head',
'put',
'trace',
'patch'])
as_view(cls,
*class_args,
**class_kwargs):
view(*args,
view.view_class(*class_args,
**class_kwargs)
self.dispatch_request(*args,
decorator(view)
view.__doc__
cls.__doc__
view.methods
cls.methods
MethodViewType(type):
type.__new__(cls,
'methods'
set(rv.methods
http_method_funcs:
methods.add(key.upper())
rv.methods
sorted(methods)
MethodView(with_metaclass(MethodViewType,
View)):
dispatch_request(self,
request.method.lower(),
'Unimplemented
meth(*args,
werkzeug.wrappers
RequestBase,
ResponseBase
BadRequest
_get_data(req,
cache):
getattr(req,
'get_data',
getter(cache=cache)
req.data
Request(RequestBase):
view_args
routing_exception
_is_old_module
max_content_length(self):
ctx.app.config['MAX_CONTENT_LENGTH']
endpoint(self):
self.url_rule.endpoint
module(self):
warn(DeprecationWarning('modules
favor
'blueprints.
'instead.'),
self._is_old_module:
blueprint(self):
self.url_rule.endpoint:
self.url_rule.endpoint.rsplit('.',
json(self):
warn(DeprecationWarning('json
'Use
get_json()
self.get_json()
is_json(self):
mt.startswith('application/')
mt.endswith('+json'):
get_json(self,
silent=False,
cache=True):
'_cached_json',
(force
self.is_json):
self.mimetype_params.get('charset')
_get_data(self,
cache)
json.loads(data,
encoding=request_charset)
json.loads(data)
self.on_json_loading_failed(e)
self._cached_json
on_json_loading_failed(self,
ctx.app.config.get('DEBUG',
BadRequest('Failed
object:
_load_form_data(self):
RequestBase._load_form_data(self)
ctx.app.debug
'multipart/form-data'
self.files:
attach_enctype_error_multidict
attach_enctype_error_multidict(self)
Response(ResponseBase):
default_mimetype
setup():
..exthook
ExtensionImporter
setup()
difflib
TEMPLATE_LOOKAHEAD
4096
_app_re_part
r'((?:[a-zA-Z_][a-zA-Z0-9_]*app)|app|application)'
_string_re_part
r"('([^'\\]*(?:\\.[^'\\]*)*)'"
r'|"([^"\\]*(?:\\.[^"\\]*)*)")'
_from_import_re
re.compile(r'^\s*from
import\s+')
_url_for_re
re.compile(r'\b(url_for\()(%s)'
_render_template_re
re.compile(r'\b(render_template\()(%s)'
_after_request_re
re.compile(r'((?:@\S+\.(?:app_)?))(after_request)(\b\s*$)(?m)')
_module_constructor_re
re.compile(r'([a-zA-Z0-9_][a-zA-Z0-9_]*)\s*=\s*Module'
r'\(__name__\s*(?:,\s*(?:name\s*=\s*)?(%s))?'
_error_handler_re
re.compile(r'%s\.error_handlers\[\s*(\d+)\s*\]'
_app_re_part)
_mod_route_re
re.compile(r'@([a-zA-Z0-9_][a-zA-Z0-9_]*)\.route')
_blueprint_related
(re.compile(r'request\.module'),
'request.blueprint'),
(re.compile(r'register_module'),
'register_blueprint'),
(re.compile(r'%s\.modules'
_app_re_part),
'\\1.blueprints')
new):
difflib.unified_diff(old.splitlines(),
new.splitlines(),
posixpath.normpath(posixpath.join('a',
posixpath.normpath(posixpath.join('b',
lineterm=''):
print(line)
looks_like_teardown_function(node):
ast.walk(node)
ast.Return)]
len(returns)
return_def
returns[0]
resp_name
node.args.args[0]
isinstance(return_def.value,
return_def.value.id
body_node
node.body:
ast.walk(body_node):
isinstance(child,
child.id
return_def.value:
resp_name.id
module_declarations=None):
module_declarations
dict(module_declarations)
annotated_lines
make_line_annotations():
annotated_lines:
contents.splitlines(True):
len(line)
annotated_lines.append((last_index,
backtrack_module_name(call_start):
make_line_annotations()
(line_end,
enumerate(annotated_lines):
line_end
call_start:
reversed(annotated_lines[:idx]):
_mod_route_re.search(line)
mapping.get(shortname)
skip_module_test:
backtrack_module_name(match.start())
ast.literal_eval(match.group(2))
repr(endpoint)
_url_for_re.sub(handle_match,
fix_teardown_funcs(contents):
is_return_line(line):
line.strip().split()
'return'
lines,
lineno):
1].startswith('def'):
inspect.getblock(lines[lineno
1:])
func_code
''.join(block_lines)
func_code[0].isspace():
ast.parse('if
1:\n'
func_code).body[0].body
ast.parse(func_code).body[0]
looks_like_teardown_function(node)
lines[:lineno]
[match.group(1)
match.group(2).replace('after_',
'teardown_')
match.group(3)]
[line.replace(response_param_name,
'exception')
is_return_line(line)]
len(block_lines)
contents.splitlines(True)
found_one
enumerate(content_lines):
_after_request_re.match(line)
content_lines,
''.join(content_lines)
get_module_autoname(filename):
directory,
os.path.splitext(filename)[0]
os.path.basename(directory)
rewrite_from_imports(prefix,
fromlist,
lineiter):
import_block
[prefix,
fromlist]
fromlist[0]
'('
')':
line.rstrip().endswith(')'):
line.rstrip().endswith('\\'):
''.join(import_block).replace('Module',
'Blueprint')
rewrite_blueprint_imports(contents):
new_file
iter(contents.splitlines(True))
_from_import_re.search(line)
new_file.extend(rewrite_from_imports(match.group(),
line[match.end():],
lineiter))
new_file.append(line)
''.join(new_file)
rewrite_for_blueprints(contents,
modules_declared
get_module_autoname(filename)
ast.literal_eval(name_param)
modules_declared.append((target,
modname))
(target,
'Blueprint(%r,
__name__'
modname)
_module_constructor_re.sub(handle_match,
modules_declared:
rewrite_blueprint_imports(new_contents)
_blueprint_related:
pattern.sub(replacement,
dict(modules_declared)
teardown):
teardown:
fix_teardown_funcs(new_contents)
rewrite_for_blueprints(new_contents,
fix_url_for(new_contents,
modules)
_error_handler_re.sub('\\1.error_handler_spec[None][\\2]',
contents):
this_file
os.path.realpath(__file__).rstrip('c')
dirnames,
filenames
dirnames[:]
dirnames
x.startswith('.')]
filenames:
os.path.join(dirpath,
this_file:
'python'
filename.endswith(('.diff',
'.patch',
'.udiff')):
f.read(TEMPLATE_LOOKAHEAD)
for'
if'
url_for'
contents:
'template'
scan_path(path=None,
teardown=True):
teardown)
'template':
OptionParser(usage='%prog
[paths]')
parser.add_option('-T',
'--no-teardown-detection',
dest='no_teardown',
help='Do
'detect
teardown
rewrites.')
parser.add_option('-b',
'--bundled-templates',
dest='bundled_tmpl',
help='Indicate
'that
modules.
Default
auto
detect.')
['.']
parser.error('Python
later
upgrade
script.')
scan_path(path,
teardown=not
options.no_teardown)
fullname.startswith(self.prefix):
activate():
types.ModuleType('flask.ext')
ext_module.__path__
sys.modules['flask.ext']
'flask.ext')
setuptools.package_index
PackageIndex
setuptools.archive_util
unpack_archive
flask_svc_url
'http://flask.pocoo.org/extensions/'
'/private/tmp'
tempfile.gettempdir()
'/flaskext-test'
flaskdir
os.path.abspath(os.path.join(os.path.dirname(__file__),
os.environ['PYTHONDONTWRITEBYTECODE']
RESULT_TEMPATE
u'''\
<!doctype
html>
<title>Flask-Extension
Results</title>
<style
type=text/css>
17px;
#000;
#004B6B;
a:hover
#6D4100;
h1,
h2,
'Garamond',
30px;
24px;
19px;
textarea,
'Consolas',
'Menlo',
'Deja
Vu
'Bitstream
Vera
monospace!important;
7px
1.3;
1.4;
border-collapse:
collapse;
td,
4px
10px;
text-align:
left;
tr.success
#D3F5CC;
tr.failed
#F5D2CB;
</style>
<h1>Flask-Extension
Results</h1>
<p>
detailed
'approved'
approved
extensions.
<h2>Summary</h2>
<table
class=results>
<thead>
<tr>
<th>Extension
<th>Version
<th>Author
<th>License
<th>Outcome
results[0].logs|dictsort
</thead>
<tbody>
{%
result.success
'failed'
<tr
class={{
}}>
result.author
result.license
<td><a
href="#{{
}}">see
log</a>
</tbody>
</table>
<h2>Test
Logs</h2>
<p>Detailed
platforms:
<h3
id="{{
}}">
[{{
}}]</h3>
<pre>{{
}}</pre>
log(msg,
print('[EXTTEST]
(msg
TestResult(object):
folder,
statuscode,
intrptr
'.tox/%s/bin/python'
interpreters[0])
self.statuscode
self.folder
self.success
fetch(field):
subprocess.Popen([intrptr,
'--'
field],
cwd=folder,
stdout=subprocess.PIPE)
c.communicate()[0].strip()
self.license
fetch('license')
self.author
fetch('author')
fetch('version')
interpreter
interpreters:
'.tox/%s/log/test.log'
interpreter)
os.path.isfile(logfile):
open(logfile).read()
create_tdir():
shutil.rmtree(tdir)
os.mkdir(tdir)
package_flask():
distfolder
'/.flask-dist'
subprocess.Popen(['python',
'--formats=gztar',
'--dist',
distfolder],
cwd=flaskdir)
c.wait()
os.path.join(distfolder,
os.listdir(distfolder)[0])
get_test_command(checkout_dir):
os.path.isfile(checkout_dir
'/Makefile'):
'make
setup.py
urllib2.Request(flask_svc_url,
headers={'accept':'application/json'})
urllib2.urlopen(req).read()
json.loads(d)
data['extensions']:
checkout_extension(name):
log('Downloading
temporary
folder',
os.path.join(tdir,
os.mkdir(root)
PackageIndex().download(name,
unpack_archive(checkout_path,
os.listdir(root):
log('Downloaded
tox_template
os.path.join(checkout_path,
'tox-flask-test.ini')
os.path.exists(tox_path):
open(tox_path,
f.write(tox_template
'env':
','.join(interpreters),
'cache':
tdir,
'deps':
iter_extensions(only_approved=True):
ext['approved']
only_approved:
ext['name']
checkout_extension(name)
checkout_path)
test_command
get_test_command(checkout_path)
log('Test
test_command)
open(checkout_path
'/flaskext-runtest.sh',
f.write(test_command
&>
"$1"
/dev/null\n')
subprocess.call(['tox',
'tox-flask-test.ini'],
cwd=checkout_path)
TestResult(name,
checkout_path,
interpreters)
create_tdir()
log('Packaging
Flask')
package_flask()
log('Temporary
Environment:
tdir)
extensions:
log('Testing
result.success:
succeeded')
results[name]
approved):
results.values()
items.sort(key=lambda
x.name.lower())
Template(RESULT_TEMPATE,
autoescape=True).render(results=items,
approved=approved)
tempfile.mkstemp(suffix='.html')
os.fdopen(fd,
'w').write(rv.encode('utf-8')
argparse.ArgumentParser(description='Runs
parser.add_argument('--all',
dest='all',
help='run
extensions,
approved')
parser.add_argument('--browse',
dest='browse',
help='show
summary')
parser.add_argument('--env',
dest='env',
default='py25,py26,py27',
help='the
environments
against')
parser.add_argument('--extension=',
dest='extension',
help='tests
extension')
args.extension
[args.extension]
args.all
iter_extensions(only_approved)
args.env.split(',')])
only_approved)
args.browse:
webbrowser.open('file:///'
filename.lstrip('/'))
print('Results
written
{}'.format(filename))
_date_clean_re
re.compile(r'(\d+)(st|nd|rd|th)')
parse_changelog():
open('CHANGES')
iter(f)
re.search('^Version\s+(.*)',
line.strip())
match.group(1).strip()
lineiter.next().count('-')
len(match.group(0)):
change_info
lineiter.next().strip()
change_info:
re.search(r'released
(\w+\s+\d+\w+\s+\d+)'
r'(?:,
(.*))?(?i)',
change_info)
datestr,
parse_date(datestr),
bump_version(version):
version.split('.'))
fail('Current
numeric')
parts[-1]
parts))
parse_date(string):
_date_clean_re.sub(r'\1',
string)
datetime.strptime(string,
'%B
%Y')
set_filename_version(filename,
version_number,
pattern):
inject_version(match):
before,
changed.append(True)
version_number
re.sub(r"^(\s*%s\s*=\s*')(.+?)(')(?sm)"
inject_version,
changed:
f.write(contents)
set_init_version(version):
info('Setting
__init__.py
set_filename_version('flask/__init__.py',
'__version__')
build_and_upload():
'release',
'bdist_wheel',
'upload']).wait()
fail(message,
print('Error:',
info(message,
print(message
get_git_tags():
set(Popen(['git',
'tag'],
stdout=PIPE).communicate()[0].splitlines())
'diff',
'--quiet']).wait()
make_git_commit(message,
'-am',
message]).wait()
make_git_tag(tag):
info('Tagging
"%s"',
tag)
'tag',
tag]).wait()
os.chdir(os.path.join(os.path.dirname(__file__),
parse_changelog()
changelog')
release_date,
dev_version
bump_version(version)
'-dev'
info('Releasing
(codename
codename,
release_date.strftime('%d/%m/%Y'))
get_git_tags()
fail('Version
tagged',
release_date.date()
date.today():
fail('Release
today
release_date.date(),
date.today())
fail('You
uncommitted
git')
set_init_version(version)
make_git_commit('Bump
make_git_tag(version)
build_and_upload()
set_init_version(dev_version)
test_apps(monkeypatch):
monkeypatch.syspath_prepend(
os.path.abspath(os.path.join(
'test_apps'))
leak_detector(request):
ensure_clean_request_context():
leaks.append(flask._request_ctx_stack.pop())
request.addfinalizer(ensure_clean_request_context)
@pytest.fixture(params=(True,
limit_loader(request,
request.param:
LimitedLoader(object):
loader):
('archive',
'Mocking
`%s.`'
getattr(self.loader,
old_get_loader
pkgutil.get_loader
get_loader(*args,
LimitedLoader(old_get_loader(*args,
monkeypatch.setattr(pkgutil,
'get_loader',
get_loader)
modules_tmpdir(tmpdir,
'''A
tmpdir.mkdir('modules_tmpdir')
modules_tmpdir_prefix(modules_tmpdir,
monkeypatch.setattr(sys,
str(modules_tmpdir))
site_packages(modules_tmpdir,
'''Create
site-packages'''
.mkdir('lib')\
.mkdir('python{x[0]}.{x[1]}'.format(x=sys.version_info))\
.mkdir('site-packages')
install_egg(modules_tmpdir,
'''Generate
inner(name,
base=modules_tmpdir):
ValueError(name)
base.join(name).ensure_dir()
base.join(name).join('__init__.py').ensure()
egg_setup
base.join('setup.py')
egg_setup.write(textwrap.dedent(.format(name)))
subprocess.check_call(
'bdist_egg'],
cwd=str(modules_tmpdir)
egg_path,
modules_tmpdir.join('dist/').listdir()
monkeypatch.syspath_prepend(str(egg_path))
egg_path
purge_module(request):
inner(name):
request.addfinalizer(lambda:
sys.modules.pop(name,
@pytest.yield_fixture(autouse=True)
catch_deprecation_warnings(recwarn):
recwarn.list
test_basic_url_generation():
app.config['PREFERRED_URL_SCHEME']
test_url_generation_requires_server_name():
test_url_generation_without_context_fails():
test_request_context_means_app_context():
test_app_context_provides_current_app():
test_app_tearing_down():
test_app_tearing_down_with_previous_exception():
test_app_tearing_down_with_handled_exception():
test_app_ctx_globals_methods():
flask.g.get('foo')
flask.g.get('foo',
lie')
pytest.raises(KeyError):
flask.g.pop('bar',
cake')
cake'
list(flask.g)
test_custom_app_ctx_globals_class():
CustomRequestGlobals(object):
self.spam
app.app_ctx_globals_class
CustomRequestGlobals
g.spam
test_context_refcounts():
called.append('request')
called.append('app')
flask._app_ctx_stack.top:
flask._request_ctx_stack.top:
flask._request_ctx_stack.top.request.environ
env['werkzeug.request']
res.status_code
res.data
['request',
'app']
test_clean_pop():
called.append('TEARDOWN')
called.append(flask.current_app.name)
['test_appctx',
'TEARDOWN']
NotFound,
Forbidden
test_options_work():
test_options_on_multiple_rules():
methods=['PUT'])
index_put():
'Aha!'
'PUT']
test_options_handling_disabled():
index.provide_automatic_options
app.route('/')(index)
index2.provide_automatic_options
app.route('/',
methods=['OPTIONS'])(index2)
['OPTIONS']
test_request_dispatching():
@app.route('/more',
test_disallow_string_for_allowed_methods():
methods='GET
POST')
"Hey"
test_url_mapping():
"7eb41166-9ebf-4d26-b771-ea3f54f8b383"
options():
app.add_url_rule('/more',
more,
app.add_url_rule('/options',
methods=['options'])
c.open('/options',
rv.data.decode("utf-8")
test_werkzeug_routing():
app.view_functions['bar']
app.view_functions['index']
test_endpoint_decorator():
@app.endpoint('bar')
@app.endpoint('index')
test_session():
@app.route('/set',
set():
flask.request.form['value']
'value
@app.route('/get')
get():
c.post('/set',
data={'value':
'42'}).data
b'value
c.get('/get').data
test_session_using_server_name():
SERVER_NAME='example.com'
test_session_using_server_name_and_port():
SERVER_NAME='example.com:8080'
test_session_using_server_name_port_and_path():
SERVER_NAME='example.com:8080',
APPLICATION_ROOT='/foo'
'http://example.com:8080/foo')
'domain=example.com'
'path=/foo'
test_session_using_application_root():
PrefixPathMiddleware(object):
environ['SCRIPT_NAME']
self.app(environ,
app.wsgi_app
PrefixPathMiddleware(app.wsgi_app,
'/bar')
APPLICATION_ROOT='/bar'
'path=/bar'
test_session_using_session_settings():
SERVER_NAME='www.example.com:8080',
APPLICATION_ROOT='/test',
SESSION_COOKIE_DOMAIN='.example.com',
SESSION_COOKIE_HTTPONLY=False,
SESSION_COOKIE_SECURE=True,
SESSION_COOKIE_PATH='/'
'http://www.example.com:8080/test/')
'path=/'
'secure'
test_missing_session():
expect_exception(f,
e.value.args
'session
unavailable'
e.value.args[0]
flask.session.get('missing_key')
expect_exception(flask.session.__setitem__,
42)
expect_exception(flask.session.pop,
test_session_expiration():
text_type(flask.session.permanent)
re.search(r'\bexpires=([^;]+)(?i)',
parse_date(match.group())
expires.year
expected.year
expires.month
expected.month
expires.day
expected.day
client.get('/test')
b'True'
re.search(r'\bexpires=([^;]+)',
test_session_stored_last():
repr(flask.session.get('foo'))
b'None'
test_session_special_types():
datetime.utcnow().replace(microsecond=0)
flask.session['m']
flask.session['u']
flask.session['dt']
flask.session['b']
flask.session['t']
pickle.dumps(dict(flask.session))
pickle.loads(c.get('/').data)
rv['m']
type(rv['m'])
flask.Markup
rv['dt']
rv['u']
rv['b']
type(rv['b'])
rv['t']
test_session_cookie_setting():
'dev
@app.route('/bump')
bump():
flask.session.get('foo',
str(rv)
@app.route('/read')
read():
str(flask.session.get('foo',
run_test(expect_header):
c.get('/read')
set_cookie
rv.headers.get('set-cookie')
(set_cookie
expect_header
run_test(expect_header=True)
test_flashes():
flask.flash('Zap')
flask.flash('Zip')
list(flask.get_flashed_messages())
['Zap',
'Zip']
test_extended_flashing():
World')
'error')
flask.flash(flask.Markup(u'<em>Testing</em>'),
'warning')
@app.route('/test/')
flask.get_flashed_messages()
@app.route('/test_with_categories/')
test_with_categories():
flask.get_flashed_messages(with_categories=True)
('error',
@app.route('/test_filter/')
test_filter():
category_filter=['message'],
[('message',
World')]
@app.route('/test_filters/')
test_filters():
'warning'],
@app.route('/test_filters_without_returning_categories/')
test_filters2():
'warning'])
messages[0]
messages[1]
c.get('/test/')
c.get('/test_with_categories/')
c.get('/test_filter/')
c.get('/test_filters/')
c.get('/test_filters_without_returning_categories/')
test_request_processing():
evts.append('before')
b'|after'
evts.append('after')
'before'
'request'
b'request|after'
test_request_preprocessing_early_return():
before_request1():
evts.append(1)
before_request2():
evts.append(2)
"hello"
before_request3():
evts.append(3)
"bye"
evts.append('index')
"damnit"
app.test_client().get('/').data.strip()
b'hello'
test_after_request_processing():
@flask.after_this_request
foo(response):
response.headers['X-Foo']
resp.headers['X-Foo']
test_teardown_request_handler():
test_teardown_request_handler_debug_mode():
test_teardown_request_handler_error():
teardown_request1(exc):
teardown_request2(exc):
fails():
test_before_after_request_order():
before1():
called.append(1)
before2():
called.append(2)
after1(response):
called.append(4)
after2(response):
called.append(3)
finish1(exc):
called.append(6)
finish2(exc):
called.append(5)
6]
test_error_handling():
not_found(e):
'internal
@app.errorhandler(Forbidden)
forbidden(e):
'forbidden',
error():
error2():
b'not
c.get('/error')
b'internal
c.get('/forbidden')
test_before_request_and_routing_errors():
attach_something():
flask.g.something
return_something(error):
flask.g.something,
test_user_error_handling():
@app.errorhandler(MyException)
handle_my_exception(e):
MyException)
MyException()
test_http_error_subclass_handling():
ForbiddenSubclass(Forbidden):
@app.errorhandler(ForbiddenSubclass)
'banana'
'apple'
@app.route('/1')
index1():
ForbiddenSubclass()
@app.route('/2')
@app.route('/3')
index3():
c.get('/1').data
b'banana'
c.get('/2').data
c.get('/3').data
test_trapping_of_bad_request_key_errors():
flask.request.form['missing_key']
c.get('/fail').status_code
app.config['TRAP_BAD_REQUEST_ERRORS']
pytest.raises(KeyError)
c.get("/fail")
e.errisinstance(BadRequest)
test_trapping_of_all_http_exceptions():
app.config['TRAP_HTTP_EXCEPTIONS']
test_enctype_debug_helper():
DebugFilesKeyError
@app.route('/fail',
flask.request.files['foo'].filename
pytest.raises(DebugFilesKeyError)
c.post('/fail',
data={'foo':
'index.txt'})
transmitted'
"index.txt"'
test_response_creation():
@app.route('/unicode')
from_unicode():
Wörld'
@app.route('/string')
from_string():
@app.route('/args')
from_tuple():
'Meh',
'Testing',
@app.route('/two_args')
from_two_args_tuple():
'Hello',
'Test',
@app.route('/args_status')
from_status_tuple():
'Hi,
status!',
@app.route('/args_header')
from_response_instance_status_tuple():
flask.Response('Hello
world',
404),
"X-Foo":
"Bar",
"X-Bar":
"Foo"
c.get('/unicode').data
c.get('/string').data
c.get('/args')
'Testing'
rv2
c.get('/two_args')
rv2.data
rv2.headers['X-Foo']
rv2.status_code
rv2.mimetype
rv3
c.get('/args_status')
rv3.data
b'Hi,
status!'
rv3.status_code
rv3.mimetype
rv4
c.get('/args_header')
rv4.data
rv4.headers['X-Foo']
'Bar'
rv4.headers['X-Bar']
rv4.status_code
test_make_response():
flask.make_response()
flask.make_response('Awesome')
flask.make_response('W00t',
b'W00t'
test_make_response_with_response_instance():
flask.jsonify({'msg':
'W00t'}),
"W00t"\n}\n'
flask.Response(''),
'text/html'}),
[('X-Foo',
'bar')])
rv.headers['Content-Type']
test_jsonify_no_prettyprint():
b'{"msg":{"submsg":"W00t"},"msg2":"foobar"}\n'
uncompressed_msg
"W00t"
"foobar"
flask.jsonify(uncompressed_msg),
test_jsonify_prettyprint():
{"msg":{"submsg":"W00t"},"msg2":"foobar"}
=\
{\n
"W00t"\n
"foobar"\n}\n'
flask.jsonify(compressed_msg),
test_jsonify_mimetype():
app.config.update({"JSONIFY_MIMETYPE":
'application/vnd.api+json'})
{"submsg":
"W00t"},
flask.jsonify(msg),
'application/vnd.api+json'
test_jsonify_args_and_kwargs_check():
pytest.raises(TypeError)
flask.jsonify('fake
args',
kwargs='fake')
'behavior
undefined'
test_url_generation():
@app.route('/hello/<name>',
hello():
x')
'/hello/test%20x'
x',
'http://localhost/hello/test%20x'
test_build_error_handler():
'spam')
RuntimeError('Test
current.')
BuildError,
app.handle_url_build_error,
app.url_build_error_handlers.append(handler)
test_build_error_handler_reraise():
handler_raises_build_error(error,
app.url_build_error_handlers.append(handler_raises_build_error)
'not.existing')
test_custom_converters():
BaseConverter
ListConverter(BaseConverter):
to_python(self,
to_url(self,
base_to_url
super(ListConverter,
self).to_url
','.join(base_to_url(x)
app.url_map.converters['list']
ListConverter
@app.route('/<list:args>')
index(args):
'|'.join(args)
c.get('/1,2,3').data
b'1|2|3'
test_static_files():
app.test_client().get('/static/index.html')
b'<h1>Hello
World!</h1>'
'/static/index.html'
test_static_path_deprecated(recwarn):
static_path='/foo')
test_static_url_path():
static_url_path='/foo')
test_none_response():
'View
"Expected
ValueError"
test_request_locals():
repr(flask.g)
'<LocalProxy
unbound>'
test_test_app_proper_environ():
subdomain():
'Foo
'http://localhost.localdomain:5000')
'https://localhost.localdomain:5000')
app.config.update(SERVER_NAME='localhost.localdomain:443')
"('localhost.localdomain:443')
('localhost.localdomain')"
'http://foo.localhost')
"('localhost.localdomain')
('foo.localhost')"
'http://foo.localhost.localdomain')
b'Foo
test_exception_propagation():
apprunner(config_key):
app.config[config_key]
pytest.raises(Exception):
'TESTING',
'PROPAGATE_EXCEPTIONS',
Thread(target=apprunner,
args=(config_key,))
@pytest.mark.parametrize('debug',
@pytest.mark.parametrize('use_debugger',
@pytest.mark.parametrize('use_reloader',
@pytest.mark.parametrize('propagate_exceptions',
test_werkzeug_passthrough_errors(monkeypatch,
use_debugger,
use_reloader,
propagate_exceptions):
kwargs.get('passthrough_errors')
app.config['PROPAGATE_EXCEPTIONS']
propagate_exceptions
app.run(debug=debug,
use_debugger=use_debugger,
use_reloader=use_reloader)
test_max_content_length():
app.config['MAX_CONTENT_LENGTH']
always_first():
@app.route('/accept',
accept_file():
@app.errorhandler(413)
catcher(error):
c.post('/accept',
data={'myfile':
100})
test_url_processors():
@app.url_defaults
app.url_map.is_endpoint_expecting(endpoint,
'lang_code'):
@app.url_value_preprocessor
values.pop('lang_code',
@app.route('/<lang_code>/')
flask.url_for('about')
@app.route('/<lang_code>/about')
flask.url_for('something_else')
something_else():
flask.url_for('about',
lang_code='en')
b'/foo'
c.get('/foo').data
b'/en/about'
test_inject_blueprint_url_defaults():
flask.Blueprint('foo.bar.baz',
template_folder='template')
bp_defaults(endpoint,
values['page']
'login'
@bp.route('/<page>')
view(page):
app.inject_url_defaults('foo.bar.baz.view',
dict(page='login')
app.test_request_context('/somepage'):
flask.url_for('foo.bar.baz.view')
'/login'
test_nonascii_pathinfo():
@app.route(u'/киртест')
c.get(u'/киртест')
test_debug_mode_complains_after_first_request():
broken():
called'
working():
app.test_client().get('/foo').data
test_before_first_request_functions():
test_before_first_request_functions_concurrent():
get_and_assert():
c.get("/")
Thread(target=get_and_assert)
get_and_assert()
test_routing_redirect_debugging():
@app.route('/foo/',
data={})
'http://localhost/foo/'
('Make
'your
POST-request
c.get('/foo',
@app.route('/foo/')
@app.route('/bar/',
for_bar():
@app.route('/bar/123',
for_bar_foo():
flask.url_for('foo')
'/foo/'
flask.url_for('bar')
'/bar/'
flask.url_for('123')
'/bar/123'
c.get('/bar/').data
c.get('/bar/123').data
b'123'
test_preserve_only_once():
range(3):
flask._request_ctx_stack.top.pop()
test_preserve_remembers_exception():
@app.route('/success')
success_func():
'Okay'
teardown_handler(exc):
errors.append(exc)
isinstance(errors[0],
errors[1]
test_get_method_on_g():
flask.g.get('x',
test_g_iteration_protocol():
'foos'
sorted(flask.g)
'foo']
test_subdomain_basic_support():
normal_index():
subdomain='test')
test_index():
'http://localhost/')
b'normal
'http://test.localhost/')
test_subdomain_matching():
'http://mitsuhiko.localhost/')
test_subdomain_matching_with_ports():
'localhost:3000'
'http://mitsuhiko.localhost:3000/')
test_multi_route_rules():
@app.route('/<test>/')
index(test='a'):
test_multi_route_class_views():
app.add_url_rule('/<test>/',
index(self,
test='a'):
View(app)
test_run_defaults(monkeypatch):
test_run_server_port(monkeypatch):
run_simple_mock(hostname,
application,
8000
app.run(hostname,
parse_cache_control_header
test_blueprint_specific_error_handling():
flask.Blueprint('backend',
sideend
flask.Blueprint('sideend',
@frontend.errorhandler(403)
frontend_forbidden(e):
'frontend
@frontend.route('/frontend-no')
frontend_no():
@backend.errorhandler(403)
backend_forbidden(e):
'backend
@backend.route('/backend-no')
backend_no():
@sideend.route('/what-is-a-sideend')
sideend_no():
app.register_blueprint(sideend)
app_forbidden(e):
c.get('/frontend-no').data
b'frontend
c.get('/backend-no').data
b'backend
c.get('/what-is-a-sideend').data
b'application
test_blueprint_specific_user_error_handling():
MyDecoratorException(Exception):
MyFunctionException(Exception):
blue
flask.Blueprint('blue',
@blue.errorhandler(MyDecoratorException)
my_decorator_exception_handler(e):
MyDecoratorException)
'boom'
my_function_exception_handler(e):
MyFunctionException)
'bam'
blue.register_error_handler(MyFunctionException,
my_function_exception_handler)
@blue.route('/decorator')
blue_deco_test():
MyDecoratorException()
@blue.route('/function')
blue_func_test():
MyFunctionException()
app.register_blueprint(blue)
c.get('/decorator').data
b'boom'
c.get('/function').data
b'bam'
test_blueprint_url_definitions():
@bp.route('/foo',
defaults={'baz':
42})
foo(bar,
baz):
'%s/%d'
(bar,
baz)
@bp.route('/bar')
bar(bar):
text_type(bar)
url_prefix='/1',
url_prefix='/2',
19})
c.get('/1/foo').data
b'23/42'
c.get('/2/foo').data
b'19/42'
c.get('/1/bar').data
b'23'
c.get('/2/bar').data
b'19'
test_blueprint_url_processors():
url_prefix='/<lang_code>')
@bp.url_value_preprocessor
values.pop('lang_code')
@bp.route('/')
flask.url_for('.about')
@bp.route('/about')
flask.url_for('.index')
b'/de/'
test_templates_and_static(test_apps):
Frontend'
c.get('/admin/')
c.get('/admin/index2')
c.get('/admin/static/test.txt')
b'Admin
File'
b'/*
nested
*/'
expected_max_age:
flask.url_for('admin.static',
filename='test.txt')
'/admin/static/test.txt'
flask.render_template('missing.html')
e.value.name
'missing.html'
flask.Flask(__name__).test_request_context():
flask.render_template('nested/nested.txt')
'I\'m
nested'
test_default_static_cache_timeout():
MyBlueprint(flask.Blueprint):
MyBlueprint('blueprint',
app.register_blueprint(blueprint)
unexpected_max_age:
blueprint.send_static_file('index.html')
test_templates_list(test_apps):
sorted(app.jinja_env.list_templates())
['admin/index.html',
'frontend/index.html']
test_dotted_names():
flask.Blueprint('myapp.frontend',
flask.Blueprint('myapp.backend',
@frontend.route('/fe')
frontend_index():
flask.url_for('myapp.backend.backend_index')
@frontend.route('/fe2')
frontend_page2():
flask.url_for('.frontend_index')
@backend.route('/be')
backend_index():
flask.url_for('myapp.frontend.frontend_index')
c.get('/fe').data.strip()
b'/be'
c.get('/fe2').data.strip()
c.get('/be').data.strip()
test_dotted_names_from_app():
app_index():
flask.url_for('test.index')
@test.route('/test/')
flask.url_for('app_index')
app.register_blueprint(test)
b'/test/'
test_empty_url_defaults():
@bp.route('/',
@bp.route('/page/<int:page>')
something(page):
str(page)
c.get('/page/2').data
@bp.route('/bar/foo')
bar_foo():
c.get('/py/bar').data
b'bp.bar'
c.get('/py/bar/123').data
b'bp.123'
c.get('/py/bar/foo').data
b'bp.bar_foo'
test_route_decorator_custom_endpoint_with_dots():
endpoint='bar.bar')
endpoint='bar.123')
foo_foo_foo():
bp.add_url_rule(
'/bar/123',
endpoint='bar.123',
view_func=foo_foo_foo
bp.route('/bar/123',
endpoint='bar.123'),
c.get('/py/bar')
c.get('/py/bar/123')
bp.add_app_template_filter(my_reverse)
@bp.app_template_filter('strrev')
test_template_filter_after_route_with_template():
bp.add_app_template_filter(super_reverse)
@bp.app_template_filter('super_reverse')
bp.add_app_template_test(is_boolean)
test_template_test_after_route_with_template():
bp.add_app_template_test(boolean)
click.testing
CliRunner
flask.cli
AppGroup,
FlaskGroup,
NoAppException,
ScriptInfo,
with_appcontext,
prepare_exec_for_file,
find_default_import_path
test_cli_name(test_apps):
cliapp.app
testapp.cli.name
testapp.name
test_find_best_app(test_apps):
Module.app
Module.application
myapp
Module.myapp
myapp1
Flask('appname1')
myapp2
Flask('appname2')
test_prepare_exec_for_file(test_apps):
os.path.realpath('/tmp/share/test.py')
os.path.dirname(realpath)
prepare_exec_for_file('/tmp/share/test.py')
os.path.realpath('/tmp/share/__init__.py')
os.path.dirname(os.path.dirname(realpath))
prepare_exec_for_file('/tmp/share/__init__.py')
'share'
pytest.raises(NoAppException):
prepare_exec_for_file('/tmp/share/test.txt')
test_locate_app(test_apps):
locate_app("cliapp.app").name
locate_app("cliapp.app:testapp").name
locate_app("cliapp.multiapp:app1").name
"app1"
"cliapp.app:notanapp")
test_find_default_import_path(test_apps,
monkeypatch,
tmpdir):
monkeypatch.delitem(os.environ,
raising=False)
'notanapp')
'notanapp'
tmpfile
tmpdir.join('testapp.py')
tmpfile.write('')
str(tmpfile))
prepare_exec_for_file(str(tmpfile))
test_scriptinfo(test_apps):
ScriptInfo(app_import_path="cliapp.app:testapp")
Flask("createapp")
ScriptInfo(create_app=create_app)
app.name
"createapp"
test_with_appcontext():
@click.command()
testcmd():
Flask("testapp"))
runner.invoke(testcmd,
'testapp\n'
test_appgroup():
@click.group(cls=AppGroup)
cli():
@cli.command(with_appcontext=True)
@cli.group()
subgroup():
@subgroup.command(with_appcontext=True)
test2():
Flask("testappgroup"))
['test'],
['subgroup',
'test2'],
test_flaskgroup():
Flask("flaskgroup")
@click.group(cls=FlaskGroup,
create_app=create_app)
cli(**params):
@cli.command()
['test'])
'flaskgroup\n'
common_object_test(app):
app.config['TEST_KEY']
'TestConfig'
app.config
test_config_from_file():
app.config.from_pyfile(__file__.rsplit('.',
test_config_from_object():
test_config_from_json():
current_dir
app.config.from_json(os.path.join(current_dir,
'static',
'config.json'))
test_config_from_mapping():
app.config.from_mapping({
'devkey',
'TEST_KEY':
app.config.from_mapping([
('SECRET_KEY',
'devkey'),
('TEST_KEY',
SECRET_KEY='devkey',
TEST_KEY='foo'
test_config_from_class():
Base(object):
Test(Base):
app.config.from_object(Test)
test_config_from_envvar():
"'FOO_SETTINGS'
__file__.rsplit('.',
'.py'}
test_config_from_envvar_missing():
'missing.cfg'}
test_config_missing():
app.config.from_pyfile('missing.cfg')
app.config.from_pyfile('missing.cfg',
test_config_missing_json():
app.config.from_json('missing.json')
msg.endswith("missing.json'")
app.config.from_json('missing.json',
test_custom_config_class():
Config(flask.Config):
Flask(flask.Flask):
isinstance(app.config,
Config)
test_session_lifetime():
app.config['PERMANENT_SESSION_LIFETIME']
app.permanent_session_lifetime.seconds
test_send_file_max_age():
timedelta(hours=2)
test_get_namespace():
app.config['FOO_OPTION_1']
app.config['FOO_OPTION_2']
app.config['BAR_STUFF_1']
app.config['BAR_STUFF_2']
app.config.get_namespace('FOO_')
foo_options['option_1']
foo_options['option_2']
lowercase=False)
bar_options['STUFF_1']
bar_options['STUFF_2']
app.config.get_namespace('FOO_',
foo_options['foo_option_1']
foo_options['foo_option_2']
lowercase=False,
bar_options['BAR_STUFF_1']
bar_options['BAR_STUFF_2']
TestRequestDeprecation(object):
test_request_json(self,
flask.request.json
{'spam':
print(flask.request.json)
data='{"spam":
42}',
test_request_module(self,
flask.request.module
disable_extwarnings(request,
flask.exthook
inner():
set(w.category
recwarn.list)
set([ExtDeprecationWarning])
recwarn.clear()
request.addfinalizer(inner)
importhook_setup(monkeypatch,
entry,
list(sys.modules.items()):
entry.startswith('flask.ext.')
entry.startswith('flask_')
entry.startswith('flaskext.')
'flaskext'
monkeypatch.delitem(sys.modules,
entry)
reload_module(ext)
type(item)
'flask.exthook'
'ExtensionImporter':
ext.__dict__:
newext_simple(modules_tmpdir):
modules_tmpdir.join('flask_newext_simple.py')
x.write('ext_id
"newext_simple"')
oldext_simple(modules_tmpdir):
flaskext.join('oldext_simple.py').write('ext_id
"oldext_simple"')
newext_package(modules_tmpdir):
pkg
modules_tmpdir.mkdir('flask_newext_package')
pkg.join('__init__.py').write('ext_id
"newext_package"')
pkg.join('submodule.py').write('def
test_function():\n
42\n')
oldext_package(modules_tmpdir):
oldext
flaskext.mkdir('oldext_package')
oldext.join('__init__.py').write('ext_id
"oldext_package"')
oldext.join('submodule.py').write('def
test_function():\n'
42')
flaskext_broken(modules_tmpdir):
modules_tmpdir.mkdir('flask_broken')
ext.join('b.py').write('\n')
ext.join('__init__.py').write('import
flask.ext.broken.b\n'
missing_module')
test_flaskext_new_simple_import_normal(newext_simple):
flask.ext.newext_simple
test_flaskext_new_simple_import_module(newext_simple):
newext_simple
newext_simple.ext_id
newext_simple.__name__
'flask_newext_simple'
test_flaskext_new_package_import_normal(newext_package):
test_flaskext_new_package_import_module(newext_package):
newext_package
newext_package.ext_id
newext_package.__name__
'flask_newext_package'
test_flaskext_new_package_import_submodule_function(newext_package):
flask.ext.newext_package.submodule
test_flaskext_new_package_import_submodule(newext_package):
'flask_newext_package.submodule'
test_flaskext_old_simple_import_normal(oldext_simple):
flask.ext.oldext_simple
test_flaskext_old_simple_import_module(oldext_simple):
oldext_simple
oldext_simple.ext_id
oldext_simple.__name__
'flaskext.oldext_simple'
test_flaskext_old_package_import_normal(oldext_package):
test_flaskext_old_package_import_module(oldext_package):
oldext_package
oldext_package.ext_id
oldext_package.__name__
'flaskext.oldext_package'
test_flaskext_old_package_import_submodule(oldext_package):
'flaskext.oldext_package.submodule'
test_flaskext_old_package_import_submodule_function(oldext_package):
flask.ext.oldext_package.submodule
test_flaskext_broken_package_no_module_caching(flaskext_broken):
pytest.raises(ImportError):
test_no_error_swallowing(flaskext_broken):
pytest.raises(ImportError)
excinfo.type
missing_module'
\'missing_module\''
excinfo.tb.tb_frame.f_globals
globals()
excinfo.tb.tb_next.tb_next
next.tb_next
os.path.join('flask_broken',
'__init__.py')
next.tb_frame.f_code.co_filename
parse_cache_control_header,
parse_options_header
has_encoding(name):
codecs.lookup(name)
LookupError:
TestJSON(object):
test_post_empty_json_adds_exception_to_response_content_in_debug(self):
test_post_empty_json_wont_add_exception_to_response_if_no_debug(self):
test_json_bad_requests(self):
flask.jsonify(foo=text_type(flask.request.get_json()))
data='malformed',
test_json_custom_mimetypes(self):
data='"foo"',
content_type='application/x+json')
test_json_body_encoding(self):
data=u'"Hällo
Wörld"'.encode('iso-8859-15'),
content_type='application/json;
charset=iso-8859-15')
test_json_as_unicode(self):
'"\\u2603"'
u'"\u2603"'
test_jsonify_basic_types(self):
test_data
's',
"longer
string",
False,)
enumerate(test_data):
'/jsonify_basic_types{0}'.format(i)
x=d:
flask.jsonify(x))
test_jsonify_dicts(self):
a=0,
b=23,
c=3.14,
d='t',
e='Hi',
f=True,
g=False,
h=['test
i={'test':'dict'}
@app.route('/kw')
return_kwargs():
flask.jsonify(**d)
@app.route('/dict')
return_dict():
flask.jsonify(d)
'/kw',
'/dict':
test_jsonify_arrays(self):
42,
't',
['test
{'test':'dict'}
@app.route('/args_unpack')
return_args_unpack():
flask.jsonify(*l)
@app.route('/array')
return_array():
flask.jsonify(l)
'/args_unpack',
'/array':
test_jsonify_date_types(self):
test_dates
datetime.datetime(1973,
datetime.date(1975,
enumerate(test_dates):
'/datetest{0}'.format(i)
val=d:
flask.jsonify(x=val))
flask.json.loads(rv.data)['x']
http_date(d.timetuple())
test_json_attr(self):
add():
text_type(json['a']
json['b'])
c.post('/add',
data=flask.json.dumps({'a':
2}),
test_template_escaping(self):
flask.render_template_string
flask.json.htmlsafe_dumps('</script>')
u'"\\u003c/script\\u003e"'
type(rv)
"</script>"|tojson
'"\\u003c/script\\u003e"'
"<\0/script>"|tojson
'"\\u003c\\u0000/script\\u003e"'
"<!--<script>"|tojson
'"\\u003c!--\\u003cscript\\u003e"'
"&"|tojson
'"\\u0026"'
"\'"|tojson
'"\\u0027"'
render("<a
ng-data='{{
data|tojson
}}'></a>",
data={'x':
"baz'"]})
'<a
ng-data=\'{"x":
"baz\\u0027"]}\'></a>'
test_json_customization(self):
MyEncoder(flask.json.JSONEncoder):
X):
'<%d>'
o.val
flask.json.JSONEncoder.default(self,
MyDecoder(flask.json.JSONDecoder):
kwargs.setdefault('object_hook',
self.object_hook)
flask.json.JSONDecoder.__init__(self,
object_hook(self,
'_foo'
X(obj['_foo'])
app.json_encoder
MyEncoder
app.json_decoder
MyDecoder
flask.json.dumps(flask.request.get_json()['x'])
data=flask.json.dumps({
'x':
{'_foo':
b'"<42>"'
test_modified_url_encoding(self):
ModifiedRequest(flask.Request):
url_charset
app.request_class
ModifiedRequest
app.url_map.charset
app.test_client().get(u'/?foo=정상처리'.encode('euc-kr'))
u'정상처리'.encode('utf-8')
has_encoding('euc-kr'):
test_modified_url_encoding
test_json_key_sorting(self):
app.config['JSON_SORT_KEYS']
dict.fromkeys(range(20),
flask.jsonify(values=d)
rv.data.strip().decode('utf-8').splitlines()]
TestSendfile(object):
test_send_file_regular(self):
test_send_file_xsendfile(self,
catch_deprecation_warnings):
test_send_file_last_modified(self):
datetime.datetime(1999,
flask.send_file(StringIO("party
it's"),
last_modified=last_modified)
test_send_file_object(self):
'static/index.html'),
mode='rb')
PyStringIO(object):
self._io
StringIO(*args,
getattr(self._io,
PyStringIO('Test')
f.name
'test.txt'
mimetype='text/plain')
test_attachment(self):
flask.send_file('static/index.html',
flask.send_file(StringIO('Test'),
as_attachment=True,
attachment_filename='index.txt',
add_etags=False)
'index.txt'
test_static_file(self):
StaticFileApp(flask.Flask):
StaticFileApp(__name__)
test_send_from_directory(self):
'hello.txt')
Subdomain'
test_send_from_directory_bad_request(self):
pytest.raises(BadRequest):
'bad\x00')
TestLogging(object):
test_logger_cache(self):
logger1.name
'/test_logger_cache'
test_debug_log(self,
capsys):
app.logger.warning('the
dead')
app.logger.debug('this
statement')
@app.route('/exc')
exc():
capsys.readouterr()
'WARNING
test_helpers
['
os.path.basename(__file__.rsplit('.',
dead'
statement'
c.get('/exc')
test_debug_log_override(self):
'flask_tests/test_debug_log_override'
test_exception_logging(self):
'flask_tests/test_exception_logging'
'Exception
[GET]'
'Traceback
(most
recent
last):'
0'
'ZeroDivisionError:'
test_processor_exceptions(self):
'before':
'before',
test_url_for_with_anchor(self):
_anchor='x
y')
'/#x%20y'
test_url_for_with_scheme(self):
test_url_for_with_scheme_not_external(self):
pytest.raises(ValueError,
test_url_for_with_alternating_schemes(self):
test_url_with_method(self):
MethodView
MyView(MethodView):
'Get
'Create'
myview
MyView.as_view('myview')
app.add_url_rule('/myview/',
app.add_url_rule('/myview/<int:id>',
app.add_url_rule('/myview/create',
methods=['POST'],
'/myview/'
id=42,
'/myview/42'
_method='POST')
'/myview/create'
TestNoImports(object):
test_name_with_import_error(self,
modules_tmpdir.join('importerror.py').write('raise
NotImplementedError()')
flask.Flask('importerror')
'Flask(import_name)
importing
import_name.'
TestStreaming(object):
test_streaming_with_context(self):
flask.Response(flask.stream_with_context(generate()))
test_streaming_with_context_as_decorator(self):
@flask.stream_with_context
generate(hello):
flask.Response(generate('Hello
'))
test_streaming_with_context_and_custom_close(self):
Wrapper(object):
gen):
self._gen
called.append(42)
next(self._gen)
flask.Response(flask.stream_with_context(
Wrapper(generate())))
TestSafeJoin(object):
test_safe_join(self):
passing
(('/',
'a/',
'b/',
'c/',
(('a',
(('/a',
'b/c',
(('a/b',
'X/../c'),
'a/b/c',
(('/a/b',
'c/X/..'),
'/a/b/c',
'/a/b/c/',
'./',
'/a/b/c/.',
'X/..'),
'a/b/c/.',
(('../',
'../a/b/c'),
(('/..',
'/..'),
passing:
flask.safe_join(*args)
test_safe_join_exceptions(self):
failing
'/c'),
'../b/c',
'b/c'),
'b/../b/../../c',
'c/../..'),
'b/../../c',
failing:
print(flask.safe_join(*args))
test_explicit_instance_paths(modules_tmpdir):
pytest.raises(ValueError)
instance_path='instance')
'must
absolute'
instance_path=str(modules_tmpdir))
str(modules_tmpdir)
test_main_module_paths(modules_tmpdir,
modules_tmpdir.join('main_app.py')
flask.Flask("__main__")')
purge_module('main_app')
main_app
os.path.abspath(os.getcwd())
os.path.join(here,
test_uninstalled_module_paths(modules_tmpdir,
modules_tmpdir.join('config_module_app.py').write(
purge_module('config_module_app')
config_module_app
test_uninstalled_package_paths(modules_tmpdir,
modules_tmpdir.mkdir('config_package_app')
init.write(
purge_module('config_package_app')
config_package_app
test_installed_module_paths(modules_tmpdir,
site_packages,
limit_loader):
site_packages.join('site_app.py').write(
purge_module('site_app')
site_app
modules_tmpdir.join('var').join('site_app-instance')
test_installed_package_paths(limit_loader,
installed_path
modules_tmpdir.mkdir('path')
monkeypatch.syspath_prepend(installed_path)
installed_path.mkdir('installed_package')
purge_module('installed_package')
installed_package
modules_tmpdir.join('var').join('installed_package-instance')
test_prefix_package_paths(limit_loader,
site_packages):
site_packages.mkdir('site_package')
purge_module('site_package')
site_package
site_package.app.instance_path
modules_tmpdir.join('var').join('site_package-instance')
test_egg_installed_paths(install_egg,
modules_tmpdir_prefix):
modules_tmpdir.mkdir('site_egg').join('__init__.py').write(
flask.Flask(__name__)'
install_egg('site_egg')
site_egg
site_egg.app.instance_path
str(modules_tmpdir.join('var/').join('site_egg-instance'))
'site_egg'
sys.modules['site_egg']
@pytest.mark.skipif(not
PY2,
reason='This
2.')
test_meta_path_loader_without_is_package(request,
modules_tmpdir.join('unimportable.py')
Loader(object):
sys.meta_path.append(Loader())
request.addfinalizer(sys.meta_path.pop)
pytest.raises(AttributeError):
unimportable
_gc_lock
assert_no_leak(object):
gc.disable()
_gc_lock.acquire()
flask._request_ctx_stack._local
loc.__storage__['FOOO']
self.old_objects
self.old_objects:
pytest.fail('Example
leaked')
_gc_lock.release()
gc.enable()
test_memory_consumption():
fire():
b'<h1>42</h1>'
'pypy_translation_info'):
assert_no_leak():
test_safe_join_toplevel_pardir():
flask.helpers
safe_join
safe_join('/foo',
'..')
test_aborting():
Foo(Exception):
whatever
@app.errorhandler(Foo)
handle_foo(e):
str(e.whatever)
flask.abort(flask.redirect(flask.url_for('test')))
rv.headers['Location']
'http://localhost/test'
c.get('/test')
test_teardown_on_pop():
test_teardown_with_previous_exception():
test_teardown_with_handled_exception():
test_proper_test_request_context():
sub():
'http://localhost.localdomain:5000/'
flask.url_for('sub',
'http://foo.localhost.localdomain:5000/'
environ_overrides={'HTTP_HOST':
"('localhost.localdomain:5000')
('localhost')"
app.config.update(SERVER_NAME='localhost')
app.config.update(SERVER_NAME='localhost:80')
'localhost:80'}):
test_context_binding():
@app.route('/meh')
meh():
app.test_request_context('/?name=World'):
app.test_request_context('/meh'):
meh()
'http://localhost/meh'
test_context_test():
test_manual_context_binding():
app.test_request_context('/?name=World')
test_greenlet_context_copying():
test_greenlet_context_copying_api():
@flask.copy_current_request_context
pytestmark
pytest.mark.skipif(
reason='Signals
library.'
test_template_rendered():
flask.template_rendered.connect(record,
flask.template_rendered.disconnect(record,
test_before_render_template():
flask.before_render_template.connect(record,
b'<h1>43</h1>'
flask.before_render_template.disconnect(record,
test_request_signals():
before_request_signal(sender):
calls.append('before-signal')
after_request_signal(sender,
calls.append('after-signal')
before_request_handler():
calls.append('before-handler')
after_request_handler(response):
calls.append('after-handler')
'stuff'
calls.append('handler')
'ignored
anyway'
flask.request_started.connect(before_request_signal,
flask.request_finished.connect(after_request_signal,
['before-signal',
'before-handler',
'handler',
'after-handler',
'after-signal']
flask.request_started.disconnect(before_request_signal,
flask.request_finished.disconnect(after_request_signal,
test_request_exception_signal():
recorded.append(exception)
flask.got_request_exception.connect(record,
app.test_client().get('/').status_code
isinstance(recorded[0],
flask.got_request_exception.disconnect(record,
test_appcontext_signals():
record_push(sender,
recorded.append('push')
record_pop(sender,
recorded.append('pop')
'Hello'
flask.appcontext_pushed.connect(record_push,
flask.appcontext_popped.connect(record_pop,
['push']
['push',
'pop']
flask.appcontext_pushed.disconnect(record_push,
flask.appcontext_popped.disconnect(record_pop,
test_flash_signal():
app.config['SECRET_KEY']
'secret'
flask.flash('This
category='notice')
flask.redirect('/other')
category):
recorded.append((message,
category))
flask.message_flashed.connect(record,
client.session_transaction():
'notice'
flask.message_flashed.disconnect(record,
test_appcontext_tearing_down_signal():
record_teardown(sender,
recorded.append(('tear_down',
kwargs))
flask.appcontext_tearing_down.connect(record_teardown,
[('tear_down',
{'exc':
None})]
flask.appcontext_tearing_down.disconnect(record_teardown,
test_suppressed_exception_logging():
SuppressedFlask(flask.Flask):
SuppressedFlask(__name__)
'flask_tests/test_suppressed_exception_logging'
test_context_processing():
{'injected_value':
flask.render_template('context_template.html',
b'<p>23|42'
test_original_win():
config=42)
test_request_less_rendering():
app.config['WORLD_NAME']
'Special
dict(foo=42)
flask.render_template_string('Hello
config.WORLD_NAME
Special
World
42'
test_standard_context():
'aha'
flask.render_template_string('''
request.args.foo
g.foo
config.DEBUG
session.test
app.test_client().get('/?foo=42')
rv.data.split()
[b'42',
b'23',
b'False',
b'aha']
test_escaping():
flask.render_template('escaping_template.html',
test_no_escaping():
flask.render_template('non_escaping_template.txt',
test_escaping_without_template_filename():
flask.render_template_string(
'&lt;test&gt;'
flask.render_template('mail.txt',
'<test>
Mail'
test_macros():
macro
flask.get_template_attribute('_macro.html',
macro('World')
app.add_template_filter(my_reverse)
@app.template_filter('strrev')
app.add_template_filter(super_reverse)
@app.template_filter('super_reverse')
test_add_template_global():
@app.template_global()
get_stuff():
'get_stuff'
app.jinja_env.globals.keys()
app.jinja_env.globals['get_stuff']
get_stuff
app.jinja_env.globals['get_stuff'](),
get_stuff()
test_custom_template_loader():
MyFlask(flask.Flask):
DictLoader
DictLoader({'index.html':
World!'})
MyFlask(__name__)
flask.render_template('index.html')
test_iterable_loader():
{'whiskey':
'Jameson'}
flask.render_template(
['no_template.xml',
'simple_template.html',
'context_template.html'],
b'<h1>Jameson</h1>'
test_templates_auto_reload():
test_template_loader_debugging(test_apps):
_TestHandler(logging.Handler):
handle(x,
str(record.msg)
'1:
"blueprintapp"'
('2:
"admin"
'(blueprintapp.apps.admin)')
('trying
"frontend"
'(blueprintapp.apps.frontend)')
'Error:
('looked
belongs
"frontend"')
'See
http://flask.pocoo.org/docs/blueprints/#templates'
app.logger.handlers
[_TestHandler()]
c.get('/missing')
'missing_template.html'
test_custom_jinja_env():
CustomEnvironment(flask.templating.Environment):
CustomFlask(flask.Flask):
CustomEnvironment
CustomFlask(__name__)
isinstance(app.jinja_env,
CustomEnvironment)
test_environ_defaults_from_config():
'example.com:1234'
'/foo'
'http://example.com:1234/foo/'
b'http://example.com:1234/foo/'
test_environ_defaults():
b'http://localhost/'
test_redirect_keep_session():
flask.redirect('/getsession')
flask.session['data']
@app.route('/getsession')
get_session():
flask.session.get('data',
'<missing>')
b'<missing>'
hasattr(c,
'redirect_client'):
test_session_transactions():
text_type(flask.session['foo'])
b'[42]'
test_session_transactions_no_null_sessions():
session'
test_session_transactions_keep_context():
c.session_transaction():
test_session_transaction_needs_cookies():
app.test_client(use_cookies=False)
'cookies'
test_test_client_context_binding():
@app.route('/other')
other():
c.get('/other')
hasattr(flask.g,
RuntimeError):
AssertionError('some
expected')
test_reuse_client():
test_test_client_calls_teardown_handlers():
remember(error):
called.append(error)
called[:]
test_full_url_request():
@app.route('/action',
action():
c.post('http://domain.com/action?vodka=42',
data={'gin':
43})
'gin'
flask.request.form
'vodka'
flask.request.args
test_subdomain():
subdomain='<company_id>')
test_nosubdomain():
@app.route('/<company_id>')
Forbidden,
InternalServerError
test_error_handler_no_match():
@app.errorhandler(CustomException)
custom_exception_handler(e):
CustomException)
'custom'
handle_500(e):
type(e).__name__
@app.route('/custom')
custom_test():
CustomException()
@app.route('/keyerror')
key_error():
KeyError()
c.get('/custom').data
b'custom'
c.get('/keyerror').data
b'KeyError'
test_error_handler_subclass():
ParentException(Exception):
ChildExceptionUnregistered(ParentException):
ChildExceptionRegistered(ParentException):
@app.errorhandler(ParentException)
parent_exception_handler(e):
ParentException)
'parent'
@app.errorhandler(ChildExceptionRegistered)
child_exception_handler(e):
ChildExceptionRegistered)
'child-registered'
@app.route('/parent')
parent_test():
ParentException()
@app.route('/child-unregistered')
ChildExceptionUnregistered()
@app.route('/child-registered')
ChildExceptionRegistered()
c.get('/parent').data
c.get('/child-unregistered').data
c.get('/child-registered').data
b'child-registered'
test_error_handler_http_subclass():
ForbiddenSubclassRegistered(Forbidden):
ForbiddenSubclassUnregistered(Forbidden):
code_exception_handler(e):
'forbidden'
@app.errorhandler(ForbiddenSubclassRegistered)
subclass_exception_handler(e):
ForbiddenSubclassRegistered)
'forbidden-registered'
forbidden_test():
@app.route('/forbidden-registered')
ForbiddenSubclassRegistered()
@app.route('/forbidden-unregistered')
ForbiddenSubclassUnregistered()
c.get('/forbidden').data
c.get('/forbidden-unregistered').data
c.get('/forbidden-registered').data
b'forbidden-registered'
test_error_handler_blueprint():
@bp.errorhandler(500)
bp_exception_handler(e):
'bp-error'
@bp.route('/error')
bp_test():
app_exception_handler(e):
'app-error'
app_test():
url_prefix='/bp')
c.get('/error').data
b'app-error'
c.get('/bp/error').data
b'bp-error'
parse_set_header
common_test(app):
c.post('/').data
c.put('/').status_code
test_basic_view():
test_method_based_view():
test_view_patching():
Other(Index):
Index.as_view('index')
Other
view_func=view)
test_view_inheritance():
BetterIndex(Index):
'DELETE'
view_func=BetterIndex.as_view('index'))
['DELETE',
test_view_decorators():
add_x_parachute(f):
new_function(*args,
flask.make_response(f(*args,
resp.headers['X-Parachute']
new_function
[add_x_parachute]
rv.headers['X-Parachute']
test_implicit_head():
flask.Response('Blub',
'X-Method':
b'Blub'
test_explicit_head():
head(self):
headers={'X-Method':
test_endpoint_override():
pytest.raises(AssertionError):
blueprintapp.apps.admin
blueprintapp.apps.frontend
app.register_blueprint(admin)
Blueprint('admin',
url_prefix='/admin',
@admin.route('/')
render_template('admin/index.html')
@admin.route('/index2')
render_template('./admin/index.html')
Blueprint('frontend',
@frontend.route('/')
render_template('frontend/index.htl')
@frontend.route('/missing')
missing_template():
render_template('missing_template.html')
Flask('testapp')
app1
Flask('app1')
app2
Flask('app2')
Module(__name__,
dict(test_suite="tests.test.suite",
include_package_data=True)
distutils.core
"ERROR:
above...exiting."
print(error,
readme():
open("README.rst")
setup(name
"boto",
"Amazon
Services
Library",
long_description
readme(),
author
"Mitch
Garnaat",
author_email
"mitch@garnaat.com",
["bin/sdbadmin",
"bin/elbadmin",
"bin/cfadmin",
"bin/s3put",
"bin/fetch_file",
"bin/launch_instance",
"bin/list_instances",
"bin/taskadmin",
"bin/kill_instance",
"bin/bundle_image",
"bin/pyami_sendmail",
"bin/lss3",
"bin/cq",
"bin/route53",
"bin/cwutil",
"bin/instance_events",
"bin/asadmin",
"bin/glacier",
"bin/mturk",
"bin/dynamodb_dump",
"bin/dynamodb_load"],
"https://github.com/boto/boto/",
["boto",
"boto.sqs",
"boto.s3",
"boto.gs",
"boto.file",
"boto.ec2",
"boto.ec2.cloudwatch",
"boto.ec2.autoscale",
"boto.ec2.elb",
"boto.sdb",
"boto.cacerts",
"boto.sdb.db",
"boto.sdb.db.manager",
"boto.mturk",
"boto.pyami",
"boto.pyami.installers",
"boto.pyami.installers.ubuntu",
"boto.mashups",
"boto.contrib",
"boto.manage",
"boto.services",
"boto.cloudfront",
"boto.roboto",
"boto.rds",
"boto.vpc",
"boto.sns",
"boto.ecs",
"boto.iam",
"boto.route53",
"boto.ses",
"boto.cloudformation",
"boto.sts",
"boto.dynamodb",
"boto.swf",
"boto.mws",
"boto.cloudsearch",
"boto.glacier",
"boto.beanstalk",
"boto.datapipeline",
"boto.elasticache",
"boto.elastictranscoder",
"boto.opsworks",
"boto.redshift",
"boto.dynamodb2",
"boto.support",
"boto.cloudtrail",
"boto.directconnect",
"boto.kinesis",
"boto.rds2",
"boto.cloudsearch2",
"boto.logs",
"boto.vendored",
"boto.route53.domains",
"boto.cognito",
"boto.cognito.identity",
"boto.cognito.sync",
"boto.cloudsearchdomain",
"boto.kms",
"boto.awslambda",
"boto.codedeploy",
"boto.configservice",
"boto.cloudhsm",
"boto.ec2containerservice",
"boto.machinelearning"],
package_data
"boto.cacerts":
["cacerts.txt"],
"boto":
["endpoints.json"],
license
"MIT",
platforms
"Posix;
MacOS
X;
Windows",
classifiers
["Development
Production/Stable",
"Intended
Developers",
"License
MIT
License",
"Operating
Independent",
"Topic
Internet",
2.6",
2.7",
3.3",
3.4"],
**extra
boto.storage_uri
BucketStorageUri,
FileStorageUri
'2.42.0'
backware
datetime.datetime.strptime('',
'Boto/%s
Python/%s
platform.python_version(),
platform.system(),
platform.release()
BUCKET_NAME_RE
re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9\._-]{1,253}[a-zA-Z0-9]$')
TOO_LONG_DNS_NAME_COMP
re.compile(r'[-_a-z0-9]{64}')
GENERATION_RE
re.compile(r'(?P<versionless_uri_str>.+)'
r'#(?P<generation>[0-9]+)$')
VERSION_RE
re.compile('(?P<versionless_uri_str>.+)#(?P<version_id>.+)$')
ENDPOINTS_PATH
init_logging():
BotoConfigLocations:
logging.config.fileConfig(os.path.expanduser(file))
NullHandler(logging.Handler):
logging.getLogger('boto')
perflog
logging.getLogger('boto.perf')
log.addHandler(NullHandler())
perflog.addHandler(NullHandler())
init_logging()
set_file_logger(name,
filepath,
level=logging.INFO,
logging.FileHandler(filepath)
set_stream_logger(name,
connect_sqs(aws_access_key_id=None,
SQSConnection(aws_access_key_id,
connect_s3(aws_access_key_id=None,
connect_gs(gs_access_key_id=None,
GSConnection(gs_access_key_id,
connect_ec2(aws_access_key_id=None,
connect_elb(aws_access_key_id=None,
ELBConnection(aws_access_key_id,
connect_autoscale(aws_access_key_id=None,
AutoScaleConnection(aws_access_key_id,
connect_cloudwatch(aws_access_key_id=None,
CloudWatchConnection(aws_access_key_id,
connect_sdb(aws_access_key_id=None,
SDBConnection(aws_access_key_id,
connect_fps(aws_access_key_id=None,
FPSConnection(aws_access_key_id,
connect_mturk(aws_access_key_id=None,
MTurkConnection(aws_access_key_id,
connect_cloudfront(aws_access_key_id=None,
CloudFrontConnection(aws_access_key_id,
connect_vpc(aws_access_key_id=None,
VPCConnection(aws_access_key_id,
connect_rds(aws_access_key_id=None,
RDSConnection(aws_access_key_id,
connect_rds2(aws_access_key_id=None,
RDSConnection(
connect_emr(aws_access_key_id=None,
EmrConnection(aws_access_key_id,
connect_sns(aws_access_key_id=None,
SNSConnection(aws_access_key_id,
connect_iam(aws_access_key_id=None,
IAMConnection(aws_access_key_id,
connect_route53(aws_access_key_id=None,
Route53Connection(aws_access_key_id,
connect_cloudformation(aws_access_key_id=None,
CloudFormationConnection(aws_access_key_id,
connect_euca(host=None,
path='/services/Eucalyptus',
'eucalyptus_host',
RegionInfo(name='eucalyptus',
endpoint=host)
region=reg,
connect_glacier(aws_access_key_id=None,
connect_ec2_endpoint(url,
purl
purl.port
purl.hostname
purl.path
kwargs['is_secure']
(purl.scheme
"https")
RegionInfo(name=purl.hostname,
endpoint=purl.hostname)
kwargs['aws_access_key_id']
kwargs['aws_secret_access_key']
return(connect_ec2(**kwargs))
connect_walrus(host=None,
path='/services/Walrus',
'walrus_host',
connect_ses(aws_access_key_id=None,
SESConnection(aws_access_key_id,
connect_sts(aws_access_key_id=None,
STSConnection(aws_access_key_id,
connect_ia(ia_access_key_id=None,
ia_secret_access_key=None,
'ia_access_key_id',
ia_access_key_id)
'ia_secret_access_key',
ia_secret_access_key)
S3Connection(access_key,
host='s3.us.archive.org',
connect_dynamodb(aws_access_key_id=None,
connect_swf(aws_access_key_id=None,
connect_cloudsearch(aws_access_key_id=None,
connect_cloudsearch2(aws_access_key_id=None,
sign_request=False,
sign_request=sign_request,
connect_cloudsearchdomain(aws_access_key_id=None,
CloudSearchDomainConnection(aws_access_key_id,
connect_beanstalk(aws_access_key_id=None,
connect_elastictranscoder(aws_access_key_id=None,
ElasticTranscoderConnection(
connect_opsworks(aws_access_key_id=None,
OpsWorksConnection(
connect_redshift(aws_access_key_id=None,
RedshiftConnection(
connect_support(aws_access_key_id=None,
SupportConnection(
connect_cloudtrail(aws_access_key_id=None,
CloudTrailConnection(
connect_directconnect(aws_access_key_id=None,
DirectConnectConnection(
connect_kinesis(aws_access_key_id=None,
KinesisConnection(
connect_logs(aws_access_key_id=None,
CloudWatchLogsConnection(
connect_route53domains(aws_access_key_id=None,
Route53DomainsConnection(
connect_cognito_identity(aws_access_key_id=None,
CognitoIdentityConnection(
connect_cognito_sync(aws_access_key_id=None,
CognitoSyncConnection(
connect_kms(aws_access_key_id=None,
KMSConnection(
connect_awslambda(aws_access_key_id=None,
AWSLambdaConnection(
connect_codedeploy(aws_access_key_id=None,
CodeDeployConnection(
connect_configservice(aws_access_key_id=None,
ConfigServiceConnection(
connect_cloudhsm(aws_access_key_id=None,
CloudHSMConnection(
connect_ec2containerservice(aws_access_key_id=None,
EC2ContainerServiceConnection(
connect_machinelearning(aws_access_key_id=None,
MachineLearningConnection(
storage_uri(uri_str,
default_scheme='file',
bucket_storage_uri_class=BucketStorageUri,
uri_str.find('://')
default_scheme.lower()
uri_str[0:end_scheme_idx].lower()
uri_str[end_scheme_idx
3:]
['file',
'gs']:
FileStorageUri(path,
is_stream)
path.split('/',
path_parts[0]
(validate
BUCKET_NAME_RE.match(bucket_name)
TOO_LONG_DNS_NAME_COMP.search(bucket_name))):
uri_str)
GENERATION_RE.search(path)
int(md['generation'])
VERSION_RE.search(path)
md['version_id']
len(path_parts)
path_parts[1]
bucket_storage_uri_class(
storage_uri_for_key(key):
boto.s3.key.Key):
InvalidUriError('Requested
'boto.s3.key.Key'
str(type(key)))
prov_name
key.bucket.connection.provider.get_provider_name()
(prov_name,
storage_uri(uri_str)
boto.plugin.load_plugins(config)
encodebytes,
AuthHandler
SIGV4_DETECT
'.cn-',
'-eu-central',
'.ap-northeast-2',
'-ap-south-1'
HmacKeys(object):
provider.access_key
provider.secret_key
boto.auth_handler.NotReadyToAuthenticate()
self.update_provider(provider)
self._provider
self._hmac
sha256:
digestmod=sha256)
algorithm(self):
'HmacSHA256'
'HmacSHA1'
_get_hmac(self):
digestmod=digestmod)
sign_string(self,
new_hmac
new_hmac.update(string_to_sign.encode('utf-8'))
encodebytes(new_hmac.digest()).decode('utf-8').strip()
pickled_dict['_hmac']
pickled_dict['_hmac_256']
self.update_provider(self._provider)
AnonAuthHandler(AuthHandler,
super(AnonAuthHandler,
HmacAuthV1Handler(AuthHandler,
['hmac-v1',
's3']
super(HmacAuthV1Handler,
self._provider)
self.sign_string(string_to_sign)
auth)
HmacAuthV2Handler(AuthHandler,
['hmac-v2',
'cloudfront']
super(HmacAuthV2Handler,
HmacAuthV3Handler(AuthHandler,
['hmac-v3',
'ses']
"AWS3-HTTPS
"Algorithm=%s,Signature=%s"
(self.algorithm(),
b64_hmac)
headers['X-Amzn-Authorization']
HmacAuthV3HTTPHandler(AuthHandler,
['hmac-v3-http']
self.host}
sorted(['%s:%s'
(n.lower().strip(),
headers_to_sign[n].strip())
'\n'.join(l)
self.canonical_headers(headers_to_sign)
'\n'.join([http_request.method,
http_request.body])
self.string_to_sign(req)
hash_value
sha256(string_to_sign.encode('utf-8')).digest()
self.sign_string(hash_value)
"AWS3
"Algorithm=%s,"
"SignedHeaders=%s,"
';'.join(headers_to_sign)
"Signature=%s"
HmacAuthV4Handler(AuthHandler,
service_name=None,
region_name=None):
_sign(self,
hex=False):
key.encode('utf-8')
hex:
sha256).hexdigest()
sha256).digest()
http_request.headers.get('Host'):
http_request.headers['Host']
query_string(self,
parameter_names
sorted(http_request.params.keys())
parameter_names:
pval
boto.utils.get_utf8_value(http_request.params[pname])
pairs.append(urllib.parse.quote(pname,
urllib.parse.quote(pval,
headers_to_sign:
c_name
header.lower().strip()
raw_value
str(headers_to_sign[header])
'"'
raw_value:
raw_value.strip()
'.join(raw_value.strip().split())
canonical.append('%s:%s'
(c_name,
c_value))
'\n'.join(sorted(canonical))
signed_headers(self,
['%s'
headers_to_sign]
sorted(l)
normalized
posixpath.normpath(path).replace('\\',
urllib.parse.quote(normalized)
len(path)
path.endswith('/'):
'read'):
boto.utils.compute_hash(body,
hash_algorithm=sha256)[0]
sha256(body).hexdigest()
canonical_request(self,
[http_request.method.upper()]
cr.append(self.canonical_uri(http_request))
cr.append(self.canonical_query_string(http_request))
cr.append(self.canonical_headers(headers_to_sign)
cr.append(self.signed_headers(headers_to_sign))
cr.append(self.payload(http_request))
'\n'.join(cr)
scope(self,
[self._provider.access_key]
split_host_parts(self,
host.split('.')
'us-gov':
'us-gov-west-1'
credential_scope(self,
http_request.timestamp
http_request.headers['X-Amz-Date'][0:8]
self.determine_region_name(http_request.host)
self.determine_service_name(http_request.host)
http_request.service_name
http_request.region_name
canonical_request):
['AWS4-HMAC-SHA256']
sts.append(http_request.headers['X-Amz-Date'])
sts.append(self.credential_scope(http_request))
sts.append(sha256(canonical_request.encode('utf-8')).hexdigest())
'\n'.join(sts)
signature(self,
self._provider.secret_key
k_date
self._sign(('AWS4'
key).encode('utf-8'),
http_request.timestamp)
k_region
self._sign(k_date,
http_request.region_name)
k_service
self._sign(k_region,
http_request.service_name)
k_signing
self._sign(k_service,
'aws4_request')
self._sign(k_signing,
hex=True)
now.strftime('%Y%m%dT%H%M%SZ')
self.query_string(req)
'unmangled_req'
self.query_string(kwargs['unmangled_req'])
qs:
canonical_request
boto.log.debug('CanonicalRequest:\n%s'
['AWS4-HMAC-SHA256
Credential=%s'
self.scope(req)]
l.append('SignedHeaders=%s'
self.signed_headers(headers_to_sign))
l.append('Signature=%s'
req.headers['Authorization']
','.join(l)
S3HmacAuthV4Handler(HmacAuthV4Handler,
self.clean_region_name(self.region_name)
clean_region_name(self,
region_name.startswith('s3-'):
region_name[3:]
urllib.parse.urlparse(http_request.path)
unquoted
urllib.parse.unquote(path.path)
urllib.parse.quote(unquoted,
safe='/~')
http_request.host
(http_request.host,
['authorization']:
self.clean_region_name(parts[0])
enumerate(reversed(parts)):
part.lower()
parts[-offset]
'amazonaws':
part.startswith('s3-'):
self.clean_region_name(part)
mangle_path_and_params(self,
copy.copy(req)
parsed_path
urllib.parse.urlparse(modified_req.auth_path)
modified_req.auth_path
parsed_path.path
req.params.copy()
raw_qs
parsed_path.query
existing_qs
parse_qs_safe(
raw_qs,
keep_blank_values=True
existing_qs.items():
existing_qs[key]
modified_req.params.update(existing_qs)
http_request.headers.get('x-amz-content-sha256'):
http_request.headers['x-amz-content-sha256']
self).payload(http_request)
'x-amz-content-sha256'
'_sha256'
req.headers.pop('_sha256')
self.payload(req)
updated_req
self.mangle_path_and_params(req)
self).add_auth(updated_req,
unmangled_req=req,
presign(self,
datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
self.determine_region_name(req.host)
self.determine_service_name(req.host)
'X-Amz-Algorithm':
'AWS4-HMAC-SHA256',
'X-Amz-Credential':
'%s/%s/%s/%s/aws4_request'
iso_date[:8],
iso_date,
'X-Amz-Expires':
'X-Amz-SignedHeaders':
params['X-Amz-Security-Token']
sorted(['%s'
params['X-Amz-SignedHeaders']
req.params.update(params)
'\n'.join(cr.split('\n')[:-1])
'\nUNSIGNED-PAYLOAD'
cr)
sts)
req.params['X-Amz-Signature']
'%s://%s%s?%s'
(req.protocol,
req.host,
req.path,
urllib.parse.urlencode(req.params))
STSAnonHandler(AuthHandler):
_escape_value(self,
_build_query_string(self,
self._escape_value(val.decode('utf-8')))
self._build_query_string(
boto.log.debug('query_string
'application/x-www-form-urlencoded'
QuerySignatureHelper(HmacKeys):
params['AWSAccessKeyId']
params['SignatureVersion']
self._calc_signature(
http_request.params,
http_request.method,
http_request.host)
urllib.parse.quote_plus(signature)
http_request.headers['Content-Length']
str(len(http_request.body))
http_request.path.split('?')[0]
(http_request.path
QuerySignatureV0AuthHandler(QuerySignatureHelper,
['sign-v0']
_calc_signature_0')
hmac.update(s.encode('utf-8'))
params.keys()
keys.sort(cmp=lambda
cmp(x.lower(),
y.lower()))
QuerySignatureV1AuthHandler(QuerySignatureHelper,
['sign-v1',
'mturk']
QuerySignatureHelper.__init__(self,
_calc_signature_1')
hmac.update(key.encode('utf-8'))
hmac.update(val)
QuerySignatureV2AuthHandler(QuerySignatureHelper,
['sign-v2',
'emr',
'fps',
'sns',
'cloudformation']
server_name):
_calc_signature_2')
'%s\n%s\n%s\n'
(verb,
server_name.lower(),
params['SignatureMethod']
params['SecurityToken']
sorted(params.keys())
pairs.append(urllib.parse.quote(key,
urllib.parse.quote(val,
boto.log.debug('query
string:
boto.log.debug('string_to_sign:
hmac.update(string_to_sign.encode('utf-8'))
b64
base64.b64encode(hmac.digest())
boto.log.debug('len(b64)=%d'
len(b64))
boto.log.debug('base64
digest:
POSTPathQSV2AuthHandler(QuerySignatureV2AuthHandler,
req.params['AWSAccessKeyId']
req.params['SignatureVersion']
req.params['Timestamp']
self._calc_signature(req.params,
req.method,
req.auth_path,
req.host)
req.headers.get('Content-Type',
(req.path
get_auth_handler(host,
ready_handlers
boto.plugin.get_plugin(AuthHandler,
requested_capability)
auth_handlers:
ready_handlers.append(handler(host,
provider))
boto.auth_handler.NotReadyToAuthenticate:
ready_handlers:
checked_handlers
[handler.__name__
checked_handlers]
boto.exception.NoAuthHandlerFound(
authenticate.
checked.'
(len(names),
str(names)))
ready_handlers[-1]
detect_potential_sigv4(func):
os.environ.get('EC2_USE_SIGV4',
boto.config.get('ec2',
getattr(self.region,
'endpoint',
''):
self.region.endpoint:
detect_potential_s3sigv4(func):
os.environ.get('S3_USE_SIGV4',
'host'):
self.host:
Plugin
NotReadyToAuthenticate(Exception):
AuthHandler(Plugin):
encodestring
os.path.expanduser('~')
os.path.expanduser
ImportError):
boto.vendored
boto.vendored.six
boto.vendored.six.moves
_thread,
boto.vendored.six.moves.queue
unquote,
boto.vendored.six.moves.urllib.request
urlopen
configparser
unquote_str(value,
byte_string
value.encode(encoding)
unquote_plus(byte_string).decode(encoding)
parse_qs_safe(qs,
strict_parsing=False,
errors='replace'):
is_text_type
isinstance(qs,
qs.encode('ascii')
parse_qs(qs,
keep_blank_values,
strict_parsing)
qs_dict.items():
decoded_name
name.decode(encoding,
[item.decode(encoding,
result[decoded_name]
auth_handler
boto.cacerts
AWSConnectionError
hasattr(ssl,
'SSLError'):
dummy_threading
ON_APP_ENGINE
'USER_IS_ADMIN',
'CURRENT_VERSION_ID',
'APPLICATION_ID'))
PORTS_BY_SECURITY
80}
os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)),
"cacerts.txt")
HostConnectionPool(object):
self.queue.append((conn,
time.time()))
range(len(self.queue)):
(conn,
self._conn_ready(conn):
self.put(conn)
_conn_ready(self,
ON_APP_ENGINE:
getattr(conn,
'_HTTPConnection__response',
(response
response.isclosed()
self._pair_stale(self.queue[0]):
_pair_stale(self,
pair):
(_conn,
return_time)
pair
return_time
ConnectionPool(object):
CLEAN_INTERVAL
STALE_DURATION
self.host_to_pool
self.mutex
'connection_stale_duration',
ConnectionPool.STALE_DURATION)
pickled_dict['host_to_pool']
pickled_dict['mutex']
self.__init__()
sum(pool.size()
self.host_to_pool.values())
self.host_to_pool[key].get()
self.host_to_pool[key]
HostConnectionPool()
self.host_to_pool[key].put(conn)
self.CLEAN_INTERVAL
now:
to_remove
self.host_to_pool.items():
pool.clean()
pool.size()
to_remove.append(host)
to_remove:
self.host_to_pool[host]
HTTPRequest(object):
self.auth_path
'PUT':
(('method:(%s)
protocol:(%s)
host(%s)
port(%s)
path(%s)
'params(%s)
headers(%s)
body(%s)')
self.body))
'!"#$%&\'()*+,/:;<=>?@[\\]^`{|}~
quote(val.encode('utf-8'),
safe)
self.headers['User-Agent']
connection._auth_handler.add_auth(self,
str(len(self.body))
HTTPResponse(http_client.HTTPResponse):
http_client.HTTPResponse.__init__(self,
self._cached_response:
http_client.HTTPResponse.read(self)
http_client.HTTPResponse.read(self,
amt)
AWSAuthConnection(object):
config.has_option('Boto',
'is_secure'):
config.getboolean('Boto',
'is_secure')
self.is_secure
config.getbool(
validate_certs)
"SSL
validation
"configuration,
dependencies
"support
feature
Certificate
"validation
"2.6
later.")
config.get_value(
'system':
PORTS_BY_SECURITY[is_secure]
self.handle_proxy(proxy,
proxy_pass)
(http_client.HTTPException,
socket.gaierror,
http_client.BadStatusLine)
self.http_unretryable_exceptions
self.http_unretryable_exceptions.append(
self.socket_exception_values
(errno.EINTR,)
https_connection_factory
https_connection_factory[0]
https_connection_factory[1]
(is_secure):
isinstance(debug,
self.http_connection_kwargs
(sys.version_info[0],
sys.version_info[1])
6):
self.http_connection_kwargs['timeout']
config.getint(
'http_socket_timeout',
70)
isinstance(provider,
Provider):
self._provider_type
Provider(self._provider_type,
self.provider.host:
self.provider.host
self.provider.port:
self.provider.port
self.provider.host_header:
self.provider.host_header
ConnectionPool()
self._last_rs
self._auth_handler
auth.get_auth_handler(
self._required_auth_capability())
'AuthServiceName',
self.AuthServiceName
_get_auth_service_name(self):
_set_auth_service_name(self,
self._auth_handler.service_name
auth_service_name
property(_get_auth_service_name,
_set_auth_service_name)
_get_auth_region_name(self):
_set_auth_region_name(self,
self._auth_handler.region_name
auth_region_name
property(_get_auth_region_name,
_set_auth_region_name)
self.get_http_connection(*self._connection)
property(connection)
aws_access_key_id(self):
property(aws_access_key_id)
gs_access_key_id
aws_secret_access_key(self):
property(aws_secret_access_key)
gs_secret_access_key
profile_name(self):
self.provider.profile_name
property(profile_name)
get_path(self,
self.suppress_consec_slashes:
re.sub('^(/*)/',
"\\1",
path.find('?')
path[pos:]
path[:pos]
self.path.split('/')
path_elements.extend(path.split('/'))
[p
p]
'/'.join(path_elements)
need_trailing:
server_name(self,
80:
((ON_APP_ENGINE
('2.6',
'2.7'))
443:
handle_proxy(self,
proxy_pass):
proxy_port
proxy_user
proxy_pass
'http_proxy'
'(?:http://)?'
'(?:(?P<user>[\w\-\.]+):(?P<pass>.*)@)?'
'(?P<host>[\w\-\.]+)'
'(?::(?P<port>\d+))?'
pattern.match(os.environ['http_proxy'])
match.group('host')
match.group('port')
match.group('user')
match.group('pass')
self.proxy_port:
self.proxy_user:
print("http_proxy
os.environ.get('no_proxy',
os.environ.get('NO_PROXY',
(self.proxy
self._pool.get_http_connection(host,
self.new_http_connection(host,
skip_proxy(self,
self.no_proxy:
"*":
self.no_proxy.split(','):
(hostonly.endswith(name)
host.endswith(name)):
new_http_connection(self,
self.server_name()
http_connection_kwargs
self.http_connection_kwargs.copy()
int(self.proxy_port)
is_secure:
'establishing
HTTPS
host=%s,
kwargs=%s',
self.proxy_ssl(host,
self.https_connection_factory(host)
https_connection.CertValidatingHTTPSConnection(
ca_certs=self.ca_certificates_file,
http_client.HTTPSConnection(
boto.log.debug('establishing
kwargs=%s'
self.https_connection_factory(
http_client.HTTPConnection(
connection.set_debuglevel(self.debug)
connection.response_class
HTTPResponse
self._pool.put_http_connection(host,
proxy_ssl(self,
self.http_connection_kwargs.get("timeout")
int(self.proxy_port)),
int(self.proxy_port)))
boto.log.debug("Proxy
HTTP/1.0\r\n",
sock.sendall("CONNECT
sock.sendall("User-Agent:
UserAgent)
self.get_proxy_auth_header().items():
sock.sendall("%s:
config.getbool('Boto',
'send_crlf_after_proxy_auth_headers',
http_client.HTTPResponse(sock,
strict=True,
debuglevel=self.debug)
resp.begin()
socket.error(-71,
talking
%s:%s:
(self.proxy,
self.proxy_port,
resp.reason))
resp.close()
http_client.HTTPConnection(host)
proxied
connection;
self.ca_certificates_file:
self.http_connection_kwargs.get('key_file',
self.http_connection_kwargs.get('cert_file',
keyfile=key_file,
certfile=cert_file,
ca_certs=self.ca_certificates_file)
sslSock.getpeercert()
https_connection.ValidateCertificateHostname(cert,
https_connection.InvalidCertificateException(
'hostname
mismatch')
hasattr(http_client,
'ssl'):
http_client.ssl.SSLSocket(sock)
socket.ssl(sock,
http_client.FakeSocket(sock,
sslSock)
h.sock
prefix_proxy_to_path(self,
(host
self.server_name())
get_proxy_auth_header(self):
encodebytes(self.proxy_user
{'Proxy-Authorization':
'Basic
auth}
get_proxy_url_with_auth(self):
self.use_proxy:
'%s:%s@'
(self.proxy_user,
'%s@'
'http://%s%s:%s'
(login_info,
self.proxy,
str(self.proxy_port
set_host_header(self,
self._auth_handler.host_header(self.host,
set_request_hook(self,
hook):
boto.log.debug('Method:
boto.log.debug('Path:
request.path)
boto.log.debug('Data:
request.body)
boto.log.debug('Headers:
boto.log.debug('Host:
request.host)
boto.log.debug('Port:
request.port)
boto.log.debug('Params:
request.params)
self.num_retries)
isinstance(request.body,
hasattr(request.body,
'encode'):
request.body.encode('utf-8')
boto.log.debug('Token:
request.authorize(connection=self)
'anon',
request.headers.get('Host'):
self.set_host_header(request)
boto.log.debug('Final
request.start_time
callable(sender):
sender(connection,
connection.request(request.method,
boto.log.debug('Response
response.getheaders())
response.getheader('location')
'chunked',
response.chunked
callable(retry_handler):
retry_handler(response,
504]:
'Retrying
response.getheader('connection')
'close':
connection.close()
self.put_http_connection(request.host,
self.is_secure,
urlparse(location)
request.host:
request.port
request.host.split(':',
'Redirecting:
request.host
e.response
self.http_unretryable_exceptions:
unretryable):
'encountered
re-raising'
reconnecting'
error=True)
'Please
Boto
Issue!'
BotoClientError(msg)
build_base_http_request(self,
self.get_path(path)
params.copy()
headers['host']
auth_path:
self.prefix_proxy_to_path(path,
headers.update(self.get_proxy_auth_header())
HTTPRequest(method,
override_num_retries,
retry_handler=retry_handler)
boto.log.debug('closing
connections')
AWSQueryConnection(AWSAuthConnection):
provider='aws'):
super(AWSQueryConnection,
get_utf8_value(self,
boto.utils.get_utf8_value(value)
self.build_base_http_request(verb,
action:
http_request.params['Action']
self.APIVersion:
http_request.params['Version']
self._mexe(http_request)
build_complex_list_params(self,
enumerate(items,
zip(names,
markers,
cls(parent)
boto.handler.XmlHandler(obj,
get_status(self,
rs.status
BotoClientError(StandardError):
super(BotoClientError,
SDBPersistenceError(StandardError):
StoragePermissionsError(BotoClientError):
S3PermissionsError(StoragePermissionsError):
GSPermissionsError(StoragePermissionsError):
BotoServerError(StandardError):
self._error_message
isinstance(self.body,
self.body.decode('utf-8')
boto.log.debug('Unable
bytes!')
hasattr(self.body,
self.body.get('RequestId',
self.body.get('Error',
error.get('Code',
error.get('Message',
handler.XmlHandlerWrapper(self,
h.parseString(self.body)
xml.sax.SAXParseException):
'RequestId'
parsed['RequestId']
'Code'
parsed['Error']['Code']
parsed['Error']['Message']
('RequestId',
'RequestID'):
StorageCreateError(BotoServerError):
S3CreateError(StorageCreateError):
GSCreateError(StorageCreateError):
StorageCopyError(BotoServerError):
S3CopyError(StorageCopyError):
GSCopyError(StorageCopyError):
SQSError(BotoServerError):
'Detail':
('detail',
'type'):
SQSDecodeError(BotoClientError):
super(SQSDecodeError,
StorageResponseError(BotoServerError):
self).startElement(
self).endElement(
('resource'):
S3ResponseError(StorageResponseError):
GSResponseError(StorageResponseError):
EC2ResponseError(BotoServerError):
(e.error_code,
e.error_message)
self._errorResultSet]
len(self.errors):
self.error_code,
self.errors[0]
'Errors':
ResultSet([('Error',
_EC2Error)])
'RequestID':
('errors'):
JSONResponseError(BotoServerError):
self.body.get('__type',
self.error_code:
self.error_code.split('#')[-1]
DynamoDBResponseError(JSONResponseError):
SWFResponseError(JSONResponseError):
EmrResponseError(BotoServerError):
_EC2Error(object):
SDBResponseError(BotoServerError):
AWSConnectionError(BotoClientError):
StorageDataError(BotoClientError):
S3DataError(StorageDataError):
GSDataError(StorageDataError):
InvalidUriError(Exception):
super(InvalidUriError,
InvalidAclError(Exception):
super(InvalidAclError,
InvalidCorsError(Exception):
super(InvalidCorsError,
NoAuthHandlerFound(Exception):
InvalidLifecycleConfigError(Exception):
super(InvalidLifecycleConfigError,
ResumableTransferDisposition(object):
START_OVER
'START_OVER'
WAIT_BEFORE_RETRY
'WAIT_BEFORE_RETRY'
ABORT_CUR_PROCESS
'ABORT_CUR_PROCESS'
ABORT
'ABORT'
ResumableUploadException(Exception):
super(ResumableUploadException,
'ResumableUploadException("%s",
ResumableDownloadException(Exception):
super(ResumableDownloadException,
'ResumableDownloadException("%s",
TooManyRecordsException(Exception):
super(TooManyRecordsException,
PleaseRetryException(Exception):
'PleaseRetryException("%s",
InvalidInstanceMetadataError(Exception):
MSG
'metadata_service_num_attempts'
increase
times
service."
final_msg
self.MSG
super(InvalidInstanceMetadataError,
self).__init__(final_msg)
new_node))
hasattr(self.nodes[-1][1],
'endNode'):
self.nodes[-1][1].endNode(self.connection)
XmlHandlerWrapper(object):
XmlHandler(root_node,
xml.sax.make_parser()
self.parser.setContentHandler(self.handler)
self.parser.setFeature(xml.sax.handler.feature_external_ges,
parseString(self,
self.parser.parse(StringIO(content))
InvalidCertificateException(http_client.HTTPException):
http_client.HTTPException.__init__(self)
self.cert
('Host
self.cert))
GetValidHostsForCert(cert):
'subjectAltName'
cert['subjectAltName']
x[0].lower()
'dns']
[x[0][1]
cert['subject']
x[0][0].lower()
'commonname']
GetValidHostsForCert(cert)
"validating
certificate:
hostname=%s,
hosts=%s",
hosts)
host_re
host.replace('.',
'\.').replace('*',
'[^.]*')
re.search('^%s$'
(host_re,),
re.I):
CertValidatingHTTPSConnection(http_client.HTTPConnection):
http_client.HTTPS_PORT
port=default_port,
ca_certs=None,
strict=None,
kwargs['strict']
strict
http_client.HTTPConnection.__init__(self,
self.key_file
self.cert_file
ca_certs
connect(self):
"Connect
(SSL)
port."
"timeout"):
self.port),
self.timeout)
socket;
self.ca_certs:
self.sock
keyfile=self.key_file,
certfile=self.cert_file,
ca_certs=self.ca_certs)
self.sock.getpeercert()
InvalidCertificateException(hostname,
'remote
'certificate'
hostname)
isinstance(t,
self.nodes.append(t)
t))
s):
xml.sax.parseString(s,
Element(dict):
stack=None,
list_marker=('Set',),
utils.mklist(list_marker)
utils.mklist(item_marker)
getattr(e,
self.stack.append(name)
self[self.get_name(name)]
self.stack[-1]
element_name,
self.stack,
self[self.get_name(element_name)]
self.stack.pop()
self.parent[self.get_name(name)]
ListElement):
self.parent.append(value)
list_marker=['Set'],
item_marker
self.item_marker:
list_marker=self.list_marker,
item_marker=self.item_marker,
pythonize_name=self.pythonize_name)
self.append(e)
self.element_name:
len(e)
empty.append(e)
empty:
self.remove(e)
Plugin(object):
is_capable(cls,
requested_capability):
cls.capability:
get_plugin(cls,
requested_capability
cls.__subclasses__():
handler.is_capable(requested_capability):
result.append(handler)
_import_module(filename):
(path,
ext)
os.path.splitext(name)
(file,
imp.find_module(name,
[path])
imp.load_module(name,
load_plugins(config):
_plugin_loaded:
config.has_option('Plugin',
'plugin_directory'):
config.get('Plugin',
'plugin_directory')
glob.glob(os.path.join(directory,
'*.py')):
_import_module(file)
CannedS3ACLStrings
HEADER_PREFIX_KEY
'header_prefix'
METADATA_PREFIX_KEY
'metadata_prefix'
'x-amz-'
'x-goog-'
ACL_HEADER_KEY
'acl-header'
AUTH_HEADER_KEY
'auth-header'
COPY_SOURCE_HEADER_KEY
'copy-source-header'
COPY_SOURCE_VERSION_ID_HEADER_KEY
'copy-source-version-id-header'
COPY_SOURCE_RANGE_HEADER_KEY
'copy-source-range-header'
DELETE_MARKER_HEADER_KEY
'delete-marker-header'
DATE_HEADER_KEY
'date-header'
METADATA_DIRECTIVE_HEADER_KEY
'metadata-directive-header'
RESUMABLE_UPLOAD_HEADER_KEY
'resumable-upload-header'
SECURITY_TOKEN_HEADER_KEY
'security-token-header'
STORAGE_CLASS_HEADER_KEY
'storage-class'
MFA_HEADER_KEY
'mfa-header'
SERVER_SIDE_ENCRYPTION_KEY
'server-side-encryption-header'
VERSION_ID_HEADER_KEY
'version-id-header'
RESTORE_HEADER_KEY
'restore-header'
STORAGE_COPY_ERROR
'StorageCopyError'
STORAGE_CREATE_ERROR
'StorageCreateError'
STORAGE_DATA_ERROR
'StorageDataError'
STORAGE_PERMISSIONS_ERROR
'StoragePermissionsError'
STORAGE_RESPONSE_ERROR
'StorageResponseError'
ProfileNotFoundError(ValueError):
Provider(object):
CredentialMap
'aws_security_token',
'aws_profile'),
('gs_access_key_id',
AclClassMap
CannedAclsMap
CannedS3ACLStrings,
HostKeyMap
ChunkedTransferSupport
MetadataServiceSupport
HeaderInfoMap
AWS_HEADER_PREFIX,
'AWS',
'copy-source-range',
'server-side-encryption',
'storage-class',
'mfa',
GOOG_HEADER_PREFIX,
'GOOG1',
'resumable',
ErrorMap
boto.exception.S3CopyError,
boto.exception.S3CreateError,
boto.exception.S3DataError,
boto.exception.S3PermissionsError,
boto.exception.S3ResponseError,
boto.exception.GSCopyError,
boto.exception.GSCreateError,
boto.exception.GSDataError,
boto.exception.GSPermissionsError,
boto.exception.GSResponseError,
self.profile_name
self.acl_class
self.AclClassMap[self.name]
self.canned_acls
self.CannedAclsMap[self.name]
shared_path
os.path.isfile(shared_path):
self.shared_credentials.load_from_path(shared_path)
self.get_credentials(access_key,
self.configure_headers()
self.configure_errors()
host_opt_name
'%s_host'
host_opt_name):
host_opt_name)
port_opt_name
'%s_port'
port_opt_name):
config.getint('Credentials',
port_opt_name)
host_header_opt_name
'%s_host_header'
host_header_opt_name):
host_header_opt_name)
get_access_key(self):
set_access_key(self,
property(get_access_key,
set_access_key)
get_secret_key(self):
set_secret_key(self,
property(get_secret_key,
set_secret_key)
get_security_token(self):
set_security_token(self,
property(get_security_token,
set_security_token)
_credentials_need_refresh(self):
(delta.microseconds
(delta.seconds
(5
60):
boto.log.debug("Credentials
refreshed.")
get_credentials(self,
access_key_name,
secret_key_name,
security_token_name,
self.CredentialMap[self.name]
profile_name_name.upper()
os.environ[profile_name_name.upper()]
access_key_name.upper()
os.environ[access_key_name.upper()]
secret_key_name.upper()
os.environ[secret_key_name.upper()]
'keyring'):
keyring_name
'keyring')
boto.log.error("The
imported.
support,
"module.")
keyring.get_password(
keyring_name,
self.access_key)
keyring.")
((security_token_name
(access_key
(secret_key
security_token_name.upper()
os.environ[security_token_name.upper()]
environment"
shared.has_option(profile_name
shared.get(profile_name
"credential
boto.log.debug("config
option")
((self._access_key
self.MetadataServiceSupport[self.name]):
self._convert_key_to_str(self._secret_key)
_populate_keys_from_metadata_server(self):
boto.log.debug("Retrieving
server.")
get_instance_metadata
'metadata_service_timeout',
1.0)
'metadata_service_num_attempts',
get_instance_metadata(
num_retries=attempts,
self._get_credentials_from_metadata(metadata)
creds[0]
creds[1]
creds[2]
creds[3]
expires_at,
boto.log.debug("Retrieved
at:
datetime.now(),
expires_at)
_get_credentials_from_metadata(self,
list(metadata.values())[0]
isinstance(creds,
string'
'type:
InvalidInstanceMetadataError("Expected
(msg))
creds['AccessKeyId']
self._convert_key_to_str(creds['SecretAccessKey'])
creds['Token']
creds['Expiration']
InvalidInstanceMetadataError(
"Credentials
"required
access_key,
_convert_key_to_str(self,
str(key)
configure_headers(self):
header_info_map
self.HeaderInfoMap[self.name]
self.metadata_prefix
header_info_map[METADATA_PREFIX_KEY]
self.header_prefix
header_info_map[HEADER_PREFIX_KEY]
self.acl_header
header_info_map[ACL_HEADER_KEY]
header_info_map[AUTH_HEADER_KEY]
self.copy_source_header
header_info_map[COPY_SOURCE_HEADER_KEY]
self.copy_source_version_id
COPY_SOURCE_VERSION_ID_HEADER_KEY]
self.copy_source_range_header
COPY_SOURCE_RANGE_HEADER_KEY]
self.date_header
header_info_map[DATE_HEADER_KEY]
header_info_map[DELETE_MARKER_HEADER_KEY]
self.metadata_directive_header
header_info_map[METADATA_DIRECTIVE_HEADER_KEY])
self.security_token_header
header_info_map[SECURITY_TOKEN_HEADER_KEY]
self.resumable_upload_header
header_info_map[RESUMABLE_UPLOAD_HEADER_KEY])
self.server_side_encryption_header
header_info_map[SERVER_SIDE_ENCRYPTION_KEY]
self.storage_class_header
header_info_map[STORAGE_CLASS_HEADER_KEY]
header_info_map[VERSION_ID_HEADER_KEY]
self.mfa_header
header_info_map[MFA_HEADER_KEY]
self.restore_header
header_info_map[RESTORE_HEADER_KEY]
configure_errors(self):
error_map
self.ErrorMap[self.name]
self.storage_copy_error
error_map[STORAGE_COPY_ERROR]
self.storage_create_error
error_map[STORAGE_CREATE_ERROR]
self.storage_data_error
error_map[STORAGE_DATA_ERROR]
self.storage_permissions_error
error_map[STORAGE_PERMISSIONS_ERROR]
self.storage_response_error
error_map[STORAGE_RESPONSE_ERROR]
supports_chunked_transfer(self):
self.ChunkedTransferSupport[self.name]
get_default():
Provider('aws')
load_endpoint_json(path):
endpoints_file:
json.load(endpoints_file)
additions):
additions.items():
defaults.setdefault(service,
defaults[service].update(region_info)
load_regions():
os.environ.get('BOTO_ENDPOINTS'):
'endpoints_path'):
'endpoints_path')
additional_path:
load_endpoint_json(additional_path)
merge_endpoints(endpoints,
additional)
get_regions(service_name,
region_cls=None,
endpoints:
"Service
endpoints."
endpoints.get(service_name,
region_objs.append(
region_cls(
name=region_name,
endpoint=endpoint,
connection_cls=connection_cls
RegionInfo(object):
'RegionInfo:%s'
'regionEndpoint':
self.connection_cls(region=self,
RequestHook
RequestLogger(RequestHook):
filename='/tmp/request_log.csv'):
self.request_log_file
self.request_log_queue
Queue.Queue(100)
Thread(target=self._request_log_worker).start()
len
response.getheader('Content-Length')
now.strftime('%Y-%m-%d
%H:%M:%S')
td
request.start_time)
(td.microseconds
long_type(td.seconds
1e6)
1e6
self.request_log_queue.put("'%s',
'%s'\n"
(time,
len,
request.params['Action']))
_request_log_worker(self):
self.request_log_queue.get(True)
self.request_log_file.write(item)
self.request_log_file.flush()
self.request_log_queue.task_done()
traceback.print_exc(file=sys.stdout)
ResultSet(list):
isinstance(marker_elem,
marker_elem
avail
t[0]:
t[1](connection)
self.append(obj)
'KeyMarker':
'NextKeyMarker':
'VersionIdMarker':
'NextVersionIdMarker':
'NextGenerationMarker':
'UploadIdMarker':
'NextUploadIdMarker':
'MaxUploads':
self.max_uploads
self.nextToken
BooleanResult(object):
self.status:
'True'
'False'
StorageUri(object):
provider_pool
BotoClientError('Attempt
StorageUri
'class')
check_response(self,
level,
InvalidUriError('\n'.join(textwrap.wrap(
refers
non-existent
meant
'operate
leaving
'cp,
mv,
bucket)'
uri),
80)))
_check_bucket_uri(self,
InvalidUriError(
_check_object_uri(self,
InvalidUriError('%s
_warn_about_args(self,
args[arg]:
sys.stderr.write(
'Warning:
ignores
argument:
%s=%s\n'
str(args[arg])))
access_key_id=None,
secret_access_key=None,
dict(self.connection_args
(hasattr(self,
'suppress_consec_slashes')
'suppress_consec_slashes'
connection_args):
connection_args['suppress_consec_slashes']
self.suppress_consec_slashes)
connection_args.update(kwargs)
self.provider_pool:
S3Connection(access_key_id,
connection_args['calling_format']
OrdinaryCallingFormat()
GSConnection(access_key_id,
FileConnection(self)
self.connection.debug
all_versions=False):
self._check_bucket_uri('list_bucket')
self.get_bucket(headers=headers)
all_versions:
(v
bucket.list_versions(
prefix=prefix,
bucket.list(prefix=prefix,
bucket.get_all_keys(headers)
self._check_bucket_uri('get_bucket')
conn.get_bucket(self.bucket_name,
self.check_response(bucket,
self._check_object_uri('new_key')
self._warn_about_args('get_key',
self._check_object_uri('get_contents_to_file')
hash_algs:
self._check_object_uri('get_contents_as_string')
key.get_contents_as_string(headers,
conn.provider.acl_class
self.check_response(acl_class,
'acl_class',
conn.provider.canned_acls
self.check_response(canned_acls,
'canned_acls',
BucketStorageUri(StorageUri):
capabilities
set([])
capabilities.
connection_args=None,
connection_args:
bool(self.generation)
bool(version_id)
_build_uri_strings(self):
(self.versionless_uri,
self.generation)
self.versionless_uri,
self.is_version_specific:
_update_from_key(self,
self._update_from_values(
'md5',
_update_from_values(self,
is_latest,
md5):
version_id=(version_id
self.version_id))
self._check_bucket_uri('clone_replace_name')
bucket_name=self.bucket_name,
object_name=new_name,
suppress_consec_slashes=self.suppress_consec_slashes)
self._check_bucket_uri('clone_replace_key')
'version_id'):
'generation'):
'is_latest'):
key.is_latest
self._check_bucket_uri('get_acl')
self._check_bucket_uri('get_def_acl')
bucket.get_def_acl(headers)
self._check_bucket_uri('get_cors')
bucket.get_cors(headers)
self.check_response(cors,
self._check_bucket_uri('set_cors
bucket.set_cors(cors,
bucket.set_cors(cors.to_xml(),
get_location(self,
self._check_bucket_uri('get_location')
bucket.get_location()
get_storage_class(self,
self._check_bucket_uri('get_storage_class')
ValueError('get_storage_class()
bucket.get_storage_class()
self._check_bucket_uri('get_subresource')
bucket.get_subresource(subresource,
self._check_bucket_uri('add_group_email_grant')
key-ful
'specify
recursive=True')
bucket.add_group_email_grant(permission,
InvalidUriError('add_group_email_grant()
self._check_bucket_uri('add_email_grant')
bucket.add_email_grant(permission,
self._check_bucket_uri('add_user_grant')
bucket.add_user_grant(permission,
self._check_bucket_uri('list_grants
self.get_bucket(headers)
bucket.list_grants(headers)
bool(self.bucket_name)
location='',
self._check_bucket_uri('create_bucket
self._check_bucket_uri('delete_bucket')
conn.delete_bucket(self.bucket_name,
conn.get_all_buckets(headers)
get_provider(self):
conn.provider
self.check_response(provider,
'provider',
self._check_bucket_uri('set_acl')
bucket.set_acl(
bucket.set_acl(acl_or_str,
self._check_bucket_uri('set_xml_acl')
bucket.set_xml_acl(
bucket.set_xml_acl(xmlstring,
self._check_bucket_uri('set_def_xml_acl')
headers).set_def_xml_acl(xmlstring,
self._check_bucket_uri('set_def_acl')
headers).set_def_acl(acl_or_str,
self._check_object_uri('set_canned_acl')
self._warn_about_args('set_canned_acl',
key.set_canned_acl(acl_str,
self._check_bucket_uri('set_def_canned_acl
key.set_def_canned_acl(acl_str,
self._check_bucket_uri('set_subresource')
bucket.set_subresource(subresource,
self._check_object_uri('set_contents_from_string')
sys.stderr.write('Warning:
GCS
'reduced_redundancy;
'set_contents_from_string')
reduced_redundancy)
res_upload_handler=None):
self._check_object_uri('set_contents_from_file')
rewind=rewind,
self._update_from_values(None,
res_upload_handler.generation,
self._warn_about_args('set_contents_from_file',
rewind=rewind)
self._check_object_uri('set_contents_from_stream')
self.new_key(False,
dst_key.set_contents_from_stream(
policy=policy,
reduced_redundancy=reduced_redundancy)
self._update_from_key(dst_key)
self._check_object_uri('copy_key')
self.get_bucket(validate=False,
src_generation=src_generation)
self._check_bucket_uri('enable_logging')
bucket.enable_logging(target_bucket,
self._check_bucket_uri('disable_logging')
bucket.disable_logging(headers=headers)
self._check_bucket_uri('get_logging_config')
bucket.get_logging_config(headers=headers)
set_website_config(self,
self._check_bucket_uri('set_website_config')
(main_page_suffix
error_key):
bucket.delete_website_configuration(headers)
bucket.configure_website(main_page_suffix,
get_website_config(self,
self._check_bucket_uri('get_website_config')
bucket.get_website_configuration(headers)
self._check_bucket_uri('get_versioning_config')
bucket.get_versioning_status(headers)
self._check_bucket_uri('configure_versioning')
bucket.configure_versioning(enabled,
self.get_key(False).set_remote_metadata(metadata_plus,
self._check_object_uri('compose')
component_keys
component_keys.append(suri.new_key())
component_keys[-1].generation
suri.generation
self.new_key().compose(
component_keys,
content_type=content_type,
self._check_bucket_uri('get_lifecycle_config')
bucket.get_lifecycle_config(headers)
self.check_response(lifecycle_config,
self._check_bucket_uri('configure_lifecycle')
bucket.configure_lifecycle(lifecycle_config,
InvalidUriError('exists
bool(key)
FileStorageUri(StorageUri):
os.sep
is_stream=False):
FileStorageUri(new_name,
self.debug,
self.stream)
self.names_directory()
self.stream:
os.path.isdir(self.object_name)
bool(self.stream)
self.get_key().close()
_headers_not_used=None):
os.path.exists(self.object_name)
logging.handlers
smtplib
email.mime.multipart
email.mime.base
email.mime.text
email.encoders
boto.compat.json
qsa_of_interest
['acl',
'defaultObjectAcl',
'location',
'logging',
'partNumber',
'policy',
'requestPayment',
'torrent',
'versioning',
'versionId',
'website',
'uploadId',
'response-content-type',
'response-content-language',
'response-expires',
'response-cache-control',
'response-content-disposition',
'response-content-encoding',
'tagging',
'storageClass',
'websiteConfig',
'compose']
_first_cap_regex
re.compile('(.)([A-Z][a-z]+)')
_number_cap_regex
re.compile('([a-z])([0-9]+)')
_end_cap_regex
re.compile('([a-z0-9])([A-Z])')
unquote_v(nv):
(nv[0],
urllib.parse.unquote(nv[1]))
canonical_string(method,
interesting_headers
lk
(lk
['content-md5',
'content-type',
'date']
lk.startswith(provider.header_prefix)):
interesting_headers[lk]
str(headers[key]).strip()
'content-type'
interesting_headers['content-type']
'content-md5'
interesting_headers['content-md5']
provider.date_header
str(expires)
sorted_header_keys
sorted(interesting_headers.keys())
sorted_header_keys:
interesting_headers[key]
key.startswith(provider.header_prefix):
"%s:%s\n"
path.split('?')
t[1].split('&')
[a.split('=',
[unquote_v(a)
a[0]
qsa_of_interest]
len(qsa)
qsa.sort(key=lambda
x[0])
['='.join(a)
'&'.join(qsa)
merge_meta(headers,
metadata.keys():
boto.s3.key.Key.base_user_settable_fields:
final_headers[k]
final_headers[metadata_prefix
get_aws_metadata(headers,
hkey
headers.keys():
hkey.lower().startswith(metadata_prefix):
urllib.parse.unquote(headers[hkey])
val.decode('utf-8')
metadata[hkey[len(metadata_prefix):]]
headers[hkey]
retry_url(url,
retry_on_404=True,
num_retries):
proxy_handler
urllib.request.ProxyHandler({})
urllib.request.build_opener(proxy_handler)
urllib.request.Request(url)
opener.open(req,
r.read()
hasattr(result,
'decode')):
result.decode('utf-8')
urllib.error.HTTPError
e.getcode()
retry_on_404:
boto.log.exception('Caught
data')
boto.log.debug('Sleeping
time.sleep(min(2
60)))
_get_instance_metadata(url,
LazyLoadMetadata(dict):
self._leaves
self._dicts
boto.utils.retry_url(self._url,
field.endswith('/'):
field[0:-1]
self._dicts.append(key)
field.find('=')
field[p
field[0:p]
'/openssh-key'
_materialize(self):
self._leaves:
self._num_retries):
boto.utils.retry_url(
urllib.parse.quote(resource,
safe="/:"),
val.find('\n')
val.split('\n')
boto.log.debug("encountered
unretryable"
re-raising"
e.__class__.__name__))
boto.log.error("Caught
data"
try"
self._num_retries:
min(
boto.log.error(
last_exception.__class__.__name__,
last_exception))
self._dicts:
LazyLoadMetadata(self._url
self._num_retries)
self).values()
self).items()
self).__str__()
'%s/%s/%s'
get_instance_metadata(version='latest',
data='meta-data/',
metadata_url
_get_instance_metadata(metadata_url,
"instance
get_instance_identity(version='latest',
'dynamic/instance-identity/')
retry_url(base_url,
retry_url(base_url
iid[field]
get_instance_userdata(version='latest',
ud_url
'user-data')
retry_url(ud_url,
user_data.split(sep)
nvpair
nvpair.split('=')
user_data[t[0].strip()]
t[1].strip()
ISO8601_MS
RFC1123
'%a,
%b
%Y
%H:%M:%S
%Z'
LOCALE_LOCK
setlocale(name):
LOCALE_LOCK:
saved
saved)
get_ts(ts=None):
ts:
time.gmtime()
time.strftime(ISO8601,
ts)
parse_ts(ts):
setlocale('C'):
ts.strip()
ISO8601_MS)
RFC1123)
find_class(module_name,
class_name=None):
(module_name,
class_name)
module_name.split('.')
modules[1:]:
getattr(c,
getattr(__import__(".".join(modules[0:-1])),
update_dme(username,
ip_address):
'https://www.dnsmadeeasy.com/servlet/updateip'
'?username=%s&password=%s&id=%s&ip=%s'
urllib.request.urlopen(dme_url
(username,
ip_address))
s.read()
fetch_file(uri,
username=None,
boto.log.info('Fetching
uri.startswith('s3://'):
uri[len('s3://'):].split('/',
boto.connect_s3(aws_access_key_id=username,
aws_secret_access_key=password)
bucket.get_key(key_name)
key.get_contents_to_file(file)
password:
passman
urllib.request.HTTPPasswordMgrWithDefaultRealm()
passman.add_password(None,
authhandler
urllib.request.HTTPBasicAuthHandler(passman)
urllib.request.build_opener(authhandler)
urllib.request.install_opener(opener)
urllib.request.urlopen(uri)
file.write(s.read())
ShellCommand(object):
wait=True,
fail_fast=False,
self.wait
fail_fast
self.run(cwd=cwd)
self.process
if(self.wait):
self.process.poll()
self.process.communicate()
self.log_fp.write(t[0])
self.log_fp.write(t[1])
boto.log.info(self.log_fp.getvalue())
self.process.returncode
Exception("Command
self.exit_code)
getOutput(self):
self.log_fp.getvalue()
property(getOutput,
STDIN
STDERR
AuthSMTPHandler(logging.handlers.SMTPHandler):
mailhost,
super(AuthSMTPHandler,
self).__init__(mailhost,
self.mailport
smtplib.SMTP_PORT
smtp
smtplib.SMTP(self.mailhost,
smtp.login(self.username,
self.format(record)
"From:
%s\r\nTo:
%s\r\nSubject:
%s\r\nDate:
%s\r\n\r\n%s"
self.fromaddr,
','.join(self.toaddrs),
self.getSubject(record),
email.utils.formatdate(),
smtp.sendmail(self.fromaddr,
self.toaddrs,
smtp.quit()
(KeyboardInterrupt,
SystemExit):
self.handleError(record)
LRUCache(dict):
_Item(object):
self.previous
self.next
repr(self.value)
self.capacity
capacity
cur:
cur.key
cur.next
self._dict.get(key)
self._Item(key,
self._insert_item(item)
repr(self._dict)
_insert_item(self,
_manage_size(self):
self.capacity:
self._dict[self.tail.key]
self.head:
self.tail.previous
self.tail.next
previous.next
item.next.previous
Password(object):
str=None,
hashfunc:
self.hashfunc(value).hexdigest()
other.encode('utf-8')
str(self.hashfunc(other).hexdigest())
self.str:
len(self.str)
notify(subject,
html_body=None,
to_string=None,
attachments=None,
append_instance_id=True):
append_instance_id:
boto.config.get_value("Instance",
"instance-id"),
'smtp_to',
'smtp_from',
'boto')
msg['Reply-To']
email.utils.formatdate(localtime=True)
msg.attach(email.mime.text.MIMEText(body))
html_body:
email.mime.base.MIMEBase('text',
part.set_payload(html_body)
email.encoders.encode_base64(part)
attachments:
smtp_host
'smtp_host',
"smtp_port"):
smtplib.SMTP(smtp_host,
"smtp_port")))
smtplib.SMTP(smtp_host)
boto.config.getbool("Notification",
"smtp_tls"):
server.starttls()
smtp_user
'smtp_user',
smtp_pass
'smtp_pass',
smtp_user:
server.login(smtp_user,
smtp_pass)
server.sendmail(from_string,
to_string,
server.quit()
boto.log.exception('notify
get_utf8_value(value):
mklist(value):
pythonize_name(name):
_first_cap_regex.sub(r'\1_\2',
_number_cap_regex.sub(r'\1_\2',
_end_cap_regex.sub(r'\1_\2',
s2).lower()
write_mime_multipart(content,
compress=False,
deftype='text/plain',
delimiter=':'):
content:
definite_type
guess_mime_type(con,
deftype)
maintype,
subtype
definite_type.split('/',
maintype
email.mime.text.MIMEText(con,
_subtype=subtype)
email.mime.base.MIMEBase(maintype,
subtype)
mime_con.set_payload(con)
email.encoders.encode_base64(mime_con)
mime_con.add_header('Content-Disposition',
filename=name)
wrapper.attach(mime_con)
wrapper.as_string()
compress:
gz
gzip.GzipFile(mode='wb',
fileobj=buf)
gz.write(rcontent)
gz.close()
buf.getvalue()
guess_mime_type(content,
deftype):
starts_with_mappings
'#include':
'text/x-include-url',
'#!':
'text/x-shellscript',
'#cloud-config':
'text/cloud-config',
'#upstart-job':
'text/upstart-job',
'#part-handler':
'text/part-handler',
'#cloud-boothook':
'text/cloud-boothook'
deftype
possible_type,
starts_with_mappings.items():
content.startswith(possible_type):
return(rtype)
buf_size,
hash_algorithm=md5)
hash_algorithm=md5):
hash_obj
hash_algorithm()
hash_obj.update(s)
hex_digest
hash_obj.hexdigest()
encodebytes(hash_obj.digest()).decode('utf-8')
base64_digest[-1]
base64_digest[0:-1]
base64_digest,
data_size)
[h
h.lower()
name.lower()]
merge_headers_by_name(name,
','.join(str(headers[h])
headers[h]
RequestHook(object):
isinstance(hostname,
hostname.startswith('['):
len(hostname.split(':'))
parse_host(hostname):
hostname.strip()
hostname.split(']:',
1)[0].strip('[]')
hostname.split(':',
get_regions('awslambda',
connection_cls=AWSLambdaConnection)
InvalidRequestContentException(BotoServerError):
InvalidParameterValueException(BotoServerError):
ServiceException(BotoServerError):
boto.awslambda
AWSLambdaConnection(AWSAuthConnection):
"2014-11-11"
"lambda.us-east-1.amazonaws.com"
"InvalidRequestContentException":
exceptions.InvalidRequestContentException,
"InvalidParameterValueException":
exceptions.InvalidParameterValueException,
"ServiceException":
exceptions.ServiceException,
super(AWSLambdaConnection,
add_event_source(self,
batch_size=None,
'EventSource':
'FunctionName':
'Role':
params['BatchSize']
delete_function(self,
get_event_source(self,
get_function(self,
get_function_configuration(self,
invoke_async(self,
invoke_args):
'/2014-11-13/functions/{0}/invoke-async/'.format(function_name)
str(len(invoke_args))
invoke_args.tell()
"``invoke_args``
str(os.fstat(invoke_args.fileno()).st_size)
expected_status=202,
data=invoke_args,
list_event_sources(self,
event_source_arn=None,
function_name=None,
query_params['EventSource']
query_params['FunctionName']
list_functions(self,
'/2014-11-13/functions/'
remove_event_source(self,
update_function_configuration(self,
handler=None,
upload_function(self,
function_zip,
runtime,
handler,
query_params['Runtime']
query_params['Mode']
str(len(function_zip))
function_zip.tell()
"``function_zip``
str(os.fstat(function_zip.fileno()).st_size)
data=function_zip,
'elasticbeanstalk',
connection_cls=boto.beanstalk.layer1.Layer1
simple(e):
code.endswith('Exception'):
code.rstrip('Exception')
getattr(sys.modules[__name__],
code)(e)
SimpleException(BotoServerError):
super(SimpleException,
self).__init__(e.status,
e.reason,
ValidationError(SimpleException):
IncompleteSignature(SimpleException):
InternalFailure(SimpleException):
InvalidAction(SimpleException):
InvalidClientTokenId(SimpleException):
InvalidParameterCombination(SimpleException):
InvalidParameterValue(SimpleException):
InvalidQueryParameter(SimpleException):
MalformedQueryString(SimpleException):
MissingAction(SimpleException):
MissingAuthenticationToken(SimpleException):
MissingParameter(SimpleException):
OptInRequired(SimpleException):
RequestExpired(SimpleException):
ServiceUnavailable(SimpleException):
Throttling(SimpleException):
TooManyApplications(SimpleException):
InsufficientPrivileges(SimpleException):
S3LocationNotInServiceRegion(SimpleException):
TooManyApplicationVersions(SimpleException):
TooManyConfigurationTemplates(SimpleException):
TooManyEnvironments(SimpleException):
S3SubscriptionRequired(SimpleException):
TooManyBuckets(SimpleException):
OperationInProgress(SimpleException):
SourceBundleDeletion(SimpleException):
'elasticbeanstalk.us-east-1.amazonaws.com'
_get_response(self,
check_dns_availability(self,
cname_prefix):
{'CNAMEPrefix':
cname_prefix}
self._get_response('CheckDNSAvailability',
self._get_response('CreateApplication',
create_application_version(self,
s3_bucket=None,
s3_key=None,
auto_create_application=None):
s3_key:
params['SourceBundle.S3Bucket']
params['SourceBundle.S3Key']
s3_key
auto_create_application:
params['AutoCreateApplication']
auto_create_application)
self._get_response('CreateApplicationVersion',
create_configuration_template(self,
source_configuration_application_name=None,
source_configuration_template_name=None,
option_settings=None):
source_configuration_application_name:
params['SourceConfiguration.ApplicationName']
source_configuration_application_name
source_configuration_template_name:
params['SourceConfiguration.TemplateName']
source_configuration_template_name
self._get_response('CreateConfigurationTemplate',
create_environment(self,
environment_name,
cname_prefix=None,
cname_prefix:
params['CNAMEPrefix']
cname_prefix
self._get_response('CreateEnvironment',
create_storage_location(self):
self._get_response('CreateStorageLocation',
terminate_env_by_force=None):
terminate_env_by_force:
params['TerminateEnvByForce']
terminate_env_by_force)
self._get_response('DeleteApplication',
delete_application_version(self,
delete_source_bundle=None):
delete_source_bundle:
params['DeleteSourceBundle']
delete_source_bundle)
self._get_response('DeleteApplicationVersion',
delete_configuration_template(self,
template_name):
self._get_response('DeleteConfigurationTemplate',
delete_environment_configuration(self,
environment_name):
self._get_response('DeleteEnvironmentConfiguration',
describe_application_versions(self,
version_labels=None):
version_labels:
version_labels,
'VersionLabels.member')
self._get_response('DescribeApplicationVersions',
describe_applications(self,
application_names:
application_names,
'ApplicationNames.member')
self._get_response('DescribeApplications',
describe_configuration_options(self,
'Options.member')
self._get_response('DescribeConfigurationOptions',
describe_configuration_settings(self,
self._get_response('DescribeConfigurationSettings',
describe_environment_resources(self,
self._get_response('DescribeEnvironmentResources',
describe_environments(self,
environment_ids=None,
environment_names=None,
include_deleted=None,
included_deleted_back_to=None):
environment_ids:
environment_ids,
'EnvironmentIds.member')
environment_names:
environment_names,
'EnvironmentNames.member')
include_deleted:
params['IncludeDeleted']
self._encode_bool(include_deleted)
included_deleted_back_to:
params['IncludedDeletedBackTo']
included_deleted_back_to
self._get_response('DescribeEnvironments',
request_id:
params['RequestId']
severity:
self._get_response('DescribeEvents',
list_available_solution_stacks(self):
self._get_response('ListAvailableSolutionStacks',
rebuild_environment(self,
self._get_response('RebuildEnvironment',
request_environment_info(self,
self._get_response('RequestEnvironmentInfo',
restart_app_server(self,
self._get_response('RestartAppServer',
retrieve_environment_info(self,
self._get_response('RetrieveEnvironmentInfo',
swap_environment_cnames(self,
source_environment_id=None,
source_environment_name=None,
destination_environment_id=None,
destination_environment_name=None):
source_environment_id:
params['SourceEnvironmentId']
source_environment_id
source_environment_name:
params['SourceEnvironmentName']
source_environment_name
destination_environment_id:
params['DestinationEnvironmentId']
destination_environment_id
destination_environment_name:
params['DestinationEnvironmentName']
destination_environment_name
self._get_response('SwapEnvironmentCNAMEs',
terminate_environment(self,
terminate_resources=None):
terminate_resources:
params['TerminateResources']
terminate_resources)
self._get_response('TerminateEnvironment',
self._get_response('UpdateApplication',
update_application_version(self,
self._get_response('UpdateApplicationVersion',
update_configuration_template(self,
options_to_remove=None):
self._get_response('UpdateConfigurationTemplate',
update_environment(self,
self._get_response('UpdateEnvironment',
validate_configuration_settings(self,
self._get_response('ValidateConfigurationSettings',
user_values,
tuple_names):
user_value
enumerate(user_values,
zip(tuple_names,
user_value):
BaseObject(object):
six.iteritems(self.__dict__):
}'
_repr_by_type(self,
value.__repr__()
self._repr_list(value)
_repr_list(self,
array):
'['
array:
','
len(result)
result[:-1]
']'
Response(BaseObject):
response['ResponseMetadata']:
ResponseMetadata(response['ResponseMetadata'])
ResponseMetadata(BaseObject):
super(ResponseMetadata,
ApplicationDescription(BaseObject):
super(ApplicationDescription,
self.configuration_templates
configuration_template
self.configuration_templates.append(configuration_template)
self.versions
self.versions.append(version)
ApplicationVersionDescription(BaseObject):
super(ApplicationVersionDescription,
response['SourceBundle']:
S3Location(response['SourceBundle'])
AutoScalingGroup(BaseObject):
super(AutoScalingGroup,
ConfigurationOptionDescription(BaseObject):
super(ConfigurationOptionDescription,
self.change_severity
str(response['ChangeSeverity'])
str(response['DefaultValue'])
int(response['MaxLength'])
response['MaxLength']
self.max_value
int(response['MaxValue'])
response['MaxValue']
self.min_value
int(response['MinValue'])
response['MinValue']
response['Regex']:
OptionRestrictionRegex(response['Regex'])
self.user_defined
str(response['UserDefined'])
self.value_options
value_option
self.value_options.append(value_option)
self.value_type
str(response['ValueType'])
ConfigurationOptionSetting(BaseObject):
super(ConfigurationOptionSetting,
str(response['Value'])
ConfigurationSettingsDescription(BaseObject):
super(ConfigurationSettingsDescription,
EnvironmentDescription(BaseObject):
super(EnvironmentDescription,
EnvironmentInfoDescription(BaseObject):
super(EnvironmentInfoDescription,
self.ec2_instance_id
str(response['Ec2InstanceId'])
self.info_type
str(response['InfoType'])
self.sample_timestamp
datetime.fromtimestamp(response['SampleTimestamp'])
EnvironmentResourceDescription(BaseObject):
super(EnvironmentResourceDescription,
self.auto_scaling_groups
auto_scaling_group
AutoScalingGroup(member)
self.auto_scaling_groups.append(auto_scaling_group)
Instance(member)
self.launch_configurations
launch_configuration
LaunchConfiguration(member)
self.launch_configurations.append(launch_configuration)
LoadBalancer(member)
self.load_balancers.append(load_balancer)
self.triggers
Trigger(member)
self.triggers.append(trigger)
EnvironmentResourcesDescription(BaseObject):
super(EnvironmentResourcesDescription,
response['LoadBalancer']:
LoadBalancerDescription(response['LoadBalancer'])
EventDescription(BaseObject):
super(EventDescription,
self.event_date
datetime.fromtimestamp(response['EventDate'])
Instance(BaseObject):
str(response['Id'])
LaunchConfiguration(BaseObject):
super(LaunchConfiguration,
Listener(BaseObject):
super(Listener,
int(response['Port'])
response['Port']
str(response['Protocol'])
LoadBalancer(BaseObject):
super(LoadBalancer,
LoadBalancerDescription(BaseObject):
super(LoadBalancerDescription,
str(response['Domain'])
Listener(member)
self.listeners.append(listener)
self.load_balancer_name
str(response['LoadBalancerName'])
OptionRestrictionRegex(BaseObject):
super(OptionRestrictionRegex,
response['Label']
self.pattern
response['Pattern']
SolutionStackDescription(BaseObject):
super(SolutionStackDescription,
self.permitted_file_types
permitted_file_type
self.permitted_file_types.append(permitted_file_type)
S3Location(BaseObject):
super(S3Location,
self.s3_key
str(response['S3Key'])
Trigger(BaseObject):
super(Trigger,
ValidationMessage(BaseObject):
super(ValidationMessage,
CheckDNSAvailabilityResponse(Response):
response['CheckDNSAvailabilityResponse']
super(CheckDNSAvailabilityResponse,
response['CheckDNSAvailabilityResult']
self.fully_qualified_cname
str(response['FullyQualifiedCNAME'])
bool(response['Available'])
CheckDnsAvailabilityResponse(CheckDNSAvailabilityResponse):
CreateApplicationResponse(Response):
response['CreateApplicationResponse']
super(CreateApplicationResponse,
response['CreateApplicationResult']
CreateApplicationVersionResponse(Response):
response['CreateApplicationVersionResponse']
super(CreateApplicationVersionResponse,
response['CreateApplicationVersionResult']
CreateConfigurationTemplateResponse(Response):
response['CreateConfigurationTemplateResponse']
super(CreateConfigurationTemplateResponse,
response['CreateConfigurationTemplateResult']
CreateEnvironmentResponse(Response):
response['CreateEnvironmentResponse']
super(CreateEnvironmentResponse,
response['CreateEnvironmentResult']
CreateStorageLocationResponse(Response):
response['CreateStorageLocationResponse']
super(CreateStorageLocationResponse,
response['CreateStorageLocationResult']
DeleteApplicationResponse(Response):
response['DeleteApplicationResponse']
super(DeleteApplicationResponse,
DeleteApplicationVersionResponse(Response):
response['DeleteApplicationVersionResponse']
super(DeleteApplicationVersionResponse,
DeleteConfigurationTemplateResponse(Response):
response['DeleteConfigurationTemplateResponse']
super(DeleteConfigurationTemplateResponse,
DeleteEnvironmentConfigurationResponse(Response):
response['DeleteEnvironmentConfigurationResponse']
super(DeleteEnvironmentConfigurationResponse,
DescribeApplicationVersionsResponse(Response):
response['DescribeApplicationVersionsResponse']
super(DescribeApplicationVersionsResponse,
response['DescribeApplicationVersionsResult']
self.application_versions
application_version
ApplicationVersionDescription(member)
self.application_versions.append(application_version)
DescribeApplicationsResponse(Response):
response['DescribeApplicationsResponse']
super(DescribeApplicationsResponse,
response['DescribeApplicationsResult']
ApplicationDescription(member)
self.applications.append(application)
DescribeConfigurationOptionsResponse(Response):
response['DescribeConfigurationOptionsResponse']
super(DescribeConfigurationOptionsResponse,
response['DescribeConfigurationOptionsResult']
ConfigurationOptionDescription(member)
self.options.append(option)
DescribeConfigurationSettingsResponse(Response):
response['DescribeConfigurationSettingsResponse']
super(DescribeConfigurationSettingsResponse,
response['DescribeConfigurationSettingsResult']
self.configuration_settings
configuration_setting
ConfigurationSettingsDescription(member)
self.configuration_settings.append(configuration_setting)
DescribeEnvironmentResourcesResponse(Response):
response['DescribeEnvironmentResourcesResponse']
super(DescribeEnvironmentResourcesResponse,
response['DescribeEnvironmentResourcesResult']
response['EnvironmentResources']:
EnvironmentResourceDescription(response['EnvironmentResources'])
DescribeEnvironmentsResponse(Response):
response['DescribeEnvironmentsResponse']
super(DescribeEnvironmentsResponse,
response['DescribeEnvironmentsResult']
self.environments
EnvironmentDescription(member)
self.environments.append(environment)
DescribeEventsResponse(Response):
response['DescribeEventsResponse']
super(DescribeEventsResponse,
response['DescribeEventsResult']
EventDescription(member)
self.events.append(event)
self.next_tokent
str(response['NextToken'])
ListAvailableSolutionStacksResponse(Response):
response['ListAvailableSolutionStacksResponse']
super(ListAvailableSolutionStacksResponse,
response['ListAvailableSolutionStacksResult']
self.solution_stack_details
solution_stack_detail
SolutionStackDescription(member)
self.solution_stack_details.append(solution_stack_detail)
self.solution_stacks
solution_stack
self.solution_stacks.append(solution_stack)
RebuildEnvironmentResponse(Response):
response['RebuildEnvironmentResponse']
super(RebuildEnvironmentResponse,
RequestEnvironmentInfoResponse(Response):
response['RequestEnvironmentInfoResponse']
super(RequestEnvironmentInfoResponse,
RestartAppServerResponse(Response):
response['RestartAppServerResponse']
super(RestartAppServerResponse,
RetrieveEnvironmentInfoResponse(Response):
response['RetrieveEnvironmentInfoResponse']
super(RetrieveEnvironmentInfoResponse,
response['RetrieveEnvironmentInfoResult']
self.environment_info
environment_info
EnvironmentInfoDescription(member)
self.environment_info.append(environment_info)
SwapEnvironmentCNAMEsResponse(Response):
response['SwapEnvironmentCNAMEsResponse']
super(SwapEnvironmentCNAMEsResponse,
SwapEnvironmentCnamesResponse(SwapEnvironmentCNAMEsResponse):
TerminateEnvironmentResponse(Response):
response['TerminateEnvironmentResponse']
super(TerminateEnvironmentResponse,
response['TerminateEnvironmentResult']
UpdateApplicationResponse(Response):
response['UpdateApplicationResponse']
super(UpdateApplicationResponse,
response['UpdateApplicationResult']
UpdateApplicationVersionResponse(Response):
response['UpdateApplicationVersionResponse']
super(UpdateApplicationVersionResponse,
response['UpdateApplicationVersionResult']
UpdateConfigurationTemplateResponse(Response):
response['UpdateConfigurationTemplateResponse']
super(UpdateConfigurationTemplateResponse,
response['UpdateConfigurationTemplateResult']
UpdateEnvironmentResponse(Response):
response['UpdateEnvironmentResponse']
super(UpdateEnvironmentResponse,
response['UpdateEnvironmentResult']
ValidateConfigurationSettingsResponse(Response):
response['ValidateConfigurationSettingsResponse']
super(ValidateConfigurationSettingsResponse,
response['ValidateConfigurationSettingsResult']
ValidationMessage(member)
self.messages.append(message)
beanstalk_wrapper(func,
_wrapped_low_level_api(*args,
exception.simple(e)
''.join([part.capitalize()
name.split('_')])
getattr(boto.beanstalk.response,
cls(response)
_wrapped_low_level_api
Layer1Wrapper(object):
beanstalk_wrapper(getattr(self.api,
AttributeError("%s
load_regions().get('cloudformation')
'cloudformation',
connection_cls=CloudFormationConnection
Stack,
StackSummary,
StackEvent
StackResource,
StackResourceSummary
boto.cloudformation.template
CloudFormationConnection(AWSQueryConnection):
'cfn_version',
'2010-05-15')
'cfn_region_name',
'cfn_region_endpoint',
'cloudformation.us-east-1.amazonaws.com')
'CREATE_FAILED',
'CREATE_COMPLETE',
'ROLLBACK_IN_PROGRESS',
'ROLLBACK_FAILED',
'ROLLBACK_COMPLETE',
'DELETE_IN_PROGRESS',
'DELETE_FAILED',
'DELETE_COMPLETE',
'UPDATE_IN_PROGRESS',
'UPDATE_COMPLETE_CLEANUP_IN_PROGRESS',
'UPDATE_COMPLETE',
'UPDATE_ROLLBACK_IN_PROGRESS',
'UPDATE_ROLLBACK_FAILED',
'UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS',
'UPDATE_ROLLBACK_COMPLETE')
CloudFormationConnection)
super(CloudFormationConnection,
_build_create_or_update_params(self,
stack_policy_during_update_url=None):
self.encode_bool(disable_rollback)}
use_previous_template
params['UsePreviousTemplate']
self.encode_bool(use_previous_template)
parameter_tuple
parameter_tuple[:2]
use_previous
(parameter_tuple[2]
len(parameter_tuple)
use_previous:
params['Parameters.member.%d.UsePreviousValue'
self.encode_bool(use_previous)
capabilities:
enumerate(capabilities):
params['Capabilities.member.%d'
enumerate(tags.items()):
params['Tags.member.%d.Key'
params['Tags.member.%d.Value'
notification_arns
len(notification_arns)
"NotificationARNs.member")
timeout_in_minutes:
params['TimeoutInMinutes']
int(timeout_in_minutes)
params['DisableRollback']
disable_rollback).lower()
params['OnFailure']
params['StackPolicyDuringUpdateBody']
params['StackPolicyDuringUpdateURL']
_do_request(self,
self.make_request(call,
disable_rollback=None,
on_failure=None,
self._do_request('CreateStack',
body['CreateStackResponse']['CreateStackResult']['StackId']
disable_rollback=False,
stack_policy_during_update_url=None,
use_previous_template,
stack_policy_during_update_body,
stack_policy_during_update_url)
self._do_request('UpdateStack',
body['UpdateStackResponse']['UpdateStackResult']['StackId']
self._do_request('DeleteStack',
describe_stack_events(self,
self.get_list('DescribeStackEvents',
StackEvent)])
describe_stack_resource(self,
logical_resource_id}
self._do_request('DescribeStackResource',
describe_stack_resources(self,
logical_resource_id:
params['LogicalResourceId']
logical_resource_id
physical_resource_id:
params['PhysicalResourceId']
physical_resource_id
self.get_list('DescribeStackResources',
StackResource)])
self.get_list('DescribeStacks',
Stack)])
get_template(self,
self._do_request('GetTemplate',
list_stack_resources(self,
{'StackName':
self.get_list('ListStackResources',
StackResourceSummary)])
list_stacks(self,
stack_status_filters=None,
stack_status_filters
len(stack_status_filters)
stack_status_filters,
"StackStatusFilter.member")
self.get_list('ListStacks',
StackSummary)])
validate_template(self,
template_url=None):
self.get_object('ValidateTemplate',
Template,
cancel_update_stack(self,
stack_name_or_id=None):
self.get_status('CancelUpdateStack',
estimate_template_cost(self,
"JSON"}
self._do_request('EstimateTemplateCost',
response['EstimateTemplateCostResponse']\
['EstimateTemplateCostResult']\
['Url']
get_stack_policy(self,
self._do_request('GetStackPolicy',
response['GetStackPolicyResponse']\
['GetStackPolicyResult']\
['StackPolicyBody']
set_stack_policy(self,
self._do_request('SetStackPolicy',
response['SetStackPolicyResponse']
Stack(object):
stack_name_reason(self):
@stack_name_reason.setter
stack_name_reason(self,
Parameter)])
Output)])
"Tags":
'NotificationARNs':
NotificationARN)])
"DisableRollback":
"StackStatusReason":
"TimeoutInMinutes":
self.connection.delete_stack(stack_name_or_id=self.stack_id)
self.connection.describe_stack_events(
describe_resource(self,
self.connection.describe_stack_resource(
logical_resource_id=logical_resource_id
describe_resources(self,
self.connection.describe_stack_resources(
logical_resource_id=logical_resource_id,
physical_resource_id=physical_resource_id
list_resources(self,
self.connection.list_stack_resources(
self.connection.describe_stacks(self.stack_id)
rs[0].stack_id
self.stack_id:
self.__dict__.update(rs[0].__dict__)
Stack
Name"
get_template(self):
self.connection.get_template(stack_name_or_id=self.stack_id)
get_policy(self):
self.connection.get_stack_policy(self.stack_id)
self.connection.set_stack_policy(self.stack_id,
stack_policy_body=stack_policy_body,
stack_policy_url=stack_policy_url)
StackSummary(object):
"DeletionTime":
'TemplateDescription':
"ParameterValue":
"Parameter:\"%s\"=\"%s\""
Output(object):
"OutputKey":
"OutputValue":
"Output:\"%s\"=\"%s\""
Capability(object):
"Capability:\"%s\""
Tag(dict):
"Key":
self._current_value:
NotificationARN(object):
"NotificationARN:\"%s\""
StackResource(object):
"StackResource:%s
StackResourceSummary(object):
"LastUpdatedTime":
"StackResourceSummary:%s
StackEvent(object):
("CREATE_IN_PROGRESS",
"CREATE_FAILED",
"CREATE_COMPLETE",
"DELETE_IN_PROGRESS",
"DELETE_FAILED",
"DELETE_COMPLETE")
"EventId":
"ResourceProperties":
"StackEvent
(self.resource_type,
self.logical_resource_id,
self.resource_status)
Capability
Template(object):
TemplateParameter)])
"CapabilitiesReason":
TemplateParameter(object):
"DefaultValue":
"NoEcho":
DistributionSummary,
StreamingDistribution,
StreamingDistributionSummary,
StreamingDistributionConfig
OriginAccessIdentitySummary
OriginAccessIdentityConfig
boto.cloudfront.invalidation
InvalidationBatch,
InvalidationSummary,
boto.cloudfront.exception
CloudFrontServerError
CloudFrontConnection(AWSAuthConnection):
'cloudfront.amazonaws.com'
https_connection_factory=None):
super(CloudFrontConnection,
get_etag(self,
['cloudfront']
_get_all_objects(self,
result_set_class=None,
result_set_kwargs=None):
resource))
result_set_class
result_set_kwargs
rs_class(tags,
**rs_kwargs)
_get_info(self,
_get_config(self,
config_class):
config_class(connection=self)
_set_config(self,
isinstance(config,
StreamingDistributionConfig):
'streaming-distribution'
'distribution'
'text/xml'}
config.to_xml())
_create_object(self,
data=config.to_xml())
_delete_object(self,
resource):
etag})
get_all_distributions(self):
self._get_all_objects('distribution',
get_distribution_info(self,
get_distribution_config(self,
DistributionConfig)
set_distribution_config(self,
create_distribution(self,
DistributionConfig(origin=origin,
delete_distribution(self,
'distribution')
get_all_streaming_distributions(self):
[('StreamingDistributionSummary',
StreamingDistributionSummary)]
self._get_all_objects('streaming-distribution',
get_streaming_distribution_info(self,
get_streaming_distribution_config(self,
StreamingDistributionConfig)
set_streaming_distribution_config(self,
create_streaming_distribution(self,
StreamingDistributionConfig(origin=origin,
delete_streaming_distribution(self,
'streaming-distribution')
get_all_origin_access_identity(self):
[('CloudFrontOriginAccessIdentitySummary',
OriginAccessIdentitySummary)]
self._get_all_objects('origin-access-identity/cloudfront',
get_origin_access_identity_info(self,
self._get_info(access_id,
get_origin_access_identity_config(self,
self._get_config(access_id,
OriginAccessIdentityConfig)
set_origin_access_identity_config(self,
self._set_config(access_id,
create_origin_access_identity(self,
OriginAccessIdentityConfig(caller_reference=caller_reference,
delete_origin_access_identity(self,
self._delete_object(access_id,
'origin-access-identity/cloudfront')
create_invalidation_request(self,
paths,
isinstance(paths,
InvalidationBatch):
InvalidationBatch(paths)
paths.connection
'/%s/distribution/%s/invalidation'
data=paths.to_xml())
invalidation_request_status(self,
'/%s/distribution/%s/invalidation/%s'
'text/xml'})
InvalidationBatch([])
get_invalidation_requests(self,
'distribution/%s/invalidation'
'?%s=%s'
params.popitem()
'&%s=%s'
tags=[('InvalidationSummary',
InvalidationSummary)]
dict(connection=self,
distribution_id=distribution_id,
self._get_all_objects(uri,
result_set_class=rs_class,
result_set_kwargs=rs_kwargs)
boto.cloudfront.object
Object,
boto.cloudfront.signers
ActiveTrustedSigners,
TrustedSigners
S3Origin,
DistributionConfig(object):
default_root_object=None,
cnames:
trusted_signers
default_root_object
"DistributionConfig:%s"
'<DistributionConfig
<Self></Self>\n'
self.default_root_object:
'<DefaultRootObject>%s</DefaultRootObject>\n'
'</DistributionConfig>\n'
'Logging':
LoggingInfo()
'DefaultRootObject':
StreamingDistributionConfig(DistributionConfig):
origin='',
super(StreamingDistributionConfig,
self).__init__(connection=connection,
origin=origin,
trusted_signers=trusted_signers,
logging=logging)
'<StreamingDistributionConfig
<Self/>\n'
'</StreamingDistributionConfig>\n'
DistributionSummary(object):
status='',
cname='',
enabled=False):
cname:
self.cnames.append(cname)
"DistributionSummary:%s"
'Origin':
'StreamingDistributionSummary':
self.connection.get_distribution_info(self.id)
StreamingDistributionSummary(DistributionSummary):
self.connection.get_streaming_distribution_info(self.id)
Distribution(object):
"Distribution:%s"
'DistributionConfig':
'ActiveTrustedSigners':
ActiveTrustedSigners()
'InProgressInvalidationBatches':
DistributionConfig(self.connection,
self.config.trusted_signers,
self.config.default_root_object)
self.connection.set_distribution_config(self.id,
enable(self):
self.update(enabled=True)
disable(self):
self.update(enabled=False)
self.connection.delete_distribution(self.id,
_get_bucket(self):
self._bucket:
bucket_dns_name
self.config.origin.dns_name
bucket_dns_name.replace('.s3.amazonaws.com',
S3Connection(self.connection.aws_access_key_id,
self.connection.aws_secret_access_key,
proxy=self.connection.proxy,
proxy_port=self.connection.proxy_port,
proxy_user=self.connection.proxy_user,
proxy_pass=self.connection.proxy_pass)
self._bucket.distribution
self._bucket.set_key_class(self._object_class)
NotImplementedError('Unable
get_objects
CustomOrigin')
get_objects(self):
objs.append(key)
set_permissions(self,
self.config.origin.origin_access_identity.split('/')[-1]
oai
self.connection.get_origin_access_identity_info(id)
object.get_acl()
policy.acl
policy.acl.add_user_grant('READ',
oai.s3_user_id)
object.set_acl(policy)
object.set_canned_acl('public-read')
set_permissions_all(self,
self.set_permissions(key,
add_object(self,
'public-read'
bucket.new_key(name)
object.set_contents_from_file(content,
policy=policy)
self.set_permissions(object,
create_signed_url(self,
self._create_signing_params(
keypair_id=keypair_id,
expire_time=expire_time,
valid_after_time=valid_after_time,
ip_address=ip_address,
policy_url=policy_url,
private_key_file=private_key_file,
private_key_string=private_key_string)
"&"
["Expires",
"Policy",
"Signature",
"Key-Pair-Id"]:
"%s=%s"
params[key])
signed_url_params.append(param)
"&".join(signed_url_params)
_create_signing_params(self,
valid_after_time
policy_url:
self._canned_policy(url,
expire_time)
params["Expires"]
str(expire_time)
self._custom_policy(policy_url,
expires=expire_time,
valid_after=valid_after_time,
ip_address=ip_address)
self._url_base64_encode(policy)
params["Policy"]
self._sign_string(policy,
private_key_file,
private_key_string)
self._url_base64_encode(signature)
params["Signature"]
params["Key-Pair-Id"]
keypair_id
_canned_policy(resource,
expires):
('{"Statement":[{"Resource":"%(resource)s",'
'"Condition":{"DateLessThan":{"AWS:EpochTime":'
'%(expires)s}}}]}'
locals())
_custom_policy(resource,
valid_after=None,
ip_address=None):
expires}
valid_after:
valid_after}
"/32"
{"AWS:SourceIp":
ip_address}
condition}]}
json.dumps(policy,
separators=(",",
":"))
_sign_string(message,
NotImplementedError("Boto
depends
"library
signed
URLs
"CloudFront")
ValueError("Only
ValueError("You
private_key_string")
isinstance(private_key_file,
open(private_key_file,
file_handle:
file_handle.read()
private_key_file.read()
private_key
rsa.PrivateKey.load_pkcs1(private_key_string)
rsa.sign(str(message),
'SHA-1')
_url_base64_encode(msg):
base64.b64encode(msg)
msg_base64.replace('+',
msg_base64.replace('=',
msg_base64.replace('/',
'~')
StreamingDistribution(Distribution):
last_modified_time,
'StreamingDistributionConfig':
StreamingDistributionConfig()
StreamingDistributionConfig(self.connection,
self.config.trusted_signers)
self.connection.set_streaming_distribution_config(self.id,
self.connection.delete_streaming_distribution(self.id,
CloudFrontServerError(BotoServerError):
OriginAccessIdentity(object):
'CloudFrontOriginAccessIdentityConfig':
OriginAccessIdentityConfig()
OriginAccessIdentityConfig(self.connection,
self.config.comment)
self.connection.set_origin_identity_config(self.id,
self.connection.delete_origin_access_identity(self.id,
uri(self):
'origin-access-identity/cloudfront/%s'
OriginAccessIdentityConfig(object):
'<CloudFrontOriginAccessIdentityConfig
xmlns="http://cloudfront.amazonaws.com/doc/2009-09-09/">\n'
'</CloudFrontOriginAccessIdentityConfig>\n'
OriginAccessIdentitySummary(object):
get_origin_access_identity(self):
self.connection.get_origin_access_identity_info(self.id)
InvalidationBatch(object):
paths=None,
distribution=None,
caller_reference=''):
self.caller_reference:
distribution:
'<InvalidationBatch:
self.paths.append(path)
remove(self,
self.paths.remove(path)
iter(self.paths)
self.paths[i]
self.paths[k]
escape(self,
p):
p[0]
"/":
urllib.parse.quote(p,
'<InvalidationBatch
xmlns="http://cloudfront.amazonaws.com/doc/%s/">\n'
self.connection.Version
self.paths:
<Path>%s</Path>\n'
self.escape(p)
'</InvalidationBatch>\n'
"InvalidationBatch":
self.paths.append(value)
"Status":
"CreateTime":
"CallerReference":
InvalidationListResultSet(object):
markers=None,
invalidations=None,
next_marker=None,
is_truncated=False):
markers
self.auto_paginate
self._inval_cache
invalidations
self.auto_paginate:
result_set.is_truncated:
conn.get_invalidation_requests(distribution_id,
marker=result_set.next_marker,
max_items=result_set.max_items)
root_elem,
root_elem:
handler(connection,
distribution_id=self.distribution_id)
self._inval_cache.append(obj)
InvalidationSummary(object):
'<InvalidationSummary:
self.connection.get_distribution_info(self.distribution_id)
get_invalidation_request(self):
self.connection.invalidation_request_status(
self.distribution_id,
LoggingInfo(object):
Object(Key):
super(Object,
self).__init__(bucket,
bucket.distribution
'<Object:
%s/%s>'
(self.distribution.config.origin,
scheme='http'):
self.distribution.domain_name
scheme.lower().startswith('rtmp'):
'/cfx/st/'
StreamingObject(Object):
scheme='rtmp'):
super(StreamingObject,
self).url(scheme)
get_oai_value(origin_access_identity):
isinstance(origin_access_identity,
OriginAccessIdentity):
origin_access_identity.uri()
S3Origin(object):
origin_access_identity=None):
'<S3Origin:
'OriginAccessIdentity':
<S3Origin>\n'
self.origin_access_identity:
get_oai_value(self.origin_access_identity)
<OriginAccessIdentity>%s</OriginAccessIdentity>\n'
</S3Origin>\n'
CustomOrigin(object):
http_port=80,
https_port=443,
origin_protocol_policy=None):
http_port
https_port
origin_protocol_policy
'<CustomOrigin:
'HTTPPort':
'HTTPSPort':
'OriginProtocolPolicy':
<CustomOrigin>\n'
<HTTPPort>%d</HTTPPort>\n'
<HTTPSPort>%d</HTTPSPort>\n'
<OriginProtocolPolicy>%s</OriginProtocolPolicy>\n'
</CustomOrigin>\n'
Signer(object):
self.key_pair_ids
'Self'
'KeyPairId':
self.key_pair_ids.append(value)
ActiveTrustedSigners(list):
'Signer':
Signer()
self.append(s)
TrustedSigners(list):
self.append(name)
get_regions('cloudhsm',
connection_cls=CloudHSMConnection)
InvalidRequestException(BotoServerError):
CloudHsmServiceException(BotoServerError):
CloudHsmInternalException(BotoServerError):
boto.cloudhsm
CloudHSMConnection(AWSQueryConnection):
"2014-05-30"
"cloudhsm.us-east-1.amazonaws.com"
"CloudHSM"
"CloudHsmFrontendService"
"CloudHsmServiceException":
exceptions.CloudHsmServiceException,
"CloudHsmInternalException":
exceptions.CloudHsmInternalException,
super(CloudHSMConnection,
create_hapg(self,
self.make_request(action='CreateHapg',
create_hsm(self,
'SshKey':
'IamRoleArn':
'SubscriptionType':
self.make_request(action='CreateHsm',
create_luna_client(self,
{'Certificate':
self.make_request(action='CreateLunaClient',
delete_hapg(self,
self.make_request(action='DeleteHapg',
delete_hsm(self,
hsm_arn):
self.make_request(action='DeleteHsm',
delete_luna_client(self,
client_arn):
{'ClientArn':
self.make_request(action='DeleteLunaClient',
describe_hapg(self,
self.make_request(action='DescribeHapg',
describe_hsm(self,
hsm_arn=None,
hsm_serial_number=None):
params['HsmArn']
params['HsmSerialNumber']
self.make_request(action='DescribeHsm',
describe_luna_client(self,
client_arn=None,
certificate_fingerprint=None):
params['ClientArn']
params['CertificateFingerprint']
self.make_request(action='DescribeLunaClient',
hapg_list):
'ClientVersion':
'HapgList':
hapg_list,
self.make_request(action='GetConfig',
list_available_zones(self):
self.make_request(action='ListAvailableZones',
list_hapgs(self,
self.make_request(action='ListHapgs',
list_hsms(self,
self.make_request(action='ListHsms',
list_luna_clients(self,
self.make_request(action='ListLunaClients',
modify_hapg(self,
label=None,
partition_serial_list=None):
params['PartitionSerialList']
self.make_request(action='ModifyHapg',
modify_hsm(self,
iam_role_arn=None,
params['IamRoleArn']
self.make_request(action='ModifyHsm',
modify_luna_client(self,
certificate):
'Certificate':
self.make_request(action='ModifyLunaClient',
connection_cls=boto.cloudsearch.layer1.Layer1
fields,
lang='en'):
'lang':
lang,
version):
version}
'500
"http://%s/2011-02-01/documents/batch"
sdf)
sdf):
Content:\n{0}\n\n'
'SDF:\n{1}'.format(_body,
'adds'
'deletes'
self.content:
documents"
=>
self.content.get('message',
{2}'\
response_num))
OptionStatus
RankExpressionStatus
data['created']
data['deleted']
data['processing']
data['requires_index_documents']
data['domain_id']
data['domain_name']
self.num_searchable_docs
data['num_searchable_docs']
data['search_instance_count']
data.get('search_instance_type',
data['search_partition_count']
data['doc_service']
doc_service_arn(self):
self._doc_service['arn']
self._doc_service['endpoint']
search_service_arn(self):
self._search_service['arn']
self._search_service['endpoint']
num_searchable_docs(self):
@num_searchable_docs.setter
num_searchable_docs(self,
get_stemming(self):
self.layer1.describe_stemming_options,
self.layer1.update_stemming_options)
get_stopwords(self):
self.layer1.describe_stopword_options,
self.layer1.update_stopword_options)
get_synonyms(self):
self.layer1.describe_synonym_options,
self.layer1.update_synonym_options)
ServicePoliciesStatus(self,
self.layer1.describe_service_access_policies,
self.layer1.update_service_access_policies)
source_attributes=[]):
searchable=searchable,
source_attributes=source_attributes)
get_rank_expressions(self,
self.layer1.describe_rank_expressions
rank_names)
[RankExpressionStatus(self,
create_rank_expression(self,
self.layer1.define_rank_expression(self.name,
RankExpressionStatus(self,
self.layer1.describe_rank_expressions)
do_bool(val):
'true']
'2011-02-01'
'cs_region_name',
'cs_region_endpoint',
'cloudsearch.us-east-1.amazonaws.com')
AWSQueryConnection.__init__(
host=self.region.endpoint,
proxy_user=proxy_user,
proxy_pass=proxy_pass,
doc_path,
list_marker=None):
boto.jsonresponse.Element(
list_marker=list_marker
'Set',
doc_path:
inner.get(p)
inner:
isinstance(inner,
dict(**inner)
('create_domain_response',
'create_domain_result',
source_attributes=None):
('define_index_field_response',
'define_index_field_result',
'IndexField.IndexFieldName':
'IndexField.IndexFieldType':
field_type}
params['IndexField.LiteralOptions.DefaultValue']
params['IndexField.LiteralOptions.FacetEnabled']
params['IndexField.LiteralOptions.ResultEnabled']
params['IndexField.LiteralOptions.SearchEnabled']
do_bool(searchable)
'uint':
params['IndexField.UIntOptions.DefaultValue']
params['IndexField.TextOptions.DefaultValue']
params['IndexField.TextOptions.FacetEnabled']
params['IndexField.TextOptions.ResultEnabled']
'DefineIndexField',
define_rank_expression(self,
rank_name,
rank_expression):
('define_rank_expression_response',
'define_rank_expression_result',
'RankExpression.RankExpression':
rank_expression,
'RankExpression.RankName':
'DefineRankExpression',
('delete_domain_response',
'delete_domain_result',
('delete_index_field_response',
'delete_index_field_result',
field_name}
'DeleteIndexField',
delete_rank_expression(self,
rank_name):
('delete_rank_expression_response',
'delete_rank_expression_result',
'RankName':
'DeleteRankExpression',
describe_default_search_field(self,
('describe_default_search_field_response',
'describe_default_search_field_result',
'DescribeDefaultSearchField',
('describe_domains_response',
'describe_domains_result',
'domain_status_list')
domain_names:
enumerate(domain_names,
params['DomainNames.member.%d'
'DescribeDomains',
list_marker='DomainStatusList')
('describe_index_fields_response',
'describe_index_fields_result',
'index_fields')
field_names:
enumerate(field_names,
params['FieldNames.member.%d'
'DescribeIndexFields',
list_marker='IndexFields')
describe_rank_expressions(self,
('describe_rank_expressions_response',
'describe_rank_expressions_result',
'rank_expressions')
rank_names:
enumerate(rank_names,
params['RankNames.member.%d'
'DescribeRankExpressions',
list_marker='RankExpressions')
('describe_service_access_policies_response',
'describe_service_access_policies_result',
'DescribeServiceAccessPolicies',
describe_stemming_options(self,
('describe_stemming_options_response',
'describe_stemming_options_result',
'DescribeStemmingOptions',
describe_stopword_options(self,
('describe_stopword_options_response',
'describe_stopword_options_result',
'DescribeStopwordOptions',
describe_synonym_options(self,
('describe_synonym_options_response',
'describe_synonym_options_result',
'DescribeSynonymOptions',
('index_documents_response',
'index_documents_result',
'field_names')
list_marker='FieldNames')
update_default_search_field(self,
default_search_field):
('update_default_search_field_response',
'update_default_search_field_result',
'DefaultSearchField':
default_search_field}
'UpdateDefaultSearchField',
('update_service_access_policies_response',
'update_service_access_policies_result',
{'AccessPolicies':
'UpdateServiceAccessPolicies',
update_stemming_options(self,
stems):
('update_stemming_options_response',
'update_stemming_options_result',
'Stems':
stems}
'UpdateStemmingOptions',
update_stopword_options(self,
stopwords):
('update_stopword_options_response',
'update_stopword_options_result',
'Stopwords':
stopwords}
'UpdateStopwordOptions',
update_synonym_options(self,
synonyms):
('update_synonym_options_response',
'update_synonym_options_result',
'Synonyms':
synonyms}
'UpdateSynonymOptions',
validate_certs=True):
Layer1(
validate_certs=validate_certs)
status['creation_date']
status['state']
status['update_date']
int(status['update_version'])
self.update(json.loads(options))
self._update_status(data['status'])
self._update_options(data['options'])
'UpdateDate':
self.updated
'UpdateVersion':
self.update_from_json_doc(value)
wait_for_state(self,
state:
self.refresh()
RankExpressionStatus(IndexFieldStatus):
"Effect":"Allow",
"Action":"*",
attrs['info']['rid']
self.cpu_time_ms
attrs['info']['cpu-time-ms']
attrs['info']['time-ms']
attrs['rank']
self.match_expression
attrs['match-expr']
'constraints'
values['constraints']))
bq
rank
self.facet_constraints
facet_constraints
self.facet_sort
facet_sort
self.facet_top_n
facet_top_n
self.t
self.bq:
params['bq']
self.rank:
params['rank']
','.join(self.rank)
params['return-fields']
','.join(self.facet)
self.facet_constraints:
six.iteritems(self.facet_constraints):
params['facet-%s-constraints'
self.facet_sort:
six.iteritems(self.facet_sort):
params['facet-%s-sort'
self.facet_top_n:
six.iteritems(self.facet_top_n):
params['facet-%s-top-n'
self.t:
six.iteritems(self.t):
params['t-%s'
"http://%s/2011-02-01/search"
query.to_params()
requests.get(url,
r.content.decode('utf-8')
SourceAttribute(object):
ValidDataFunctions
('Copy',
'TrimTitle',
'Map')
self.data_copy
self.ValidDataFunctions[0]
self.data_map
self.data_trim_title
data_function(self):
@data_function.setter
data_function(self,
self.ValidDataFunctions:
'|'.join(self.ValidDataFunctions)
ValueError('data_function
valid)
connection_cls=boto.cloudsearch2.layer1.CloudSearchConnection
self.domain.layer1.get_proxy_url_with_auth()}
_id):
_id}
_commit_with_auth(self,
self.domain_connection.upload_documents(sdf,
_commit_without_auth(self,
"http://%s/%s/documents/batch"
session.proxies
self._commit_with_auth(sdf,
self._commit_without_auth(sdf,
signed_request=self.sign_request)
signed_request=False):
self.signed_request
signed_request
Content:\n{0}'
'\n\nSDF:\n{1}'.format(_body,
boto.log.debug(self.response)
boto.log.debug(self.response.content)
{2}'
response_num)
exc.errors
ExpressionStatus
AvailabilityOptionsStatus
ScalingParametersStatus
data['Created']
data['Deleted']
data['Processing']
data['RequiresIndexDocuments']
data['DomainId']
data['DomainName']
data['SearchInstanceCount']
data.get('SearchInstanceType',
data['SearchPartitionCount']
data['DocService']
data['ARN']
data['SearchService']
service_arn(self):
self._doc_service['Endpoint']
self._search_service['Endpoint']
get_analysis_schemes(self):
self.layer1.describe_analysis_schemes(self.name)
get_availability_options(self):
AvailabilityOptionsStatus(
refresh_fn=self.layer1.describe_availability_options,
refresh_key=['DescribeAvailabilityOptionsResponse',
'DescribeAvailabilityOptionsResult',
'AvailabilityOptions'],
save_fn=self.layer1.update_availability_options)
get_scaling_options(self):
ScalingParametersStatus(
refresh_fn=self.layer1.describe_scaling_parameters,
refresh_key=['DescribeScalingParametersResponse',
'DescribeScalingParametersResult',
'ScalingParameters'],
save_fn=self.layer1.update_scaling_parameters)
ServicePoliciesStatus(
refresh_fn=self.layer1.describe_service_access_policies,
refresh_key=['DescribeServiceAccessPoliciesResponse',
'DescribeServiceAccessPoliciesResult',
'AccessPolicies'],
save_fn=self.layer1.update_service_access_policies)
(data['DescribeIndexFieldsResponse']
['DescribeIndexFieldsResult']
['IndexFields'])
returnable=False,
sortable=False,
highlight=False,
source_field=None,
analysis_scheme=None):
'IndexFieldType':
index['LiteralOptions']
index['LiteralOptions']['DefaultValue']
index['LiteralOptions']['SourceField']
'literal-array':
index['LiteralArrayOptions']
index['LiteralArrayOptions']['DefaultValue']
index['LiteralArrayOptions']['SourceFields']
'int':
index['IntOptions']
index['IntOptions']['DefaultValue']
index['IntOptions']['SourceField']
'int-array':
index['IntArrayOptions']
index['IntArrayOptions']['DefaultValue']
index['IntArrayOptions']['SourceFields']
'date':
index['DateOptions']
index['DateOptions']['DefaultValue']
index['DateOptions']['SourceField']
'date-array':
index['DateArrayOptions']
index['DateArrayOptions']['DefaultValue']
index['DateArrayOptions']['SourceFields']
'double':
index['DoubleOptions']
index['DoubleOptions']['DefaultValue']
index['DoubleOptions']['SourceField']
'double-array':
index['DoubleArrayOptions']
index['DoubleArrayOptions']['DefaultValue']
index['DoubleArrayOptions']['SourceFields']
index['TextOptions']
highlight,
index['TextOptions']['DefaultValue']
index['TextOptions']['SourceField']
index['TextOptions']['AnalysisScheme']
'text-array':
index['TextArrayOptions']
index['TextArrayOptions']['DefaultValue']
index['TextArrayOptions']['SourceFields']
index['TextArrayOptions']['AnalysisScheme']
'latlon':
index['LatLonOptions']
index['LatLonOptions']['DefaultValue']
index['LatLonOptions']['SourceField']
(data['DefineIndexFieldResponse']
['DefineIndexFieldResult']
['IndexField'])
get_expressions(self,
names=None):
self.layer1.describe_expressions
(data['DescribeExpressionsResponse']
['DescribeExpressionsResult']
['Expressions'])
[ExpressionStatus(self,
create_expression(self,
self.layer1.define_expression(self.name,
(data['DefineExpressionResponse']
['DefineExpressionResult']
['Expression'])
ExpressionStatus(self,
self.layer1.describe_expressions)
InvalidTypeException(BotoServerError):
InternalException(BotoServerError):
DisabledOperationException(BotoServerError):
BaseException(BotoServerError):
CloudSearchConnection(AWSQueryConnection):
"InvalidTypeException":
exceptions.InvalidTypeException,
"InternalException":
exceptions.InternalException,
"DisabledOperationException":
exceptions.DisabledOperationException,
"BaseException":
exceptions.BaseException,
kwargs.pop('sign_request',
super(CloudSearchConnection,
build_suggesters(self,
action='BuildSuggesters',
action='CreateDomain',
define_analysis_scheme(self,
analysis_scheme):
'AnalysisScheme',
analysis_scheme)
action='DefineAnalysisScheme',
define_expression(self,
'Expression',
action='DefineExpression',
index_field):
'IndexField',
index_field)
action='DefineIndexField',
define_suggester(self,
suggester):
'Suggester',
suggester)
action='DefineSuggester',
delete_analysis_scheme(self,
analysis_scheme_name):
'AnalysisSchemeName':
analysis_scheme_name,
action='DeleteAnalysisScheme',
action='DeleteDomain',
delete_expression(self,
expression_name):
'ExpressionName':
expression_name,
action='DeleteExpression',
index_field_name):
index_field_name,
action='DeleteIndexField',
delete_suggester(self,
suggester_name):
'SuggesterName':
suggester_name,
action='DeleteSuggester',
describe_analysis_schemes(self,
analysis_scheme_names=None,
analysis_scheme_names
analysis_scheme_names,
'AnalysisSchemeNames.member')
action='DescribeAnalysisSchemes',
describe_availability_options(self,
action='DescribeAvailabilityOptions',
domain_names
domain_names,
'DomainNames.member')
action='DescribeDomains',
describe_expressions(self,
expression_names=None,
expression_names
expression_names,
'ExpressionNames.member')
action='DescribeExpressions',
field_names=None,
field_names
field_names,
'FieldNames.member')
action='DescribeIndexFields',
describe_scaling_parameters(self,
action='DescribeScalingParameters',
action='DescribeServiceAccessPolicies',
describe_suggesters(self,
suggester_names=None,
suggester_names
suggester_names,
'SuggesterNames.member')
action='DescribeSuggesters',
action='IndexDocuments',
list_domain_names(self):
action='ListDomainNames',
update_availability_options(self,
multi_az):
action='UpdateAvailabilityOptions',
update_scaling_parameters(self,
scaling_parameters):
'ScalingParameters',
scaling_parameters)
action='UpdateScalingParameters',
'AccessPolicies':
action='UpdateServiceAccessPolicies',
build_complex_param(self,
k2,
v.items():
sign_request=False):
boto.cloudsearch2.regions():
CloudSearchConnection(
sign_request=sign_request)
(domain_data['DescribeDomainsResponse']
['DescribeDomainsResult']
['DomainStatusList'])
data['CreateDomainResponse']
refresh_key=None,
self.refresh_key
refresh_key
status['CreationDate']
status['State']
status['UpdateDate']
int(status['UpdateVersion'])
data[key]
self._update_status(data['Status'])
self._update_options(data['Options'])
AvailabilityOptionsStatus(OptionStatus):
ScalingParametersStatus(IndexFieldStatus):
ExpressionStatus(IndexFieldStatus):
"*",
SIMPLE
STRUCTURED
'structured'
LUCENE
'lucene'
DISMAX
'dismax'
attrs['status']['rid']
attrs['status']['time-ms']
'buckets'
values.get('buckets',
fq
self.expr
self.sort
self.highlight
params['q.parser']
params['fq']
params['expr.%s'
params['facet.%s'
params['highlight.%s'
params['q.options']
params['return']
to_domain_connection_params(self):
params['query_parser']
params['filter_query']
expr['expr.%s'
params['expr']
facet['facet.%s'
highlight['highlight.%s'
params['highlight']
params['query_options']
params['ret']
self.session.proxies['http']
self.domain.layer1.get_proxy_url_with_auth()
expr=rank,
_search_with_auth(self,
self.domain_connection.search(params.pop("q",
""),
_search_without_auth(self,
"http://%s/%s/search"
self.session.get(url,
resp.content.decode('utf-8'),
'status_code':
resp.status_code}
self._search_with_auth(query.to_domain_connection_params())
self._search_without_auth(query.to_params(),
r['body']
r['status_code']
_body)
_body,
get_regions('cloudsearchdomain',
connection_cls=CloudSearchDomainConnection)
SearchException(BotoServerError):
DocumentServiceException(BotoServerError):
boto.cloudsearchdomain
CloudSearchDomainConnection(AWSAuthConnection):
"SearchException":
exceptions.SearchException,
"DocumentServiceException":
exceptions.DocumentServiceException,
'CloudSearchDomainConnection
'specific
domain\'s
'requests
CloudSearch
Domain.'
super(CloudSearchDomainConnection,
cursor=None,
filter_query=None,
query_options=None,
query_parser=None,
ret=None,
start=None):
'/2013-01-01/search'
query_params['cursor']
query_params['expr']
query_params['facet']
query_params['fq']
query_params['highlight']
query_params['partial']
query_params['q.options']
query_params['q.parser']
query_params['return']
query_params['sort']
query_params['start']
suggest(self,
suggester,
'/2013-01-01/suggest'
query_params['suggester']
upload_documents(self,
documents,
content_type):
'/2013-01-01/documents/batch'
data=documents,
JSONResponseError(response.status,
get_regions('cloudtrail',
connection_cls=CloudTrailConnection)
InvalidSnsTopicNameException(BotoServerError):
InvalidS3BucketNameException(BotoServerError):
TrailAlreadyExistsException(BotoServerError):
InsufficientSnsTopicPolicyException(BotoServerError):
InvalidTrailNameException(BotoServerError):
TrailNotFoundException(BotoServerError):
S3BucketDoesNotExistException(BotoServerError):
TrailNotProvidedException(BotoServerError):
InvalidS3PrefixException(BotoServerError):
MaximumNumberOfTrailsExceededException(BotoServerError):
InsufficientS3BucketPolicyException(BotoServerError):
InvalidMaxResultsException(BotoServerError):
InvalidLookupAttributesException(BotoServerError):
InvalidCloudWatchLogsLogGroupArnException(BotoServerError):
InvalidCloudWatchLogsRoleArnException(BotoServerError):
CloudWatchLogsDeliveryUnavailableException(BotoServerError):
CloudTrailConnection(AWSQueryConnection):
"2013-11-01"
"cloudtrail.us-east-1.amazonaws.com"
"CloudTrail"
"com.amazonaws.cloudtrail.v20131101.CloudTrail_20131101"
"InvalidMaxResultsException":
exceptions.InvalidMaxResultsException,
"InvalidSnsTopicNameException":
exceptions.InvalidSnsTopicNameException,
"InvalidS3BucketNameException":
exceptions.InvalidS3BucketNameException,
"TrailAlreadyExistsException":
exceptions.TrailAlreadyExistsException,
"InvalidLookupAttributesException":
exceptions.InvalidLookupAttributesException,
"InsufficientSnsTopicPolicyException":
exceptions.InsufficientSnsTopicPolicyException,
"InvalidCloudWatchLogsLogGroupArnException":
exceptions.InvalidCloudWatchLogsLogGroupArnException,
"InvalidCloudWatchLogsRoleArnException":
exceptions.InvalidCloudWatchLogsRoleArnException,
"InvalidTrailNameException":
exceptions.InvalidTrailNameException,
"CloudWatchLogsDeliveryUnavailableException":
exceptions.CloudWatchLogsDeliveryUnavailableException,
"TrailNotFoundException":
exceptions.TrailNotFoundException,
"S3BucketDoesNotExistException":
exceptions.S3BucketDoesNotExistException,
"InvalidS3PrefixException":
exceptions.InvalidS3PrefixException,
"MaximumNumberOfTrailsExceededException":
exceptions.MaximumNumberOfTrailsExceededException,
"InsufficientS3BucketPolicyException":
exceptions.InsufficientS3BucketPolicyException,
super(CloudTrailConnection,
create_trail(self,
self.make_request(action='CreateTrail',
delete_trail(self,
self.make_request(action='DeleteTrail',
describe_trails(self,
trail_name_list=None):
params['trailNameList']
self.make_request(action='DescribeTrails',
get_trail_status(self,
self.make_request(action='GetTrailStatus',
lookup_events(self,
lookup_attributes=None,
params['LookupAttributes']
self.make_request(action='LookupEvents',
start_logging(self,
self.make_request(action='StartLogging',
stop_logging(self,
self.make_request(action='StopLogging',
update_trail(self,
s3_bucket_name=None,
params['S3BucketName']
self.make_request(action='UpdateTrail',
get_regions('codedeploy',
connection_cls=CodeDeployConnection)
InvalidDeploymentIdException(BotoServerError):
InvalidDeploymentGroupNameException(BotoServerError):
DeploymentConfigAlreadyExistsException(BotoServerError):
RoleRequiredException(BotoServerError):
DeploymentGroupAlreadyExistsException(BotoServerError):
DeploymentConfigLimitExceededException(BotoServerError):
InvalidDeploymentConfigNameException(BotoServerError):
InvalidSortByException(BotoServerError):
InstanceDoesNotExistException(BotoServerError):
InvalidMinimumHealthyHostValueException(BotoServerError):
ApplicationLimitExceededException(BotoServerError):
ApplicationNameRequiredException(BotoServerError):
InvalidEC2TagException(BotoServerError):
DeploymentDoesNotExistException(BotoServerError):
DeploymentLimitExceededException(BotoServerError):
InvalidInstanceStatusException(BotoServerError):
RevisionRequiredException(BotoServerError):
InvalidBucketNameFilterException(BotoServerError):
DeploymentGroupLimitExceededException(BotoServerError):
DeploymentGroupDoesNotExistException(BotoServerError):
DeploymentConfigNameRequiredException(BotoServerError):
DeploymentAlreadyCompletedException(BotoServerError):
RevisionDoesNotExistException(BotoServerError):
DeploymentGroupNameRequiredException(BotoServerError):
DeploymentIdRequiredException(BotoServerError):
DeploymentConfigDoesNotExistException(BotoServerError):
BucketNameFilterRequiredException(BotoServerError):
ApplicationDoesNotExistException(BotoServerError):
InvalidRevisionException(BotoServerError):
InvalidSortOrderException(BotoServerError):
InvalidOperationException(BotoServerError):
InvalidAutoScalingGroupException(BotoServerError):
InvalidApplicationNameException(BotoServerError):
DescriptionTooLongException(BotoServerError):
ApplicationAlreadyExistsException(BotoServerError):
InvalidDeployedStateFilterException(BotoServerError):
DeploymentNotStartedException(BotoServerError):
DeploymentConfigInUseException(BotoServerError):
InstanceIdRequiredException(BotoServerError):
InvalidKeyPrefixFilterException(BotoServerError):
InvalidDeploymentStatusException(BotoServerError):
boto.codedeploy
CodeDeployConnection(AWSQueryConnection):
"2014-10-06"
"codedeploy.us-east-1.amazonaws.com"
"codedeploy"
"CodeDeploy_20141006"
"InvalidDeploymentIdException":
exceptions.InvalidDeploymentIdException,
"InvalidDeploymentGroupNameException":
exceptions.InvalidDeploymentGroupNameException,
"DeploymentConfigAlreadyExistsException":
exceptions.DeploymentConfigAlreadyExistsException,
"RoleRequiredException":
exceptions.RoleRequiredException,
"DeploymentGroupAlreadyExistsException":
exceptions.DeploymentGroupAlreadyExistsException,
"DeploymentConfigLimitExceededException":
exceptions.DeploymentConfigLimitExceededException,
"InvalidDeploymentConfigNameException":
exceptions.InvalidDeploymentConfigNameException,
"InvalidSortByException":
exceptions.InvalidSortByException,
"InstanceDoesNotExistException":
exceptions.InstanceDoesNotExistException,
"InvalidMinimumHealthyHostValueException":
exceptions.InvalidMinimumHealthyHostValueException,
"ApplicationLimitExceededException":
exceptions.ApplicationLimitExceededException,
"ApplicationNameRequiredException":
exceptions.ApplicationNameRequiredException,
"InvalidEC2TagException":
exceptions.InvalidEC2TagException,
"DeploymentDoesNotExistException":
exceptions.DeploymentDoesNotExistException,
"DeploymentLimitExceededException":
exceptions.DeploymentLimitExceededException,
"InvalidInstanceStatusException":
exceptions.InvalidInstanceStatusException,
"RevisionRequiredException":
exceptions.RevisionRequiredException,
"InvalidBucketNameFilterException":
exceptions.InvalidBucketNameFilterException,
"DeploymentGroupLimitExceededException":
exceptions.DeploymentGroupLimitExceededException,
"DeploymentGroupDoesNotExistException":
exceptions.DeploymentGroupDoesNotExistException,
"DeploymentConfigNameRequiredException":
exceptions.DeploymentConfigNameRequiredException,
"DeploymentAlreadyCompletedException":
exceptions.DeploymentAlreadyCompletedException,
"RevisionDoesNotExistException":
exceptions.RevisionDoesNotExistException,
"DeploymentGroupNameRequiredException":
exceptions.DeploymentGroupNameRequiredException,
"DeploymentIdRequiredException":
exceptions.DeploymentIdRequiredException,
"DeploymentConfigDoesNotExistException":
exceptions.DeploymentConfigDoesNotExistException,
"BucketNameFilterRequiredException":
exceptions.BucketNameFilterRequiredException,
"ApplicationDoesNotExistException":
exceptions.ApplicationDoesNotExistException,
"InvalidRevisionException":
exceptions.InvalidRevisionException,
"InvalidSortOrderException":
exceptions.InvalidSortOrderException,
"InvalidOperationException":
exceptions.InvalidOperationException,
"InvalidAutoScalingGroupException":
exceptions.InvalidAutoScalingGroupException,
"InvalidApplicationNameException":
exceptions.InvalidApplicationNameException,
"DescriptionTooLongException":
exceptions.DescriptionTooLongException,
"ApplicationAlreadyExistsException":
exceptions.ApplicationAlreadyExistsException,
"InvalidDeployedStateFilterException":
exceptions.InvalidDeployedStateFilterException,
"DeploymentNotStartedException":
exceptions.DeploymentNotStartedException,
"DeploymentConfigInUseException":
exceptions.DeploymentConfigInUseException,
"InstanceIdRequiredException":
exceptions.InstanceIdRequiredException,
"InvalidKeyPrefixFilterException":
exceptions.InvalidKeyPrefixFilterException,
"InvalidDeploymentStatusException":
exceptions.InvalidDeploymentStatusException,
super(CodeDeployConnection,
batch_get_applications(self,
params['applicationNames']
self.make_request(action='BatchGetApplications',
batch_get_deployments(self,
params['deploymentIds']
self.make_request(action='BatchGetDeployments',
self.make_request(action='CreateApplication',
revision=None,
ignore_application_stop_failures=None):
params['revision']
params['ignoreApplicationStopFailures']
create_deployment_config(self,
minimum_healthy_hosts=None):
params['minimumHealthyHosts']
self.make_request(action='CreateDeploymentConfig',
create_deployment_group(self,
self.make_request(action='CreateDeploymentGroup',
self.make_request(action='DeleteApplication',
delete_deployment_config(self,
self.make_request(action='DeleteDeploymentConfig',
delete_deployment_group(self,
self.make_request(action='DeleteDeploymentGroup',
get_application(self,
self.make_request(action='GetApplication',
get_application_revision(self,
revision):
self.make_request(action='GetApplicationRevision',
get_deployment(self,
self.make_request(action='GetDeployment',
get_deployment_config(self,
self.make_request(action='GetDeploymentConfig',
get_deployment_group(self,
self.make_request(action='GetDeploymentGroup',
get_deployment_instance(self,
'deploymentId':
self.make_request(action='GetDeploymentInstance',
list_application_revisions(self,
s_3_bucket=None,
s_3_key_prefix=None,
deployed=None,
params['sortBy']
params['sortOrder']
params['s3Bucket']
params['s3KeyPrefix']
params['deployed']
self.make_request(action='ListApplicationRevisions',
list_applications(self,
self.make_request(action='ListApplications',
list_deployment_configs(self,
self.make_request(action='ListDeploymentConfigs',
list_deployment_groups(self,
self.make_request(action='ListDeploymentGroups',
list_deployment_instances(self,
instance_status_filter=None):
params['instanceStatusFilter']
self.make_request(action='ListDeploymentInstances',
list_deployments(self,
include_only_statuses=None,
create_time_range=None,
params['includeOnlyStatuses']
params['createTimeRange']
self.make_request(action='ListDeployments',
register_application_revision(self,
self.make_request(action='RegisterApplicationRevision',
stop_deployment(self,
self.make_request(action='StopDeployment',
new_application_name=None):
params['newApplicationName']
self.make_request(action='UpdateApplication',
update_deployment_group(self,
new_deployment_group_name=None,
'currentDeploymentGroupName':
params['newDeploymentGroupName']
self.make_request(action='UpdateDeploymentGroup',
get_regions('cognito-identity',
connection_cls=CognitoIdentityConnection)
DeveloperUserAlreadyRegisteredException(BotoServerError):
boto.cognito.identity
CognitoIdentityConnection(AWSQueryConnection):
"cognito-identity.us-east-1.amazonaws.com"
"CognitoIdentity"
"AWSCognitoIdentityService"
"DeveloperUserAlreadyRegisteredException":
exceptions.DeveloperUserAlreadyRegisteredException,
super(CognitoIdentityConnection,
create_identity_pool(self,
self.make_request(action='CreateIdentityPool',
delete_identity_pool(self,
self.make_request(action='DeleteIdentityPool',
describe_identity_pool(self,
self.make_request(action='DescribeIdentityPool',
get_id(self,
'AccountId':
self.make_request(action='GetId',
get_open_id_token(self,
{'IdentityId':
self.make_request(action='GetOpenIdToken',
get_open_id_token_for_developer_identity(self,
token_duration=None):
params['TokenDuration']
self.make_request(action='GetOpenIdTokenForDeveloperIdentity',
list_identities(self,
self.make_request(action='ListIdentities',
list_identity_pools(self,
{'MaxResults':
self.make_request(action='ListIdentityPools',
lookup_developer_identity(self,
developer_user_identifier=None,
params['DeveloperUserIdentifier']
self.make_request(action='LookupDeveloperIdentity',
merge_developer_identities(self,
'SourceUserIdentifier':
'DestinationUserIdentifier':
self.make_request(action='MergeDeveloperIdentities',
unlink_developer_identity(self,
developer_user_identifier):
'DeveloperUserIdentifier':
developer_user_identifier,
self.make_request(action='UnlinkDeveloperIdentity',
unlink_identity(self,
logins_to_remove):
'LoginsToRemove':
logins_to_remove,
self.make_request(action='UnlinkIdentity',
update_identity_pool(self,
self.make_request(action='UpdateIdentityPool',
get_regions('cognito-sync',
connection_cls=CognitoSyncConnection)
InvalidConfigurationException(BotoServerError):
boto.cognito.sync
CognitoSyncConnection(AWSAuthConnection):
"cognito-sync.us-east-1.amazonaws.com"
"InvalidConfigurationException":
exceptions.InvalidConfigurationException,
super(CognitoSyncConnection,
delete_dataset(self,
describe_dataset(self,
describe_identity_pool_usage(self,
'/identitypools/{0}'.format(identity_pool_id)
describe_identity_usage(self,
identity_id):
'/identitypools/{0}/identities/{1}'.format(
get_identity_pool_configuration(self,
list_datasets(self,
'/identitypools/{0}/identities/{1}/datasets'.format(
list_identity_pool_usage(self,
'/identitypools'
list_records(self,
last_sync_count=None,
sync_session_token=None):
'/identitypools/{0}/identities/{1}/datasets/{2}/records'.format(
query_params['lastSyncCount']
query_params['syncSessionToken']
register_device(self,
'/identitypools/{0}/identity/{1}/device'.format(
{'Platform':
set_identity_pool_configuration(self,
push_sync=None):
params['PushSync']
subscribe_to_dataset(self,
unsubscribe_from_dataset(self,
update_records(self,
device_id=None,
record_patches=None,
client_context=None):
{'SyncSessionToken':
params['DeviceId']
params['RecordPatches']
get_regions('configservice',
connection_cls=ConfigServiceConnection)
InvalidLimitException(BotoServerError):
NoSuchBucketException(BotoServerError):
InvalidSNSTopicARNException(BotoServerError):
ResourceNotDiscoveredException(BotoServerError):
MaxNumberOfDeliveryChannelsExceededException(BotoServerError):
LastDeliveryChannelDeleteFailedException(BotoServerError):
InsufficientDeliveryPolicyException(BotoServerError):
NoSuchDeliveryChannelException(BotoServerError):
NoSuchConfigurationRecorderException(BotoServerError):
InvalidS3KeyPrefixException(BotoServerError):
InvalidDeliveryChannelNameException(BotoServerError):
NoRunningConfigurationRecorderException(BotoServerError):
ValidationException(BotoServerError):
NoAvailableConfigurationRecorderException(BotoServerError):
InvalidConfigurationRecorderNameException(BotoServerError):
NoAvailableDeliveryChannelException(BotoServerError):
MaxNumberOfConfigurationRecordersExceededException(BotoServerError):
boto.configservice
ConfigServiceConnection(AWSQueryConnection):
"2014-11-12"
"config.us-east-1.amazonaws.com"
"ConfigService"
"StarlingDoveService"
"InvalidLimitException":
exceptions.InvalidLimitException,
"NoSuchBucketException":
exceptions.NoSuchBucketException,
"InvalidSNSTopicARNException":
exceptions.InvalidSNSTopicARNException,
"ResourceNotDiscoveredException":
exceptions.ResourceNotDiscoveredException,
"MaxNumberOfDeliveryChannelsExceededException":
exceptions.MaxNumberOfDeliveryChannelsExceededException,
"LastDeliveryChannelDeleteFailedException":
exceptions.LastDeliveryChannelDeleteFailedException,
"InsufficientDeliveryPolicyException":
exceptions.InsufficientDeliveryPolicyException,
"NoSuchDeliveryChannelException":
exceptions.NoSuchDeliveryChannelException,
"NoSuchConfigurationRecorderException":
exceptions.NoSuchConfigurationRecorderException,
"InvalidS3KeyPrefixException":
exceptions.InvalidS3KeyPrefixException,
"InvalidDeliveryChannelNameException":
exceptions.InvalidDeliveryChannelNameException,
"NoRunningConfigurationRecorderException":
exceptions.NoRunningConfigurationRecorderException,
"NoAvailableConfigurationRecorderException":
exceptions.NoAvailableConfigurationRecorderException,
"InvalidConfigurationRecorderNameException":
exceptions.InvalidConfigurationRecorderNameException,
"NoAvailableDeliveryChannelException":
exceptions.NoAvailableDeliveryChannelException,
"MaxNumberOfConfigurationRecordersExceededException":
exceptions.MaxNumberOfConfigurationRecordersExceededException,
super(ConfigServiceConnection,
delete_delivery_channel(self,
{'DeliveryChannelName':
self.make_request(action='DeleteDeliveryChannel',
deliver_config_snapshot(self,
{'deliveryChannelName':
self.make_request(action='DeliverConfigSnapshot',
describe_configuration_recorder_status(self,
self.make_request(action='DescribeConfigurationRecorderStatus',
describe_configuration_recorders(self,
self.make_request(action='DescribeConfigurationRecorders',
describe_delivery_channel_status(self,
self.make_request(action='DescribeDeliveryChannelStatus',
describe_delivery_channels(self,
self.make_request(action='DescribeDeliveryChannels',
get_resource_config_history(self,
later_time=None,
earlier_time=None,
chronological_order=None,
params['laterTime']
params['earlierTime']
params['chronologicalOrder']
self.make_request(action='GetResourceConfigHistory',
put_configuration_recorder(self,
configuration_recorder):
{'ConfigurationRecorder':
configuration_recorder,
self.make_request(action='PutConfigurationRecorder',
put_delivery_channel(self,
delivery_channel):
{'DeliveryChannel':
delivery_channel,
self.make_request(action='PutDeliveryChannel',
start_configuration_recorder(self,
self.make_request(action='StartConfigurationRecorder',
stop_configuration_recorder(self,
self.make_request(action='StopConfigurationRecorder',
yaml
YAMLMessage(Message):
super(YAMLMessage,
yaml.safe_load(body)
yaml.dump(self.data)
boto.datapipeline.layer1
DataPipelineConnection
get_regions('datapipeline',
connection_cls=DataPipelineConnection)
PipelineDeletedException(JSONResponseError):
InvalidRequestException(JSONResponseError):
TaskNotFoundException(JSONResponseError):
PipelineNotFoundException(JSONResponseError):
InternalServiceError(JSONResponseError):
DataPipelineConnection(AWSQueryConnection):
"2012-10-29"
"datapipeline.us-east-1.amazonaws.com"
"PipelineDeletedException":
exceptions.PipelineDeletedException,
"TaskNotFoundException":
exceptions.TaskNotFoundException,
"PipelineNotFoundException":
exceptions.PipelineNotFoundException,
"InternalServiceError":
exceptions.InternalServiceError,
super(DataPipelineConnection,
activate_pipeline(self,
self.make_request(action='ActivatePipeline',
'uniqueId':
self.make_request(action='CreatePipeline',
self.make_request(action='DeletePipeline',
describe_objects(self,
evaluate_expressions=None):
params['evaluateExpressions']
self.make_request(action='DescribeObjects',
describe_pipelines(self,
pipeline_ids):
{'pipelineIds':
pipeline_ids,
self.make_request(action='DescribePipelines',
evaluate_expression(self,
object_id):
'objectId':
object_id,
'expression':
self.make_request(action='EvaluateExpression',
get_pipeline_definition(self,
version=None):
params['version']
self.make_request(action='GetPipelineDefinition',
self.make_request(action='ListPipelines',
poll_for_task(self,
{'workerGroup':
params['instanceIdentity']
self.make_request(action='PollForTask',
put_pipeline_definition(self,
self.make_request(action='PutPipelineDefinition',
query_objects(self,
'sphere':
params['query']
self.make_request(action='QueryObjects',
report_task_progress(self,
task_id):
self.make_request(action='ReportTaskProgress',
report_task_runner_heartbeat(self,
worker_group=None,
hostname=None):
{'taskrunnerId':
params['workerGroup']
self.make_request(action='ReportTaskRunnerHeartbeat',
set_status(self,
self.make_request(action='SetStatus',
set_task_status(self,
error_id=None,
error_message=None,
error_stack_trace=None):
'taskStatus':
params['errorId']
params['errorMessage']
params['errorStackTrace']
self.make_request(action='SetTaskStatus',
validate_pipeline_definition(self,
self.make_request(action='ValidatePipelineDefinition',
get_regions('directconnect',
connection_cls=DirectConnectConnection)
DirectConnectClientException(Exception):
DirectConnectServerException(Exception):
boto.directconnect
DirectConnectConnection(AWSQueryConnection):
"2012-10-25"
"directconnect.us-east-1.amazonaws.com"
"DirectConnect"
"OvertureService"
"DirectConnectClientException":
exceptions.DirectConnectClientException,
"DirectConnectServerException":
exceptions.DirectConnectServerException,
super(DirectConnectConnection,
allocate_connection_on_interconnect(self,
vlan):
'interconnectId':
'vlan':
vlan,
self.make_request(action='AllocateConnectionOnInterconnect',
allocate_private_virtual_interface(self,
new_private_virtual_interface_allocation):
'newPrivateVirtualInterfaceAllocation':
new_private_virtual_interface_allocation,
self.make_request(action='AllocatePrivateVirtualInterface',
allocate_public_virtual_interface(self,
new_public_virtual_interface_allocation):
'newPublicVirtualInterfaceAllocation':
new_public_virtual_interface_allocation,
self.make_request(action='AllocatePublicVirtualInterface',
confirm_connection(self,
self.make_request(action='ConfirmConnection',
confirm_private_virtual_interface(self,
virtual_gateway_id):
'virtualInterfaceId':
'virtualGatewayId':
virtual_gateway_id,
self.make_request(action='ConfirmPrivateVirtualInterface',
confirm_public_virtual_interface(self,
self.make_request(action='ConfirmPublicVirtualInterface',
create_connection(self,
connection_name):
self.make_request(action='CreateConnection',
create_interconnect(self,
location):
'interconnectName':
self.make_request(action='CreateInterconnect',
create_private_virtual_interface(self,
new_private_virtual_interface):
'newPrivateVirtualInterface':
new_private_virtual_interface,
self.make_request(action='CreatePrivateVirtualInterface',
create_public_virtual_interface(self,
new_public_virtual_interface):
'newPublicVirtualInterface':
new_public_virtual_interface,
self.make_request(action='CreatePublicVirtualInterface',
delete_connection(self,
self.make_request(action='DeleteConnection',
delete_interconnect(self,
self.make_request(action='DeleteInterconnect',
delete_virtual_interface(self,
self.make_request(action='DeleteVirtualInterface',
describe_connections(self,
connection_id=None):
self.make_request(action='DescribeConnections',
describe_connections_on_interconnect(self,
self.make_request(action='DescribeConnectionsOnInterconnect',
describe_interconnects(self,
interconnect_id=None):
params['interconnectId']
self.make_request(action='DescribeInterconnects',
describe_locations(self):
self.make_request(action='DescribeLocations',
describe_virtual_gateways(self):
self.make_request(action='DescribeVirtualGateways',
describe_virtual_interfaces(self,
connection_id=None,
virtual_interface_id=None):
params['virtualInterfaceId']
self.make_request(action='DescribeVirtualInterfaces',
connection_cls=boto.dynamodb.layer2.Layer2)
Batch(object):
key_list.append(k)
batch_dict['Keys']
self.attributes_to_get:
batch_dict['AttributesToGet']
self.consistent_read:
BatchWrite(object):
self.puts
puts
deletes
op_list
self.puts:
{'Item':
self.table.layer2.dynamize_item(item)}
{'PutRequest':
d}
op_list.append(d)
self.deletes:
{'Key':
k}
op_list.append({'DeleteRequest':
(self.table.name,
op_list)
BatchList(list):
self.append(Batch(table,
consistent_read))
resubmit(self):
self[:]
self.unprocessed:
table_req
six.iteritems(self.unprocessed):
table_keys
table_req['Keys']
self.layer2.get_table(table_name)
table_keys:
key['HashKeyElement']
key['RangeKeyElement']
keys.append((h,
'AttributesToGet'
table_req:
table_req['AttributesToGet']
self.add_batch(table,
attributes_to_get=attributes_to_get)
self.submit()
self.layer2.batch_get_item(self)
'UnprocessedKeys'
res['UnprocessedKeys']
b['Keys']:
d[batch.table.name]
BatchWriteList(list):
self.append(BatchWrite(table,
puts,
deletes))
self.layer2.batch_write_item(self)
d[table_name]
dynamize_value
Condition(object):
Condition):
self.to_dict()
other.to_dict()
ConditionNoArgs(Condition):
{'ComparisonOperator':
ConditionOneArg(Condition):
v1):
self.v1)
[dynamize_value(self.v1)],
ConditionTwoArgs(Condition):
v2):
self.v2
'%s(%s,
self.v1,
(self.v1,
values],
ConditionSeveralArgs(Condition):
self.values
'.join(self.values))
self.values],
EQ(ConditionOneArg):
NE(ConditionOneArg):
LE(ConditionOneArg):
LT(ConditionOneArg):
GE(ConditionOneArg):
GT(ConditionOneArg):
NULL(ConditionNoArgs):
NOT_NULL(ConditionNoArgs):
CONTAINS(ConditionOneArg):
NOT_CONTAINS(ConditionOneArg):
IN(ConditionSeveralArgs):
BETWEEN(ConditionTwoArgs):
DynamoDBExpiredTokenError(BotoServerError):
DynamoDBKeyNotFoundError(BotoClientError):
DynamoDBItemError(BotoClientError):
DynamoDBNumberError(BotoClientError):
DynamoDBConditionalCheckFailedError(DynamoDBResponseError):
DynamoDBValidationError(DynamoDBResponseError):
DynamoDBThroughputExceededError(DynamoDBResponseError):
DynamoDBItemError
self.table.schema.hash_key_name
self.table.schema.range_key_name
attrs.get(self._hash_key_name,
attrs.get(self._range_key_name,
self[self._range_key_name]
attrs.items():
hash_key(self):
range_key(self):
self.get(self._range_key_name)
add_attribute(self,
("ADD",
delete_attribute(self,
attr_value=None):
("DELETE",
put_attribute(self,
("PUT",
self.table.layer2.update_item(self,
self.table.layer2.delete_item(self,
self.table.layer2.put_item(self,
self.put_attribute(key,
self.delete_attribute(key)
self.__dict__.update(d)
'DynamoDB'
'20111205'
ThruputError
"ProvisionedThroughputExceededException"
SessionExpiredError
'com.amazon.coral.service#ExpiredTokenException'
ConditionalCheckFailedError
ValidationError
validate_checksums=True,
boto.dynamodb.regions():
_get_session_token(self):
Provider(self._provider_type)
self._auth_handler.update_provider(self.provider)
'%s_%s.%s'
elapsed
(time.time()
response.getheader('x-amzn-RequestId')
boto.log.debug('RequestId:
boto.perflog.debug('%s:
id=%s
time=%sms',
headers['X-Amz-Target'],
int(elapsed))
self.ThruputError
(self.ThruputError,
self._exponential_time(i)
dynamodb_exceptions.DynamoDBThroughputExceededError(
self.SessionExpiredError
'Renewing
Token'
self._get_session_token()
self.ConditionalCheckFailedError
dynamodb_exceptions.DynamoDBConditionalCheckFailedError(
self.ValidationError
dynamodb_exceptions.DynamoDBValidationError(
response.read().decode('utf-8'))
self._exponential_time(i))
_exponential_time(self,
start_table=None):
data['ExclusiveStartTableName']
self.make_request('ListTables',
self.make_request('DescribeTable',
self.make_request('CreateTable',
self.make_request('UpdateTable',
self.make_request('DeleteTable',
self.make_request('GetItem',
dynamodb_exceptions.DynamoDBKeyNotFoundError(
"Key
exist."
request_items:
self.make_request('BatchGetItem',
self.make_request('BatchWriteItem',
item}
self.make_request('PutItem',
attribute_updates,
'AttributeUpdates':
attribute_updates}
self.make_request('UpdateItem',
self.make_request('DeleteItem',
hash_key_value,
range_key_conditions=None,
'HashKeyValue':
hash_key_value}
range_key_conditions:
data['RangeKeyCondition']
range_key_conditions
scan_index_forward:
self.make_request('Query',
data['ScanFilter']
self.make_request('Scan',
BatchList,
BatchWriteList
LossyFloatDynamizer,
NonBooleanDynamizer
TableGenerator(object):
remaining,
kwargs):
self.callable
item_class
self.kwargs
scanned_count(self):
consumed_units(self):
response(self):
next_response(self):
self.kwargs.get('limit')
(self.remaining
(limit
self.remaining)):
self.callable(**self.kwargs)
self._response.get('ConsumedCapacityUnits',
self._response.get('Count',
self._response.get('ScannedCount',
'LastEvaluatedKey'
lek
self._response['LastEvaluatedKey']
self.table.layer2.dynamize_last_evaluated_key(lek)
self.kwargs['exclusive_start_key']
(lek['HashKeyElement'],)
lek:
(lek['RangeKeyElement'],)
response.get('Items',
self.item_class(self.table,
attrs=item)
dynamizer=LossyFloatDynamizer,
dynamizer()
use_decimals(self,
use_boolean=False):
use_boolean
dynamize_attribute_updates(self,
pending_updates):
pending_updates:
pending_updates[attr_name]
action}
self.dynamizer.encode(value)}
dynamize_item(self,
self.dynamizer.encode(item[attr_name])
dynamize_range_key_condition(self,
range_key_condition):
range_key_condition.to_dict()
dynamize_scan_filter(self,
scan_filter):
scan_filter[attr_name]
condition.to_dict()
dynamize_expected_value(self,
expected_value[attr_name]
self.dynamizer.encode(expected_value[attr_name])
dynamize_last_evaluated_key(self,
last_evaluated_key):
last_evaluated_key['HashKeyElement']
self.dynamizer.encode(hash_key)}
last_evaluated_key['RangeKeyElement']
d['RangeKeyElement']
build_key_from_values(self,
self.dynamizer.encode(hash_key)
schema.hash_key_type:
'Hashkey
schema.hash_key_type
dynamodb_key['HashKeyElement']
schema.range_key_type:
'RangeKey
schema.range_key_type
dynamodb_key['RangeKeyElement']
new_batch_list(self):
BatchList(self)
new_batch_write_list(self):
BatchWriteList(self)
min(this_round_limit,
self.layer1.list_tables(limit=this_round_limit,
start_table=start_table)
tables.extend(result.get('TableNames',
result.get('LastEvaluatedTableName',
table_from_schema(self,
Table.create_from_schema(self,
schema)
get_table(self,
get_table
self.layer1.create_table(name,
schema.dict,
self.layer1.update_table(table.name,
self.layer1.delete_table(table.name)
create_schema(self,
hash_key_name,
hash_key_proto_value,
range_key_name=None,
range_key_proto_value=None):
(hash_key_name,
get_dynamodb_type(hash_key_proto_value))
range_key_proto_value
(range_key_name,
get_dynamodb_type(range_key_proto_value))
Schema.create(hash_key,
self.layer1.get_item(table.name,
item_class(table,
response['Item'])
self.layer1.batch_get_item(request_items,
self.layer1.batch_write_item(request_items,
self.layer1.put_item(item.table.name,
self.dynamize_item(item),
attr_updates
self.dynamize_attribute_updates(item._updates)
self.layer1.update_item(item.table.name,
attr_updates,
item._updates.clear()
self.layer1.delete_item(item.table.name,
expected=expected_value,
return_values=return_values,
range_key_condition=None,
range_key_condition:
self.dynamize_range_key_condition(range_key_condition)
'hash_key_value':
self.dynamizer.encode(hash_key),
'range_key_conditions':
rkc,
'scan_index_forward':
scan_index_forward,
self.layer1.query,
'scan_filter':
self.dynamize_scan_filter(scan_filter),
self.layer1.scan,
Schema(object):
schema_dict):
schema_dict
'Schema(%s:%s)'
(self.hash_key_name,
self.range_key_name)
'Schema(%s)'
'HashKeyElement':
hash_key[0],
hash_key[1],
reconstructed['RangeKeyElement']
range_key[0],
range_key[1],
cls(None)
instance._dict
dict(self):
self._dict['HashKeyElement']['AttributeName']
hash_key_type(self):
self._dict['HashKeyElement']['AttributeType']
self._dict['RangeKeyElement']['AttributeName']
range_key_type(self):
self._dict['RangeKeyElement']['AttributeType']
(self.hash_key_name
other.hash_key_name
other.hash_key_type
other.range_key_name
other.range_key_type)
TableBatchGenerator(object):
_queue_unprocessed(self,
u'UnprocessedKeys'
res[u'UnprocessedKeys']:
res[u'UnprocessedKeys'][self.table.name][u'Keys']
key[u'HashKeyElement']
key[u'RangeKeyElement']
u'RangeKeyElement'
self.keys.append((h,
BatchList(self.table.layer2)
batch.add_batch(self.table,
self.keys[:100],
self.attributes_to_get)
batch.submit()
res[u'Responses']:
res[u'Responses'][self.table.name][u'ConsumedCapacityUnits']
res[u'Responses'][self.table.name][u'Items']:
self.keys[100:]
self._queue_unprocessed(res)
create_from_schema(cls,
cls(layer2,
{'Table':
name}})
table._schema
'Table(%s)'
self._dict['TableName']
create_time(self):
self._dict.get('CreationDateTime',
self._dict.get('TableStatus',
item_count(self):
self._dict.get('ItemCount',
size_bytes(self):
self._dict.get('TableSizeBytes',
read_units(self):
self._dict['ProvisionedThroughput']['ReadCapacityUnits']
write_units(self):
self._dict['ProvisionedThroughput']['WriteCapacityUnits']
update_from_response(self,
'Table'
self._dict.update(response['Table'])
'TableDescription'
self._dict.update(response['TableDescription'])
'KeySchema'
Schema(self._dict['KeySchema'])
wait_for_active=False,
retry_seconds=5):
self.layer2.describe_table(self.name)
wait_for_active:
time.sleep(retry_seconds)
self.layer2.update_throughput(self,
self.layer2.delete_table(self)
self.layer2.get_item(self,
item_class)
get_item
self.get_item(hash_key,
range_key=range_key,
attributes_to_get=[hash_key],
dynamodb_exceptions.DynamoDBKeyNotFoundError:
attrs=None,
item_class(self,
self.layer2.query(self,
self.layer2.scan(self,
attributes_to_get=None):
TableBatchGenerator(self,
attributes_to_get)
(Decimal,
DecimalException,
Context,
Clamped,
Underflow,
Rounded)
Mapping
Context(
Emin=-128,
Emax=126,
rounding=None,
prec=38,
traps=[Clamped,
Rounded,
Underflow])
float_to_decimal(f):
f.as_integer_ratio()
numerator,
denominator
Decimal(n),
Decimal(d)
ctx.flags[Inexact]:
ctx.flags[Inexact]
ctx.prec
is_num(n,
boolean_as_int=True):
boolean_as_int:
Decimal,
Decimal)
types)
basestring))
Binary)
serialize_num(val):
str(int(val))
str(val)
convert_num(s):
float(s)
convert_binary(n):
Binary(base64.b64decode(n))
get_dynamodb_type(val,
use_boolean=True):
is_num(val):
use_boolean:
is_str(val):
(set,
frozenset)):
map(is_num,
map(is_str,
map(is_binary,
is_binary(val):
Mapping):
(type(val),
dynamize_value(val):
get_dynamodb_type(val)
serialize_num(val)}
'NS':
list(map(serialize_num,
val))}
'SS':
[n
Binary(val)
val.encode()}
'BS':
[n.encode()
Binary(object):
TypeError('Value
data!')
value.encode("utf-8")
base64.b64encode(self.value).decode('utf-8')
Binary):
other.value
hash(self.value)
Binary(bytes):
base64.b64encode(self).decode('utf-8')
value(self):
bytes(self)
item_object_hook(dct):
len(dct.keys())
dct['S']
convert_num(dct['N'])
set(dct['SS'])
set(map(convert_num,
dct['NS']))
convert_binary(dct['B'])
set(map(convert_binary,
dct['BS']))
Dynamizer(object):
get_dynamodb_type(attr)
self._get_dynamodb_type(attr)
'_encode_%s'
ValueError("Unable
dynamodb_type)
encoder(attr)}
hasattr(Decimal,
'from_float'):
str(float_to_decimal(attr))
str(DYNAMODB_CONTEXT.create_decimal(attr))
('Infinity',
'NaN'))):
TypeError('Infinity
NaN
DecimalException)
numeric
`{1}`\n{2}'.format(
DynamoDBNumberError(msg)
_encode_s(self,
attr.decode('utf-8')
str(attr)
list(map(self._encode_n,
_encode_ss(self,
[self._encode_s(n)
_encode_b(self,
Binary(attr)
attr.encode()
_encode_bs(self,
[self._encode_b(n)
_encode_null(self,
_encode_m(self,
self.encode(v))
_encode_l(self,
[self.encode(i)
len(attr)
is_str(attr):
list(attr.keys())[0]
dynamodb_type.lower()
dynamodb_type:
'_decode_%s'
decoder(attr[dynamodb_type])
DYNAMODB_CONTEXT.create_decimal(attr)
_decode_s(self,
_decode_ss(self,
set(map(self._decode_s,
_decode_b(self,
convert_binary(attr)
_decode_bs(self,
set(map(self._decode_b,
_decode_null(self,
_decode_bool(self,
_decode_m(self,
self.decode(v))
_decode_l(self,
[self.decode(i)
NonBooleanDynamizer(Dynamizer):
get_dynamodb_type(attr,
use_boolean=False)
LossyFloatDynamizer(NonBooleanDynamizer):
serialize_num(attr)
[str(i)
convert_num(attr)
connection_cls=DynamoDBConnection)
ProvisionedThroughputExceededException(JSONResponseError):
ConditionalCheckFailedException(JSONResponseError):
ItemCollectionSizeLimitExceededException(JSONResponseError):
DynamoDBError(Exception):
UnknownSchemaFieldError(DynamoDBError):
UnknownIndexFieldError(DynamoDBError):
UnknownFilterTypeError(DynamoDBError):
QueryError(DynamoDBError):
ItemNotFound(DynamoDBError):
BaseSchemaField(object):
data_type=STRING):
self.attr_type,
HashKey(BaseSchemaField):
RangeKey(BaseSchemaField):
BaseIndexField(object):
parts):
self.parts
definition.append({
part.name,
part.data_type,
key_schema
key_schema.append(part.schema())
self.projection_type,
AllIndex(BaseIndexField):
KeysOnlyIndex(BaseIndexField):
IncludeIndex(BaseIndexField):
kwargs.pop('includes',
schema_data['Projection']['NonKeyAttributes']
GlobalBaseIndexField(BaseIndexField):
schema_data['ProvisionedThroughput']
GlobalAllIndex(GlobalBaseIndexField):
GlobalKeysOnlyIndex(GlobalBaseIndexField):
GlobalIncludeIndex(GlobalBaseIndexField,
IncludeIndex):
IncludeIndex.__init__(self,
kwargs['throughput']
GlobalBaseIndexField.__init__(self,
IncludeIndex.schema(self)
schema_data.update(GlobalBaseIndexField.schema(self))
deepcopy
NEWVALUE(object):
Item(object):
loaded=False):
loaded
table._dynamizer
isinstance(self._data,
self._data._data
self._data.keys()
self._data.values()
self._data.items()
__bool__(self):
bool(self._data)
__nonzero__
__bool__
_determine_alterations(self):
'changes':
orig_keys
set(self._orig_data.keys())
data_keys
set(self._data.keys())
orig_keys.intersection(data_keys):
self._orig_data[key]:
alterations['changes'][key]
data_keys.difference(orig_keys):
alterations['adds'][key]
orig_keys.difference(data_keys):
needs_save(self,
['adds',
'changes',
'deletes']:
len(data[kind]):
mark_clean(self):
mark_dirty(self):
field_value
data.get('Item',
self[field_name]
self._dynamizer.decode(field_value)
get_keys(self):
self.table.get_key_fields()
key_fields:
key_data[key]
get_raw_keys(self):
self.get_keys().items():
raw_key_data[key]
build_expects(self,
fields=None):
list(self._data.keys())
list(self._orig_data.keys())
set(fields)
expects[key]
provided."
self._orig_data.get(key,
current_value:
self._orig_data:
expects[key]['Exists']
expects[key]['Value']
_is_storable(self,
prepare_full(self):
self._data.items():
self._is_storable(value):
prepare_partial(self):
alterations['adds'].items():
alterations['changes'].items():
alterations['deletes']:
partial_save(self):
self.prepare_partial()
fieldname,
key.items():
final_data[fieldname]
fields.remove(fieldname)
self.build_expects(fields=fields)
self.table._update_item(key,
self.needs_save()
overwrite:
self.prepare_full()
self.build_expects()
self.table._put_item(final_data,
self.table.delete_item(**key_data)
DynamoDBConnection(AWSQueryConnection):
"2012-08-10"
"dynamodb.us-east-1.amazonaws.com"
"DynamoDB"
"DynamoDB_20120810"
"ConditionalCheckFailedException":
exceptions.ConditionalCheckFailedException,
"ItemCollectionSizeLimitExceededException":
exceptions.ItemCollectionSizeLimitExceededException,
validate_checksums
kwargs.pop('validate_checksums',
boto.dynamodb2.regions():
super(DynamoDBConnection,
return_consumed_capacity=None):
self.make_request(action='BatchGetItem',
return_item_collection_metrics=None):
self.make_request(action='BatchWriteItem',
local_secondary_indexes=None,
global_secondary_indexes=None):
'AttributeDefinitions':
params['LocalSecondaryIndexes']
params['GlobalSecondaryIndexes']
self.make_request(action='CreateTable',
self.make_request(action='DeleteItem',
self.make_request(action='DeleteTable',
self.make_request(action='DescribeTable',
expression_attribute_names=None):
self.make_request(action='GetItem',
exclusive_start_table_name=None,
params['ExclusiveStartTableName']
self.make_request(action='ListTables',
self.make_request(action='PutItem',
scan_index_forward=None,
'KeyConditions':
params['IndexName']
params['QueryFilter']
params['ScanIndexForward']
self.make_request(action='Query',
params['ScanFilter']
params['TotalSegments']
params['Segment']
self.make_request(action='Scan',
attribute_updates=None,
update_expression=None,
params['AttributeUpdates']
params['UpdateExpression']
self.make_request(action='UpdateItem',
provisioned_throughput=None,
attribute_definitions=None):
params['AttributeDefinitions']
params['ProvisionedThroughput']
params['GlobalSecondaryIndexUpdates']
self.make_request(action='UpdateTable',
'ProvisionedThroughputExceededException'
'ProvisionedThroughputExceededException',
self._truncated_exponential_time(i)
exceptions.ProvisionedThroughputExceededException(
exceptions.ConditionalCheckFailedException(
exceptions.ValidationException(
self._truncated_exponential_time(i))
_truncated_exponential_time(self,
ResultSet(object):
max_page_size=None):
super(ResultSet,
max_page_size
first_key(self):
'exclusive_start_key'
_reset(self):
len(self._results)
self._results_left:
self._results[self._offset]
to_call(self,
the_callable,
callable(the_callable):
supply
called.'
kwargs.pop('limit',
the_callable
kwargs[self.first_key]
self._limit:
new_results
results.get('results',
results.get('last_key',
len(new_results):
BatchGetResultSet(ResultSet):
kwargs.pop('keys',
self._max_batch_get
kwargs.pop('max_batch_get',
super(BatchGetResultSet,
kwargs['keys']
self._keys_left[:self._max_batch_get]
self._keys_left[self._max_batch_get:]
len(results.get('results',
enumerate(results.get('unprocessed_keys',
self._keys_left.insert(offset,
key_data)
self.call_kwargs.get('limit'):
self.call_kwargs['limit']
(NonBooleanDynamizer,
QUERY_OPERATORS,
max_batch_get
_PROJECTION_TYPE_TO_INDEX
global_indexes=dict(
ALL=GlobalAllIndex,
KEYS_ONLY=GlobalKeysOnlyIndex,
INCLUDE=GlobalIncludeIndex,
local_indexes=dict(
ALL=AllIndex,
KEYS_ONLY=KeysOnlyIndex,
INCLUDE=IncludeIndex,
schema=None,
use_boolean(self):
cls(table_name=table_name,
table.schema
table.throughput
table.indexes
table.global_indexes
attr_defs
seen_attrs
table.schema:
raw_schema.append(field.schema())
int(table.throughput['read']),
int(table.throughput['write']),
kwarg_map
'indexes':
'local_secondary_indexes',
'global_indexes':
'global_secondary_indexes',
index_attr
('indexes',
'global_indexes'):
table_indexes
getattr(table,
index_attr)
index_field
raw_indexes.append(index_field.schema())
index_field.parts:
field.name
seen_attrs:
kwargs[kwarg_map[index_attr]]
table.connection.create_table(
table_name=table.table_name,
attribute_definitions=attr_defs,
key_schema=raw_schema,
provisioned_throughput=raw_throughput,
_introspect_schema(self,
raw_schema,
raw_attributes=None):
sane_attributes
sane_attributes[field['AttributeName']]
field['AttributeType']
raw_schema:
sane_attributes.get(field['AttributeName'],
'HASH':
HashKey(field['AttributeName'],
'RANGE':
RangeKey(field['AttributeName'],
exceptions.UnknownSchemaFieldError(
_introspect_all_indexes(self,
map_indexes_projection):
raw_indexes:
'parts':
'ALL':
'KEYS_ONLY':
map_indexes_projection.get('KEYS_ONLY')
'INCLUDE':
map_indexes_projection.get('INCLUDE')
kwargs['includes']
field['Projection']['NonKeyAttributes']
exceptions.UnknownIndexFieldError(
field['IndexName']
kwargs['parts']
self._introspect_schema(field['KeySchema'],
indexes.append(index_klass(name,
_introspect_indexes(self,
raw_indexes):
self._PROJECTION_TYPE_TO_INDEX.get('local_indexes'))
_introspect_global_indexes(self,
raw_global_indexes):
raw_global_indexes,
self._PROJECTION_TYPE_TO_INDEX.get('global_indexes'))
self.connection.describe_table(self.table_name)
self.throughput['read']
int(raw_throughput['ReadCapacityUnits'])
self.throughput['write']
int(raw_throughput['WriteCapacityUnits'])
result['Table'].get('KeySchema',
raw_attributes
result['Table'].get('AttributeDefinitions',
self._introspect_schema(raw_schema,
raw_attributes)
self.indexes:
result['Table'].get('LocalSecondaryIndexes',
self._introspect_indexes(raw_indexes)
raw_global_indexes
result['Table'].get('GlobalSecondaryIndexes',
self._introspect_global_indexes(raw_global_indexes)
global_indexes=None):
provisioned_throughput=data,
'global_indexes
create_global_secondary_index(self,
global_index):
global_index:
gsi_data_attr_def
"Create":
global_index.schema()
attr_def
global_index.parts:
gsi_data_attr_def.append(attr_def.definition())
attribute_definitions=gsi_data_attr_def
global_index
'create_global_secondary_index
delete_global_secondary_index(self,
global_index_name):
global_index_name:
"Delete":
global_index_name
'delete_global_secondary_index
update_global_secondary_index(self,
global_indexes):
'update_global_secondary_index
self.connection.delete_table(self.table_name)
_encode_keys(self,
keys):
keys.items():
item_data
self.connection.get_item(
consistent_read=consistent
item_data:
exceptions.ItemNotFound("Item
couldn't
found."
item.load(item_data)
(JSONResponseError,
exceptions.ItemNotFound):
kwargs[self.schema[x].name]
ret.keys():
data[self.schema[x].name]
item.save(overwrite=overwrite)
_put_item(self,
self.connection.put_item(self.table_name,
self._encode_keys(key)
self.connection.update_item(self.table_name,
self._build_filters(expected,
self.connection.delete_item(self.table_name,
expected=expected,
conditional_operator=conditional_operator)
exceptions.ConditionalCheckFailedException:
get_key_fields(self):
[field.name
self.schema]
batch_write(self):
BatchTable(self)
_build_filters(self,
using=QUERY_OPERATORS):
filter_kwargs
field_and_op,
filter_kwargs.items():
field_bits
field_and_op.split('__')
'__'.join(field_bits[:-1])
using[field_bits[-1]]
exceptions.UnknownFilterTypeError(
"Operator
recognized."
field_bits[-1],
field_and_op
lookup['AttributeValueList']
self._dynamizer.encode(value[0])
self._dynamizer.encode(value[1])
lookup['AttributeValueList'].append(self._dynamizer.encode(val))
set(value)
filters[fieldname]
self.query_2(limit=limit,
index=index,
reverse=reverse,
max_page_size=max_page_size,
**filter_kwargs)
query_2(self,
len(self.schema)
len(filter_kwargs)
len(self.global_indexes):
exceptions.QueryError(
on."
'SPECIFIC_ATTRIBUTES'
'reverse':
reverse,
'consistent':
'query_filter':
results.to_call(self._query,
query_count(self,
key_conditions
built_query_filter
index_name=index,
consistent_read=consistent,
select='COUNT',
key_conditions=key_conditions,
query_filter=built_query_filter,
conditional_operator=conditional_operator,
scan_index_forward=scan_index_forward,
exclusive_start_key=last_evaluated_key
int(raw_results.get('Count',
raw_results.get('LastEvaluatedKey')
_query(self,
'index_name':
reverse:
kwargs['scan_index_forward']
kwargs['key_conditions']
kwargs['query_filter']
'attributes':
results.to_call(self._scan,
_scan(self,
kwargs['scan_filter']
self.connection.scan(
batch_get(self,
BatchGetResultSet(keys=keys,
max_batch_get=self.max_batch_get)
results.to_call(self._batch_get,
attributes=attributes)
_batch_get(self,
consistent:
items[self.table_name]['ConsistentRead']
items[self.table_name]['AttributesToGet']
key_data.items():
items[self.table_name]['Keys'].append(raw_key)
self.connection.batch_get_item(request_items=items)
unprocessed_keys
raw_results['Responses'].get(self.table_name,
raw_unproccessed
raw_results.get('UnprocessedKeys',
raw_unproccessed.get('Keys',
py_key
raw_key.items():
py_key[key]
unprocessed_keys.append(py_key)
unprocessed_keys,
info['Table'].get('ItemCount',
BatchTable(object):
self._unprocessed:
self.resend_unprocessed()
self._to_put.append(data)
self._to_delete.append(kwargs)
should_flush(self):
len(self._to_put)
len(self._to_delete)
25:
self._to_put:
data=put)
item.prepare_full(),
self.table._encode_keys(delete),
handle_unprocessed(self,
len(resp.get('UnprocessedItems',
self.table.table_name
resp['UnprocessedItems'].get(table_name,
unprocessed.
Storing
later."
boto.log.info(msg
len(unprocessed))
self._unprocessed.extend(unprocessed)
resend_unprocessed(self):
"Re-sending
items."
len(self._unprocessed):
self._unprocessed[:25]
self._unprocessed[25:]
boto.log.info("Sending
items"
len(to_resend))
left"
NonBooleanDynamizer,
Dynamizer
NUMBER
BINARY
STRING_SET
NUMBER_SET
BINARY_SET
NULL
BOOLEAN
MAP
LIST
QUERY_OPERATORS
FILTER_OPERATORS
'ne':
'NE',
'nnull':
'NOT_NULL',
'NULL',
'contains':
'CONTAINS',
'ncontains':
'NOT_CONTAINS',
load_regions().get('ec2',
connection_cls=EC2Connection)
'region'
kw_params
isinstance(kw_params['region'],
RegionInfo)\
kw_params['region'].name:
EC2Connection(**kw_params)
get_region(region_name,
Address(EC2Object):
super(Address,
'Address:%s'
'publicIp':
'allocationId':
'associationId':
'networkInterfaceOwnerId':
release(self,
associate(self,
disassociate(self,
self.association_id:
association_id=self.association_id,
AccountAttribute(object):
'attributeValueSet':
AttributeValues()
'attributeName':
AttributeValues(list):
'attributeValue':
VPCAttribute(object):
('enableDnsHostnames',
'enableDnsSupport'):
'enableDnsHostnames':
'enableDnsSupport':
BlockDeviceType(object):
ephemeral_name=None,
no_device=False,
volume_id=None,
attach_time=None,
encrypted=None):
ephemeral_name
attach_time
'virtualname':
'nodevice':
'snapshotid':
'volumesize':
'attachtime':
'volumetype':
EBSBlockDeviceType
BlockDeviceMapping(dict):
['ebs',
'virtualname']:
BlockDeviceType(self)
['device',
'devicename']:
['item',
'member']:
self[self.current_name]
ec2_build_list_params(self,
'%sBlockDeviceMapping'
autoscale_build_list_params(self,
'%sBlockDeviceMappings.member'
'%s.%d'
block_dev
self[dev_name]
block_dev.ephemeral_name:
params['%s.VirtualName'
block_dev.ephemeral_name
block_dev.no_device:
params['%s.NoDevice'
block_dev.snapshot_id:
params['%s.Ebs.SnapshotId'
block_dev.snapshot_id
block_dev.size:
params['%s.Ebs.VolumeSize'
block_dev.size
block_dev.delete_on_termination:
block_dev.volume_type:
params['%s.Ebs.VolumeType'
block_dev.volume_type
params['%s.Ebs.Iops'
block_dev.encrypted
block_dev.encrypted:
BundleInstanceTask(EC2Object):
super(BundleInstanceTask,
'BundleInstanceTask:%s'
'bundleId':
'progress':
'uploadPolicy':
'uploadPolicySignature':
'm2.xlarge',
'm2.4xlarge',
'cc1.4xlarge',
't1.micro']
BuyReservation(object):
StringProperty(name='region',
Region',
BuyReservation()
obj.get(params)
obj.ec2.get_all_reserved_instances_offerings(instance_type=params['instance_type'],
availability_zone=params['zone'].name)
print('\nThe
Reserved
Instances
Offerings
available:\n')
offerings:
StringProperty(name='offering',
verbose_name='Offering',
choices=offerings)
print('\nYou
chosen
offering:')
float(offering.fixed_price)
total_price
print('!!!
purchase
these
$%.2f
!!!'
(params['quantity'],
total_price))
six.moves.input('Are
this?
so,
YES:
answer.strip().lower()
'yes':
offering.purchase(params['quantity'])
print('Purchase
cancelled')
CopyImage
InstanceAttribute
SnapshotAttribute
ReservedInstancesOffering
ReservedInstanceListing
ModifyReservedInstancesResult
ReservedInstancesModification
SpotInstanceRequest
boto.ec2.spotpricehistory
SpotPriceHistory
boto.ec2.spotdatafeedsubscription
SpotDatafeedSubscription
boto.ec2.bundleinstance
BundleInstanceTask
boto.ec2.placementgroup
PlacementGroup
boto.ec2.instancetype
InstanceType
InstanceStatusSet
boto.ec2.volumestatus
VolumeStatusSet
boto.ec2.attributes
AccountAttribute,
VPCAttribute
BlockDeviceMapping,
EC2Connection(AWSQueryConnection):
'ec2_version',
'2014-10-01')
'ec2_region_name',
'ec2_region_endpoint',
'ec2.us-east-1.amazonaws.com')
super(EC2Connection,
api_version:
param_names
['aws_access_key_id',
'is_secure',
'port',
'https_connection_factory']
param_names:
params[name]
build_filter_params(self,
dict(filters)
aws_name.startswith('tag:'):
name.replace('_',
params['Filter.%d.Name'
filters[name]
params['Filter.%d.Value.%d'
get_all_images(self,
image_ids=None,
executable_by=None,
image_ids:
image_ids,
executable_by:
executable_by,
'ExecutableBy')
get_all_kernels(self,
kernel_ids=None,
kernel_ids:
kernel_ids,
'kernel'}
get_all_ramdisks(self,
ramdisk_ids=None,
ramdisk_ids:
ramdisk_ids,
'ramdisk'}
get_image(self,
self.get_all_images(image_ids=[image_id],
those
register_image(self,
image_location=None,
root_device_name=None,
sriov_net_support=None,
delete_root_volume_on_termination=False):
architecture:
image_location:
params['ImageLocation']
image_location
root_device_name:
params['RootDeviceName']
root_device_name
BlockDeviceType(snapshot_id=snapshot_id,
delete_on_termination=delete_root_volume_on_termination)
block_device_map
block_device_map[root_device_name]
virtualization_type:
sriov_net_support:
params['SriovNetSupport']
sriov_net_support
self.get_object('RegisterImage',
getattr(rs,
'imageId',
deregister_image(self,
delete_snapshot:
self.get_image(image_id)
image.block_device_mapping:
"/dev/sda1":
image.block_device_mapping[key].snapshot_id
self.get_status('DeregisterImage',
self.delete_snapshot(snapshot_id)
block_device_mapping=None,
no_reboot:
params['NoReboot']
block_device_mapping:
block_device_mapping.ec2_build_list_params(params)
self.get_object('CreateImage',
get_image_attribute(self,
self.get_object('DescribeImageAttribute',
modify_image_attribute(self,
product_codes=None,
product_codes:
product_codes,
'ProductCode')
self.get_status('ModifyImageAttribute',
reset_image_attribute(self,
self.get_status('ResetImageAttribute',
get_all_instances(self,
warnings.warn(('The
get_all_instances
'replaced
get_all_reservations.'),
PendingDeprecationWarning)
max_results=max_results)
get_only_instances(self,
max_results=max_results,
retval.extend([instance
reservation.instances])
reservations.next_token
get_all_reservations(self,
'group-id'
gid
filters.get('group-id')
gid.startswith('sg-')
len(gid)
group-id
"identifier
"by
'group-name'
instead.",
self.get_list('DescribeInstances',
Reservation)],
get_all_instance_status(self,
include_all_instances=False):
include_all_instances:
params['IncludeAllInstances']
self.get_object('DescribeInstanceStatus',
InstanceStatusSet,
run_instances(self,
max_count}
'SecurityGroup')
params['AddressingType']
params['Placement.AvailabilityZone']
params['Placement.GroupName']
tenancy:
params['Placement.Tenancy']
params['Monitoring.Enabled']
disable_api_termination:
params['DisableApiTermination']
instance_initiated_shutdown_behavior:
instance_initiated_shutdown_behavior
params['InstanceInitiatedShutdownBehavior']
client_token:
additional_info:
params['IamInstanceProfile.Name']
params['IamInstanceProfile.Arn']
network_interfaces.build_list_params(params)
self.get_object('RunInstances',
terminate_instances(self,
self.get_list('TerminateInstances',
stop_instances(self,
self.get_list('StopInstances',
start_instances(self,
self.get_list('StartInstances',
[instance_id],
self.get_object('GetConsoleOutput',
reboot_instances(self,
self.get_status('RebootInstances',
confirm_product_instance(self,
{'ProductCode':
self.get_object('ConfirmProductInstance',
(rs.status,
rs.ownerId)
get_instance_attribute(self,
self.get_object('DescribeInstanceAttribute',
InstanceAttribute,
modify_network_interface_attribute(self,
interface_id,
attachment_id=None,
'deleteontermination',
['true',
'false']:
boolean,
"false"!'
interface_id}
params['SecurityGroupId.%s'
params['Description.Value']
'sourcedestcheck':
params['SourceDestCheck.Value']
params['Attachment.DeleteOnTermination']
attachment_id:
ValueError('You
attachment_id')
params['Attachment.AttachmentId']
(attr,))
self.get_status(
modify_instance_attribute(self,
('disableapitermination',
'ebsoptimized')
params['GroupId.%s'
'blockdevicemapping':
dev_name,
kv.partition('=')
'BlockDeviceMapping.%d'
attribute[0].upper()
attribute[1:]
attribute]
self.get_status('ModifyInstanceAttribute',
reset_instance_attribute(self,
self.get_status('ResetInstanceAttribute',
get_all_spot_instance_requests(self,
request_ids=None,
lgid
filters.get('launch.group-id')
lgid.startswith('sg-')
len(lgid)
filtering
accordingly.",
self.get_list('DescribeSpotInstanceRequests',
get_spot_price_history(self,
self.get_list('DescribeSpotPriceHistory',
SpotPriceHistory)],
request_spot_instances(self,
type='one-time',
valid_from=None,
valid_until=None,
launch_group=None,
availability_zone_group=None,
'LaunchSpecification'
{'%s.ImageId'
ls:
price}
params['InstanceCount']
valid_from:
params['ValidFrom']
valid_from
valid_until:
params['ValidUntil']
valid_until
launch_group:
params['LaunchGroup']
launch_group
availability_zone_group:
params['AvailabilityZoneGroup']
availability_zone_group
params['%s.KeyName'
'%s.SecurityGroupId'
'%s.SecurityGroup'
params['%s.UserData'
base64.b64encode(user_data)
params['%s.AddressingType'
params['%s.InstanceType'
params['%s.Placement.AvailabilityZone'
params['%s.KernelId'
params['%s.RamdiskId'
params['%s.Monitoring.Enabled'
params['%s.SubnetId'
params['%s.Placement.GroupName'
block_device_map.ec2_build_list_params(params,
'%s.'
params['%s.IamInstanceProfile.Name'
params['%s.IamInstanceProfile.Arn'
params['%s.EbsOptimized'
network_interfaces.build_list_params(params,
prefix=ls
self.get_list('RequestSpotInstances',
cancel_spot_instance_requests(self,
self.get_list('CancelSpotInstanceRequests',
get_spot_datafeed_subscription(self,
self.get_object('DescribeSpotDatafeedSubscription',
create_spot_datafeed_subscription(self,
{'Bucket':
bucket}
self.get_object('CreateSpotDatafeedSubscription',
delete_spot_datafeed_subscription(self,
self.get_status('DeleteSpotDatafeedSubscription',
get_all_zones(self,
zones=None,
'ZoneName')
self.get_list('DescribeAvailabilityZones',
Zone)],
get_all_addresses(self,
addresses=None,
allocation_ids=None,
addresses:
addresses,
'PublicIp')
allocation_ids:
allocation_ids,
'AllocationId')
self.get_list('DescribeAddresses',
Address)],
allocate_address(self,
params['Domain']
self.get_object('AllocateAddress',
assign_private_ip_addresses(self,
allow_reassignment=False,
params['SecondaryPrivateIpAddressCount']
allow_reassignment:
params['AllowReassignment']
self.get_status('AssignPrivateIpAddresses',
_associate_address(self,
allow_reassociation:
params['AllowReassociation']
self.get_status('AssociateAddress',
self.get_object('AssociateAddress',
associate_address(self,
self._associate_address(True,
associate_address_object(self,
self._associate_address(False,
disassociate_address(self,
association_id=None,
params['AssociationId']
self.get_status('DisassociateAddress',
release_address(self,
self.get_status('ReleaseAddress',
unassign_private_ip_addresses(self,
self.get_status('UnassignPrivateIpAddresses',
get_all_volumes(self,
self.get_list('DescribeVolumes',
Volume)],
get_all_volume_status(self,
self.get_object('DescribeVolumeStatus',
VolumeStatusSet,
enable_volume_io(self,
self.get_status('EnableVolumeIO',
get_volume_attribute(self,
attribute='autoEnableIO',
self.get_object('DescribeVolumeAttribute',
VolumeAttribute,
modify_volume_attribute(self,
'AutoEnableIO':
params['AutoEnableIO.Value']
self.get_status('ModifyVolumeAttribute',
snapshot=None,
kms_key_id=None,
{'AvailabilityZone':
zone}
params['Size']
snapshot:
isinstance(snapshot,
Snapshot):
volume_type:
str(iops)
encrypted:
kms_key_id:
self.get_object('CreateVolume',
delete_volume(self,
self.get_status('DeleteVolume',
'VolumeId':
'Device':
device}
self.get_status('AttachVolume',
device=None,
device:
params['Device']
self.get_status('DetachVolume',
get_all_snapshots(self,
snapshot_ids=None,
snapshot_ids:
snapshot_ids,
'SnapshotId')
owner:
owner,
restorable_by:
restorable_by,
'RestorableBy')
self.get_list('DescribeSnapshots',
Snapshot)],
description[0:255]
self.get_object('CreateSnapshot',
self.get_all_volumes([volume_id],
volume.tags.get('Name')
snapshot.add_tag('Name',
volume_name)
delete_snapshot(self,
snapshot_id}
self.get_status('DeleteSnapshot',
copy_snapshot(self,
self.get_object('CopySnapshot',
hourly_backups=8,
daily_backups=7,
weekly_backups=4,
monthly_backups=True):
last_hour
now.day,
now.hour)
last_midnight
last_sunday
timedelta(days=(now.weekday()
datetime(2007,
hourly_backups):
target_backup_times.append(last_hour
timedelta(hours=hour))
day
daily_backups):
target_backup_times.append(last_midnight
timedelta(days=day))
week
weekly_backups):
target_backup_times.append(last_sunday
timedelta(weeks=week))
timedelta(days=1)
(start_of_month
(monthly_backups
monthly_backups)):
target_backup_times.append(start_of_month)
datetime(start_of_month.year,
start_of_month.month,
target_backup_times:
temp.__contains__(t)
temp.append(t)
sorted(temp)
all_snapshots
self.get_all_snapshots(owner
'self')
all_snapshots.sort(key=lambda
x.start_time)
snaps_for_each_volume
all_snapshots:
snap.tags.get('Name')
snaps_for_each_volume.get(volume_name)
snaps_for_volume:
snaps_for_volume.append(snap)
snaps_for_each_volume:
snaps[:-1]
never
newest
target_backup_times.__len__():
datetime.strptime(snap.start_time,
'%Y-%m-%dT%H:%M:%S.000Z')
target_backup_times[time_period_number]:
snap.tags.get('preserve_snapshot'):
self.delete_snapshot(snap.id)
boto.log.info('Trimmed
boto.log.error('Attempt
trim
Possible
race
trimming
server?'
get_snapshot_attribute(self,
{'Attribute':
self.get_object('DescribeSnapshotAttribute',
SnapshotAttribute,
modify_snapshot_attribute(self,
self.get_status('ModifySnapshotAttribute',
reset_snapshot_attribute(self,
self.get_status('ResetSnapshotAttribute',
get_all_key_pairs(self,
keynames=None,
keynames:
keynames,
self.get_list('DescribeKeyPairs',
KeyPair)],
get_key_pair(self,
keyname,
self.get_all_key_pairs(
keynames=[keyname],
)[0]
'InvalidKeyPair.NotFound':
create_key_pair(self,
self.get_object('CreateKeyPair',
delete_key_pair(self,
self.get_status('DeleteKeyPair',
import_key_pair(self,
public_key_material,
public_key_material
base64.b64encode(public_key_material)
'PublicKeyMaterial':
public_key_material}
self.get_object('ImportKeyPair',
get_all_security_groups(self,
group_ids=None,
groupnames
group_ids
group_ids,
'GroupId')
self.get_list('DescribeSecurityGroups',
SecurityGroup)],
create_security_group(self,
'GroupDescription':
self.get_object('CreateSecurityGroup',
SecurityGroup,
group.vpc_id
delete_security_group(self,
self.get_status('DeleteSecurityGroup',
authorize_security_group_deprecated(self,
authorize_security_group(self,
self.authorize_security_group_deprecated(
enumerate(cidr_ip):
params['IpPermissions.1.IpRanges.%d.CidrIp'
authorize_security_group_egress(self,
'GroupId':
'IpPermissions.1.IpProtocol':
self.get_status('AuthorizeSecurityGroupEgress',
revoke_security_group_deprecated(self,
revoke_security_group(self,
self.revoke_security_group_deprecated(
revoke_security_group_egress(self,
self.get_status('RevokeSecurityGroupEgress',
get_all_regions(self,
region_names=None,
region_names,
'RegionName')
self.get_list('DescribeRegions',
RegionInfo)],
region.connection_cls
get_all_reserved_instances_offerings(self,
reserved_instances_offering_ids=None,
include_marketplace=None,
min_duration=None,
max_duration=None,
max_instance_count=None,
reserved_instances_offering_ids
reserved_instances_offering_ids,
'ReservedInstancesOfferingId')
include_marketplace
include_marketplace:
min_duration
params['MinDuration']
str(min_duration)
max_duration
params['MaxDuration']
str(max_duration)
max_instance_count
params['MaxInstanceCount']
str(max_instance_count)
str(max_results)
self.get_list('DescribeReservedInstancesOfferings',
ReservedInstancesOffering)],
get_all_reserved_instances(self,
reserved_instances_id=None,
reserved_instances_id:
self.get_list('DescribeReservedInstances',
ReservedInstance)],
purchase_reserved_instance_offering(self,
limit_price=None,
instance_count}
limit_price
params['LimitPrice.Amount']
str(limit_price[0])
params['LimitPrice.CurrencyCode']
str(limit_price[1])
self.get_object('PurchaseReservedInstancesOffering',
ReservedInstance,
create_reserved_instances_listing(self,
price_schedules,
str(instance_count),
enumerate(price_schedules):
params['PriceSchedules.%s.Price'
str(price)
params['PriceSchedules.%s.Term'
str(term)
self.get_list('CreateReservedInstancesListing',
cancel_reserved_instances_listing(self,
reserved_instances_listing_ids=None,
reserved_instances_listing_ids
reserved_instances_listing_ids,
'ReservedInstancesListingId')
self.get_list('CancelReservedInstancesListing',
build_configurations_param_list(self,
tc
enumerate(target_configurations):
'ReservedInstancesConfigurationSetItemType.%d.'
'AvailabilityZone']
'Platform']
'InstanceCount']
'InstanceType']
modify_reserved_instances(self,
reserved_instance_ids
target_configurations
self.build_configurations_param_list(params,
target_configurations)
mrir
ModifyReservedInstancesResult,
mrir.modification_id
describe_reserved_instances_modifications(self,
reserved_instances_modification_ids=None,
reserved_instances_modification_ids:
reserved_instances_modification_ids,
'ReservedInstancesModificationId')
self.get_list('DescribeReservedInstancesModifications',
ReservedInstancesModification)],
monitor_instances(self,
self.get_list('MonitorInstances',
monitor_instance(self,
self.monitor_instances([instance_id],
unmonitor_instances(self,
self.get_list('UnmonitorInstances',
unmonitor_instance(self,
self.unmonitor_instances([instance_id],
bundle_instance(self,
s3_upload_policy,
'Storage.S3.Bucket':
'Storage.S3.Prefix':
'Storage.S3.UploadPolicy':
s3_upload_policy}
s3auth
boto.auth.get_auth_handler(None,
boto.config,
['s3'])
params['Storage.S3.AWSAccessKeyId']
s3auth.sign_string(s3_upload_policy)
params['Storage.S3.UploadPolicySignature']
self.get_object('BundleInstance',
get_all_bundle_tasks(self,
bundle_ids=None,
bundle_ids:
bundle_ids,
'BundleId')
self.get_list('DescribeBundleTasks',
BundleInstanceTask)],
cancel_bundle_task(self,
bundle_id,
{'BundleId':
bundle_id}
self.get_object('CancelBundleTask',
get_password_data(self,
self.get_object('GetPasswordData',
rs.passwordData
get_all_placement_groups(self,
groupnames:
self.get_list('DescribePlacementGroups',
PlacementGroup)],
create_placement_group(self,
strategy='cluster',
'Strategy':
strategy}
self.get_status('CreatePlacementGroup',
delete_placement_group(self,
self.get_status('DeletePlacementGroup',
build_tag_param_list(self,
sorted(tags.keys())
tags[key]
params['Tag.%d.Key'
params['Tag.%d.Value'
Tag)],
create_tags(self,
self.get_status('CreateTags',
{}.fromkeys(tags,
get_all_network_interfaces(self,
network_interface_ids=None,
network_interface_ids:
network_interface_ids,
'NetworkInterfaceId')
self.get_list('DescribeNetworkInterfaces',
NetworkInterface)],
create_network_interface(self,
ids.append(group.id)
ids.append(group)
self.get_object('CreateNetworkInterface',
NetworkInterface,
attach_network_interface(self,
'DeviceIndex':
device_index}
self.get_status('AttachNetworkInterface',
detach_network_interface(self,
{'AttachmentId':
attachment_id}
self.get_status('DetachNetworkInterface',
delete_network_interface(self,
network_interface_id}
self.get_status('DeleteNetworkInterface',
get_all_instance_types(self):
self.get_list('DescribeInstanceTypes',
InstanceType)],
copy_image(self,
kms_key_id=None):
self.get_object('CopyImage',
CopyImage,
describe_account_attributes(self,
self.get_list('DescribeAccountAttributes',
AccountAttribute)],
describe_vpc_attribute(self,
attribute=None,
self.get_object('DescribeVpcAttribute',
VPCAttribute,
get_all_classic_link_instances(self,
self.get_list('DescribeClassicLinkInstances',
EC2Object(object):
hasattr(self.connection,
connection.region
TaggedEC2Object(EC2Object):
super(TaggedEC2Object,
value='',
self.add_tags({key:
self.connection.create_tags(
self.tags.update(tags)
remove_tag(self,
self.remove_tags({key:
self.connection.delete_tags(
tags.items():
self.tags:
self.tags[key]:
self.tags[key]
Group(object):
ProductCodes(list):
BillingProducts(list):
'billingProduct':
Image(TaggedEC2Object):
BillingProducts()
'Image:%s'
'billingProducts':
'imageLocation':
'imageState':
'imageOwnerId':
'isPublic':
isPublic
'imageType':
'imageOwnerAlias':
'instanceLifecycle':
'sriovNetSupport':
self.connection.get_all_images([self.id],
self._update(img)
self.connection.run_instances(self.id,
max_count,
user_data,
addressing_type,
placement,
kernel_id,
ramdisk_id,
monitoring_enabled,
block_device_map,
disable_api_termination,
instance_initiated_shutdown_behavior,
private_ip_address,
placement_group,
security_group_ids=security_group_ids,
additional_info=additional_info,
instance_profile_name=instance_profile_name,
instance_profile_arn=instance_profile_arn,
tenancy=tenancy,
deregister(self,
self.connection.deregister_image(
delete_snapshot,
get_launch_permissions(self,
img_attrs.attrs
set_launch_permissions(self,
remove_launch_permissions(self,
reset_launch_attributes(self,
self.connection.reset_image_attribute(
get_kernel(self,
img_attrs.kernel
get_ramdisk(self,
img_attrs.ramdisk
ImageAttribute(object):
'launchPermission':
'launch_permission'
'product_codes'
self.attrs['product_codes'].append(value)
self.attrs['product_codes']
'kernel':
'ramdisk':
CopyImage(object):
ProductCodes
InstancePlacement(object):
tenancy=None):
'tenancy':
Reservation(EC2Object):
super(Reservation,
'Reservation:%s'
'instancesSet':
'reservationId':
stop_all(self,
instance.stop(dry_run=dry_run)
Instance(TaggedEC2Object):
InstancePlacement()
'Instance:%s'
self._state.name
state_code(self):
self._state.code
previous_state(self):
self._previous_state.name
previous_state_code(self):
self._previous_state.code
placement(self):
self._placement.zone
placement_group(self):
self._placement.group_name
placement_tenancy(self):
self._placement.tenancy
'stateReason':
SubParse('stateReason')
SubParse('eventsSet')
'networkInterfaceSet':
NetworkInterface)])
'currentState':
'instanceState':
'dnsName'
'publicDnsName':
'privateDnsName':
'amiLaunchIndex':
self.previous_state
'launchTime':
'requesterId':
'persistent':
self.connection.get_all_reservations([self.id],
self._update(i)
self.connection.terminate_instances([self.id],
self.connection.stop_instances([self.id],
self.connection.start_instances([self.id],
reboot(self,
self.connection.reboot_instances([self.id],
self.connection.get_console_output(self.id,
confirm_product(self,
self.connection.confirm_product_instance(
use_ip(self,
isinstance(ip_address,
Address):
ip_address.public_ip
monitor(self,
self.connection.monitor_instance(self.id,
unmonitor(self,
self.connection.unmonitor_instance(self.id,
get_attribute(self,
self.connection.get_instance_attribute(
modify_attribute(self,
self.connection.modify_instance_attribute(
reset_attribute(self,
self.connection.reset_instance_attribute(
self.connection.create_image(
no_reboot,
InstanceAttribute(dict):
ValidValues
['instanceType',
'userData',
'disableApiTermination',
'instanceInitiatedShutdownBehavior',
'rootDeviceName',
'blockDeviceMapping',
'groupSet']
self.ValidValues:
SubParse(dict):
self.section
self.section:
InstanceInfo(object):
'InstanceInfo:%s'
'instanceId'
Details(dict):
Details()
'Status:%s'
InstanceStatus(object):
events=None,
state_code=None,
state_name=None):
state_code
state_name
'InstanceStatus:%s'
'systemStatus':
'instanceStatus':
InstanceStatusSet(list):
InstanceStatus()
InstanceType(EC2Object):
cores=None,
memory=None,
disk=None):
super(InstanceType,
cores
disk
'InstanceType:%s-%s,%s,%s'
self.cores,
self.memory,
self.disk)
'cpu':
'disk':
'memory':
KeyPair(EC2Object):
super(KeyPair,
'KeyPair:%s'
'keyFingerprint':
'keyMaterial':
self.connection.delete_key_pair(self.name,
directory_path):
self.material:
directory_path
os.path.expanduser(directory_path)
os.path.join(directory_path,
os.path.exists(file_path):
overwritten'
file_path)
fp.write(self.material)
BotoClientError('KeyPair
material')
rconn.create_key_pair(self.name,
SubParse
GroupList(list):
LaunchSpecification(EC2Object):
super(LaunchSpecification,
'LaunchSpecification(%s)'
'Attachment:%s'
'attachmentId':
'deviceIndex':
'instanceOwnerId':
'deleteOnTermination':
NetworkInterface(TaggedEC2Object):
'NetworkInterface:%s'
'attachment':
'privateIpAddressesSet':
PrivateIPAddress)])
'requesterManaged':
'macAddress':
'sourceDestCheck':
self.connection.get_all_network_interfaces(
self.connection.attach_network_interface(
getattr(self.attachment,
self.connection.detach_network_interface(
self.connection.delete_network_interface(
PrivateIPAddress(object):
primary=None):
primary
'primary':
"PrivateIPAddress(%s,
primary=%s)"
(self.private_ip_address,
self.primary)
NetworkInterfaceCollection(list):
*interfaces):
self.extend(interfaces)
enumerate(self):
'%sNetworkInterface.%s.'
spec.network_interface_id
'NetworkInterfaceId']
str(spec.network_interface_id)
spec.device_index
str(spec.device_index)
spec.subnet_id
'SubnetId']
str(spec.subnet_id)
spec.description
'Description']
str(spec.description)
'DeleteOnTermination']
spec.secondary_private_ip_address_count
'SecondaryPrivateIpAddressCount']
str(spec.secondary_private_ip_address_count)
spec.private_ip_address
'PrivateIpAddress']
str(spec.private_ip_address)
spec.groups
enumerate(spec.groups):
query_param_key
'%sSecurityGroupId.%s'
params[query_param_key]
str(group_id)
spec.private_ip_addresses
enumerate(spec.private_ip_addresses):
query_param_key_prefix
'%sPrivateIpAddresses.%s'
k))
'.PrivateIpAddress']
str(ip_addr.private_ip_address)
'.Primary']
spec.associate_public_ip_address
'0'):
'AssociatePublicIpAddress'
spec.associate_public_ip_address:
NetworkInterfaceSpecification(object):
device_index=None,
delete_on_termination=None,
associate_public_ip_address=None):
device_index
self.secondary_private_ip_address_count
PlacementGroup(EC2Object):
strategy=None,
super(PlacementGroup,
strategy
'PlacementGroup:%s'
'strategy':
self.connection.delete_placement_group(
EC2RegionInfo(RegionInfo):
super(EC2RegionInfo,
EC2Connection)
ReservedInstancesOffering(EC2Object):
recurring_charges=None,
pricing_details=None):
super(ReservedInstancesOffering,
fixed_price
usage_price
recurring_charges
pricing_details
'ReservedInstanceOffering:%s'
'recurringCharges':
RecurringCharge)])
'pricingDetailsSet':
PricingDetail)])
'reservedInstancesOfferingId':
'duration':
'fixedPrice':
'usagePrice':
'offeringType':
'marketplace':
print('ID=%s'
print('\tInstance
Type=%s'
print('\tZone=%s'
self.availability_zone)
print('\tDuration=%s'
self.duration)
print('\tFixed
self.fixed_price)
print('\tUsage
self.usage_price)
print('\tDescription=%s'
purchase(self,
self.connection.purchase_reserved_instance_offering(
RecurringCharge(object):
frequency=None,
amount=None):
self.frequency
frequency
PricingDetail(object):
self.count
ReservedInstance(ReservedInstancesOffering):
fixed_price,
usage_price,
'ReservedInstance:%s'
'end':
ReservedInstanceListing(EC2Object):
listing_id=None,
listing_id
'instanceCounts':
InstanceCount)])
'priceSchedules':
PriceSchedule)])
'reservedInstancesListingId':
InstanceCount(object):
instance_count=None):
PriceSchedule(object):
term=None,
active=None):
'term':
ReservedInstancesConfiguration(object):
ModifyReservedInstancesResult(object):
modification_id=None):
ModificationResult(object):
ReservedInstancesModification(object):
reserved_instances=None,
modification_results=None,
effective_date=None,
reserved_instances
modification_results
effective_date
'reservedInstancesSet':
ReservedInstance)
'modificationResultSet':
ModificationResult)
'effectiveDate':
SecurityGroup(TaggedEC2Object):
'SecurityGroup:%s'
'ipPermissions':
'ipPermissionsEgress':
'groupDescription':
'ipRanges':
group_id=self.id,
IPPermissions(self)
self.rules.append(rule)
rule.add_grant(
remove_rule(self,
rules")
src_group_group_id:
src_group_owner_id:
target_grant:
rule.grants.remove(target_grant)
len(rule.grants)
self.rules.remove(target_rule)
self.connection.authorize_security_group(group_name,
self.add_rule(ip_protocol,
single_cidr_ip,
self.connection.revoke_security_group(group_name,
self.remove_rule(ip_protocol,
rconn.create_security_group(
self.description,
source_groups
grant_nom:
source_groups:
source_groups.append(grant_nom)
sg.authorize(None,
grant,
sg.authorize(rule.ip_protocol,
rule.from_port,
rule.to_port,
grant.cidr_ip,
instances(self,
filters={'instance.group-id':
filters={'group-id':
[i
IPPermissionsList(list):
self.append(IPPermissions(self))
self[-1]
IPPermissions(object):
'IPPermissions:%s(%s-%s)'
(self.ip_protocol,
self.grants.append(GroupOrCIDR(self))
'ipProtocol':
'fromPort':
'toPort':
GroupOrCIDR(self)
GroupOrCIDR(object):
self.cidr_ip:
'%s-%s'
(self.name
self.group_id,
self.owner_id)
'cidrIp':
Snapshot(TaggedEC2Object):
AttrName
'createVolumePermission'
super(Snapshot,
'Snapshot:%s'
'ownerAlias':
'volumeSize':
updated.progress
updated.status
self.connection.get_all_snapshots([self.id],
self.connection.delete_snapshot(self.id,
get_permissions(self,
self.connection.get_snapshot_attribute(
attrs.attrs
share(self,
unshare(self,
reset_permissions(self,
self.connection.reset_snapshot_attribute(
self.connection.create_volume(
volume_type,
self.encrypted,
SnapshotAttribute(object):
'createVolumePermission':
'create_volume_permission'
SpotInstanceStateFault
SpotDatafeedSubscription(EC2Object):
fault=None):
super(SpotDatafeedSubscription,
fault
'SpotDatafeedSubscription:%s'
self.connection.delete_spot_datafeed_subscription(
boto.ec2.launchspecification
LaunchSpecification
SpotInstanceStateFault(object):
'(%s,
SpotInstanceStatus(object):
update_time=None,
update_time
'<Status:
SpotInstanceRequest(TaggedEC2Object):
'SpotInstanceRequest:%s'
'launchSpecification':
LaunchSpecification(connection)
SpotInstanceStatus()
'validFrom':
'validUntil':
'launchGroup':
'availabilityZoneGroup':
'launchedAvailabilityZone':
self.connection.cancel_spot_instance_requests(
SpotPriceHistory(EC2Object):
super(SpotPriceHistory,
'SpotPriceHistory(%s):%2f'
(self.instance_type,
self.price)
TagSet(dict):
res_id=None,
res_type=None,
res_id
res_type
'Tag:%s'
Volume(TaggedEC2Object):
'Volume:%s'
'volumeType':
self.connection.get_all_volumes(
x.id
self.id]
self.connection.delete_volume(self.id,
self.connection.attach_volume(
self.connection.detach_volume(
self.connection.create_snapshot(
volume_state(self):
attachment_state(self):
snapshots(self,
self.connection.get_all_snapshots(
owner=owner,
restorable_by=restorable_by,
snap.volume_id
mine.append(snap)
AttachmentSet(object):
'AttachmentSet:%s'
'device':
VolumeAttribute(object):
'autoEnableIO':
Status,
Details
Action(object):
'Action:%s'
ActionSet(list):
Action()
self.append(action)
VolumeStatus(object):
'VolumeStatus:%s'
'actionsSet':
ActionSet()
'volumeStatus':
VolumeStatusSet(list):
VolumeStatus()
MessageSet(list):
Zone(EC2Object):
super(Zone,
'Zone:%s'
'messageSet':
MessageSet()
'zoneName':
'zoneState':
AdjustmentType
MetricCollectionTypes
TerminationPolicies
boto.ec2.autoscale.limits
AccountLimits
load_regions().get('autoscaling',
get_regions('autoscaling',
connection_cls=AutoScaleConnection)
AutoScaleConnection(AWSQueryConnection):
'autoscale_version',
'2011-01-01')
'autoscale_endpoint',
'autoscaling.us-east-1.amazonaws.com')
'autoscale_region_name',
use_block_device_types=False):
AutoScaleConnection)
super(AutoScaleConnection,
six.iteritems(items[i
1]):
kk,
six.iteritems(v):
params['%s.member.%d.%s.%s'
kk)]
params['%s.member.%d.%s'
params['%s.member.%d'
_update_group(self,
as_group.name,
as_group.launch_config_name,
as_group.min_size,
as_group.max_size}
as_group.availability_zones
'AvailabilityZones')
as_group.vpc_zone_identifier:
params['VPCZoneIdentifier']
as_group.vpc_zone_identifier
as_group.health_check_period:
params['HealthCheckGracePeriod']
as_group.health_check_period
as_group.health_check_type:
params['HealthCheckType']
as_group.health_check_type
as_group.default_cooldown:
params['DefaultCooldown']
as_group.default_cooldown
as_group.placement_group:
params['PlacementGroup']
as_group.placement_group
as_group.instance_id:
as_group.instance_id
as_group.termination_policies:
as_group.termination_policies,
'TerminationPolicies')
op.startswith('Create'):
as_group.load_balancers:
as_group.load_balancers,
'LoadBalancerNames')
as_group.tags:
enumerate(as_group.tags):
self.get_object(op,
attach_instances(self,
self.get_status('AttachInstances',
detach_instances(self,
decrement_capacity
self.get_status('DetachInstances',
create_auto_scaling_group(self,
self._update_group('CreateAutoScalingGroup',
as_group)
delete_auto_scaling_group(self,
if(force_delete):
'ForceDelete':
self.get_object('DeleteAutoScalingGroup',
create_launch_configuration(self,
launch_config):
launch_config.image_id,
launch_config.name,
launch_config.instance_type}
launch_config.key_name:
launch_config.key_name
launch_config.user_data:
launch_config.user_data
launch_config.kernel_id:
launch_config.kernel_id
launch_config.ramdisk_id:
launch_config.ramdisk_id
launch_config.block_device_mappings:
[x.autoscale_build_list_params(params)
launch_config.block_device_mappings]
launch_config.security_groups:
launch_config.security_groups,
'SecurityGroups')
launch_config.instance_monitoring:
launch_config.spot_price
params['SpotPrice']
str(launch_config.spot_price)
params['IamInstanceProfile']
launch_config.ebs_optimized:
launch_config.volume_type:
launch_config.volume_type
launch_config.delete_on_termination:
launch_config.iops:
launch_config.iops
launch_config.classic_link_vpc_id:
params['ClassicLinkVPCId']
launch_config.classic_link_vpc_id
launch_config.classic_link_vpc_security_groups:
self.build_list_params(
launch_config.classic_link_vpc_security_groups,
'ClassicLinkVPCSecurityGroups'
self.get_object('CreateLaunchConfiguration',
get_account_limits(self):
self.get_object('DescribeAccountLimits',
AccountLimits)
create_scaling_policy(self,
scaling_policy):
{'AdjustmentType':
scaling_policy.adjustment_type,
scaling_policy.as_name,
scaling_policy.name,
scaling_policy.scaling_adjustment}
scaling_policy.adjustment_type
"PercentChangeInCapacity"
params['MinAdjustmentStep']
params['Cooldown']
self.get_object('PutScalingPolicy',
delete_launch_configuration(self,
launch_config_name):
{'LaunchConfigurationName':
launch_config_name}
self.get_object('DeleteLaunchConfiguration',
names=None,
'AutoScalingGroupNames')
self.get_list('DescribeAutoScalingGroups',
AutoScalingGroup)])
get_all_launch_configurations(self,
kwargs.get('max_records',
kwargs.get('names',
'LaunchConfigurationNames')
kwargs.get('next_token')
self.get_list('DescribeLaunchConfigurations',
LaunchConfiguration)])
get_all_activities(self,
activity_ids:
'ActivityIds')
self.get_list('DescribeScalingActivities',
Activity)])
get_termination_policies(self):
self.get_object('DescribeTerminationPolicyTypes',
TerminationPolicies)
delete_scheduled_action(self,
scheduled_action_name,
{'ScheduledActionName':
scheduled_action_name}
self.get_status('DeleteScheduledAction',
decrement_capacity:
self.get_object('TerminateInstanceInAutoScalingGroup',
self.get_status('DeletePolicy',
get_all_adjustment_types(self):
self.get_list('DescribeAdjustmentTypes',
AdjustmentType)])
get_all_autoscaling_instances(self,
self.get_list('DescribeAutoScalingInstances',
get_all_metric_collection_types(self):
self.get_object('DescribeMetricCollectionTypes',
get_all_policies(self,
policy_names=None,
policy_names:
policy_names,
'PolicyNames')
self.get_list('DescribePolicies',
ScalingPolicy)])
get_all_scaling_process_types(self):
self.get_list('DescribeScalingProcessTypes',
ProcessType)])
self.get_status('SuspendProcesses',
self.get_status('ResumeProcesses',
create_scheduled_group_action(self,
time=None,
recurrence=None):
params['Recurrence']
params['Time']
time.isoformat()
params['MinSize']
params['MaxSize']
self.get_status('PutScheduledUpdateGroupAction',
get_all_scheduled_actions(self,
scheduled_actions=None,
scheduled_actions:
scheduled_actions,
'ScheduledActionNames')
self.get_list('DescribeScheduledActions',
ScheduledUpdateGroupAction)])
disable_metrics_collection(self,
self.get_status('DisableMetricsCollection',
enable_metrics_collection(self,
granularity,
granularity}
self.get_status('EnableMetricsCollection',
execute_policy(self,
honor_cooldown=None):
honor_cooldown
self.get_status('ExecutePolicy',
notification_types,
'NotificationTypes')
self.get_status('PutNotificationConfiguration',
self.get_status('DeleteNotificationConfiguration',
set_instance_health(self,
health_status,
should_respect_grace_period=True):
health_status}
should_respect_grace_period:
self.get_status('SetInstanceHealth',
set_desired_capacity(self,
desired_capacity,
honor_cooldown=False):
desired_capacity}
self.get_status('SetDesiredCapacity',
create_or_update_tags(self,
self.get_status('CreateOrUpdateTags',
Activity(object):
'Activity<%s>:
group:%s,
progress:%s,
cause:%s'
(self.activity_id,
self.group_name,
self.status_message,
self.cause)
'ActivityId':
'Progress':
'Cause':
'StatusMessage':
ProcessType(object):
'ProcessType(%s)'
SuspendedProcess(object):
'SuspendedProcess(%s,
(self.process_name,
self.reason)
'SuspensionReason':
EnabledMetric(object):
granularity=None):
granularity
'EnabledMetric(%s,
(self.metric,
self.granularity)
AutoScalingGroup(object):
launch_config=None,
availability_zones=None,
load_balancers=None,
default_cooldown=None,
health_check_type=None,
health_check_period=None,
vpc_zone_identifier=None,
termination_policies=None,
kwargs.get('group_name')
int(min_size)
int(max_size)
kwargs.get('cooldown')
int(default_cooldown)
isinstance(launch_config,
LaunchConfiguration):
launch_config.name
ListElement(lbs)
availability_zones
ListElement(zones)
health_check_period
health_check_type
type(vpc_zone_identifier)
','.join(vpc_zone_identifier)
ListElement(termination_policies)
_get_cooldown(self):
_set_cooldown(self,
cooldown
property(_get_cooldown,
_set_cooldown)
'AutoScaleGroup<%s>'
'LoadBalancerNames':
'EnabledMetrics':
EnabledMetric)])
'SuspendedProcesses':
SuspendedProcess)])
'TerminationPolicies':
'AutoScalingGroupARN':
'DefaultCooldown':
'PlacementGroup':
'HealthCheckGracePeriod':
'HealthCheckType':
set_capacity(self,
capacity}
self.connection.get_object('SetDesiredCapacity',
self.connection.last_request
self.connection._update_group('UpdateAutoScalingGroup',
shutdown_instances(self):
self.update()
self.connection.delete_auto_scaling_group(self.name,
force_delete)
get_activities(self,
max_records=50):
self.connection.get_all_activities(self,
max_records)
self.connection.put_notification_configuration(self,
notification_types)
self.connection.delete_notification_configuration(self,
topic)
self.connection.suspend_processes(self.name,
self.connection.resume_processes(self.name,
AutoScalingGroupMetric(object):
'AutoScalingGroupMetric:%s'
Instance(object):
'Instance<id:%s,
state:%s,
health:%s'
self.lifecycle_state,
self.health_status)
self.group_name:
group:%s'
'LifecycleState':
BDM
Ebs(object):
volume_size=None):
volume_size
'Ebs(%s,
(self.snapshot_id,
self.volume_size)
'VolumeSize':
InstanceMonitoring(object):
enabled='false'):
'InstanceMonitoring(%s)'
BlockDeviceMapping(object):
device_name=None,
virtual_name=None,
ebs=None,
no_device=None):
virtual_name
ebs
'BlockDeviceMapping(%s,
(self.device_name,
self.virtual_name)
'Ebs':
Ebs(self)
'DeviceName':
'VirtualName':
'NoDevice':
LaunchConfiguration(object):
image_id=None,
block_device_mappings=None,
instance_monitoring=False,
spot_price=None,
associate_public_ip_address=None,
use_block_device_types=False,
classic_link_vpc_id=None,
classic_link_vpc_security_groups=None):
block_device_mappings
sec_groups
ListElement(sec_groups)
instance_monitoring
spot_price
classic_link_vpc_id
classic_link_vpc_sec_groups
classic_link_vpc_security_groups
ListElement(classic_link_vpc_sec_groups)
connection.use_block_device_types
'LaunchConfiguration:%s'
'ClassicLinkVPCSecurityGroups':
'BlockDeviceMappings':
self.use_block_device_types:
BDM()
BlockDeviceMapping)])
InstanceMonitoring(self)
'KeyName':
boto.utils.parse_ts(value)
'KernelId':
'RamdiskId':
'LaunchConfigurationARN':
'IamInstanceProfile':
self.connection.delete_launch_configuration(self.name)
AccountLimits(object):
'AccountLimits:
[%s,
(self.max_autoscaling_groups,
self.max_launch_configurations)
'MaxNumberOfAutoScalingGroups':
'MaxNumberOfLaunchConfigurations':
Alarm(object):
'Alarm:%s'
'AlarmARN':
AdjustmentType(object):
'AdjustmentType:%s'
MetricCollectionTypes(object):
BaseType(object):
(self.arg,
self.arg:
Metric(BaseType):
'Metric'
Granularity(BaseType):
'Granularity'
'MetricCollectionTypes:<%s,
(self.metrics,
self.granularities)
'Granularities':
self.Granularity)])
'Metrics':
self.Metric)])
ScalingPolicy(object):
kwargs.get('name',
kwargs.get('adjustment_type',
kwargs.get('as_name',
kwargs.get('scaling_adjustment',
kwargs.get('cooldown',
kwargs.get('min_adjustment_step',
'ScalingPolicy(%s
group:%s
adjustment:%s)'
self.as_name,
self.adjustment_type)
'Alarms':
Alarm)])
'PolicyARN':
self.policy_arn
'Cooldown':
self.connection.delete_policy(self.name,
self.as_name)
Request(object):
'Request:%s'
ScheduledUpdateGroupAction(object):
'ScheduledUpdateGroupAction:%s'
'ScheduledActionARN':
'Time':
propagate_at_launch=False,
resource_id=None,
resource_type='auto-scaling-group'):
propagate_at_launch
resource_id
resource_type
'Tag(%s=%s)'
'PropagateAtLaunch':
'ResourceType':
build_params(self,
'Tags.member.%d.'
'ResourceId']
'ResourceType']
'Key']
'Value']
self.propagate_at_launch:
self.connection.delete_tags([self])
boto.ec2.cloudwatch.metric
Metric
MetricAlarm,
MetricAlarms,
AlarmHistoryItem
boto.ec2.cloudwatch.datapoint
Datapoint
load_regions().get('cloudwatch',
get_regions('cloudwatch',
connection_cls=CloudWatchConnection)
CloudWatchConnection(AWSQueryConnection):
'cloudwatch_version',
'2010-08-01')
'cloudwatch_region_name',
'cloudwatch_region_endpoint',
'monitoring.us-east-1.amazonaws.com')
'eu-west-1':
validate_certs
super(CloudWatchConnection,
build_dimension_param(self,
dimension,
'Dimensions.member'
dimension:
dimension[dim_name]
isinstance(dim_value,
[dim_value]
six.iteritems(item):
'Name')]
'Value')]
build_put_params(self,
timestamp)
max(map(lambda
aslist(a):
length:
equal
elements;
length)
[a]
t)
enumerate(zip(*map(aslist,
args))):
metric_data
n}
timestamp:
metric_data['Timestamp']
t.isoformat()
metric_data['Unit']
self.build_dimension_param(d,
metric_data)
statistics:
metric_data['StatisticValues.Maximum']
s['maximum']
metric_data['StatisticValues.Minimum']
s['minimum']
metric_data['StatisticValues.SampleCount']
s['samplecount']
metric_data['StatisticValues.Sum']
s['sum']
'metric.Posting
value.'
boto.log.warn(msg)
metric_data['Value']
put.')
six.iteritems(metric_data):
params['MetricData.member.%d.%s'
get_metric_statistics(self,
{'Period':
start_time.isoformat(),
end_time.isoformat()}
'Statistics.member.%d')
self.get_list('GetMetricStatistics',
Datapoint)])
list_metrics(self,
metric_name=None,
namespace=None):
metric_name:
params['MetricName']
metric_name
params['Namespace']
self.get_list('ListMetrics',
Metric)])
put_metric_data(self,
{'Namespace':
self.build_put_params(params,
statistics=statistics)
self.get_status('PutMetricData',
action_prefix=None,
alarm_name_prefix=None,
alarm_names=None,
state_value=None,
action_prefix:
params['ActionPrefix']
action_prefix
alarm_name_prefix:
params['AlarmNamePrefix']
alarm_name_prefix
alarm_names:
state_value:
params['StateValue']
state_value
self.get_list('DescribeAlarms',
[('MetricAlarms',
MetricAlarms)])
result[0]
ret.next_token
result.next_token
describe_alarm_history(self,
alarm_name=None,
alarm_name:
params['AlarmName']
alarm_name
params['StartDate']
start_date.isoformat()
params['EndDate']
end_date.isoformat()
history_item_type:
params['HistoryItemType']
history_item_type
self.get_list('DescribeAlarmHistory',
AlarmHistoryItem)])
describe_alarms_for_metric(self,
period:
params['Period']
statistic:
params['Statistic']
self.get_list('DescribeAlarmsForMetric',
MetricAlarm)])
put_metric_alarm(self,
alarm):
alarm.name,
alarm.metric,
alarm.namespace,
alarm.statistic,
alarm.comparison,
alarm.threshold,
alarm.evaluation_periods,
alarm.period,
params['ActionsEnabled']
alarm.alarm_actions:
alarm.alarm_actions,
'AlarmActions.member.%s')
alarm.description:
params['AlarmDescription']
alarm.description
alarm.dimensions:
self.build_dimension_param(alarm.dimensions,
alarm.insufficient_data_actions:
alarm.insufficient_data_actions,
'InsufficientDataActions.member.%s')
alarm.ok_actions:
alarm.ok_actions,
'OKActions.member.%s')
alarm.unit:
alarm.unit
alarm.connection
self.get_status('PutMetricAlarm',
create_alarm
update_alarm
delete_alarms(self,
alarms):
alarms,
self.get_status('DeleteAlarms',
set_alarm_state(self,
state_value,
state_reason_data=None):
{'AlarmName':
state_value}
state_reason_data:
params['StateReasonData']
json.dumps(state_reason_data)
self.get_status('SetAlarmState',
enable_alarm_actions(self,
self.get_status('EnableAlarmActions',
disable_alarm_actions(self,
self.get_status('DisableAlarmActions',
boto.ec2.cloudwatch.listelement
MetricAlarms(list):
MetricAlarm(connection)
self.append(metric_alarm)
MetricAlarm(object):
ALARM
'ALARM'
INSUFFICIENT_DATA
'INSUFFICIENT_DATA'
_cmp_map
'>=':
'GreaterThanOrEqualToThreshold',
'>':
'GreaterThanThreshold',
'<':
'LessThanThreshold',
'<=':
'LessThanOrEqualToThreshold',
_rev_cmp_map
dict((v,
six.iteritems(_cmp_map))
comparison=None,
threshold=None,
evaluation_periods=None,
description='',
ok_actions=None):
threshold
float(threshold)
self.comparison
self._cmp_map.get(comparison)
int(period)
evaluation_periods
int(evaluation_periods)
alarm_actions
insufficient_data_actions
ok_actions
'MetricAlarm:%s[%s(%s)
self.metric,
self.statistic,
self.comparison,
self.threshold)
'AlarmActions':
'InsufficientDataActions':
'OKActions':
'ActionsEnabled':
'AlarmArn':
'AlarmConfigurationUpdatedTimestamp':
'AlarmDescription':
'comparison',
self._rev_cmp_map[value])
'Unit':
set_state(self,
self.connection.set_alarm_state(self.name,
self.connection.update_alarm(self)
enable_actions(self):
self.connection.enable_alarm_actions([self.name])
disable_actions(self):
self.connection.disable_alarm_actions([self.name])
describe_history(self,
self.connection.describe_alarm_history(self.name,
start_date,
end_date,
max_records,
history_item_type,
next_token)
add_alarm_action(self,
Raise
instead?
self.alarm_actions.append(action_arn)
add_insufficient_data_action(self,
self.insufficient_data_actions.append(action_arn)
add_ok_action(self,
self.ok_actions.append(action_arn)
self.connection.delete_alarms([self.name])
AlarmHistoryItem(object):
'AlarmHistory:%s[%s
self.summary,
self.timestamp)
'HistoryData':
'HistoryItemType':
self.tem_type
'HistorySummary':
self.summary
Datapoint(dict):
'Minimum',
'SampleCount']:
Dimension(dict):
self[self._name].append(value)
MetricAlarm
Metric(object):
Statistics
['Minimum',
'Average',
'SampleCount']
Units
['Seconds',
'Microseconds',
'Milliseconds',
'Bytes',
'Kilobytes',
'Megabytes',
'Gigabytes',
'Terabytes',
'Bits',
'Kilobits',
'Megabits',
'Gigabits',
'Terabits',
'Percent',
'Count',
'Bytes/Second',
'Kilobytes/Second',
'Megabytes/Second',
'Gigabytes/Second',
'Terabytes/Second',
'Bits/Second',
'Kilobits/Second',
'Megabits/Second',
'Gigabits/Second',
'Terabits/Second',
'Count/Second',
'Metric:%s'
period=60):
isinstance(statistics,
[statistics]
self.connection.get_metric_statistics(period,
self.dimensions,
create_alarm(self,
ok_actions=None,
MetricAlarm(self.connection,
alarm_actions,
insufficient_data_actions,
ok_actions)
self.connection.put_metric_alarm(alarm):
self.connection.describe_alarms_for_metric(self.name,
LoadBalancer,
LoadBalancerZones
boto.ec2.elb.instancestate
InstanceState
load_regions().get('elasticloadbalancing',
get_regions('elasticloadbalancing',
connection_cls=ELBConnection)
ELBConnection(AWSQueryConnection):
'elb_version',
'2012-06-01')
'elb_region_name',
boto.config.get(
'elb_region_endpoint',
'elasticloadbalancing.us-east-1.amazonaws.com')
super(ELBConnection,
get_all_load_balancers(self,
load_balancer_names=None,
load_balancer_names:
load_balancer_names,
'LoadBalancerNames.member.%d')
self.get_list('DescribeLoadBalancers',
LoadBalancer)])
create_load_balancer(self,
subnets=None,
scheme='internet-facing',
scheme}
subnets:
self.get_object('CreateLoadBalancer',
load_balancer.name
load_balancer.listeners
load_balancer.availability_zones
load_balancer.subnets
load_balancer.security_groups
create_load_balancer_listeners(self,
self.get_status('CreateLoadBalancerListeners',
delete_load_balancer(self,
self.get_status('DeleteLoadBalancer',
delete_load_balancer_listeners(self,
ports):
enumerate(ports):
params['LoadBalancerPorts.member.%d'
self.get_status('DeleteLoadBalancerListeners',
enable_availability_zones(self,
zones_to_add):
zones_to_add,
self.get_object('EnableAvailabilityZonesForLoadBalancer',
disable_availability_zones(self,
zones_to_remove):
zones_to_remove,
self.get_object('DisableAvailabilityZonesForLoadBalancer',
modify_lb_attribute(self,
('crosszoneloadbalancing',)
params['LoadBalancerAttributes.CrossZoneLoadBalancing.Enabled'
params['LoadBalancerAttributes.AccessLog.Enabled']
params['LoadBalancerAttributes.AccessLog.S3BucketName']
value.s3_bucket_name
params['LoadBalancerAttributes.AccessLog.S3BucketPrefix']
value.s3_bucket_prefix
params['LoadBalancerAttributes.AccessLog.EmitInterval']
value.emit_interval
params['LoadBalancerAttributes.ConnectionDraining.Enabled']
params['LoadBalancerAttributes.ConnectionDraining.Timeout']
value.timeout
params['LoadBalancerAttributes.ConnectionSettings.IdleTimeout']
value.idle_timeout
ValueError('InvalidAttribute',
self.get_status('ModifyLoadBalancerAttributes',
verb='GET')
get_all_lb_attributes(self,
load_balancer_name):
self.get_object('DescribeLoadBalancerAttributes',
LbAttributes)
get_lb_attribute(self,
self.get_all_lb_attributes(load_balancer_name)
attributes.access_log
attributes.cross_zone_load_balancing.enabled
attributes.connection_draining
attributes.connecting_settings
self.get_list('RegisterInstancesWithLoadBalancer',
self.get_list('DeregisterInstancesFromLoadBalancer',
describe_instance_health(self,
self.get_list('DescribeInstanceHealth',
InstanceState)])
'HealthCheck.Timeout':
health_check.timeout,
'HealthCheck.Target':
health_check.target,
'HealthCheck.Interval':
health_check.interval,
'HealthCheck.UnhealthyThreshold':
health_check.unhealthy_threshold,
'HealthCheck.HealthyThreshold':
health_check.healthy_threshold}
self.get_object('ConfigureHealthCheck',
HealthCheck)
set_lb_listener_SSL_certificate(self,
ssl_certificate_id}
self.get_status('SetLoadBalancerListenerSSLCertificate',
{'CookieName':
self.get_status('CreateAppCookieStickinessPolicy',
create_lb_cookie_stickiness_policy(self,
params['CookieExpirationPeriod']
self.get_status('CreateLBCookieStickinessPolicy',
policy_attributes):
'PolicyTypeName':
policy_type}
enumerate(six.iteritems(policy_attributes),
params['PolicyAttributes.member.%d.AttributeName'
params['PolicyAttributes.member.%d.AttributeValue'
params['PolicyAttributes']
self.get_status('CreateLoadBalancerPolicy',
delete_lb_policy(self,
self.get_status('DeleteLoadBalancerPolicy',
set_lb_policies_of_listener(self,
lb_port}
len(policies):
self.get_status('SetLoadBalancerPoliciesOfListener',
set_lb_policies_of_backend_server(self,
instance_port}
self.get_status('SetLoadBalancerPoliciesForBackendServer',
apply_security_groups_to_lb(self,
self.get_list('ApplySecurityGroupsToLoadBalancer',
attach_lb_to_subnets(self,
self.get_list('AttachLoadBalancerToSubnets',
detach_lb_from_subnets(self,
self.get_list('DetachLoadBalancerFromSubnets',
ConnectionSettingAttribute(object):
'ConnectionSettingAttribute(%s)'
self.idle_timeout)
'IdleTimeout':
CrossZoneLoadBalancingAttribute(object):
'CrossZoneLoadBalancingAttribute(%s)'
self.enabled)
AccessLogAttribute(object):
'AccessLog(%s,
self.s3_bucket_name,
self.s3_bucket_prefix,
'S3BucketPrefix':
'EmitInterval':
ConnectionDrainingAttribute(object):
'ConnectionDraining(%s,
LbAttributes(object):
CrossZoneLoadBalancingAttribute(
AccessLogAttribute(self.connection)
ConnectionDrainingAttribute(self.connection)
ConnectionSettingAttribute(self.connection)
'LbAttributes(%s,
repr(self.cross_zone_load_balancing),
repr(self.access_log),
repr(self.connection_draining),
repr(self.connecting_settings))
'CrossZoneLoadBalancing':
'AccessLog':
'ConnectionDraining':
'ConnectionSettings':
access_point=None,
interval=30,
healthy_threshold=3,
timeout=5,
unhealthy_threshold=5):
self.access_point
access_point
healthy_threshold
unhealthy_threshold
'HealthCheck:%s'
'Interval':
'Target':
'HealthyThreshold':
'UnhealthyThreshold':
self.access_point:
new_hc
self.connection.configure_health_check(self.access_point,
new_hc.interval
new_hc.target
new_hc.healthy_threshold
new_hc.unhealthy_threshold
new_hc.timeout
reason_code=None):
reason_code
'InstanceState:(%s,%s)'
self.state)
'ReasonCode':
Listener(object):
load_balancer_port=0,
instance_port=0,
protocol='',
ssl_certificate_id=None,
instance_protocol=None):
load_balancer_port
instance_port
instance_protocol
ssl_certificate_id
"(%d,
(self.load_balancer_port,
self.instance_protocol:
self.ssl_certificate_id:
(self.ssl_certificate_id)
'InstanceProtocol':
get_tuple(self):
get_complex_tuple(self):
boto.ec2.elb.policies
Policies,
OtherPolicy
boto.ec2.elb.securitygroup
Backend(object):
'Backend(%r:%r)'
(self.instance_port,
self.policies)
LoadBalancerZones(object):
LoadBalancer(object):
endpoints=None):
'LoadBalancer:%s'
'HealthCheck':
HealthCheck(self)
'ListenerDescriptions':
Listener)])
'Policies':
Policies(self)
'SourceSecurityGroup':
'Subnets':
"BackendServerDescriptions":
Backend)])
self.instances.append(value)
'CanonicalHostedZoneName':
'CanonicalHostedZoneNameID':
enable_zones(self,
self.connection.enable_availability_zones(self.name,
disable_zones(self,
self.connection.disable_availability_zones(
self.connection.get_all_lb_attributes(self.name)
is_cross_zone_load_balancing(self,
self.get_attributes(force).cross_zone_load_balancing.enabled
enable_cross_zone_load_balancing(self):
disable_cross_zone_load_balancing(self):
self.connection.register_instances(self.name,
self.connection.deregister_instances(self.name,
self.connection.delete_load_balancer(self.name)
self.connection.configure_health_check(self.name,
health_check)
get_instance_health(self,
self.connection.describe_instance_health(self.name,
create_listeners(self,
self.connection.create_load_balancer_listeners(self.name,
create_listener(self,
inPort,
outPort=None,
proto="tcp"):
inPort
self.create_listeners([(inPort,
outPort,
proto)])
delete_listeners(self,
self.connection.delete_load_balancer_listeners(self.name,
delete_listener(self,
inPort):
self.delete_listeners([inPort])
self.connection.delete_lb_policy(self.name,
set_policies_of_listener(self,
self.connection.set_lb_policies_of_listener(self.name,
set_policies_of_backend_server(self,
self.connection.set_lb_policies_of_backend_server(
create_cookie_stickiness_policy(self,
self.connection.create_lb_cookie_stickiness_policy(
self.connection.create_app_cookie_stickiness_policy(name,
set_listener_SSL_certificate(self,
self.connection.set_lb_listener_SSL_certificate(
ssl_certificate_id)
policy_attribute):
self.connection.create_lb_policy(
policy_attribute)
attach_subnets(self,
self.connection.attach_lb_to_subnets(self.name,
detach_subnets(self,
self.connection.detach_lb_from_subnets(
apply_security_groups(self,
isinstance(security_groups,
[security_groups]
self.connection.apply_security_groups_to_lb(
security_groups)
AppCookieStickinessPolicy(object):
'AppCookieStickiness(%s,
self.cookie_name)
'CookieName':
LBCookieStickinessPolicy(object):
'LBCookieStickiness(%s,
self.cookie_expiration_period)
'CookieExpirationPeriod':
OtherPolicy(object):
'OtherPolicy(%s)'
(self.policy_name)
Policies(object):
'AppCookieStickiness%s'
'LBCookieStickiness%s'
'Other%s'
'Policies(%s,%s,%s)'
(app,
lb,
other)
'AppCookieStickinessPolicies':
AppCookieStickinessPolicy)])
'LBCookieStickinessPolicies':
LBCookieStickinessPolicy)])
'OtherPolicies':
SecurityGroup(object):
'SecurityGroup(%s,
self.owner_alias)
'OwnerAlias':
get_regions('ec2containerservice',
connection_cls=EC2ContainerServiceConnection)
ServerException(BotoServerError):
ClientException(BotoServerError):
EC2ContainerServiceConnection(AWSQueryConnection):
"2014-11-13"
"ecs.us-east-1.amazonaws.com"
"ServerException":
exceptions.ServerException,
"ClientException":
exceptions.ClientException,
super(EC2ContainerServiceConnection,
cluster_name=None):
params['clusterName']
cluster):
{'cluster':
cluster,
deregister_container_instance(self,
force=None):
{'containerInstance':
params['force']
force).lower()
action='DeregisterContainerInstance',
deregister_task_definition(self,
action='DeregisterTaskDefinition',
clusters=None):
clusters,
'clusters.member')
describe_container_instances(self,
action='DescribeContainerInstances',
describe_task_definition(self,
action='DescribeTaskDefinition',
describe_tasks(self,
'tasks.member')
action='DescribeTasks',
discover_poll_endpoint(self,
container_instance=None):
action='DiscoverPollEndpoint',
action='ListClusters',
list_container_instances(self,
action='ListContainerInstances',
list_task_definitions(self,
family_prefix=None,
params['familyPrefix']
action='ListTaskDefinitions',
list_tasks(self,
container_instance=None,
family=None,
params['family']
action='ListTasks',
register_container_instance(self,
instance_identity_document=None,
instance_identity_document_signature=None,
total_resources=None):
params['instanceIdentityDocument']
params['instanceIdentityDocumentSignature']
total_resources
total_resources,
'totalResources.member',
'doubleValue',
'longValue',
'integerValue',
'stringSetValue'))
action='RegisterContainerInstance',
register_task_definition(self,
container_definitions):
{'family':
container_definitions,
'containerDefinitions.member',
'cpu',
'memory',
'links',
'portMappings',
'essential',
'entryPoint',
'environment'))
action='RegisterTaskDefinition',
run_task(self,
overrides=None,
params['count']
action='RunTask',
start_task(self,
overrides=None):
action='StartTask',
stop_task(self,
{'task':
action='StopTask',
submit_container_state_change(self,
container_name=None,
exit_code=None,
network_bindings=None):
params['containerName']
params['exitCode']
network_bindings
network_bindings,
'networkBindings.member',
('bindIP',
'containerPort',
'hostPort'))
action='SubmitContainerStateChange',
submit_task_state_change(self,
action='SubmitTaskStateChange',
boto.ecs.item
ItemSet
ECSConnection(AWSQueryConnection):
host='ecs.amazonaws.com',
super(ECSConnection,
['ecs']
page=0,
itemSet=None):
params['Service']
"AWSECommerceService"
params['ItemPage']
"/onca/xml")
ItemSet(self,
rs.is_valid:
'{Code}:
{Message}'.format(**rs.errors[0]))
item_search(self,
search_index,
params['SearchIndex']
search_index
self.get_response('ItemSearch',
item_lookup(self,
self.get_response('ItemLookup',
ResponseGroup(xml.sax.ContentHandler):
nodename=None):
self._nodename
self._nodepath
self._xml
self.__dict__)
self.__dict__.get(name)
self.__dict__[name]
"<%s>%s</%s>"
(self._nodename,
self._xml.getvalue(),
self._nodename)
self._xml.write("<%s>"
self._nodepath.append(name)
ResponseGroup(self._connection)
self._curobj.startElement(name,
self._xml.write("%s</%s>"
(cgi.escape(value).replace("&amp;amp;",
"&amp;"),
curval
curval:
self._curobj.endElement(name,
self._nodepath.pop()
Item(ResponseGroup):
"Item")
ItemSet(ResponseGroup):
page=0):
"Items")
Item(self._connection)
self.curItem.startElement(name,
'TotalResults':
'TotalPages':
self.errors.append({'Code':
self.errors[-1]['Message']
self.objs.append(self.curItem)
self._xml.write(self.curItem.to_xml())
self.curItem.endElement(name,
iter(self.objs)
next(self.iter)
int(self.page)
int(self.total_pages):
self._connection.get_response(self.action,
self.page,
next(self)
ResponseGroup.to_xml(self)
get_regions('elasticache',
connection_cls=ElastiCacheConnection)
ElastiCacheConnection(AWSQueryConnection):
"2013-06-15"
"elasticache.us-east-1.amazonaws.com"
super(ElastiCacheConnection,
authorize_cache_security_group_ingress(self,
action='AuthorizeCacheSecurityGroupIngress',
create_cache_cluster(self,
snapshot_arns=None,
preferred_availability_zone=None,
snapshot_arns
snapshot_arns,
'SnapshotArns.member')
params['PreferredAvailabilityZone']
action='CreateCacheCluster',
create_cache_parameter_group(self,
action='CreateCacheParameterGroup',
create_cache_security_group(self,
action='CreateCacheSecurityGroup',
create_cache_subnet_group(self,
'CacheSubnetGroupName':
'CacheSubnetGroupDescription':
action='CreateCacheSubnetGroup',
create_replication_group(self,
replication_group_description):
'ReplicationGroupId':
'PrimaryClusterId':
'ReplicationGroupDescription':
replication_group_description,
action='CreateReplicationGroup',
delete_cache_cluster(self,
cache_cluster_id):
action='DeleteCacheCluster',
delete_cache_parameter_group(self,
cache_parameter_group_name):
action='DeleteCacheParameterGroup',
delete_cache_security_group(self,
cache_security_group_name):
action='DeleteCacheSecurityGroup',
delete_cache_subnet_group(self,
cache_subnet_group_name):
action='DeleteCacheSubnetGroup',
delete_replication_group(self,
replication_group_id):
action='DeleteReplicationGroup',
describe_cache_clusters(self,
cache_cluster_id=None,
show_cache_node_info=None):
params['CacheClusterId']
show_cache_node_info
params['ShowCacheNodeInfo']
show_cache_node_info).lower()
action='DescribeCacheClusters',
describe_cache_engine_versions(self,
cache_parameter_group_family=None,
default_only=None):
params['CacheParameterGroupFamily']
action='DescribeCacheEngineVersions',
describe_cache_parameter_groups(self,
action='DescribeCacheParameterGroups',
describe_cache_parameters(self,
action='DescribeCacheParameters',
describe_cache_security_groups(self,
cache_security_group_name=None,
params['CacheSecurityGroupName']
action='DescribeCacheSecurityGroups',
describe_cache_subnet_groups(self,
action='DescribeCacheSubnetGroups',
describe_replication_groups(self,
action='DescribeReplicationGroups',
describe_reserved_cache_nodes(self,
action='DescribeReservedCacheNodes',
describe_reserved_cache_nodes_offerings(self,
action='DescribeReservedCacheNodesOfferings',
modify_cache_cluster(self,
cache_node_ids_to_remove=None,
cache_node_ids_to_remove
cache_node_ids_to_remove,
'CacheNodeIdsToRemove.member')
action='ModifyCacheCluster',
modify_cache_parameter_group(self,
parameter_name_values):
action='ModifyCacheParameterGroup',
modify_cache_subnet_group(self,
cache_subnet_group_description=None,
params['CacheSubnetGroupDescription']
action='ModifyCacheSubnetGroup',
modify_replication_group(self,
replication_group_description=None,
primary_cluster_id=None):
params['ReplicationGroupDescription']
params['PrimaryClusterId']
action='ModifyReplicationGroup',
purchase_reserved_cache_nodes_offering(self,
cache_node_count=None):
'ReservedCacheNodesOfferingId':
params['CacheNodeCount']
action='PurchaseReservedCacheNodesOffering',
reboot_cache_cluster(self,
cache_node_ids_to_reboot):
cache_node_ids_to_reboot,
'CacheNodeIdsToReboot.member')
action='RebootCacheCluster',
reset_cache_parameter_group(self,
reset_all_parameters=None):
action='ResetCacheParameterGroup',
revoke_cache_security_group_ingress(self,
action='RevokeCacheSecurityGroupIngress',
'elastictranscoder',
connection_cls=ElasticTranscoderConnection
AccessDeniedException(JSONResponseError):
InternalServiceException(JSONResponseError):
IncompatibleVersionException(JSONResponseError):
ElasticTranscoderConnection(AWSAuthConnection):
"2012-09-25"
"elastictranscoder.us-east-1.amazonaws.com"
"IncompatibleVersionException":
exceptions.IncompatibleVersionException,
"AccessDeniedException":
exceptions.AccessDeniedException,
"InternalServiceException":
exceptions.InternalServiceException,
cancel_job(self,
create_job(self,
input_name=None,
outputs=None,
output_key_prefix=None,
playlists=None):
'/2012-09-25/jobs'
params['Input']
params['Output']
params['Outputs']
params['OutputKeyPrefix']
params['Playlists']
'/2012-09-25/pipelines'
create_preset(self,
container=None,
video=None,
audio=None,
thumbnails=None):
'/2012-09-25/presets'
params['Container']
params['Video']
params['Audio']
params['Thumbnails']
delete_preset(self,
list_jobs_by_pipeline(self,
'/2012-09-25/jobsByPipeline/{0}'.format(pipeline_id)
list_jobs_by_status(self,
'/2012-09-25/jobsByStatus/{0}'.format(status)
'/2012-09-25/pipelines'.format()
list_presets(self,
'/2012-09-25/presets'.format()
read_job(self,
read_pipeline(self,
read_preset(self,
test_role(self,
topics=None):
'/2012-09-25/roleTests'
params['Topics']
update_pipeline(self,
update_pipeline_notifications(self,
notifications=None):
'/2012-09-25/pipelines/{0}/notifications'.format(id)
update_pipeline_status(self,
'/2012-09-25/pipelines/{0}/status'.format(id)
StreamingStep,
boto.emr.bootstrap_action
BootstrapAction
get_regions('elasticmapreduce',
connection_cls=EmrConnection)
BootstrapAction(object):
bootstrap_action_args):
isinstance(bootstrap_action_args,
[bootstrap_action_args]
self.bootstrap_action_args
self.bootstrap_action_args:
args.extend(self.bootstrap_action_args)
path=%r,
bootstrap_action_args=%r)'
self.bootstrap_action_args)
HadoopStep,
StepSummaryList
EmrConnection(AWSQueryConnection):
'emr_version',
'2009-03-31')
'emr_region_name',
'emr_region_endpoint',
'elasticmapreduce.us-east-1.amazonaws.com')
DebuggingJar
's3://{region_name}.elasticmapreduce/libs/script-runner/script-runner.jar'
DebuggingArgs
's3://{region_name}.elasticmapreduce/libs/state-pusher/0.1/fetch'
super(EmrConnection,
'elasticmapreduce'
describe_cluster(self,
self.get_object('DescribeCluster',
Cluster)
describe_jobflow(self,
jobflows
self.describe_jobflows(jobflow_ids=[jobflow_id])
jobflows:
jobflows[0]
describe_jobflows(self,
states=None,
jobflow_ids=None,
created_before=None):
states:
states,
'JobFlowStates.member')
jobflow_ids:
self.get_list('DescribeJobFlows',
JobFlow)])
describe_step(self,
step_id):
step_id
self.get_object('DescribeStep',
HadoopStep)
list_bootstrap_actions(self,
self.get_object('ListBootstrapActions',
BootstrapActionList)
created_before=None,
cluster_states=None,
cluster_states:
cluster_states,
'ClusterStates.member')
self.get_object('ListClusters',
ClusterSummaryList)
list_instance_groups(self,
self.get_object('ListInstanceGroups',
InstanceGroupList)
list_instances(self,
instance_group_id=None,
instance_group_types=None,
instance_group_id:
params['InstanceGroupId']
instance_group_id
instance_group_types:
instance_group_types,
'InstanceGroupTypes.member')
self.get_object('ListInstances',
InstanceList)
list_steps(self,
step_states=None,
step_states:
step_states,
'StepStates.member')
self.get_object('ListSteps',
StepSummaryList)
isinstance(resource_id,
params.update(self._build_tag_list(tags))
self.get_status('AddTags',
params.update(self._build_string_list('TagKeys',
tags))
self.get_status('RemoveTags',
terminate_jobflow(self,
self.terminate_jobflows([jobflow_id])
terminate_jobflows(self,
jobflow_ids):
self.get_status('TerminateJobFlows',
add_jobflow_steps(self,
'AddJobFlowSteps',
add_instance_groups(self,
params.update(self._build_instance_group_list_args(instance_groups))
self.get_object('AddInstanceGroups',
modify_instance_groups(self,
instance_group_ids,
new_sizes):
isinstance(instance_group_ids,
instance_group_ids
[instance_group_ids]
isinstance(new_sizes,
new_sizes
[new_sizes]
zip(instance_group_ids,
new_sizes)
params['InstanceGroups.member.%d.InstanceGroupId'
ig[0]
params['InstanceGroups.member.%d.InstanceCount'
ig[1]
self.get_object('ModifyInstanceGroups',
run_jobflow(self,
log_uri=None,
ec2_keyname=None,
master_instance_type='m1.small',
slave_instance_type='m1.small',
num_instances=1,
keep_alive=False,
enable_debugging=False,
hadoop_version=None,
steps=None,
bootstrap_actions=[],
instance_groups=None,
ami_version=None,
api_params=None,
visible_to_all_users=None,
job_flow_role=None,
service_role=None):
action_on_failure:
params['ActionOnFailure']
log_uri:
params['LogUri']
log_uri
common_params
self._build_instance_common_args(ec2_keyname,
hadoop_version)
params.update(common_params)
instance_groups:
self._build_instance_count_and_type_args(
num_instances)
list_args
self._build_instance_group_list_args(instance_groups)
('Instances.%s'
six.iteritems(list_args)
enable_debugging:
debugging_step
JarStep(name='Setup
Hadoop
Debugging',
jar=self.DebuggingJar.format(region_name=self.region.name),
step_args=self.DebuggingArgs.format(region_name=self.region.name))
steps.insert(0,
debugging_step)
steps:
bootstrap_actions:
[self._build_bootstrap_action_args(bootstrap_action)
bootstrap_actions]
params.update(self._build_bootstrap_action_list(bootstrap_action_args))
ami_version:
params['AmiVersion']
ami_version
api_params:
six.iteritems(api_params):
params.pop(key,
visible_to_all_users
visible_to_all_users:
params['JobFlowRole']
params['ServiceRole']
response.jobflowid
set_termination_protection(self,
termination_protection_status):
termination_protection_status
params['TerminationProtected']
(termination_protection_status
self.get_status('SetTerminationProtection',
set_visible_to_all_users(self,
visibility):
visibility
(visibility
self.get_status('SetVisibleToAllUsers',
_build_bootstrap_action_args(self,
bootstrap_action):
bootstrap_action_params['ScriptBootstrapAction.Path']
bootstrap_action.path
bootstrap_action_params['Name']
bootstrap_action.name
bootstrap_action.args()
self.build_list_params(bootstrap_action_params,
'ScriptBootstrapAction.Args.member')
_build_step_args(self,
step):
step_params['ActionOnFailure']
step.action_on_failure
step_params['HadoopJarStep.Jar']
step.jar()
step.main_class()
main_class:
step_params['HadoopJarStep.MainClass']
step.args()
self.build_list_params(step_params,
'HadoopJarStep.Args.member')
step_params['Name']
step.name
_build_bootstrap_action_list(self,
bootstrap_actions):
isinstance(bootstrap_actions,
bootstrap_actions
[bootstrap_actions]
enumerate(bootstrap_actions):
six.iteritems(bootstrap_action):
params['BootstrapActions.member.%s.%s'
_build_step_list(self,
enumerate(steps):
six.iteritems(step):
params['Steps.member.%s.%s'
_build_string_list(self,
params['%s.member.%s'
_build_tag_list(self,
enumerate(sorted(six.iteritems(tags)),
'Tags.member.%s'
params['%s.Key'
_build_instance_common_args(self,
ec2_keyname,
hadoop_version):
'Instances.KeepJobFlowAliveWhenNoSteps':
str(keep_alive).lower(),
hadoop_version:
params['Instances.HadoopVersion']
hadoop_version
ec2_keyname:
params['Instances.Ec2KeyName']
ec2_keyname
params['Instances.Placement.AvailabilityZone']
_build_instance_count_and_type_args(self,
num_instances):
{'Instances.MasterInstanceType':
'Instances.SlaveInstanceType':
'Instances.InstanceCount':
num_instances}
_build_instance_group_args(self,
instance_group):
{'InstanceCount':
instance_group.num_instances,
'InstanceRole':
instance_group.role,
instance_group.type,
instance_group.name,
'Market':
instance_group.market}
instance_group.market
params['BidPrice']
instance_group.bidprice
_build_instance_group_list_args(self,
ig_dict
self._build_instance_group_args(instance_group)
six.iteritems(ig_dict):
params['InstanceGroups.member.%d.%s'
EmrObject(object):
self.Fields:
name.lower(),
RunJobFlowResponse(EmrObject):
set(['JobFlowId'])
AddInstanceGroupsResponse(EmrObject):
set(['InstanceGroupIds',
'JobFlowId'])
ModifyInstanceGroupsResponse(EmrObject):
set(['RequestId'])
Arg(EmrObject):
StepId(Arg):
SupportedProduct(Arg):
JobFlowStepList(EmrObject):
__ini__(self,
'StepIds':
StepId)])
BootstrapAction(EmrObject):
'Path',
'ScriptPath',
KeyValue(EmrObject):
Step(EmrObject):
'ActionOnFailure',
'MainClass',
InstanceGroup(EmrObject):
'InstanceGroupId',
'InstanceRequestCount',
'InstanceRole',
'InstanceRunningCount',
'LaunchGroup',
JobFlow(EmrObject):
'AmiVersion',
'AvailabilityZone',
'HadoopVersion',
'InstanceCount',
'JobFlowId',
'KeepJobFlowAliveWhenNoSteps',
'MasterInstanceId',
'MasterInstanceType',
'RequestId',
'SlaveInstanceType',
'Type',
Step)])
InstanceGroup)])
'SupportedProducts':
SupportedProduct)])
ClusterTimeline(EmrObject):
'EndDateTime'
ClusterStateChangeReason(EmrObject):
'Code',
ClusterStatus(EmrObject):
'StateChangeReason',
'Timeline'
'Timeline':
ClusterTimeline()
'StateChangeReason':
ClusterStateChangeReason()
Ec2InstanceAttributes(EmrObject):
'Ec2SubnetId',
'Ec2AvailabilityZone',
'IamInstanceProfile'
Application(EmrObject):
'AdditionalInfo'
Cluster(EmrObject):
'RequestedAmiVersion',
'RunningAmiVersion',
'AutoTerminate',
'ServiceRole'
'Ec2InstanceAttributes':
Ec2InstanceAttributes()
'Applications':
Application)])
ClusterSummary(EmrObject):
'NormalizedInstanceHours'
ClusterSummaryList(EmrObject):
'Clusters':
ClusterSummary)])
StepConfig(EmrObject):
'MainClass'
HadoopStep(EmrObject):
'ActionOnFailure'
InstanceGroupInfo(EmrObject):
'InstanceGroupType',
'RequestedInstanceCount',
'RunningInstanceCount'
InstanceGroupList(EmrObject):
InstanceGroupInfo)])
InstanceInfo(EmrObject):
'Ec2InstanceId',
'PublicDnsName',
'PublicIpAddress',
'PrivateDnsName',
'PrivateIpAddress'
InstanceList(EmrObject):
StepSummary(EmrObject):
StepSummaryList(EmrObject):
StepSummary)])
BootstrapActionList(EmrObject):
InstanceGroup(object):
num_instances,
market,
bidprice=None):
self.num_instances
num_instances
bidprice:
ValueError('bidprice
SPOT')
self.bidprice
str(bidprice)
bidprice
self.market,
self.bidprice)
self.market)
Step(object):
JarStep(Step):
step_args=None):
StreamingStep(Step):
mapper,
reducer=None,
combiner=None,
cache_files=None,
cache_archives=None,
step_args=None,
jar='/home/hadoop/contrib/streaming/hadoop-streaming.jar'):
self.mapper
mapper
self.reducer
reducer
self.combiner
combiner
self.cache_files
cache_files
self.cache_archives
cache_archives
self.input
args.extend(['-mapper',
self.mapper])
self.combiner:
args.extend(['-combiner',
self.combiner])
self.reducer:
args.extend(['-reducer',
self.reducer])
args.extend(['-jobconf',
'mapred.reduce.tasks=0'])
isinstance(self.input,
input))
self.input))
self.output:
args.extend(('-output',
self.output))
cache_file
args.extend(('-cacheFile',
cache_file))
cache_archive
args.extend(('-cacheArchive',
cache_archive))
mapper=%r,
reducer=%r,
action_on_failure=%r,
cache_files=%r,
cache_archives=%r,
step_args=%r,
input=%r,
output=%r,
jar=%r)'
self.mapper,
self.reducer,
self.action_on_failure,
self.cache_files,
self.cache_archives,
self.step_args,
self.input,
self.output,
self._jar)
ScriptRunnerStep(JarStep):
ScriptRunnerJar
's3n://us-east-1.elasticmapreduce/libs/script-runner/script-runner.jar'
super(ScriptRunnerStep,
self.ScriptRunnerJar,
PigBase(ScriptRunnerStep):
['s3n://us-east-1.elasticmapreduce/libs/pig/pig-script',
's3n://us-east-1.elasticmapreduce/libs/pig/']
InstallPigStep(PigBase):
InstallPigName
Pig'
pig_versions='latest'):
step_args.extend(['--install-pig'])
super(InstallPigStep,
self).__init__(self.InstallPigName,
PigStep(PigBase):
pig_file,
pig_versions='latest',
pig_args=[]):
step_args.extend(['--run-pig-script',
pig_file])
step_args.extend(pig_args)
super(PigStep,
HiveBase(ScriptRunnerStep):
['s3n://us-east-1.elasticmapreduce/libs/hive/hive-script',
's3n://us-east-1.elasticmapreduce/libs/hive/']
InstallHiveStep(HiveBase):
InstallHiveName
Hive'
hive_site=None):
step_args.extend(['--install-hive'])
hive_site
step_args.extend(['--hive-site=%s'
hive_site])
super(InstallHiveStep,
self).__init__(self.InstallHiveName,
HiveStep(HiveBase):
hive_file,
hive_args=None):
step_args.extend(['--run-hive-script',
hive_file])
hive_args
step_args.extend(hive_args)
super(HiveStep,
['Connection',
'Bucket']
boto.file.simpleresultset
SimpleResultSet
contained_key):
contained_key
'anonymous
file://'
os.remove(key_name)
self.contained_key)
SimpleResultSet([key])
key_type=Key.KEY_STREAM_READABLE)
key_type=Key.KEY_STREAM_WRITABLE)
os.path.dirname(key_name)
os.path.exists(dir_name):
os.makedirs(dir_name)
FileConnection(object):
file_storage_uri):
self.file_storage_uri
file_storage_uri
Bucket(bucket_name,
self.file_storage_uri.object_name)
KEY_STREAM_READABLE
0x01
KEY_STREAM_WRITABLE
0x02
KEY_STREAM
(KEY_STREAM_READABLE
KEY_STREAM_WRITABLE)
KEY_REGULAR_FILE
0x00
key_type=KEY_REGULAR_FILE):
os.stat(name).st_size
'<STDIN>'
'<STDOUT>'
readable')
shutil.copyfileobj(key_file,
md5=None):
writable')
replace
os.path.exists(self.full_path):
shutil.copyfileobj(fp,
key_file)
num_cb=None,
shutil.copyfileobj(self.fp,
self.get_contents_to_file(fp)
(self.key_type
self.KEY_STREAM)
self.fp.close()
SimpleResultSet(list):
input_list):
input_list:
self.append(x)
boto.fps.exception
['FPSConnection']
'response')
complex_amounts(*fields):
filter(kw.has_key,
'.Value']
str(amount))
'.CurrencyCode']
'CurrencyCode',
"{0}\nComplex
Amounts:
len(filter(kw.has_key,
len(filter(hasgroup,
groups)):
"".format(getattr(func,
'Method'),
needs_caller_reference(func):
kw.setdefault('CallerReference',
uuid.uuid4())
"{0}\nUses
CallerReference,
uuid.uuid4()".format(func.__doc__)
api_action(*api):
ResponseFactory(action)
hasattr(boto.fps.response,
'Response'):
getattr(boto.fps.response,
wrapper.action,
wrapper.response
"FPS
call\n{1}".format(action,
FPSConnection(AWSQueryConnection):
'2010-08-28'
currencycode
'USD'
self.currencycode
kw.pop('CurrencyCode',
'fps.sandbox.amazonaws.com')
super(FPSConnection,
['fps']
@complex_amounts('SettlementAmount')
'SettlementAmount.Value',
'SettlementAmount.CurrencyCode'])
settle_debt(self,
get_transaction_status(self,
@requires(['StartDate'])
get_account_activity(self,
get_transaction(self,
get_outstanding_debt_balance(self,
@requires(['PrepaidInstrumentId'])
get_prepaid_balance(self,
get_total_prepaid_liability(self,
get_account_balance(self,
@requires(['PaymentInstruction',
'TokenType'])
install_payment_instruction(self,
@requires(['returnURL',
'pipelineName'])
cbui_url(self,
sandbox
'sandbox'
'payments-sandbox'
'payments'
'authorize.{0}.amazon.com'.format(sandbox)
'/cobranded-ui/actions/start'
validpipelines
('SingleUse',
'MultiUse',
'Recurring',
'Recipient',
'SetupPrepaid',
'SetupPostpaid',
'EditToken')
kw['pipelineName']
validpipelines,
pipelineName"
kw.update({
'signatureMethod':
'HmacSHA256',
'signatureVersion':
kw.setdefault('callerKey',
safestr
str(x)
safequote
urllib.quote(safestr(x),
safe='~')
sorted([(k,
safequote(v))
kw.items()])
p:
'&'.join([k
p])
'\n'.join(['GET',
encoded(payload)])
self._auth_handler.sign_string(canonical)
[('signature',
safequote(signature))]
payload.sort()
'https://{0}{1}?{2}'.format(endpoint,
encoded(payload))
reserve(self,
pay(self,
@requires(['ReserveTransactionId',
settle(self,
@requires(['TransactionId',
'RefundAmount.Value',
'CallerReference',
'RefundAmount.CurrencyCode'])
@requires(['RecipientTokenId'])
get_recipient_verification_status(self,
@requires(['CallerReference'],
['TokenId'])
get_token_by_caller(self,
@requires(['UrlEndPoint',
'HttpParameters'])
verify_signature(self,
get_tokens(self,
get_token_usage(self,
cancel_token(self,
@complex_amounts('FundingAmount')
@requires(['PrepaidInstrumentId',
'FundingAmount.Value',
'FundingAmount.CurrencyCode'])
fund_prepaid(self,
@requires(['CreditInstrumentId'])
get_debt_balance(self,
@complex_amounts('AdjustmentAmount')
'AdjustmentAmount.Value',
'AdjustmentAmount.CurrencyCode'])
write_off_debt(self,
get_transactions_for_subscription(self,
get_subscription_details(self,
cancel_subscription_and_refund(self,
RefundAmount,
"you
CallerReference."
'RefundAmount.Value'
'CallerReference'
get_payment_instruction(self,
ResponseErrorFactory(BotoServerError):
BotoServerError(*args,
newclass
globals().get(error.error_code,
newclass.__new__(newclass,
obj.__dict__.update(error.__dict__)
'{0}({1},
{2},\n\t{3})'.format(self.__class__.__name__,
self.error_message)
'FPS
{0.status}
{0.__class__.__name__}
{1}\n'
'{2}\n'
'{0.error_message}'.format(self,
self.__doc__.strip())
AccessFailure(RetriableResponseError):
AccountClosed(RetriableResponseError):
AccountLimitsExceeded(RetriableResponseError):
AmountOutOfRange(ResponseError):
AuthFailure(RetriableResponseError):
ConcurrentModification(RetriableResponseError):
DuplicateRequest(ResponseError):
InactiveInstrument(ResponseError):
IncompatibleTokens(ResponseError):
InstrumentAccessDenied(ResponseError):
InstrumentExpired(ResponseError):
InsufficientBalance(RetriableResponseError):
InternalError(RetriableResponseError):
InvalidAccountState(RetriableResponseError):
InvalidAccountState_Caller(RetriableResponseError):
InvalidAccountState_Recipient(RetriableResponseError):
InvalidAccountState_Sender(RetriableResponseError):
InvalidCallerReference(ResponseError):
InvalidClientTokenId(ResponseError):
InvalidDateRange(ResponseError):
InvalidParams(ResponseError):
InvalidPaymentInstrument(ResponseError):
InvalidPaymentMethod(ResponseError):
InvalidRecipientForCCTransaction(ResponseError):
InvalidSenderRoleForAccountType(ResponseError):
InvalidTokenId(ResponseError):
InvalidTokenId_Recipient(ResponseError):
InvalidTokenId_Sender(ResponseError):
InvalidTokenType(ResponseError):
InvalidTransactionId(ResponseError):
InvalidTransactionState(ResponseError):
NotMarketplaceApp(RetriableResponseError):
OriginalTransactionFailed(ResponseError):
OriginalTransactionIncomplete(RetriableResponseError):
PaymentInstrumentNotCC(ResponseError):
PaymentMethodNotDefined(ResponseError):
PrepaidFundingLimitExceeded(RetriableResponseError):
RefundAmountExceeded(ResponseError):
SameSenderAndRecipient(ResponseError):
SameTokenIdUsedMultipleTimes(ResponseError):
SenderNotOriginalRecipient(ResponseError):
SettleAmountGreaterThanDebt(ResponseError):
SettleAmountGreaterThanReserveAmount(ResponseError):
SignatureDoesNotMatch(ResponseError):
TokenAccessDenied(ResponseError):
TokenNotActive(ResponseError):
TokenNotActive_Recipient(ResponseError):
TokenNotActive_Sender(ResponseError):
TokenUsageError(ResponseError):
TransactionDenied(ResponseError):
TransactionFullyRefundedAlready(ResponseError):
TransactionTypeNotRefundable(ResponseError):
UnverifiedAccount_Recipient(ResponseError):
UnverifiedAccount_Sender(ResponseError):
UnverifiedBankAccount(ResponseError):
UnverifiedEmailAddress_Caller(ResponseError):
UnverifiedEmailAddress_Recipient(ResponseError):
UnverifiedEmailAddress_Sender(ResponseError):
ResponseFactory(action):
FPSResponse(Response):
_Result
globals().get(action
'Response':
super(FPSResponse,
FPSResponse
ResponseElement(object):
'{!s}:
{!r}'.format(*pair)
'Undefined'
'ResponseMetadata':
ResponseElement(name=name))
'Result':
self._Result(name=name))
self.Value)
AmountCollection(ResponseElement):
AccountBalance(AmountCollection):
'AvailableBalances':
super(AccountBalance,
GetAccountBalanceResult(ResponseElement):
'AccountBalance':
AccountBalance(name=name))
super(GetAccountBalanceResult,
GetTotalPrepaidLiabilityResult(ResponseElement):
'OutstandingPrepaidLiability':
super(GetTotalPrepaidLiabilityResult,
GetPrepaidBalanceResult(ResponseElement):
'PrepaidBalance':
super(GetPrepaidBalanceResult,
GetOutstandingDebtBalanceResult(ResponseElement):
'OutstandingDebt':
super(GetOutstandingDebtBalanceResult,
TransactionPart(ResponseElement):
'FeesPaid':
super(TransactionPart,
Transaction(ResponseElement):
self.TransactionPart
'TransactionPart':
name).append(TransactionPart(name=name))
('TransactionAmount',
'FPSFees',
'Balance'):
GetAccountActivityResult(ResponseElement):
self.Transaction
name).append(Transaction(name=name))
GetTransactionResult(ResponseElement):
Transaction(name=name))
super(GetTransactionResult,
GetTokensResult(ResponseElement):
self.Token
name).append(ResponseElement(name=name))
get_regions('glacier',
connection_cls=Layer2)
DEFAULT_PART_SIZE,
UploadArchiveError,
DownloadArchiveError,
logging.getLogger('boto.glacier.concurrent')
ConcurrentTransferer(object):
self._num_threads
num_threads
self._threads
_calculate_required_part_size(self,
minimum_part_size(total_size)
min_part_size_required:
log.debug("The
smaller
Using
self._part_size,
int(math.ceil(total_size
float(part_size)))
_shutdown_threads(self):
log.debug("Shutting
thread.join()
log.debug("Threads
exited.")
_add_work_items_to_queue(self,
log.debug("Adding
queue.")
worker_queue.put((i,
part_size))
worker_queue.put(_END_SENTINEL)
ConcurrentUploader(ConcurrentTransferer):
super(ConcurrentUploader,
upload(self,
os.stat(filename).st_size
self._api.initiate_multipart_upload(self._vault_name,
response['UploadId']
self._start_upload_threads(result_queue,
self._wait_for_upload_threads(hash_chunks,
"aborting
multipart
self._api.abort_multipart_upload(self._vault_name,
log.debug("Completing
self._api.complete_multipart_upload(
bytes_to_hex(tree_hash(hash_chunks)),
log.debug("Upload
finished.")
terminating
"threads:
UploadArchiveError("An
UploadWorkerThread(self._api,
TransferThread(threading.Thread):
result_queue):
super(TransferThread,
self._worker_queue
self._result_queue
self.should_continue
self.should_continue:
self._worker_queue.get(timeout=1)
Empty:
_END_SENTINEL:
self._process_chunk(work)
self._result_queue.put(result)
UploadWorkerThread(TransferThread):
super(UploadWorkerThread,
self._filename
self._fileobj
self._upload_id
range(self._num_retries
self._upload_chunk(work)
"vault
attempt:
%s),
"exception:
self._filename,
e.__class__,
_upload_chunk(self,
self._fileobj.seek(start_byte)
self._fileobj.read(part_size)
hashlib.sha256(contents).hexdigest()
tree_hash_bytes
tree_hash(chunk_hashes(contents))
len(contents)
log.debug("Uploading
self._api.upload_part(self._vault_name,
self._upload_id,
bytes_to_hex(tree_hash_bytes),
tree_hash_bytes)
self._fileobj.close()
ConcurrentDownloader(ConcurrentTransferer):
super(ConcurrentDownloader,
self._job.archive_size
self._start_download_threads(result_queue,
worker_queue)
self._wait_for_download_threads(filename,
log.debug("Download
completed.")
"terminating
DownloadArchiveError(
"An
actual_hash,
f.seek(start_byte)
f.write(data)
bytes_to_hex(tree_hash(hash_chunks))
log.debug("Verifying
final
expecting:
"actual:
self._job.sha256_treehash,
final_hash)
self._job.sha256_treehash
final_hash:
entire
(self._job.sha256_treehash,
final_hash))
DownloadWorkerThread(self._job,
DownloadWorkerThread(TransferThread):
super(DownloadWorkerThread,
range(self._num_retries):
self._download_chunk(work)
"job
self._job,)
_download_chunk(self,
log.debug("Downloading
self._job.get_output(byte_range)
bytes_to_hex(tree_hash(chunk_hashes(data)))
actual_hash:
actual_hash))
binascii.unhexlify(actual_hash),
UnexpectedHTTPResponseError(Exception):
expected_responses,
body["code"]
expected_responses
'(%d,
code=%s,
message=%s)'
body["message"])
(%d,
(expected_responses,
super(UnexpectedHTTPResponseError,
self).__init__(msg)
ArchiveError(Exception):
UploadArchiveError(ArchiveError):
DownloadArchiveError(ArchiveError):
TreeHashDoesNotMatchError(ArchiveError):
TreeHashDoesNotMatchError,
tree_hash_from_str
Job(object):
(('Action',
('ArchiveId',
'archive_id',
('ArchiveSizeInBytes',
'archive_size',
('Completed',
'completed',
('CompletionDate',
'completion_date',
('InventorySizeInBytes',
'inventory_size',
('JobDescription',
('JobId',
('SHA256TreeHash',
'sha256_treehash',
('SNSTopic',
'sns_topic',
('StatusCode',
'status_code',
('StatusMessage',
'status_message',
response_data[response_name])
'Job(%s)'
get_output(self,
validate_checksum=False):
self.vault.layer1.get_job_output(self.vault.name,
byte_range)
validate_checksum
'TreeHash'
_calc_num_chunks(self,
int(math.ceil(self.archive_size
download_to_file(self,
output_file:
download_to_fileobj(self,
output_file,
_download_to_fileob(self,
range(num_chunks):
chunk_size),
self._download_byte_range(
verify_hashes:
expected_tree_hash,
fileobj.write(data)
_download_byte_range(self,
range(5):
self.get_output(byte_range)
DownloadArchiveError("There
downloading"
"byte
(byte_range,
UnexpectedHTTPResponseError
ResettingFileSender
'2012-06-01'
account_id='-',
region_name='us-east-1',
boto.glacier.regions():
self.account_id
self).__init__(region.endpoint,
suppress_consec_slashes,
ok_responses=(200,),
headers['x-amz-glacier-version']
(self.account_id,
self).make_request(verb,
ok_responses:
GlacierResponse(response,
UnexpectedHTTPResponseError(ok_responses,
list_vaults(self,
'vaults',
describe_vault(self,
response_headers=[('Location',
'Location')])
get_vault_notifications(self,
set_vault_notifications(self,
notification_config):
json_config
json.dumps(notification_config)
data=json_config,
delete_vault_notifications(self,
status_code=None,
status_code:
params['statuscode']
params['completed']
describe_job(self,
'vaults/%s/jobs/%s'
ok_responses=(200,))
initiate_job(self,
job_data):
u'JobId'),
json_job_data
json.dumps(job_data)
data=json_job_data,
ok_responses=(202,),
get_job_output(self,
job_id,
byte_range=None):
u'TreeHash'),
('Content-Range',
u'ContentRange'),
('Content-Type',
u'ContentType')]
byte_range:
{'Range':
'vaults/%s/jobs/%s/output'
ok_responses=(200,
206),
u'Location'),
('x-amz-sha256-tree-hash',
'vaults/%s/archives'
str(len(archive))
str(os.fstat(archive.fileno()).st_size)
content_length}
self._is_file_like(archive):
ResettingFileSender(archive)
data=archive,
_is_file_like(self,
'tell')
'vaults/%s/archives/%s'
[('x-amz-multipart-upload-id',
u'UploadId'),
{'x-amz-part-size':
str(part_size)}
archive_size):
{'x-amz-sha256-tree-hash':
'x-amz-archive-size':
str(archive_size)}
abort_multipart_upload(self,
list_parts(self,
'Content-Range':
%d-%d/*'
(str(vault_name),
data=part_data,
ok_responses=(204,),
"layer1"
kwargs["layer1"]
self.layer1.create_vault(name)
self.get_vault(name)
self.layer1.delete_vault(name)
get_vault(self,
self.layer1.describe_vault(name)
Vault(self.layer1,
list_vaults(self):
self.layer1.list_vaults(marker=marker,
limit=1000)
vaults.extend([Vault(self.layer1,
rd)
rd
response_data['VaultList']])
response_data.get('Marker')
GlacierResponse(dict):
http_response,
self[u'RequestId']
http_response.getheader('x-amzn-requestid')
header_name,
self[item_name]
http_response.getheader(header_name)
http_response.getheader('Content-Type')
json.loads(http_response.read().decode('utf-8'))
self.update(body)
http_response.getheader('Content-Length',
"Reads
bytes."
self.http_response.read(amt)
minimum_part_size(size_in_bytes,
default_part_size=DEFAULT_PART_SIZE):
(default_part_size
MAXIMUM_NUMBER_OF_PARTS)
size_in_bytes:
(4096
10000):
ValueError("File
large:
size_in_bytes)
min_part_size
min_part_size:
math.ldexp(_MEGABYTE,
power)
int(part_size)
chunk_hashes(bytestring,
chunk_size=_MEGABYTE):
chunk_count
int(math.ceil(len(bytestring)
range(chunk_count):
hashes.append(hashlib.sha256(bytestring[start:end]).digest())
hashes:
tree_hash(fo):
hashes.extend(fo)
new_hashes
new_hashes.append(hashlib.sha256(first
second).digest())
new_hashes.append(only)
hashes.extend(new_hashes)
hashes[0]
compute_hashes_from_fileobj(fileobj,
chunk_size=1024
1024):
hasattr(fileobj,
'mode')
fileobj.mode:
ValueError('File-like
mode!')
hashlib.sha256()
chunk.encode(getattr(fileobj,
linear_hash.update(chunk)
chunks.append(hashlib.sha256(chunk).digest())
chunks:
linear_hash.hexdigest(),
bytes_to_hex(tree_hash(chunks))
bytes_to_hex(str_as_bytes):
binascii.hexlify(str_as_bytes)
tree_hash_from_str(str_as_bytes):
bytes_to_hex(tree_hash(chunk_hashes(str_as_bytes)))
ResettingFileSender(object):
self._archive
self._starting_offset
archive.tell()
connection.request(method,
self._archive,
self._archive.seek(self._starting_offset)
compute_hashes_from_fileobj,
resume_file_upload,
Writer
ConcurrentUploader
MAXIMUM_ARCHIVE_SIZE
Vault(object):
SingleOperationThreshold
(('VaultName',
('LastInventoryDate',
'last_inventory_date',
('SizeInBytes',
'size',
('NumberOfArchives',
'number_of_archives',
response_data[response_name]
'Vault("%s")'
self.layer1.delete_vault(self.name)
self.SingleOperationThreshold:
self.create_archive_from_file(filename,
description=description)
self._upload_archive_single_operation(filename,
_upload_archive_single_operation(self,
fileobj:
compute_hashes_from_fileobj(fileobj)
self.layer1.upload_archive(self.name,
create_archive_writer(self,
part_size=DefaultPartSize,
self.layer1.initiate_multipart_upload(self.name,
Writer(self,
response['UploadId'],
create_archive_from_file(self,
file_obj=None,
upload_id_callback=None):
self.DefaultPartSize
minimum_part_size(file_size,
UploadArchiveError("File
exceeds
"40,000
GB
Glacier.")
self.create_archive_writer(
description=description,
upload_id_callback:
upload_id_callback(writer.upload_id)
file_obj.read(part_size)
writer.write(data)
writer.close()
writer.get_archive_id()
_range_string_to_part_index(range_string,
[int(value)
range_string.split('-')]
boundary")
(length
bigger
size"
resume_archive_from_file(self,
file_obj=None):
part_list_response
self.list_all_parts(upload_id)
part_list_response['PartSizeInBytes']
part_desc
part_list_response['Parts']:
self._range_string_to_part_index(
part_desc['RangeInBytes'],
codecs.decode(part_desc['SHA256TreeHash'],
'hex_codec')
file_obj,
part_hash_map)
concurrent_create_archive_from_file(self,
ConcurrentUploader(self.layer1,
uploader.upload(filename,
retrieve_archive(self,
archive_id,
'archive-retrieval',
'ArchiveId':
archive_id}
self.get_job(response['JobId'])
retrieve_inventory(self,
'inventory-retrieval'}
job_data['RetrievalByteRange']
rparams['StartDate']
start_date.strftime('%Y-%m-%dT%H:%M:%S%Z')
rparams['EndDate']
end_date.strftime('%Y-%m-%dT%H:%M:%S%Z')
rparams['Limit']
job_data['InventoryRetrievalParameters']
response['JobId']
retrieve_inventory_job(self,
job_id
self.retrieve_inventory(**kwargs)
self.get_job(job_id)
self.layer1.delete_archive(self.name,
get_job(self,
self.layer1.describe_job(self.name,
Job(self,
status_code=None):
self.layer1.list_jobs(self.name,
completed,
status_code)
[Job(self,
jd)
jd
response_data['JobList']]
list_all_parts(self,
self.layer1.list_parts(self.name,
additional_result
self.layer1.list_parts(
result['Parts'].extend(additional_result['Parts'])
additional_result['Marker']
_ONE_MEGABYTE
_Partitioner(object):
send_fn):
self.send_fn
send_fn
self._buffer.append(data)
_send_part(self):
b''.join(self._buffer)
[data[self.part_size:]]
len(self._buffer[0])
data[:self.part_size]
self.send_fn(part)
_Uploader(object):
self._tree_hashes
_insert_tree_hash(self,
raw_tree_hash):
list_length
len(self._tree_hashes)
list_length:
self._tree_hashes.extend([None]
(list_length
self._tree_hashes[index]
raw_tree_hash
bytes_to_hex(part_tree_hash)
hashlib.sha256(part_data).hexdigest()
content_range
self.vault.layer1.upload_part(self.vault.name,
content_range,
len(part_data)
skip_part(self,
part_length):
part_length
self._tree_hashes:
RuntimeError("Some
uploaded.")
bytes_to_hex(tree_hash(self._tree_hashes))
self.vault.layer1.complete_multipart_upload(
self.vault.name,
self._uploaded_size)
resume_file_upload(vault,
enumerate(
(part_index
part_tree_hash):
uploader.upload_part(part_index,
uploader.skip_part(part_index,
uploader.close()
uploader.archive_id
Writer(object):
self.partitioner
_Partitioner(part_size,
self._upload_part)
self.partitioner.write(data)
_upload_part(self,
self.uploader.upload_part(self.next_part_index,
self.partitioner.flush()
self.uploader.close()
get_archive_id(self):
self.uploader.archive_id
current_tree_hash(self):
tree_hash(self.uploader._tree_hashes)
current_uploaded_size(self):
self.uploader._uploaded_size
upload_id(self):
self.uploader.upload_id
vault(self):
self.uploader.vault
'AccessControlList'
ALL_AUTHENTICATED_USERS
'AllAuthenticatedUsers'
ALL_USERS
'AllUsers'
DISPLAY_NAME
'DisplayName'
DOMAIN
'Domain'
EMAIL_ADDRESS
'EmailAddress'
'Entry'
'Entries'
GROUP_BY_DOMAIN
'GroupByDomain'
GROUP_BY_EMAIL
'GroupByEmail'
GROUP_BY_ID
'GroupById'
'ID'
NAME
OWNER
'Owner'
PERMISSION
'Permission'
'Scope'
'type'
USER_BY_EMAIL
'UserByEmail'
USER_BY_ID
'UserById'
'project-private',
'bucket-owner-full-control']
acl(self):
['Owner:%s'
self.owner.__repr__()]
acl_entries.entry_list:
Entry(type=USER_BY_EMAIL,
Entry(permission=permission,
type=USER_BY_ID,
id=user_id)
Entry(type=GROUP_BY_EMAIL,
Entry(type=GROUP_BY_ID,
id=group_id,
acl_entries.to_xml()
Entries(object):
self.entry_list
'<Entries:
Entry(self)
self.entry_list.append(entry)
entry.to_xml()
Entry(object):
scope=None,
permission=None):
(self.scope.__repr__(),
self.permission.__repr__())
InvalidAclError('Missing
ACL'
(TYPE,
SCOPE))
attrs[TYPE])
SupportedPermissions:
Permission
self.scope.to_xml()
(PERMISSION,
self.permission,
PERMISSION)
Scope(object):
ALLOWED_SCOPE_TYPE_SUB_ELEMS
ALL_USERS.lower()
GROUP_BY_DOMAIN.lower()
[DOMAIN.lower()],
USER_BY_EMAIL.lower()
USER_BY_ID.lower()
NAME.lower()]
self.ALLOWED_SCOPE_TYPE_SUB_ELEMS:
self.email_address:
named_entity:
(self.type,
named_entity)
self.ALLOWED_SCOPE_TYPE_SUB_ELEMS[self.type.lower()]):
InvalidAclError('Element
SCOPE,
DOMAIN.lower():
EMAIL_ADDRESS.lower():
ID.lower():
NAME.lower():
ALL_USERS.lower()):
GROUP_BY_DOMAIN.lower():
(DOMAIN,
self.domain,
DOMAIN)
USER_BY_EMAIL.lower()):
(EMAIL_ADDRESS,
self.email_address,
EMAIL_ADDRESS)
USER_BY_ID.lower()):
(ID,
ID)
ACL,
GSPermissions
boto.gs.bucketlistresultset
S3Bucket
'defaultObjectAcl'
CORS_ARG
'cors'
LIFECYCLE_ARG
'lifecycle'
ERROR_DETAILS_REGEX
re.compile(r'<Details>(?P<details>.*)</Details>')
Bucket(S3Bucket):
'<VersioningConfiguration><Status>%s</Status>'
'</VersioningConfiguration>')
WebsiteBody
'<WebsiteConfiguration>%s%s</WebsiteConfiguration>')
WebsiteMainPageFragment
'<MainPageSuffix>%s</MainPageSuffix>'
WebsiteErrorFragment
'<NotFoundPage>%s</NotFoundPage>'
key_class=GSKey):
key_class)
urllib.quote(rv)))
'Forbidden'
e.reason:
("Access
'gs://%s/%s'."
headers['x-goog-copy-source-generation']
str(src_generation)
self).copy_key(
generation_marker,
['version_id_marker',
'generation_marker',
'max_keys'])
self.set_def_xml_acl(acl_or_str.to_xml(),
self.set_def_canned_acl(acl_or_str,
_get_xml_acl_helper(self,
ERROR_DETAILS_REGEX.search(body)
match.group('details')
(('<Details>%s.
Full
Control
access'
ACLs.</Details>')
re.sub(ERROR_DETAILS_REGEX,
_get_acl_helper(self,
self._get_acl_helper(key_name,
self._get_acl_helper('',
DEF_OBJ_ACL)
_set_acl_helper(self,
canned=False):
canned:
metageneration
"meaning
generation.")
(if_generation
if_metageneration):
"parameter
bucket.")
headers['x-goog-if-metageneration-match']
str(if_metageneration)
get_utf8_value(key_name),
data=get_utf8_value(data),
query_args='acl',
key_name=key_name,
if_metageneration=None,
self.set_xml_acl(acl_str,
query_args=DEF_OBJ_ACL)
data=get_utf8_value(cors),
get_storage_class(self):
query_args='storageClass')
rs.StorageClass
acl.entries
encoding="UTF-8"?><Logging/>'
encoding="UTF-8"?><Logging>'
'<LogBucket>%s</LogBucket>'
target_prefix:
'<LogObjectPrefix>%s</LogObjectPrefix>'
'</Logging>'
get_logging_config_with_xml(self,
self.get_logging_config_with_xml(headers)[0]
main_page_suffix:
self.WebsiteMainPageFragment
main_page_suffix
error_key:
self.WebsiteErrorFragment
self.WebsiteBody
(main_page_frag,
error_frag)
data=get_utf8_value(body),
self.configure_website(headers=headers)
boto.jsonresponse.XmlHandler(resp_json,
None).parse(body)
resp_json['VersioningConfiguration']
('Status'
resp_json)
(resp_json['Status']
('Enabled')
('Suspended')
self.set_subresource('versioning',
req_body,
handler.XmlHandler(lifecycle_config,
data=get_utf8_value(xml),
generation_marker=generation_marker,
max_keys=999)
rs.next_generation_marker
self.generation_marker
generation_marker=self.generation_marker,
headers=self.headers)
boto.gs.bucket
SubdomainCallingFormat
check_lowercase_bucketname
'US'
GSConnection(S3Connection):
'storage.googleapis.com'
'Signature=%s&Expires=%d&GoogleAccessId=%s'
gs_access_key_id=None,
calling_format=SubdomainCallingFormat(),
suppress_consec_slashes=True):
super(GSConnection,
self).__init__(gs_access_key_id,
calling_format,
"google",
Bucket,
suppress_consec_slashes=suppress_consec_slashes)
storage_class='STANDARD'):
{self.provider.acl_header
Location.DEFAULT
location_elem
('<LocationConstraint>%s</LocationConstraint>'
('<StorageClass>%s</StorageClass>'
('<CreateBucketConfiguration>%s%s</CreateBucketConfiguration>'
(location_elem,
storage_class_elem))
self.make_request(
get_utf8_value(bucket_name),
data=get_utf8_value(data))
bucket.get_all_keys(headers,
maxkeys=0)
InvalidCorsError
'CorsConfig'
'Cors'
'Origins'
ORIGIN
'Origin'
'Methods'
METHOD
'Method'
'ResponseHeaders'
'ResponseHeader'
MAXAGESEC
'MaxAgeSec'
Cors(handler.ContentHandler):
self.cors
self.legal_collections
[ORIGIN],
[METHOD],
[HEADER],
MAXAGESEC:
self.legal_elements
[ORIGIN,
METHOD,
HEADER]
validateParseLevel(self,
level):
level:
InvalidCorsError('Invalid
%d:
self.parse_level))
self.cors.append(self.collections)
self.collection:
(self.collection,
self.legal_collections[name]:
self.elements))
self.element:
(self.element,
self.elements.append((name,
self.cors:
(collection,
elements_or_value)
collections:
isinstance(elements_or_value,
elements_or_value
elements_or_value:
S3Key
Key(S3Key):
self).__init__(bucket=bucket,
self.meta_generation
self.cloud_hashes
self.metageneration:
'#%s.%s'
(self.generation,
%s,%s%s>'
None,%s%s>'
'Generation':
'MetaGeneration':
resp.getheader('x-goog-metageneration',
resp.getheader('x-goog-generation',
'x-goog-hash':
hash_pair
value.split(','):
alg,
b64_digest
hash_pair.strip().split('=',
self.cloud_hashes[alg]
binascii.a2b_base64(b64_digest)
'x-goog-component-count':
'x-goog-generation':
'x-goog-stored-content-encoding':
'x-goog-stored-content-length':
'generation=%s'
self).open_read(headers=headers,
['generation=%s'
self.generation]
hash_algs=hash_algs,
compute_hash(self,
algorithm,
compute_hash(
hash_algorithm=algorithm)
acl.add_group_grant(permission,
group_id)
'"size"
uploads.')
res_upload_handler.send_file(self,
reduced_redundancy=None,
res_upload_handler,
StringIO(get_utf8_value(s))
kwargs.pop('if_generation',
kwargs.get('headers',
kwargs['headers']
self).set_contents_from_stream(*args,
self.bucket.set_acl(acl_or_str,
self.bucket.set_canned_acl(
if_metageneration=if_metageneration
compose_req
self.bucket.name:
key.generation:
('<Generation>%s</Generation>'
str(key.generation))
compose_req.append('<Component><Name>%s</Name>%s</Component>'
generation_tag))
compose_req_xml
('<ComposeRequest>%s</ComposeRequest>'
''.join(compose_req))
content_type:
get_utf8_value(self.bucket.name),
query_args='compose',
data=get_utf8_value(compose_req_xml))
self.bucket.connection.provider.storage_response_error(
resp.reason,
resp.read())
resp.getheader('x-goog-generation')
InvalidLifecycleConfigError
'LifecycleConfiguration'
'Rule'
'Action'
DELETE
'Delete'
'Condition'
AGE
'Age'
CREATED_BEFORE
'CreatedBefore'
NUM_NEWER_VERSIONS
'NumberOfNewerVersions'
IS_LIVE
'IsLive'
LEGAL_ACTIONS
[DELETE]
LEGAL_ACTION_PARAMS
LEGAL_CONDITIONS
[AGE,
CREATED_BEFORE,
NUM_NEWER_VERSIONS,
IS_LIVE]
LEGAL_ACTION_ACTION_PARAMS
DELETE:
action=None,
action_params=None,
conditions=None):
self.action_params
action_params
self.conditions
validateStartTag(self,
tag'
self.current_tag))
validateEndTag(self,
self.current_tag:
'Mismatched
(self.current_tag,
tag))
ACTION)
action'
LEGAL_ACTION_ACTION_PARAMS[self.action]:
self.action))
CONDITION)
'Found
self.validateEndTag(name)
self.validate()
self.action_params[name]
self.conditions[name]
self.action:
self.action)
self.action_params[param]
'/>'
self.conditions[condition]
LifecycleConfig(list):
InvalidLifecycleConfigError('Invalid
Rule(action,
ResumableUploadHandler(object):
BUFFER_SIZE
SERVER_HAS_NOTHING
Byte
check.
self._load_tracker_uri_from_file()
_load_tracker_uri_from_file(self):
self._set_tracker_uri(uri)
'(%s).
(uri,
self.tracker_file_name))
_save_tracker_uri_to_file(self):
os.fdopen(os.open(self.tracker_file_name,
os.O_WRONLY
os.O_CREAT,
0o600),
f.write(self.tracker_uri)
_set_tracker_uri(self,
parse_result
urlparse.urlparse(uri)
(parse_result.scheme.lower()
['http',
'https']
parse_result.netloc):
self.tracker_uri_host
parse_result.netloc
self.tracker_uri_path
'%s?%s'
parse_result.path,
parse_result.query)
get_tracker_uri(self):
get_upload_id(self):
'?upload_id='
self.tracker_uri[self.tracker_uri.index(delim)
len(delim):]
_build_content_range_header(self,
range_spec='*',
length_spec='*'):
(range_spec,
length_spec)
_query_server_state(self,
self._build_content_range_header('*',
'0'
AWSAuthConnection.make_request(conn,
auth_path=self.tracker_uri_path,
_query_server_pos(self,
308:
non-308
query'
range_spec
resp.getheader('range')
range_spec:
re.search('bytes=(\d+)-(\d+)',
range_spec)
long(m.group(1))
long(m.group(2))
got_valid_response:
str(resp.getheaders()),
print('Server
has:
Range:
server_end))
_start_new_resumable_upload(self,
upload.')
post_headers
(disallowed)',
post_headers[k]
headers[k]
post_headers[conn.provider.resumable_upload_header]
conn.make_request(
post_headers)
resp.read()
'Will
wait/retry'
'Aborting'
tracker_uri
resp.getheader('Location')
tracker_uri:
initiation
'POST
self._set_tracker_uri(tracker_uri)
self._save_tracker_uri_to_file()
_upload_file_bytes(self,
self.BUFFER_SIZE
(num_cb-2)
'%d-%d'
str(file_length
total_bytes_uploaded)
AWSAuthConnection.build_base_http_request(
http_conn.putrequest('PUT',
http_request.path)
put_headers:
http_conn.putheader(k,
put_headers[k])
buf:
http_conn.send(buf)
self.digesters[alg].update(buf)
len(buf)
file.'
file_length),
(resp.getheader('etag'),
resp.getheader('x-goog-generation'),
resp.getheader('x-goog-metageneration'))
[408,
ResumableTransferDisposition.WAIT_BEFORE_RETRY
ResumableTransferDisposition.ABORT
attempting
(resp.status,
resp.reason),
_attempt_resumable_upload(self,
num_cb):
self._query_server_pos(conn,
server_end:
print('Catching
digest(s)
resumed
upload')
bytes_to_go:
fp.read(min(key.BufferSize,
bytes_to_go))
'Hit
'catchup.
under\n'
circumstances,
'server
transfer\nthan'
upload.',
self.digesters[alg].update(chunk)
transfer.')
resume
transfer
(%s).'
total_bytes_uploaded:
fp.seek(total_bytes_uploaded)
http_conn
conn.new_http_connection(self.tracker_uri_host,
self._upload_file_bytes(conn,
(ResumableUploadException,
socket.error):
'state
attempt.
'can
various
ACL)
attempts',
http_conn.close()
_check_final_md5(self,
print('Checking
etag.')
etag.strip('"\''):
key.open_read()
'(incorrect
uploaded
deleted)',
handle_resumable_upload_exception(self,
debug):
retaining
retry'
track_progress_less_iterations(self,
server_had_bytes_before_attempt,
roll_back_md5=True,
debug=0):
server_had_bytes_before_attempt:
progress,
reset
counter.
roll_back_md5:
(2**self.progress_less_iterations)
(self.progress_less_iterations,
server_had_bytes_before_attempt
self.digesters[alg].copy())
self.digesters)
self.generation,
self._attempt_resumable_upload(key,
num_cb))
key.local_hashes[alg]
self.digesters[alg].digest()
self._check_final_md5(key,
etag)
key.bucket.connection.connection.close()
self.handle_resumable_upload_exception(e,
self.track_progress_less_iterations(server_had_bytes_before_attempt,
name=''):
'<Name>%s</Name>'
IAMRegionInfo(RegionInfo):
region_cls=IAMRegionInfo,
IAMRegionInfo(
endpoint='iam.amazonaws.com',
boto.iam.summarymap
SummaryMap
DEFAULT_POLICY_DOCUMENTS
['ec2.amazonaws.com']
'amazonaws.com.cn':
['ec2.amazonaws.com.cn']
ASSUME_ROLE_POLICY_DOCUMENT
json.dumps(DEFAULT_POLICY_DOCUMENTS['default'])
IAMConnection(AWSQueryConnection):
'2010-05-08'
host='iam.amazonaws.com',
super(IAMConnection,
list_marker='Set'):
boto.jsonresponse.Element(list_marker=list_marker,
self.get_response('ListGroups',
self.get_response('GetGroup',
create_group(self,
self.get_response('CreateGroup',
delete_group(self,
self.get_response('DeleteGroup',
update_group(self,
new_group_name=None,
new_group_name:
params['NewGroupName']
new_group_name
self.get_response('UpdateGroup',
add_user_to_group(self,
self.get_response('AddUserToGroup',
remove_user_from_group(self,
self.get_response('RemoveUserFromGroup',
put_group_policy(self,
self.get_response('PutGroupPolicy',
get_all_group_policies(self,
self.get_response('ListGroupPolicies',
get_group_policy(self,
self.get_response('GetGroupPolicy',
delete_group_policy(self,
self.get_response('DeleteGroupPolicy',
get_all_users(self,
{'PathPrefix':
path_prefix}
self.get_response('ListUsers',
self.get_response('CreateUser',
delete_user(self,
self.get_response('DeleteUser',
self.get_response('GetUser',
update_user(self,
new_user_name=None,
new_user_name:
params['NewUserName']
new_user_name
self.get_response('UpdateUser',
get_all_user_policies(self,
self.get_response('ListUserPolicies',
put_user_policy(self,
self.get_response('PutUserPolicy',
get_user_policy(self,
self.get_response('GetUserPolicy',
delete_user_policy(self,
self.get_response('DeleteUserPolicy',
get_groups_for_user(self,
self.get_response('ListGroupsForUser',
get_all_access_keys(self,
self.get_response('ListAccessKeys',
list_marker='AccessKeyMetadata')
create_access_key(self,
self.get_response('CreateAccessKey',
update_access_key(self,
self.get_response('UpdateAccessKey',
delete_access_key(self,
access_key_id}
self.get_response('DeleteAccessKey',
get_all_signing_certs(self,
self.get_response('ListSigningCertificates',
list_marker='Certificates')
update_signing_cert(self,
self.get_response('UpdateSigningCertificate',
upload_signing_cert(self,
{'CertificateBody':
cert_body}
self.get_response('UploadSigningCertificate',
delete_signing_cert(self,
cert_id}
self.get_response('DeleteSigningCertificate',
list_server_certs(self,
self.get_response('ListServerCertificates',
list_marker='ServerCertificateMetadataList')
get_all_server_certs
list_server_certs
update_server_cert(self,
new_cert_name=None,
new_cert_name:
params['NewServerCertificateName']
new_cert_name
self.get_response('UpdateServerCertificate',
upload_server_cert(self,
cert_chain=None,
'CertificateBody':
'PrivateKey':
private_key}
cert_chain:
params['CertificateChain']
cert_chain
self.get_response('UploadServerCertificate',
get_server_certificate(self,
self.get_response('GetServerCertificate',
delete_server_cert(self,
self.get_response('DeleteServerCertificate',
get_all_mfa_devices(self,
self.get_response('ListMFADevices',
list_marker='MFADevices')
enable_mfa_device(self,
self.get_response('EnableMFADevice',
deactivate_mfa_device(self,
serial_number):
serial_number}
self.get_response('DeactivateMFADevice',
resync_mfa_device(self,
self.get_response('ResyncMFADevice',
get_login_profiles(self,
self.get_response('GetLoginProfile',
create_login_profile(self,
self.get_response('CreateLoginProfile',
delete_login_profile(self,
self.get_response('DeleteLoginProfile',
update_login_profile(self,
self.get_response('UpdateLoginProfile',
create_account_alias(self,
self.get_response('CreateAccountAlias',
delete_account_alias(self,
self.get_response('DeleteAccountAlias',
get_account_alias(self):
self.get_response('ListAccountAliases',
list_marker='AccountAliases')
get_signin_url(self,
service='ec2'):
self.get_account_alias()
alias:
alias.get('list_account_aliases_response',
resp.get('list_account_aliases_result',
aliases
result.get('account_aliases',
len(aliases):
aliases[0]
'iam.us-gov.amazonaws.com':
"https://%s.signin.amazonaws-us-gov.com/console/%s"
self.host.endswith('amazonaws.com.cn'):
"https://%s.signin.amazonaws.cn/console/%s"
"https://%s.signin.aws.amazon.com/console/%s"
get_account_summary(self):
self.get_object('GetAccountSummary',
SummaryMap)
add_role_to_instance_profile(self,
self.get_response('AddRoleToInstanceProfile',
create_instance_profile(self,
instance_profile_name}
self.get_response('CreateInstanceProfile',
_build_policy(self,
assume_role_policy_document=None):
isinstance(assume_role_policy_document,
tld,
DEFAULT_POLICY_DOCUMENTS.items():
tld
self.host.endswith(tld):
assume_role_policy_document:
DEFAULT_POLICY_DOCUMENTS['default']
json.dumps(assume_role_policy_document)
create_role(self,
assume_role_policy_document=None,
self._build_policy(
self.get_response('CreateRole',
delete_instance_profile(self,
'DeleteInstanceProfile',
delete_role(self,
self.get_response('DeleteRole',
delete_role_policy(self,
'DeleteRolePolicy',
get_instance_profile(self,
self.get_response('GetInstanceProfile',
get_role(self,
self.get_response('GetRole',
get_role_policy(self,
self.get_response('GetRolePolicy',
list_instance_profiles(self,
self.get_response('ListInstanceProfiles',
list_instance_profiles_for_role(self,
self.get_response('ListInstanceProfilesForRole',
list_role_policies(self,
self.get_response('ListRolePolicies',
list_roles(self,
self.get_response('ListRoles',
list_marker='Roles')
put_role_policy(self,
self.get_response('PutRolePolicy',
remove_role_from_instance_profile(self,
self.get_response('RemoveRoleFromInstanceProfile',
update_assume_role_policy(self,
self.get_response('UpdateAssumeRolePolicy',
create_saml_provider(self,
self.get_response('CreateSAMLProvider',
list_saml_providers(self):
self.get_response('ListSAMLProviders',
list_marker='SAMLProviderList')
get_saml_provider(self,
self.get_response('GetSAMLProvider',
update_saml_provider(self,
saml_metadata_document):
self.get_response('UpdateSAMLProvider',
delete_saml_provider(self,
self.get_response('DeleteSAMLProvider',
generate_credential_report(self):
self.get_response('GenerateCredentialReport',
get_credential_report(self):
self.get_response('GetCredentialReport',
create_virtual_mfa_device(self,
device_name):
self.get_response('CreateVirtualMFADevice',
get_account_password_policy(self):
self.get_response('GetAccountPasswordPolicy',
delete_account_password_policy(self):
self.get_response('DeleteAccountPasswordPolicy',
update_account_password_policy(self,
allow_users_to_change_password=None,
hard_expiry=None,
max_password_age=None
minimum_password_length=None
password_reuse_prevention=None,
require_lowercase_characters=None,
require_numbers=None,
require_symbols=None
require_uppercase_characters=None):
allow_users_to_change_password
params['AllowUsersToChangePassword']
str(allow_users_to_change_password).lower()
hard_expiry
params['HardExpiry']
str(hard_expiry).lower()
params['MaxPasswordAge']
params['MinimumPasswordLength']
params['PasswordReusePrevention']
require_lowercase_characters
params['RequireLowercaseCharacters']
str(require_lowercase_characters).lower()
require_numbers
params['RequireNumbers']
str(require_numbers).lower()
require_symbols
params['RequireSymbols']
str(require_symbols).lower()
require_uppercase_characters
params['RequireUppercaseCharacters']
str(require_uppercase_characters).lower()
self.get_response('UpdateAccountPasswordPolicy',
create_policy(self,
str(description)
self.get_response('CreatePolicy',
create_policy_version(
set_as_default=None):
policy_document}
type(set_as_default)
params['SetAsDefault']
str(set_as_default).lower()
self.get_response('CreatePolicyVersion',
self.get_response('DeletePolicy',
delete_policy_version(self,
self.get_response('DeletePolicyVersion',
self.get_response('GetPolicy',
get_policy_version(self,
self.get_response('GetPolicyVersion',
list_policies(self,
only_attached=None,
scope=None):
type(only_attached)
params['OnlyAttached']
str(only_attached).lower()
params['Scope']
list_marker='Policies')
list_policy_versions(self,
list_marker='Versions')
set_default_policy_version(self,
self.get_response('SetDefaultPolicyVersion',
list_entities_for_policy(self,
entity_filter=None):
params['EntityFilter']
self.get_response('ListEntitiesForPolicy',
list_marker=('PolicyGroups',
'PolicyUsers',
'PolicyRoles'))
attach_group_policy(self,
self.get_response('AttachGroupPolicy',
attach_role_policy(self,
self.get_response('AttachRolePolicy',
attach_user_policy(self,
self.get_response('AttachUserPolicy',
detach_group_policy(self,
self.get_response('DetachGroupPolicy',
detach_role_policy(self,
self.get_response('DetachRolePolicy',
detach_user_policy(self,
self.get_response('DetachUserPolicy',
SummaryMap(dict):
get_regions('kinesis',
connection_cls=KinesisConnection)
ProvisionedThroughputExceededException(BotoServerError):
ExpiredIteratorException(BotoServerError):
InvalidArgumentException(BotoServerError):
SubscriptionRequiredException(BotoServerError):
KinesisConnection(AWSQueryConnection):
"2013-12-02"
"kinesis.us-east-1.amazonaws.com"
"Kinesis"
"Kinesis_20131202"
"ExpiredIteratorException":
exceptions.ExpiredIteratorException,
"InvalidArgumentException":
exceptions.InvalidArgumentException,
"SubscriptionRequiredException":
exceptions.SubscriptionRequiredException
super(KinesisConnection,
add_tags_to_stream(self,
self.make_request(action='AddTagsToStream',
create_stream(self,
shard_count):
'ShardCount':
shard_count,
self.make_request(action='CreateStream',
delete_stream(self,
stream_name):
self.make_request(action='DeleteStream',
describe_stream(self,
exclusive_start_shard_id=None):
params['ExclusiveStartShardId']
self.make_request(action='DescribeStream',
get_records(self,
b64_decode=True):
{'ShardIterator':
self.make_request(action='GetRecords',
b64_decode:
response.get('Records',
record['Data']
record['Data'].encode('utf-8')).decode('utf-8')
get_shard_iterator(self,
starting_sequence_number=None):
'ShardId':
'ShardIteratorType':
params['StartingSequenceNumber']
self.make_request(action='GetShardIterator',
list_streams(self,
exclusive_start_stream_name=None):
params['ExclusiveStartStreamName']
self.make_request(action='ListStreams',
list_tags_for_stream(self,
exclusive_start_tag_key=None,
params['ExclusiveStartTagKey']
self.make_request(action='ListTagsForStream',
merge_shards(self,
adjacent_shard_to_merge):
'ShardToMerge':
'AdjacentShardToMerge':
adjacent_shard_to_merge,
self.make_request(action='MergeShards',
put_record(self,
explicit_hash_key=None,
sequence_number_for_ordering=None,
exclusive_minimum_sequence_number=None,
params['ExplicitHashKey']
params['SequenceNumberForOrdering']
isinstance(params['Data'],
params['Data'].encode('utf-8')
base64.b64encode(params['Data']).decode('utf-8')
self.make_request(action='PutRecord',
put_records(self,
{'Records':
range(len(params['Records'])):
base64.b64encode(
data).decode('utf-8')
self.make_request(action='PutRecords',
remove_tags_from_stream(self,
'TagKeys':
self.make_request(action='RemoveTagsFromStream',
split_shard(self,
new_starting_hash_key):
'ShardToSplit':
'NewStartingHashKey':
new_starting_hash_key,
self.make_request(action='SplitShard',
boto.log.debug(response.getheaders())
get_regions('kms',
connection_cls=KMSConnection)
InvalidGrantTokenException(BotoServerError):
DisabledException(BotoServerError):
DependencyTimeoutException(BotoServerError):
InvalidMarkerException(BotoServerError):
AlreadyExistsException(BotoServerError):
InvalidCiphertextException(BotoServerError):
KeyUnavailableException(BotoServerError):
InvalidAliasNameException(BotoServerError):
UnsupportedOperationException(BotoServerError):
InvalidArnException(BotoServerError):
KMSInternalException(BotoServerError):
InvalidKeyUsageException(BotoServerError):
MalformedPolicyDocumentException(BotoServerError):
NotFoundException(BotoServerError):
boto.kms
KMSConnection(AWSQueryConnection):
"2014-11-01"
"kms.us-east-1.amazonaws.com"
"KMS"
"TrentService"
"InvalidGrantTokenException":
exceptions.InvalidGrantTokenException,
"DisabledException":
exceptions.DisabledException,
"DependencyTimeoutException":
exceptions.DependencyTimeoutException,
"InvalidMarkerException":
exceptions.InvalidMarkerException,
"AlreadyExistsException":
exceptions.AlreadyExistsException,
"InvalidCiphertextException":
exceptions.InvalidCiphertextException,
"KeyUnavailableException":
exceptions.KeyUnavailableException,
"InvalidAliasNameException":
exceptions.InvalidAliasNameException,
"UnsupportedOperationException":
exceptions.UnsupportedOperationException,
"InvalidArnException":
exceptions.InvalidArnException,
"KMSInternalException":
exceptions.KMSInternalException,
"InvalidKeyUsageException":
exceptions.InvalidKeyUsageException,
"MalformedPolicyDocumentException":
exceptions.MalformedPolicyDocumentException,
"NotFoundException":
exceptions.NotFoundException,
super(KMSConnection,
create_alias(self,
target_key_id):
'AliasName':
'TargetKeyId':
target_key_id,
self.make_request(action='CreateAlias',
create_grant(self,
retiring_principal=None,
operations=None,
'GranteePrincipal':
params['RetiringPrincipal']
params['Operations']
params['Constraints']
self.make_request(action='CreateGrant',
create_key(self,
key_usage=None):
params['KeyUsage']
self.make_request(action='CreateKey',
decrypt(self,
{'CiphertextBlob':
ciphertext_blob.decode('utf-8'),
self.make_request(action='Decrypt',
delete_alias(self,
alias_name):
{'AliasName':
self.make_request(action='DeleteAlias',
describe_key(self,
self.make_request(action='DescribeKey',
disable_key(self,
self.make_request(action='DisableKey',
disable_key_rotation(self,
self.make_request(action='DisableKeyRotation',
enable_key(self,
self.make_request(action='EnableKey',
enable_key_rotation(self,
self.make_request(action='EnableKeyRotation',
encrypt(self,
plaintext,
isinstance(plaintext,
``plaintext``
plaintext
base64.b64encode(plaintext)
'Plaintext':
plaintext.decode('utf-8'),
self.make_request(action='Encrypt',
generate_data_key(self,
self.make_request(action='GenerateDataKey',
generate_data_key_without_plaintext(self,
self.make_request(action='GenerateDataKeyWithoutPlaintext',
generate_random(self,
number_of_bytes=None):
self.make_request(action='GenerateRandom',
get_key_policy(self,
self.make_request(action='GetKeyPolicy',
get_key_rotation_status(self,
self.make_request(action='GetKeyRotationStatus',
list_aliases(self,
self.make_request(action='ListAliases',
self.make_request(action='ListGrants',
list_key_policies(self,
self.make_request(action='ListKeyPolicies',
list_keys(self,
self.make_request(action='ListKeys',
put_key_policy(self,
policy):
'Policy':
self.make_request(action='PutKeyPolicy',
re_encrypt(self,
source_encryption_context=None,
destination_encryption_context=None,
'CiphertextBlob':
'DestinationKeyId':
params['SourceEncryptionContext']
params['DestinationEncryptionContext']
self.make_request(action='ReEncrypt',
retire_grant(self,
grant_token):
{'GrantToken':
grant_token,
self.make_request(action='RetireGrant',
revoke_grant(self,
grant_id):
'GrantId':
grant_id,
self.make_request(action='RevokeGrant',
update_key_description(self,
self.make_request(action='UpdateKeyDescription',
get_regions('logs',
connection_cls=CloudWatchLogsConnection)
DataAlreadyAcceptedException(BotoServerError):
ServiceUnavailableException(BotoServerError):
ResourceAlreadyExistsException(BotoServerError):
OperationAbortedException(BotoServerError):
InvalidSequenceTokenException(BotoServerError):
CloudWatchLogsConnection(AWSQueryConnection):
"2014-03-28"
"logs.us-east-1.amazonaws.com"
"CloudWatchLogs"
"Logs_20140328"
"DataAlreadyAcceptedException":
exceptions.DataAlreadyAcceptedException,
"ServiceUnavailableException":
exceptions.ServiceUnavailableException,
"ResourceAlreadyExistsException":
exceptions.ResourceAlreadyExistsException,
"OperationAbortedException":
exceptions.OperationAbortedException,
"InvalidSequenceTokenException":
exceptions.InvalidSequenceTokenException,
super(CloudWatchLogsConnection,
create_log_group(self,
self.make_request(action='CreateLogGroup',
create_log_stream(self,
self.make_request(action='CreateLogStream',
delete_log_group(self,
self.make_request(action='DeleteLogGroup',
delete_log_stream(self,
self.make_request(action='DeleteLogStream',
delete_metric_filter(self,
filter_name):
self.make_request(action='DeleteMetricFilter',
delete_retention_policy(self,
self.make_request(action='DeleteRetentionPolicy',
describe_log_groups(self,
log_group_name_prefix=None,
params['logGroupNamePrefix']
self.make_request(action='DescribeLogGroups',
describe_log_streams(self,
log_stream_name_prefix=None,
params['logStreamNamePrefix']
self.make_request(action='DescribeLogStreams',
describe_metric_filters(self,
filter_name_prefix=None,
params['filterNamePrefix']
self.make_request(action='DescribeMetricFilters',
get_log_events(self,
start_from_head=None):
params['startTime']
params['endTime']
params['startFromHead']
self.make_request(action='GetLogEvents',
put_log_events(self,
sequence_token=None):
'logEvents':
params['sequenceToken']
self.make_request(action='PutLogEvents',
put_metric_filter(self,
metric_transformations):
'metricTransformations':
metric_transformations,
self.make_request(action='PutMetricFilter',
put_retention_policy(self,
self.make_request(action='PutRetentionPolicy',
set_retention(self,
self.make_request(action='SetRetention',
test_metric_filter(self,
log_event_messages):
'logEventMessages':
log_event_messages,
self.make_request(action='TestMetricFilter',
get_regions('machinelearning',
connection_cls=MachineLearningConnection)
InternalServerException(BotoServerError):
IdempotentParameterMismatchException(BotoServerError):
PredictorNotMountedException(BotoServerError):
InvalidInputException(BotoServerError):
boto.machinelearning
MachineLearningConnection(AWSQueryConnection):
"2014-12-12"
'machinelearning'
"machinelearning.us-east-1.amazonaws.com"
"MachineLearning"
"AmazonML_20141212"
"InternalServerException":
exceptions.InternalServerException,
"IdempotentParameterMismatchException":
exceptions.IdempotentParameterMismatchException,
"PredictorNotMountedException":
exceptions.PredictorNotMountedException,
"InvalidInputException":
exceptions.InvalidInputException,
super(MachineLearningConnection,
create_batch_prediction(self,
batch_prediction_name=None):
'BatchPredictionDataSourceId':
'OutputUri':
params['BatchPredictionName']
self.make_request(action='CreateBatchPrediction',
create_data_source_from_rds(self,
'RDSData':
self.make_request(action='CreateDataSourceFromRDS',
create_data_source_from_redshift(self,
self.make_request(action='CreateDataSourceFromRedshift',
create_data_source_from_s3(self,
self.make_request(action='CreateDataSourceFromS3',
create_evaluation(self,
evaluation_name=None):
'EvaluationDataSourceId':
params['EvaluationName']
self.make_request(action='CreateEvaluation',
create_ml_model(self,
recipe=None,
recipe_uri=None):
'MLModelType':
'TrainingDataSourceId':
params['Recipe']
params['RecipeUri']
self.make_request(action='CreateMLModel',
create_realtime_endpoint(self,
self.make_request(action='CreateRealtimeEndpoint',
delete_batch_prediction(self,
self.make_request(action='DeleteBatchPrediction',
delete_data_source(self,
data_source_id):
self.make_request(action='DeleteDataSource',
delete_evaluation(self,
self.make_request(action='DeleteEvaluation',
delete_ml_model(self,
self.make_request(action='DeleteMLModel',
delete_realtime_endpoint(self,
self.make_request(action='DeleteRealtimeEndpoint',
describe_batch_predictions(self,
self.make_request(action='DescribeBatchPredictions',
describe_data_sources(self,
self.make_request(action='DescribeDataSources',
describe_evaluations(self,
self.make_request(action='DescribeEvaluations',
describe_ml_models(self,
self.make_request(action='DescribeMLModels',
get_batch_prediction(self,
self.make_request(action='GetBatchPrediction',
get_data_source(self,
self.make_request(action='GetDataSource',
get_evaluation(self,
self.make_request(action='GetEvaluation',
get_ml_model(self,
self.make_request(action='GetMLModel',
predict(self,
predict_endpoint):
urlsplit(predict_endpoint).hostname
predict_endpoint
'Record':
'PredictEndpoint':
predict_host,
self.make_request(action='Predict',
body=json.dumps(params),
host=predict_host)
update_batch_prediction(self,
batch_prediction_name):
'BatchPredictionName':
batch_prediction_name,
self.make_request(action='UpdateBatchPrediction',
update_data_source(self,
data_source_name):
'DataSourceName':
data_source_name,
self.make_request(action='UpdateDataSource',
update_evaluation(self,
evaluation_name):
'EvaluationName':
evaluation_name,
self.make_request(action='UpdateEvaluation',
update_ml_model(self,
score_threshold=None):
params['ScoreThreshold']
self.make_request(action='UpdateMLModel',
http_request_kwargs
'method':'POST',
'path':'/',
'auth_path':'/',
'params':{},
'data':body
headers['Host']
http_request_kwargs['host']
self.build_base_http_request(**http_request_kwargs)
SSHClient(object):
paramiko.RSAKey.from_private_key_file(server.ssh_key_file,
password=ssh_pwd)
self._ssh_client.connect(self.server.hostname,
username=self.uname,
pkey=self._pkey,
socket.error
xxx_todo_changeme:
xxx_todo_changeme.args
(51,
61,
111):
print('SSH
refused,
paramiko.BadHostKeyException:
print("%s
~/.ssh/known_hosts
match"
self.server.hostname)
print('Edit
raw_input('Hit
Enter
ready')
print('Unexpected
Connection,
print('Could
establish
connection')
open_sftp(self):
self._ssh_client.open_sftp()
sftp_client.get(src,
sftp_client.put(src,
bufsize=-1):
sftp_client.open(filename,
bufsize)
sftp_client.listdir(path)
-d
self._ssh_client.invoke_shell()
self._ssh_client.exec_command(command)
paramiko.SSHException:
std_out
std_err
t[2].read()
t[0].close()
t[1].close()
t[2].close()
boto.log.debug('stdout:
std_out)
boto.log.debug('stderr:
std_out,
run_pty(self,
self._ssh_client.get_transport().open_session()
channel.get_pty()
channel.exec_command(command)
self._ssh_client.get_transport()
transport.close()
self.server.reset_cmdshell()
LocalClient(object):
host_key_file=None,
os.listdir(path)
os.path.isdir(path)
os.path.exists(path)
NotImplementedError('shell
LocalClient')
boto.log.info(log_fp.getvalue())
boto.log.info('output:
(process.returncode,
FakeServer(object):
instance,
ssh_key_file):
self.instance
ssh_key_file
instance.dns_name
start(server):
server.instance_id:
LocalClient(server)
sshclient_from_instance(instance,
ssh_key_file,
user_name='root',
FakeServer(instance,
ssh_key_file)
SSHClient(s,
host_key_file,
ssh_pwd)
get(prop,
choices=None):
prop.verbose_name
prompt:
callable(choices):
choices()
prop.get_choices()
len(choices)
range(min,
max+1):
choices[i-1]
raw_input('%s
[%d-%d]:
int_value
choices[int_value-1]
value[1]
range[%d-%d]'
(min,
prop.validate(value)
prop.empty(value)
prop.required:
required')
BotoConfigPath,
BooleanProperty,
'm2.4xlarge']
Bundler(object):
self.ssh_client
uname=uname)
copy_x509(self,
cert_file):
self.ssh_client.open_sftp()
self.remote_key_file
self.ssh_client.put_file(key_file,
self.remote_cert_file
self.ssh_client.put_file(cert_file,
self.remote_cert_file)
'ec2-bundle-vol
(self.remote_cert_file,
self.server._reservation.owner_id
self.server.ec2.aws_access_key_id
self.server.ec2.aws_secret_access_key
bundle(self,
ssh_key=None,
clear_history=True):
ssh_key:
ssh_key
self.server.get_ssh_key_file()
self.copy_x509(key_file,
cert_file)
/mnt/boto.cfg;
~/.ssh/authorized_keys
/mnt/authorized_keys;
clear_history:
fp.write('history
-c;
fp.write(self.bundle_image(prefix,
fp.write(self.upload_bundle(bucket,
/mnt/boto.cfg
/mnt/authorized_keys
~/.ssh/authorized_keys')
print('running
remote
server:')
print(command)
self.ssh_client.run(command)
self.server.ec2.register_image(name=prefix,
image_location='%s/%s.manifest.xml'
get_ami_list(self):
self.ec2.get_all_images():
ami.location.find('pyami')
my_amis.append((ami.location,
ami))
boto.ec2.get_region(region)
get_description(self,
params.get('description',
self.cls.find_property('description')
get_ami_id(self,
StringProperty(name='ami',
verbose_name='AMI')
self.ec2.get_all_images([ami])
params.get('group',
group_list
self.ec2.get_all_security_groups()
group_list:
StringProperty(name='group',
Group',
choices=self.ec2.get_all_security_groups)
params.get('keypair',
isinstance(keypair,
self.ec2.get_all_key_pairs()
key_list:
StringProperty(name='keypair',
KeyPair',
choices=self.ec2.get_all_key_pairs)
propget.get(prop).name
self.get_description(params)
self.get_ami_id(params)
self.get_group(params)
self.get_key(params)
Elastic
Address")
BooleanProperty(verbose_name="Is
Production",
CalculatedProperty(verbose_name="AMI
ID",
CalculatedProperty(verbose_name="Availability
CalculatedProperty(verbose_name="Public
CalculatedProperty(verbose_name="Private
CalculatedProperty(verbose_name="Security
Groups",
calculated_type=list,
CalculatedProperty(verbose_name="Primary
CalculatedProperty(verbose_name="Key
CalculatedProperty(verbose_name="Instance
Type",
CalculatedProperty(verbose_name="Current
Status",
CalculatedProperty(verbose_name="Server
Launch
Time",
CalculatedProperty(verbose_name="Console
Output",
calculated_type=open,
plugins
add_credentials(cls,
cfg,
cfg.has_section('Credentials'):
cfg.add_section('Credentials')
aws_access_key_id)
aws_secret_access_key)
cfg.has_section('DB_Server'):
cfg.add_section('DB_Server')
cls._manager.domain.name)
Config(path=config_file)
cfg.has_section('EC2'):
cfg.options('EC2'):
params[option]
cfg.get('EC2',
cls.add_credentials(cfg,
ec2.aws_access_key_id,
params.get('ami')
params.get('keypair')
params.get('group')
cfg.set('EBS',
'logical_volume_name',
logical_volume.name)
cfg_fp
cfg.write(cfg_fp)
isinstance(kp,
KeyPair):
kp.name
max_count=params.get('quantity',
key_name=kp,
security_groups=[group],
instance_type=params.get('instance_type'),
cfg_fp.getvalue())
params.get('elastic_ip')
reservation.instances
instances.__len__()
print('Waiting
address...')
instance.use_ip(elastic_ip)
print('set
elastic_ip)
i==0
params.get('description')
s.elastic_ip
l.append(s)
create_from_instance_id(cls,
ec2.get_all_reservations([instance_id])
s._reservation.instances:
s._instance
create_from_current_instances(cls):
ec2.get_all_reservations()
reservation.instances:
next(Server.find(instance_id=instance.id))
boto.log.info('Server
instance.id)
servers.append(s)
_setup_ec2(self):
boto.ec2.regions():
rs[0].instances:
_status(self):
self._instance.update()
self._instance.state
_hostname(self):
self._instance.public_dns_name
_private_hostname(self):
self._instance.private_dns_name
_instance_type(self):
self._instance.instance_type
_launch_time(self):
self._instance.launch_time
_console_output(self):
self._instance.get_console_output()
_groups(self):
_security_group(self):
self._groups()
len(groups)
groups[0].id
_zone(self):
self._instance.placement
_key_name(self):
self._instance.key_name
put(self):
self).put()
self._instance.stop()
terminate(self):
self._instance.terminate()
self._instance.reboot()
get_ssh_key_file(self):
ssh_dir
os.path.expanduser('~/.ssh')
os.path.isdir(ssh_dir):
os.path.join(ssh_dir,
os.path.isfile(ssh_file):
get_cmdshell(self):
self._cmdshell:
cmdshell
cmdshell.start(self)
reset_cmdshell(self):
closing(self.get_cmdshell())
cmd.run(command)
get_bundler(self,
Bundler(self,
uname)
SSHClient(self,
uname=uname,
ssh_pwd=ssh_pwd)
install(self,
pkg):
pkg)
subprocess,
check_hour(val):
'*':
23:
Task(Model):
validator=check_hour,
default='*')
last_executed
last_status
last_output
message_id
start_all(cls,
task.start(queue_name)
super(Task,
check(self):
boto.log.info('checking
Task[%s]-now=%s,
last=%s'
self.now,
self.last_executed))
self.hourly:
60*60:
60*60
82800
hours,
_run(self,
vtimeout):
running:%s'
self.command))
boto.log.info('nsecs=%s,
timeout=%s'
(nsecs,
current_timeout:
msg.change_visibility(current_timeout)
log_fp.getvalue()))
self.last_status
process.returncode
self.last_output
log_fp.getvalue()[0:1023]
self.check()
delay=%s
delay))
self._run(msg,
msg.queue
queue.write(new_msg)
id=%s'
new_msg.id))
msg.delete()
msg.id))
boto.log.info('new_vtimeout:
delay)
msg.change_visibility(delay)
queue_name))
queue.write(msg)
successful'
TaskPoller(object):
self.sqs
self.sqs.lookup(queue_name)
wait=60,
self.queue.read(vtimeout)
Task.get_by_id(m.get_body())
task:
task.message_id
m.id
task.message_id:
(task.name,
task.run(m,
extraneous
ignoring'
task.name)
time.sleep(wait)
Volume')
Volume.create()
print(volume)
server_list
Server.create()
server_list[0]
print(server)
server.status
print('*')
running')
Now
volume.make_ready
volume.make_ready(server)
Do
"ls
-al"
filesystem')
server.run('ls
-al
volume.mount_point)
get_size(self,
params.get('size',
IntegerProperty(name='size',
verbose_name='Size
(GB)')
params['size']
get_mount_point(self,
params.get('mount_point',
self.cls.find_property('mount_point')
params['mount_point']
get_device(self,
params.get('device',
self.cls.find_property('device')
params['device']
self.get_size(params)
self.get_mount_point(params)
self.get_device(params)
Volume(Model):
verbose_name='Name')
zone_name
StringProperty(verbose_name='Mount
Point')
StringProperty(verbose_name="Device
default='/dev/sdp')
past_volume_ids
ListProperty(item_type=str)
ReferenceProperty(Server,
collection_name='volumes',
verbose_name='Server
Attached
To')
volume_state
CalculatedProperty(verbose_name="Volume
attachment_state
CalculatedProperty(verbose_name="Attachment
CalculatedProperty(verbose_name="Size
(GB)",
params.get('size')
zone.name)
params.get('mount_point')
params.get('device')
create_from_volume_id(cls,
boto.ec2.connect_to_region(region_name)
ec2.get_all_volumes([volume_id])
vol.volume_id
v.id
vol.name
vol.region_name
v.region.name
vol.zone_name
v.zone
vol.put()
create_from_latest_snapshot(self,
self.get_snapshots()[-1]
self.create_from_snapshot(name,
create_from_snapshot(self,
current_volume
current_volume.zone
self.zone_name,
snapshot)
get_ec2_connection(self):
self.server.ec2
'ec2')
boto.ec2.connect_to_region(self.region_name)
_volume_state(self):
rs[0].volume_state()
_attachment_state(self):
rs[0].attachment_state()
_size(self):
'__size'):
rs[0].size
install_xfs(self):
self.server.install('xfsprogs
get_snapshots(self):
ec2.get_all_snapshots()
all_vols
[self.volume_id]
self.past_volume_ids
snapshot.volume_id
all_vols:
snapshot.progress
'100%':
snapshot.date
boto.utils.parse_ts(snapshot.start_time)
snapshot.keep
snaps.append(snapshot)
snaps.sort(cmp=lambda
cmp(x.date,
y.date))
server=None):
'attached':
attached')
server:
'available'
'detaching':
detached')
ec2.detach_volume(self.volume_id,
self.device,
force)
checkfs(self,
use_cmd=None):
use_cmd
cmd.run('xfs_check
cmd.close()
status[1].startswith('bad
superblock
0'):
cmd.exists(self.device):
format(self):
self.checkfs(cmd):
cmd.run('mkfs
mount(self):
cmd.isdir(self.mount_point):
cmd.run("mkdir
status[1].split('\n')
cmd.run('umount
cmd.run('chmod
cmd.run("mount
cmd.run('xfs_growfs
make_ready(self,
server):
self.install_xfs()
self.wait()
self.format()
self.mount()
-f
unfreeze(self):
snapshot(self):
self.freeze()
self.get_ec2_connection().create_snapshot(self.volume_id)
self.server.ec2.create_snapshot(self.volume_id)
created:
snapshot))
boto.log.info(traceback.format_exc())
self.unfreeze()
get_snapshot_range(self,
snaps,
end_date=None):
delete=False):
self.get_snapshots()
len(snaps)
snaps[1:-1]
datetime.datetime.now(snaps[0].date.tzinfo)
midnight
datetime.datetime(year=now.year,
month=now.month,
day=now.day,
tzinfo=now.tzinfo)
datetime.timedelta(days=7,
seconds=60*60)
print(midnight-one_week,
previous_week
midnight-one_week,
print(previous_week)
snap.date.day:
snap.date.day
previous_week[0].date
week_boundary.weekday()
datetime.timedelta(days=week_boundary.weekday())
partial_week
week_boundary,
previous_week[0].date)
len(partial_week)
partial_week[1:]:
weeks_worth
week_boundary-one_week,
week_boundary)
len(weeks_worth)
weeks_worth[1:]:
end_date=week_boundary)
remainder:
snap.date.month:
snap.date.month
delete:
snap.keep:
boto.log.info('Deleting
%s(%s)
(snap,
snap.date,
snap.delete()
grow(self,
snapshot):
get_snapshot_from_date(self,
delete_ebs_volume=False):
delete_ebs_volume:
self.detach()
ec2.delete_volume(self.volume_id)
archive(self):
termios
tty
interactive_shell(chan):
has_termios:
posix_shell(chan)
windows_shell(chan)
posix_shell(chan):
oldtty
termios.tcgetattr(sys.stdin)
tty.setraw(sys.stdin.fileno())
tty.setcbreak(sys.stdin.fileno())
chan.settimeout(0.0)
select.select([chan,
sys.stdin],
chan
chan.recv(1024)
print('\r\n***
EOF\r\n',
sys.stdout.write(x)
socket.timeout:
chan.send(x)
termios.tcsetattr(sys.stdin,
termios.TCSADRAIN,
oldtty)
windows_shell(chan):
sys.stdout.write("Line-buffered
terminal
emulation.
Press
F6
^Z
EOF.\r\n\r\n")
writeall(sock):
sock.recv(256)
sys.stdout.write('\r\n***
***\r\n\r\n')
sys.stdout.write(data)
threading.Thread(target=writeall,
args=(chan,))
writer.start()
chan.send(d)
int_val_fn(v):
int(v)
IObject(object):
choose_from_list(self,
item_list,
search_str='',
prompt='Enter
Selection'):
Choices
Available')
choice:
item))
choices.append(item)
desc:
desc.find(search_str)
desc))
id.find(search_str)
id))
raw_input('%s[1-%d]:
val.startswith('/'):
val[1:]
choices[int_val-1]
range[1-%d]'
(val,
print("No
matched
pattern")
get_string(self,
prompt,
validation_fn=None):
validation_fn:
validation_fn(val)
get_filename(self,
os.path.expanduser(val)
os.path.isfile(val):
os.path.isdir(val):
self.choose_from_list(os.listdir(path))
get_int(self,
self.get_string(prompt,
int_val_fn)
boto.mashups.server
Server,
ServerSet
boto.sdb.persist
get_domain,
set_domain
'c1.xlarge']
Item(IObject):
set_userdata(self,
get_userdata(self,
set_region(self,
region=None):
[(r,
r.name,
r.endpoint)
boto.ec2.regions()]
set_name(self,
self.get_string('Name')
set_instance_type(self,
self.choose_from_list(InstanceTypes,
'Instance
Type')
set_quantity(self,
n=0):
self.get_int('Quantity')
set_zone(self,
zone:
[(z,
z.name,
z.state)
self.ec2.get_all_zones()]
set_ami(self,
ami=None):
[(a,
a.id,
a.location)
self.ec2.get_all_images()]
AMI')
add_group(self,
group=None):
self.groups.append(group)
[(s,
s.name,
s.description)
self.ec2.get_all_security_groups()]
self.groups.append(self.choose_from_list(l,
Group'))
set_key(self,
key=None):
k.name,
self.ec2.get_all_key_pairs()]
Keypair')
update_config(self):
self.config.has_section('Credentials'):
self.config.add_section('Credentials')
self.ec2.aws_access_key_id)
self.ec2.aws_secret_access_key)
self.config.has_section('Pyami'):
self.config.add_section('Pyami')
sdb_domain
sdb_domain:
sdb_domain)
config_path=None):
config_path:
config_path
self.get_filename('Specify
Config(path=config_path)
get_userdata_string(self):
self.config.write(s)
s.getvalue()
enter(self,
self.set_region()
self.region.connect()
self.set_name()
self.instance_type:
self.set_instance_type()
self.zone)
self.zone:
self.set_zone()
self.quantity)
self.quantity:
self.set_quantity()
self.ami)
self.ami:
self.set_ami()
params.get('groups',
self.groups)
self.groups:
self.add_group()
params.get('key',
self.key)
self.key:
self.set_key()
params.get('config',
self.config)
self.config:
self.set_config()
self.update_config()
Order(IObject):
self.reservation
add_item(self,
item.enter(**params)
display(self):
print('This
Order
consists
items')
print('QTY\tNAME\tTYPE\nAMI\t\tGroups\t\t\tKeyPair')
print('%s\t%s\t%s\t%s\t%s\t%s'
(item.quantity,
item.instance_type,
item.ami.id,
item.groups,
item.key.name))
place(self,
block=True):
print('SDB
Persistence
self.get_string('Specify
Domain')
set_domain(domain_name)
item.ami.run(min_count=1,
max_count=item.quantity,
key_name=item.key.name,
user_data=item.get_userdata_string(),
security_groups=item.groups,
instance_type=item.instance_type,
placement=item.zone.name)
block:
states.count('running')
len(states):
print(states)
Server()
server.name
item.name
server.instance_id
server.reservation
server.save()
s.append(server)
s[0]
StringProperty
ServerSet(list):
getattr(server,
callable(val):
results.append(val)
results.append(None)
is_callable:
self.map_list
self.map
map(self,
self.map_list:
results.append(fn(*args))
ec2(self):
Inventory(cls):
cls.find()
l.append(server)
Register(cls,
s.save()
config_uri
StringProperty(verbose_name="AMI
StringProperty(verbose_name="Availability
Zone")
StringProperty(verbose_name="Security
Group",
default="default")
StringProperty(verbose_name="Key
StringProperty(verbose_name="Elastic
IP")
Type")
getInstance(self):
self._reservation.instances[0]
property(getInstance,
getAMI(self):
self.instance.image_id
property(getAMI,
self.instance.update()
self.instance.state
getHostname(self):
self.instance.public_dns_name
property(getHostname,
getPrivateHostname(self):
self.instance.private_dns_name
property(getPrivateHostname,
getLaunchTime(self):
self.instance.launch_time
property(getLaunchTime,
started')
getConsoleOutput(self):
self.instance.get_console_output()
property(getConsoleOutput,
getGroups(self):
property(getGroups,
Groups
controlling
getConfig(self):
remote_file
self.get_file(remote_file,
local_file)
Config(local_file)
setConfig(self,
open(local_file)
self.put_file(local_file,
property(getConfig,
setConfig,
self._config.dump_to_sdb("botoConfigs",
load_config(self):
self._config.load_from_sdb("botoConfigs",
self.instance.stop()
ec2.get_all_images(image_ids
[str(self.ami_id)])[0]
ec2.get_all_security_groups(groupnames=[str(self.security_group)])
self.load_config()
self._config.has_section("Credentials"):
self._config.add_section("Credentials")
"aws_access_key_id",
"aws_secret_access_key",
self._config.has_section("Pyami"):
self._config.add_section("Pyami")
self._manager.domain:
self._config.set('Pyami',
self._manager.domain.name)
self._config.set("Pyami",
self._config.write(cfg)
cfg.getvalue()
key_name=self.key_name,
self.instance_type,
self.zone,
cfg)
r.instances[0]
self.elastic_ip:
ec2.associate_address(self.instance_id,
self.elastic_ip)
self.instance.reboot()
yet!')
self._ssh_client:
paramiko.RSAKey.from_private_key_file(key_file)
self._ssh_client.connect(self.instance.public_dns_name,
username=uname,
pkey=self._pkey)
remotepath,
localpath):
sftp_client.get(remotepath,
localpath)
localpath,
sftp_client.put(localpath,
remotepath)
sftp_client.listdir(remotepath)
shell(self,
key_file=None):
self.get_ssh_client(key_file)
ssh_client.invoke_shell()
print('bundling
remote_key_file
self.put_file(key_file,
remote_cert_file
self.put_file(cert_file,
remote_cert_file)
print('\tdeleting
sftp_client.remove(BotoConfigPath)
'sudo
ec2-bundle-vol
(remote_cert_file,
self._reservation.owner_id
self.instance.instance_type
print('uploading
bundle...')
self.ec2.aws_access_key_id
self.ec2.aws_secret_access_key
self.bundle_image(prefix,
self.upload_bundle(bucket,
self.ec2.register_image('%s/%s.manifest.xml'
volume,
device="/dev/sdp"):
self.ec2.attach_volume(volume_id=volume_id,
device=device)
volume):
self.ec2.detach_volume(volume_id=volume_id,
instance_id=self.instance_id)
install_package(self,
package_name):
print('installing
%s...'
package_name)
'yum
package_name
boto.mturk.price
boto.mturk.notification
QuestionForm,
ExternalQuestion,
HTMLQuestion
MTurkRequestError(EC2ResponseError):
MTurk
Requests"
MTurkConnection(AWSQueryConnection):
'2014-08-15'
config.has_option('MTurk',
config.get('MTurk',
'mechanicalturk.amazonaws.com'
super(MTurkConnection,
['mturk']
get_account_balance(self):
self._process_request('GetAccountBalance',
[('AvailableBalance',
Price),
('OnHoldBalance',
Price)])
register_hit_type(self,
reward,
qual_req=None):
AssignmentDurationInSeconds=self.duration_as_seconds(duration),
params.update(MTurkConnection.get_price_as_price(reward).get_as_params('Reward'))
params['AutoApprovalDelayInSeconds']
qual_req
params.update(qual_req.get_as_params())
self._process_request('RegisterHITType',
[('HITTypeId',
HITTypeId)])
set_email_notification(self,
'Email',
set_rest_notification(self,
set_sqs_notification(self,
"SQS",
send_test_event_notification(self,
test_event_type='Ping'):
'SendTestEventNotification',
test_event_type)
_set_notification(self,
test_event_type=None):
{'HITTypeId':
notification_params
{'Destination':
'Transport':
boto.mturk.notification.NotificationMessage.NOTIFICATION_VERSION,
'Active':
event_types:
self.build_list_params(notification_params,
'EventType')
notification_rest_params
notification_params:
notification_rest_params['Notification.%d.%s'
(num,
notification_params[key]
params.update(notification_rest_params)
test_event_type:
params.update({'TestEventType':
test_event_type})
self._process_request(request_type,
create_hit(self,
question=None,
hit_layout=None,
lifetime=datetime.timedelta(days=7),
max_assignments=1,
title=None,
reward=None,
duration=datetime.timedelta(days=7),
annotation=None,
questions=None,
qualifications=None,
layout_params=None,
{'LifetimeInSeconds':
self.duration_as_seconds(lifetime),
'MaxAssignments':
max_assignments,
both:
instance),
question:
[question]
QuestionForm(questions)
QuestionForm):
ExternalQuestion):
HTMLQuestion):
params['Question']
question_param.get_as_xml()
neither:
hit_layout")
params['HITLayoutId']
layout_params:
params.update(layout_params.get_as_params())
hit_type:
params['HITTypeId']
MTurkConnection.get_keywords_as_string(keywords)
MTurkConnection.get_price_as_price(reward)
final_duration
self.duration_as_seconds(duration)
additional_params
Keywords=final_keywords,
AssignmentDurationInSeconds=final_duration,
additional_params.update(final_price.get_as_params('Reward'))
additional_params['AutoApprovalDelayInSeconds']
params.update(additional_params)
params['RequesterAnnotation']
params.update(qualifications.get_as_params())
self._process_request('CreateHIT',
change_hit_type_of_hit(self,
hit_type):
'HITTypeId':
self._process_request('ChangeHITTypeOfHIT',
get_reviewable_hits(self,
status='Reviewable',
{'Status':
params.update({'HITTypeId':
hit_type})
self._process_request('GetReviewableHITs',
_get_pages(page_size,
total_records):
bool(total_records
page_size)
get_all_hits(self):
search_rs
self.search_hits(page_size=page_size)
int(search_rs.TotalNumResults)
get_page_hits
self.search_hits(page_size=page_size,
page_number=page)
hit_sets
itertools.imap(get_page_hits,
itertools.chain.from_iterable(hit_sets)
search_hits(self,
sort_by='CreationTime',
{'SortProperty':
self._process_request('SearchHITs',
get_assignment(self,
self._process_request('GetAssignment',
Assignment),
('HIT',
get_assignments(self,
sort_by='SubmitTime',
params['AssignmentStatus']
self._process_request('GetAssignmentsForHIT',
Assignment)])
approve_assignment(self,
self._process_request('ApproveAssignment',
reject_assignment(self,
self._process_request('RejectAssignment',
approve_rejected_assignment(self,
self._process_request('ApproveRejectedAssignment',
get_file_upload_url(self,
question_identifier):
question_identifier}
self._process_request('GetFileUploadURL',
[('FileUploadURL',
FileUploadURL)])
get_hit(self,
self._process_request('GetHIT',
set_reviewing(self,
revert=None):
revert:
params['Revert']
revert
self._process_request('SetHITAsReviewing',
disable_hit(self,
self._process_request('DisableHIT',
dispose_hit(self,
self._process_request('DisposeHIT',
expire_hit(self,
self._process_request('ForceExpireHIT',
extend_hit(self,
assignments_increment=None,
expiration_increment=None):
expiration_increment,
assignments_increment:
params['MaxAssignmentsIncrement']
expiration_increment:
params['ExpirationIncrementInSeconds']
self._process_request('ExtendHIT',
get_help(self,
help_type='Operation'):
{'About':
'HelpType':
help_type}
self._process_request('Help',
grant_bonus(self,
bonus_price,
bonus_price.get_as_params('BonusAmount',
params['WorkerId']
worker_id
params['AssignmentId']
assignment_id
params['Reason']
self._process_request('GrantBonus',
block_worker(self,
self._process_request('BlockWorker',
unblock_worker(self,
self._process_request('UnblockWorker',
notify_workers(self,
message_text):
{'Subject':
'MessageText':
message_text}
'WorkerId')
self._process_request('NotifyWorkers',
create_qualification_type(self,
answer_key_xml=None,
auto_granted=False,
auto_granted_value=1):
'QualificationTypeStatus':
assert(test_duration
auto_granted:
assert(test
self._process_request('CreateQualificationType',
get_qualification_type(self,
self._process_request('GetQualificationType',
get_all_qualifications_for_qual_type(self,
search_qual
self.get_qualifications_for_qualification_type(qualification_type_id)
int(search_qual.TotalNumResults)
get_page_quals
self.get_qualifications_for_qualification_type(qualification_type_id
page_size=page_size,
qual_sets
itertools.imap(get_page_quals,
itertools.chain.from_iterable(qual_sets)
get_qualifications_for_qualification_type(self,
page_size=100,
self._process_request('GetQualificationsForQualificationType',
update_qualification_type(self,
auto_granted=None,
auto_granted_value=None):
params['QualificationTypeStatus']
self._process_request('UpdateQualificationType',
dispose_qualification_type(self,
self._process_request('DisposeQualificationType',
search_qualification_types(self,
sort_by='Name',
must_be_requestable=True,
must_be_owned_by_caller=True):
{'Query':
page_number,
'MustBeRequestable':
must_be_requestable,
'MustBeOwnedByCaller':
must_be_owned_by_caller}
self._process_request('SearchQualificationTypes',
get_qualification_requests(self,
self._process_request('GetQualificationRequests',
[('QualificationRequest',
QualificationRequest)])
grant_qualification(self,
integer_value=1):
{'QualificationRequestId':
'IntegerValue':
integer_value}
self._process_request('GrantQualification',
revoke_qualification(self,
{'SubjectId':
'QualificationTypeId':
self._process_request('RevokeQualification',
assign_qualification(self,
send_notification=True):
'WorkerId'
'SendNotification'
send_notification}
self._process_request('AssignQualification',
get_qualification_score(self,
worker_id):
worker_id}
self._process_request('GetQualificationScore',
update_qualification_score(self,
self._process_request('UpdateQualificationScore',
request_type
self._process_response(response,
marker_elems)
_process_response(self,
print(body)
'<Errors>'
body.decode('utf-8'):
ResultSet(marker_elems)
MTurkRequestError(response.status,
get_keywords_as_string(keywords):
'.join(keywords)
keywords.encode('utf-8')
TypeError("keywords
strings;
type(keywords))
get_price_as_price(reward):
isinstance(reward,
Price):
Price(reward)
duration_as_seconds(duration):
isinstance(duration,
datetime.timedelta):
duration.days
duration.seconds
int(duration)
TypeError("Duration
int-castable,
type(duration))
BaseAutoResultElement(object):
HIT(BaseAutoResultElement):
_has_expired(self):
'Expiration'):
datetime.datetime.strptime(self.Expiration,
expiration)
ValueError("ERROR:
HIT!")
property(_has_expired)
FileUploadURL(BaseAutoResultElement):
HITTypeId(BaseAutoResultElement):
Qualification(BaseAutoResultElement):
QualificationType(BaseAutoResultElement):
QualificationRequest(BaseAutoResultElement):
Assignment(BaseAutoResultElement):
QuestionFormAnswer(BaseAutoResultElement):
super(QuestionFormAnswer,
self.fields
['FreeText',
'SelectionIdentifier',
'OtherSelectionText']
self.qid:
self.fields.append(value)
LayoutParameters(object):
layoutParameters=None):
self.layoutParameters
self.layoutParameters.append(req)
assert(len(self.layoutParameters)
25)
layoutParameter
enumerate(self.layoutParameters):
layoutParameter.get_as_params()
kv:
params['HITLayoutParameter.%s.%s'
kv[key]
LayoutParameter(object):
self.value,
NotificationMessage(object):
NOTIFICATION_WSDL
"http://mechanicalturk.amazonaws.com/AWSMechanicalTurk/2006-05-05/AWSMechanicalTurkRequesterNotification.wsdl"
NOTIFICATION_VERSION
'2006-05-05'
SERVICE_NAME
"AWSMechanicalTurkRequesterNotification"
OPERATION_NAME
"Notify"
EVENT_PATTERN
r"Event\.(?P<n>\d+)\.(?P<param>\w+)"
EVENT_RE
re.compile(EVENT_PATTERN)
d['Signature']
vH6ZbE0NhkF/hfNyxz2OgmzXYKs=
d['Timestamp']
2006-05-23T23:22:30Z
d['Version']
2006-05-05
d['method']
NotificationMessage.OPERATION_NAME,
"Method
'Event'
d['Event']
k.startswith('Event.'):
ed
NotificationMessage.EVENT_RE.search(k).groupdict()
int(ed['n'])
str(ed['param'])
events_dict[n]
events_dict[n][param]
self.events.append(Event(events_dict[n]))
verify(self,
secret_key):
NotificationMessage.SERVICE_NAME
hmac.new(key=secret_key,
h.update(verification_input)
base64.b64encode(h.digest())
self.event_type
d['EventType']
self.event_time_str
d['EventTime']
self.hit_type
d['HITTypeId']
self.hit_id
d['HITId']
'AssignmentId'
self.assignment_id
d['AssignmentId']
"<boto.mturk.notification.Event:
HIT
(self.event_type,
self.hit_id)
Price(object):
amount=0.0,
currency_code='USD'):
self.formatted_price:
str(self.amount)
'Amount':
'CurrencyCode':
'FormattedPrice':
ord=1):
{'%s.%d.Amount'%(label,
str(self.amount),
'%s.%d.CurrencyCode'%(label,
self.currency_code}
Qualifications(object):
requirements=None):
self.requirements
self.requirements.append(req)
assert(len(self.requirements)
enumerate(self.requirements):
reqparams
req.get_as_params()
reqparams:
params['QualificationRequirement.%s.%s'
rp)
reqparams[rp]
Requirement(object):
self.qualification_type_id
comparator
self.required_to_preview
required_to_preview
enumerate(self.integer_value,
params['IntegerValue.%d'
('Exists',
'DoesNotExist')
params['IntegerValue']
PercentAssignmentsSubmittedRequirement(Requirement):
super(PercentAssignmentsSubmittedRequirement,
self).__init__(qualification_type_id="00000000000000000000",
PercentAssignmentsAbandonedRequirement(Requirement):
super(PercentAssignmentsAbandonedRequirement,
self).__init__(qualification_type_id="00000000000000000070",
PercentAssignmentsReturnedRequirement(Requirement):
super(PercentAssignmentsReturnedRequirement,
self).__init__(qualification_type_id="000000000000000000E0",
PercentAssignmentsApprovedRequirement(Requirement):
super(PercentAssignmentsApprovedRequirement,
self).__init__(qualification_type_id="000000000000000000L0",
PercentAssignmentsRejectedRequirement(Requirement):
super(PercentAssignmentsRejectedRequirement,
self).__init__(qualification_type_id="000000000000000000S0",
NumberHitsApprovedRequirement(Requirement):
super(NumberHitsApprovedRequirement,
self).__init__(qualification_type_id="00000000000000000040",
LocaleRequirement(Requirement):
locale,
super(LocaleRequirement,
self).__init__(qualification_type_id="00000000000000000071",
enumerate(self.locale,
isinstance(locale,
locale[0]
params['LocaleValue.%d.Subdivision'
locale[1]
isinstance(self.locale,
self.locale[0]
params['LocaleValue.Subdivision']
self.locale[1]
AdultRequirement(Requirement):
super(AdultRequirement,
self).__init__(qualification_type_id="00000000000000000060",
Question(object):
"<Question>%(items)s</Question>"
answer_spec,
is_required=False,
label='Question'):
SimpleField('QuestionIdentifier',
self.identifier),
SimpleField('IsRequired',
str(self.is_required).lower()),
self.content,
self.answer_spec,
items.insert(1,
SimpleField('DisplayName',
self.display_name))
schema_src_file
urllib2.urlopen(self.schema_url)
schema_doc
etree.parse(schema_src_file)
etree.XMLSchema(schema_doc)
etree.fromstring(self.get_as_xml())
schema.assertValid(doc)
ExternalQuestion(ValidatingXML):
"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2006-07-14/ExternalQuestion.xsd"
'<ExternalQuestion
xmlns="%(schema_url)s"><ExternalURL>%%(external_url)s</ExternalURL><FrameHeight>%%(frame_height)s</FrameHeight></ExternalQuestion>'
self.external_url
xml.sax.saxutils.escape(
label='ExternalQuestion'):
XMLTemplate(object):
SimpleField(XMLTemplate):
'<%(field)s>%(value)s</%(field)s>'
self.field
Binary(XMLTemplate):
subtype,
alt_text):
List(list):
''.join('<ListItem>%s</ListItem>'
'<List>%s</List>'
Application(object):
"<Application><%(class_)s>%(content)s</%(class_)s></Application>"
parameter_template
"<Name>%(name)s</Name><Value>%(value)s</Value>"
**parameters):
self.width
self.height
content.append_field('Width',
self.width)
content.append_field('Height',
self.height)
self.parameters.items():
self.parameter_template
content.append_field('ApplicationParameter',
self.get_inner_content(content)
content.get_as_xml()
HTMLQuestion(ValidatingXML):
'http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd'
'<HTMLQuestion
xmlns=\"%(schema_url)s\"><HTMLContent><![CDATA[<!DOCTYPE
html>%%(html_form)s]]></HTMLContent><FrameHeight>%%(frame_height)s</FrameHeight></HTMLQuestion>'
html_form,
self.html_form
html_form
label="HTMLQuestion"):
JavaApplet(Application):
content.append_field('AppletPath',
self.path)
content.append_field('AppletFilename',
self.filename)
Flash(Application):
content.append_field('FlashMovieURL',
FormattedContent(XMLTemplate):
'http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2006-07-14/FormattedContentXHTMLSubset.xsd'
'<FormattedContent><![CDATA[%(content)s]]></FormattedContent>'
OrderedContent(list):
append_field(self,
self.append(SimpleField(field,
Overview(OrderedContent):
'<Overview>%(content)s</Overview>'
label='Overview'):
super(Overview,
QuestionForm(ValidatingXML,
"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd"
xml_template
is_valid(self):
any(isinstance(item,
Question)
all(isinstance(item,
(Question,
Overview))
self.is_valid(),
"QuestionForm
elements"
self.xml_template
QuestionContent(OrderedContent):
'<QuestionContent>%(content)s</QuestionContent>'
super(QuestionContent,
AnswerSpecification(object):
'<AnswerSpecification>%(spec)s</AnswerSpecification>'
spec):
self.spec
self.spec.get_as_xml()
Constraints(OrderedContent):
'<Constraints>%(content)s</Constraints>'
super(Constraints,
Constraint(object):
'%s="%d"'
self.get_attributes()
NumericConstraint(Constraint):
'minValue',
'maxValue'
'<IsNumeric
min_value=None,
max_value=None):
min_value,
max_value
LengthConstraint(Constraint):
'minLength',
'maxLength'
'<Length
min_length=None,
min_length,
RegExConstraint(Constraint):
'regex',
'errorText',
'flags'
'<AnswerFormatRegex
error_text=None,
flags=None):
error_text,
'%s="%s"'
NumberOfLinesSuggestion(object):
'<NumberOfLinesSuggestion>%(num_lines)s</NumberOfLinesSuggestion>'
num_lines=1):
FreeTextAnswer(object):
'<FreeTextAnswer>%(items)s</FreeTextAnswer>'
num_lines=None):
Constraints()
Constraints(constraints)
[self.constraints]
self.default:
items.append(SimpleField('DefaultText',
self.default))
self.num_lines:
items.append(NumberOfLinesSuggestion(self.num_lines))
FileUploadAnswer(object):
min_bytes,
max_bytes):
self.min_bytes
self.max_bytes
SelectionAnswer(object):
SELECTIONANSWER_XML_TEMPLATE
SELECTION_XML_TEMPLATE
(identifier,
SELECTION_VALUE_XML_TEMPLATE
STYLE_XML_TEMPLATE
(style)
MIN_SELECTION_COUNT_XML_TEMPLATE
MAX_SELECTION_COUNT_XML_TEMPLATE
ACCEPTED_STYLES
['radiobutton',
'dropdown',
'checkbox',
'list',
'combobox',
'multichooser']
OTHER_SELECTION_ELEMENT_NAME
'OtherSelection'
min=1,
max=1,
style=None,
selections=None,
type='text',
other=False):
SelectionAnswer.ACCEPTED_STYLES:
ValueError("style
recognized;
(style,
'.join(SelectionAnswer.ACCEPTED_STYLES)))
ValueError("SelectionAnswer.__init__():
identifier)
tuples")
self.selections
self.max_selections
len(selections)
self.min_selections,
less
self.other
"Text"
'binary':
"Binary"
ValueError("illegal
'binary'"
str(self.type))
tpl
self.selections:
value_xml
SelectionAnswer.SELECTION_VALUE_XML_TEMPLATE
(TYPE_TAG,
tpl[0],
TYPE_TAG)
SelectionAnswer.SELECTION_XML_TEMPLATE
(tpl[1],
self.other:
hasattr(self.other,
'get_as_xml'):
isinstance(self.other,
FreeTextAnswer),
'OtherSelection
FreeTextAnswer'
self.other.get_as_xml().replace('FreeTextAnswer',
'OtherSelection')
"<OtherSelection
/>"
SelectionAnswer.STYLE_XML_TEMPLATE
'radiobutton':
SelectionAnswer.MIN_SELECTION_COUNT_XML_TEMPLATE
%self.min_selections
SelectionAnswer.MAX_SELECTION_COUNT_XML_TEMPLATE
%self.max_selections
SelectionAnswer.SELECTIONANSWER_XML_TEMPLATE
boto.mws.exception
['MWSConnection']
api_version_path
'Feeds':
'Reports':
'Orders':
('2013-09-01',
'/Orders/2013-09-01'),
'Products':
('2011-10-01',
'/Products/2011-10-01'),
'Sellers':
('2011-07-01',
'/Sellers/2011-07-01'),
'Inbound':
'/FulfillmentInboundShipment/2010-10-01'),
'Outbound':
'/FulfillmentOutboundShipment/2010-10-01'),
'Inventory':
'/FulfillmentInventory/2010-10-01'),
'Recommendations':
('2013-04-01',
'/Recommendations/2013-04-01'),
'CustomerInfo':
'/CustomerInformation/2014-03-01'),
'CartInfo':
'/CartInformation/2014-03-01'),
'Subscriptions':
('2013-07-01',
'/Subscriptions/2013-07-01'),
'OffAmazonPayments':
('2013-01-01',
'/OffAmazonPayments/2013-01-01'),
content_md5
encodebytes(hashlib.md5(c).digest()).strip()
'section',
'quota',
'version')
api_call_map
to.__wrapped__
structured_lists(*fields):
[f.split('.')
fields]:
newkey
(acc
range(len(kw[key])):
kw[newkey
str(i
kw[key][i]
kw.pop(key)
"{0}\nLists:
http_body(field):
any([f
'content_type')]):
"building
body".format(func.action,
kw['body']
kw['headers']
kw.pop('content_type'),
content_md5(kw['body']),
"{0}\nRequired
Body:
"{1}".format(func.__doc__,
destructure_object(value,
members=False):
boto.mws.response.ResponseElement):
destructure_object(value.__dict__,
collections.Mapping):
destructure_object(value[name],
collections.Iterable):
(members
'.member.'
str(index
destructure_object(element,
structured_objects(*fields,
kwargs.get('members',
destructure_object(kw.pop(field),
"{0}\nElement|Iter|Map:
{1}\n"
"(ResponseElement
anything
iterable/dict-like)"
.format(func.__doc__,
groups))):
exclusive(*groups):
groups)))
{1}"
"{0}\nEither:
dependent(field,
*groups):
any(hasgroup(g)
groups):
{2}"
"{0}\n{1}
requires:
{2}".format(func.__doc__,
requires_some_of(*fields):
"{0}\nSome
Required:
boolean_arguments(*fields):
isinstance(kw.get(f),
bool)]:
kw[field]
str(kw[field]).lower()
"{0}\nBooleans:
api_action(section,
*api):
decorator(func,
quota=int(quota),
restore=float(restore)):
api_version_path[section]
kw.setdefault(accesskey,
kw[accesskey]
"MWSConnection.{2}
attribute?"
"".format(action,
accesskey)
kw['Action']
kw['Version']
self._response_factory(action,
dict(path=path,
quota=quota,
restore=restore)
setattr(wrapper,
locals().get(attr))
"MWS
{0}/{1}
call;
quota={2}
restore={3:.2f}\n"
"{4}".format(action,
api_call_map[action]
MWSConnection(AWSQueryConnection):
boto.mws.response.ResponseFactory
boto.mws.exception.ResponseErrorFactory
'mws.amazonservices.com')
self._sandboxed
kw.pop('sandbox',
kw.pop('Merchant',
kw.get('SellerId')
self.SellerId
kw.pop('SellerId',
self._setup_factories(kw.pop('factory_scopes',
[]),
super(MWSConnection,
_setup_factories(self,
extrascopes,
(scope,
Default)
'response_factory':
(boto.mws.response,
self.ResponseFactory),
'response_error_factory':
(boto.mws.exception,
self.ResponseErrorFactory),
kw.pop(factory))
extrascopes
[scope]
Default(scopes=scopes))
_sandboxify(self,
self._sandboxed:
splat
splat[-2]
'_Sandbox'
'/'.join(splat)
_post_request(self,
self._sandboxify(request['path'])
self._mexe(request,
override_num_retries=None)
bs:
self._response_error_factory(bs.status,
bs.reason,
bs.body)
response.getheader('Content-MD5')
content_md5(body)
contenttype
response.getheader('Content-Type')
self._parse_response(parser,
contenttype.startswith('text/xml'):
XmlHandler(parser,
method_for(self,
string.capwords(name,
api_call_map:
api_call_map[action])
iter_call(self,
self.method_for(call)
"{0}"'.format(call)
self.iter_response(method(*args,
iter_response(self,
self.method_for(response._action
'ByNextToken')
response._result.HasNext
more(NextToken=response._result.NextToken)
@requires(['FeedType'])
@boolean_arguments('PurgeAndReplace')
@http_body('FeedContent')
120)
submit_feed(self,
'FeedTypeList.Type',
get_feed_submission_list(self,
get_feed_submission_list_by_next_token(self,
@structured_lists('FeedTypeList.Type',
get_feed_submission_count(self,
'FeedTypeList.Type')
cancel_feed_submissions(self,
@requires(['FeedSubmissionId'])
get_feed_submission_result(self,
get_service_status(self,
sections
'.join(map(str.lower,
api_version_path.keys()))
{0}.get_(section)_service_status(),
"where
(section)
following:
"{1}".format(self.__class__.__name__,
sections)
AttributeError(message)
@requires(['ReportType'])
@boolean_arguments('ReportOptions=ShowSalesChannel')
request_report(self,
'ReportTypeList.Type',
get_report_request_list(self,
get_report_request_list_by_next_token(self,
@structured_lists('ReportTypeList.Type',
get_report_request_count(self,
cancel_report_requests(self,
'ReportTypeList.Type')
get_report_list(self,
get_report_list_by_next_token(self,
get_report_count(self,
@requires(['ReportId'])
get_report(self,
@requires(['ReportType',
'Schedule'])
manage_report_schedule(self,
get_report_schedule_list(self,
get_report_schedule_list_by_next_token(self,
get_report_schedule_count(self,
@requires(['ReportIdList'])
@structured_lists('ReportIdList.Id')
update_report_acknowledgements(self,
@requires(['ShipFromAddress',
'InboundShipmentPlanRequestItems'])
@structured_objects('ShipFromAddress',
'InboundShipmentPlanRequestItems')
create_inbound_shipment_plan(self,
@requires(['ShipmentId',
'InboundShipmentHeader',
'InboundShipmentItems'])
create_inbound_shipment(self,
@requires(['ShipmentId'])
update_inbound_shipment(self,
@requires_some_of('ShipmentIdList',
'ShipmentStatusList')
@structured_lists('ShipmentIdList.Id',
'ShipmentStatusList.Status')
list_inbound_shipments(self,
list_inbound_shipments_by_next_token(self,
@requires(['ShipmentId'],
['LastUpdatedAfter',
'LastUpdatedBefore'])
list_inbound_shipment_items(self,
list_inbound_shipment_items_by_next_token(self,
get_inbound_service_status(self,
@requires(['SellerSkus'],
['QueryStartDateTime'])
@structured_lists('SellerSkus.member')
list_inventory_supply(self,
list_inventory_supply_by_next_token(self,
get_inventory_service_status(self,
@requires(['PackageNumber'])
get_package_tracking_details(self,
@requires(['Address',
@structured_objects('Address',
get_fulfillment_preview(self,
@requires(['SellerFulfillmentOrderId',
'DisplayableOrderId',
'ShippingSpeedCategory',
'DisplayableOrderDateTime',
'DestinationAddress',
'DisplayableOrderComment',
@structured_objects('DestinationAddress',
create_fulfillment_order(self,
get_fulfillment_order(self,
list_all_fulfillment_orders(self,
list_all_fulfillment_orders_by_next_token(self,
cancel_fulfillment_order(self,
get_outbound_service_status(self,
@requires(['CreatedAfter'],
@dependent('CreatedBefore',
['CreatedAfter'])
@exclusive(['LastUpdatedAfter'],
['BuyerEmail'],
['SellerOrderId'])
@dependent('LastUpdatedBefore',
['LastUpdatedBefore'])
@structured_objects('OrderTotal',
'ShippingAddress',
'PaymentExecutionDetail')
@structured_lists('MarketplaceId.Id',
'OrderStatus.Status',
'FulfillmentChannel.Channel',
'PaymentMethod.')
list_orders(self,
toggle
set(('FulfillmentChannel.Channel.1',
'OrderStatus.Status.1',
'PaymentMethod.1',
'LastUpdatedAfter',
'LastUpdatedBefore'))
do,
'BuyerEmail':
toggle.union(['SellerOrderId']),
'SellerOrderId':
toggle.union(['BuyerEmail']),
"Don't
"{1}".format('
'.join(dont),
do)
list_orders_by_next_token(self,
@structured_lists('AmazonOrderId.Id')
get_order(self,
list_order_items(self,
list_order_items_by_next_token(self,
get_orders_service_status(self,
'Query'])
list_matching_products(self,
get_matching_product(self,
'IdType',
'IdList'])
@structured_lists('IdList.Id')
get_matching_product_for_id(self,
'GetCompetitivePricingForSKU')
get_competitive_pricing_for_sku(self,
'GetCompetitivePricingForASIN')
get_competitive_pricing_for_asin(self,
'GetLowestOfferListingsForSKU')
get_lowest_offer_listings_for_sku(self,
'GetLowestOfferListingsForASIN')
get_lowest_offer_listings_for_asin(self,
'SellerSKU'])
'GetProductCategoriesForSKU')
get_product_categories_for_sku(self,
'ASIN'])
'GetProductCategoriesForASIN')
get_product_categories_for_asin(self,
get_products_service_status(self,
'GetMyPriceForSKU')
get_my_price_for_sku(self,
'GetMyPriceForASIN')
get_my_price_for_asin(self,
list_marketplace_participations(self,
list_marketplace_participations_by_next_token(self,
get_last_updated_time_for_recommendations(self,
@structured_lists('CategoryQueryList.CategoryQuery')
list_recommendations(self,
list_recommendations_by_next_token(self,
get_recommendations_service_status(self,
list_customers(self,
list_customers_by_next_token(self,
@requires(['CustomerIdList'])
@structured_lists('CustomerIdList.CustomerId')
get_customers_for_customer_id(self,
get_customerinfo_service_status(self,
@requires(['DateRangeStart'])
list_carts(self,
list_carts_by_next_token(self,
@requires(['CartIdList'])
@structured_lists('CartIdList.CartId')
get_carts(self,
get_cartinfo_service_status(self,
register_destination(self,
deregister_destination(self,
list_registered_destinations(self,
send_test_notification_to_destination(self,
create_subscription(self,
get_subscription(self,
delete_subscription(self,
list_subscriptions(self,
update_subscription(self,
get_subscriptions_service_status(self,
'OrderReferenceAttributes'])
@structured_objects('OrderReferenceAttributes')
set_order_reference_details(self,
get_order_reference_details(self,
confirm_order_reference(self,
cancel_order_reference(self,
close_order_reference(self,
'AuthorizationReferenceId',
'AuthorizationAmount'])
@structured_objects('AuthorizationAmount')
get_authorization_details(self,
@requires(['AmazonAuthorizationId',
'CaptureReferenceId',
'CaptureAmount'])
@structured_objects('CaptureAmount')
capture(self,
@requires(['AmazonCaptureId'])
get_capture_details(self,
close_authorization(self,
@requires(['AmazonCaptureId',
'RefundReferenceId',
'RefundAmount'])
@structured_objects('RefundAmount')
@requires(['AmazonRefundId'])
get_refund_details(self,
get_offamazonpayments_service_status(self,
ResponseErrorFactory(ResponseFactory):
BotoServerError(status,
self.find_element(server.error_code,
print(supplied.__name__)
supplied(status,
'{0.__name__}({1.reason}:
"{1.message}")'
.format(self.__class__,
self.__doc__.strip()
'{1.__name__}:
{0.reason}
{2}\n{3}'
'{0.message}'.format(self,
self.__class__,
InvalidParameterValue(ResponseError):
InvalidParameter(ResponseError):
InvalidAddress(ResponseError):
ComplexType(dict):
'{0}{1}'.format(getattr(self,
self.copy())
DeclarativeType(object):
JITResponse(ResponseElement):
JITResponse
self._hint.__name__
'JIT_{0}/{1}'.format(self.__class__.__name__,
hex(id(self._hint))[2:])
kw.items():
setattr(self._hint,
'_parent',
'<{0}_{1}/{2}_{3}>'.format(self.__class__.__name__,
parent._name
'?',
'_name',
'?'),
hex(id(self.__class__)))
setup(self,
self._clone
self.__class__(_hint=self._hint)
self._clone._parent
self._clone._name
self._clone)
self._value)
Element(DeclarativeType):
SimpleList(DeclarativeType):
super(SimpleList,
ElementList(SimpleList):
MemberList(Element):
_member=None,
`member`
specification
{0}'.format(self.__class__.__name__)
'member'
_member
member=ElementList(**kw))
self).__init__(_hint=_hint)
issubclass(_member,
_member(**kw)
ElementList(_member,
member=member)
'Nonsensical
hint
{1!r}'.format(self.__class__.__name__,
_hint)
isinstance(self._value.member,
self).teardown(*args,
ResponseFactory(object):
scopes=None):
self.scopes
element_factory(self,
DynamicElement(parent):
setattr(DynamicElement,
str(name))
DynamicElement
search_scopes(self,
self.scopes:
getattr(scope,
'__getitem__'):
scope[key]
find_element(self,
self.search_scopes(action
action.endswith('ByNextToken'):
self.search_scopes(action[:-len('ByNextToken')]
element)
'Result'):
setattr(response,
Element(result))
response(connection=connection)
strip_namespace(func):
name.startswith(self._namespace
':'):
name[len(self._namespace
':'):]
ResponseElement(dict):
_override
parent._namespace
self._declared('setup',
attrs=attrs)
attrs.copy()
_declared(self,
inherit(obj):
'__bases__',
()):
result.update(inherit(cls))
result.update(obj.__dict__)
inherit(self.__class__)
scope.update(self.__dict__)
declared
isinstance(attr[1],
DeclarativeType)
filter(declared,
scope.items()):
getattr(node,
op)(self,
parentname=self._name,
'{0!s}:
{1!r}'.format(*pair)
name.startswith('JIT_'):
'^{0}^'.format(self._name
'{0}{1!r}({2})'.format(
self.copy(),
_type_for(self,
self._override.get(name,
globals().get(name,
ResponseElement))
attribute.start(name=name,
attrs=attrs,
attrs.getLength():
ComplexType(attrs.copy()))
self._declared('teardown')
attribute.end(name=name,
ComplexType):
setattr(attribute,
attribute._value,
ResponseMetadata
self.update(attrs)
_result(self):
_action(self):
(self._name
self.__class__.__name__)[:-len('Response')]
ResponseResultList(Response):
ResponseElement
ElementList(self._ResultClass))
super(ResponseResultList,
FeedSubmissionInfo(ResponseElement):
SubmitFeedResult(ResponseElement):
Element(FeedSubmissionInfo)
GetFeedSubmissionListResult(ResponseElement):
ElementList(FeedSubmissionInfo)
GetFeedSubmissionCountResult(ResponseElement):
CancelFeedSubmissionsResult(GetFeedSubmissionListResult):
GetServiceStatusResult(ResponseElement):
Messages
Element(Messages=ElementList())
ReportRequestInfo(ResponseElement):
RequestReportResult(ResponseElement):
GetReportRequestListResult(RequestReportResult):
CancelReportRequestsResult(RequestReportResult):
GetReportListResult(ResponseElement):
ReportInfo
ManageReportScheduleResult(ResponseElement):
ReportSchedule
GetReportScheduleListResult(ManageReportScheduleResult):
UpdateReportAcknowledgementsResult(GetReportListResult):
CreateInboundShipmentPlanResult(ResponseElement):
InboundShipmentPlans
MemberList(ShipToAddress=Element(),
Items=MemberList())
ListInboundShipmentsResult(ResponseElement):
ShipmentData
MemberList(ShipFromAddress=Element())
ListInboundShipmentItemsResult(ResponseElement):
ItemData
ListInventorySupplyResult(ResponseElement):
InventorySupplyList
EarliestAvailability=Element(),
SupplyDetail=MemberList(
EarliestAvailableToPick=Element(),
LatestAvailableToPick=Element(),
float(getattr(self,
self._amount):
self._amount:
ComplexMoney(ComplexAmount):
'Amount'
ComplexWeight(ResponseElement):
{1}'.format(self.Value,
self.Unit)
('Unit',
ComplexWeight'.format(name)
Dimension(ComplexType):
ComplexDimensions(ResponseElement):
_dimensions
('Height',
'Length',
'Width',
'Weight')
[getattr(self,
self._dimensions]
filter(None,
'x'.join(map('{0.Value:0.2f}{0[Units]}'.format,
values))
ComplexDimensions'.format(name)
Dimension(attrs.copy()))
Decimal(value
ResponseElement.endElement(self,
FulfillmentPreviewItem(ResponseElement):
FulfillmentPreview(ResponseElement):
EstimatedFees
MemberList(Amount=Element(ComplexAmount))
UnfulfillablePreviewItems
MemberList(FulfillmentPreviewItem)
FulfillmentPreviewShipments
FulfillmentPreviewItems=MemberList(FulfillmentPreviewItem),
GetFulfillmentPreviewResult(ResponseElement):
FulfillmentPreviews
MemberList(FulfillmentPreview)
FulfillmentOrder(ResponseElement):
DestinationAddress
NotificationEmailList
GetFulfillmentOrderResult(ResponseElement):
FulfillmentOrder
Element(FulfillmentOrder)
FulfillmentShipment
FulfillmentShipmentItem=MemberList(),
FulfillmentShipmentPackage=MemberList(),
FulfillmentOrderItem
ListAllFulfillmentOrdersResult(ResponseElement):
FulfillmentOrders
MemberList(FulfillmentOrder)
GetPackageTrackingDetailsResult(ResponseElement):
ShipToAddress
TrackingEvents
MemberList(EventAddress=Element())
Image(ResponseElement):
AttributeSet(ResponseElement):
ItemDimensions
ListPrice
PackageDimensions
SmallImage
Element(Image)
ItemAttributes(AttributeSet):
Languages
Element(Language=ElementList())
('Actor',
'Artist',
'Author',
'Creator',
'Director',
'Feature',
'Format',
'GemType',
'MaterialType',
'MediaType',
'OperatingSystem',
'Platform')
SimpleList())
super(ItemAttributes,
VariationRelationship(ResponseElement):
GemType
MaterialType
OperatingSystem
Price(ResponseElement):
LandedPrice
ListingPrice
Shipping
CompetitivePrice(ResponseElement):
CompetitivePriceList(ResponseElement):
CompetitivePrice
ElementList(CompetitivePrice)
CompetitivePricing(ResponseElement):
CompetitivePrices
Element(CompetitivePriceList)
NumberOfOfferListings
TradeInValue
SalesRank(ResponseElement):
LowestOfferListing(ResponseElement):
Qualifiers
Element(ShippingTime=Element())
Offer(ResponseElement):
BuyingPrice
RegularPrice
Product(ResponseElement):
'ns2'
AttributeSets
ItemAttributes=ElementList(ItemAttributes),
Relationships
VariationParent=ElementList(VariationRelationship),
CompetitivePricing
ElementList(CompetitivePricing)
SalesRankings
SalesRank=ElementList(SalesRank),
LowestOfferListings
LowestOfferListing=ElementList(LowestOfferListing),
Offers
Offer=ElementList(Offer),
ListMatchingProductsResult(ResponseElement):
Products
Element(Product=ElementList(Product))
ProductsBulkOperationResult(ResponseElement):
Product
Element(Product)
ProductsBulkOperationResponse(ResponseResultList):
ProductsBulkOperationResult
GetMatchingProductResponse(ProductsBulkOperationResponse):
GetMatchingProductForIdResult(ListMatchingProductsResult):
GetMatchingProductForIdResponse(ResponseResultList):
GetMatchingProductForIdResult
GetCompetitivePricingForSKUResponse(ProductsBulkOperationResponse):
GetCompetitivePricingForASINResponse(ProductsBulkOperationResponse):
GetLowestOfferListingsForSKUResponse(ProductsBulkOperationResponse):
GetLowestOfferListingsForASINResponse(ProductsBulkOperationResponse):
GetMyPriceForSKUResponse(ProductsBulkOperationResponse):
GetMyPriceForASINResponse(ProductsBulkOperationResponse):
ProductCategory(ResponseElement):
'Parent',
Element(ProductCategory))
super(ProductCategory,
GetProductCategoriesResult(ResponseElement):
Self
ElementList(ProductCategory)
GetProductCategoriesForSKUResult(GetProductCategoriesResult):
GetProductCategoriesForASINResult(GetProductCategoriesResult):
Order(ResponseElement):
ShippingAddress
PaymentExecutionDetail
PaymentExecutionDetailItem=ElementList(
PaymentExecutionDetailItem=Element(
Payment=Element(ComplexMoney)
ListOrdersResult(ResponseElement):
Orders
Element(Order=ElementList(Order))
GetOrderResult(ListOrdersResult):
OrderItem(ResponseElement):
ItemPrice
ShippingPrice
GiftWrapPrice
ItemTax
ShippingTax
GiftWrapTax
ShippingDiscount
PromotionDiscount
PromotionIds
CODFee
CODFeeDiscount
ListOrderItemsResult(ResponseElement):
OrderItems
Element(OrderItem=ElementList(OrderItem))
ListMarketplaceParticipationsResult(ResponseElement):
ListParticipations
Element(Participation=ElementList())
ListMarketplaces
Element(Marketplace=ElementList())
ListRecommendationsResult(ResponseElement):
ListingQualityRecommendations
MemberList(ItemIdentifier=Element())
Customer(ResponseElement):
PrimaryContactInfo
ShippingAddressList
Element(ShippingAddress=ElementList())
AssociatedMarketplaces
Element(MarketplaceDomain=ElementList())
ListCustomersResult(ResponseElement):
CustomerList
Element(Customer=ElementList(Customer))
GetCustomersForCustomerIdResult(ListCustomersResult):
CartItem(ResponseElement):
CurrentPrice
SalePrice
Cart(ResponseElement):
ActiveCartItemList
SavedCartItemList
ListCartsResult(ResponseElement):
CartList
Element(Cart=ElementList(Cart))
GetCartsResult(ListCartsResult):
Destination(ResponseElement):
AttributeList
ListRegisteredDestinationsResult(ResponseElement):
DestinationList
MemberList(Destination)
Subscription(ResponseElement):
Element(Destination)
GetSubscriptionResult(ResponseElement):
Subscription
Element(Subscription)
ListSubscriptionsResult(ResponseElement):
SubscriptionList
MemberList(Subscription)
OrderReferenceDetails(ResponseElement):
Buyer
Element(PhysicalDestination=Element())
SellerOrderAttributes
OrderReferenceStatus
Constraints
SetOrderReferenceDetailsResult(ResponseElement):
OrderReferenceDetails
Element(OrderReferenceDetails)
GetOrderReferenceDetailsResult(SetOrderReferenceDetailsResult):
AuthorizationDetails(ResponseElement):
AuthorizationAmount
CapturedAmount
AuthorizationFee
AuthorizationStatus
AuthorizeResult(ResponseElement):
AuthorizationDetails
Element(AuthorizationDetails)
GetAuthorizationDetailsResult(AuthorizeResult):
CaptureDetails(ResponseElement):
CaptureAmount
RefundedAmount
CaptureFee
CaptureStatus
CaptureResult(ResponseElement):
CaptureDetails
Element(CaptureDetails)
GetCaptureDetailsResult(CaptureResult):
RefundDetails(ResponseElement):
RefundAmount
FeeRefunded
RefundStatus
RefundResult(ResponseElement):
RefundDetails
Element(RefundDetails)
GetRefundDetails(RefundResult):
get_regions('opsworks',
connection_cls=OpsWorksConnection)
OpsWorksConnection(AWSQueryConnection):
"2013-02-18"
"opsworks.us-east-1.amazonaws.com"
"OpsWorks"
"OpsWorks_20130218"
super(OpsWorksConnection,
assign_instance(self,
self.make_request(action='AssignInstance',
assign_volume(self,
self.make_request(action='AssignVolume',
associate_elastic_ip(self,
self.make_request(action='AssociateElasticIp',
attach_elastic_load_balancer(self,
self.make_request(action='AttachElasticLoadBalancer',
clone_stack(self,
clone_permissions=None,
clone_app_ids=None,
'SourceStackId':
params['Region']
params['ClonePermissions']
params['CloneAppIds']
self.make_request(action='CloneStack',
create_app(self,
self.make_request(action='CreateApp',
custom_json=None):
'Command':
params['Comment']
create_instance(self,
root_device_type=None,
params['RootDeviceType']
self.make_request(action='CreateInstance',
create_layer(self,
'Shortname':
self.make_request(action='CreateLayer',
'DefaultInstanceProfileArn':
self.make_request(action='CreateStack',
create_user_profile(self,
self.make_request(action='CreateUserProfile',
delete_app(self,
app_id):
self.make_request(action='DeleteApp',
delete_instance(self,
delete_elastic_ip=None,
delete_volumes=None):
params['DeleteElasticIp']
params['DeleteVolumes']
self.make_request(action='DeleteInstance',
delete_layer(self,
self.make_request(action='DeleteLayer',
self.make_request(action='DeleteStack',
delete_user_profile(self,
iam_user_arn):
self.make_request(action='DeleteUserProfile',
deregister_elastic_ip(self,
self.make_request(action='DeregisterElasticIp',
deregister_instance(self,
self.make_request(action='DeregisterInstance',
deregister_rds_db_instance(self,
rds_db_instance_arn):
self.make_request(action='DeregisterRdsDbInstance',
deregister_volume(self,
self.make_request(action='DeregisterVolume',
describe_apps(self,
app_ids=None):
params['AppIds']
self.make_request(action='DescribeApps',
describe_commands(self,
deployment_id=None,
command_ids=None):
params['DeploymentId']
params['CommandIds']
self.make_request(action='DescribeCommands',
describe_deployments(self,
params['DeploymentIds']
self.make_request(action='DescribeDeployments',
describe_elastic_ips(self,
ips=None):
params['Ips']
self.make_request(action='DescribeElasticIps',
describe_elastic_load_balancers(self,
self.make_request(action='DescribeElasticLoadBalancers',
describe_instances(self,
layer_id=None,
instance_ids=None):
params['LayerId']
self.make_request(action='DescribeInstances',
describe_layers(self,
self.make_request(action='DescribeLayers',
describe_load_based_auto_scaling(self,
{'LayerIds':
self.make_request(action='DescribeLoadBasedAutoScaling',
describe_my_user_profile(self):
self.make_request(action='DescribeMyUserProfile',
describe_permissions(self,
iam_user_arn=None,
stack_id=None):
params['IamUserArn']
self.make_request(action='DescribePermissions',
describe_raid_arrays(self,
raid_array_ids=None):
params['RaidArrayIds']
self.make_request(action='DescribeRaidArrays',
describe_rds_db_instances(self,
rds_db_instance_arns=None):
params['RdsDbInstanceArns']
self.make_request(action='DescribeRdsDbInstances',
describe_service_errors(self,
service_error_ids=None):
params['ServiceErrorIds']
self.make_request(action='DescribeServiceErrors',
describe_stack_provisioning_parameters(self,
self.make_request(action='DescribeStackProvisioningParameters',
describe_stack_summary(self,
self.make_request(action='DescribeStackSummary',
stack_ids=None):
params['StackIds']
self.make_request(action='DescribeStacks',
describe_time_based_auto_scaling(self,
{'InstanceIds':
self.make_request(action='DescribeTimeBasedAutoScaling',
describe_user_profiles(self,
iam_user_arns=None):
params['IamUserArns']
self.make_request(action='DescribeUserProfiles',
describe_volumes(self,
raid_array_id=None,
volume_ids=None):
params['RaidArrayId']
params['VolumeIds']
self.make_request(action='DescribeVolumes',
detach_elastic_load_balancer(self,
self.make_request(action='DetachElasticLoadBalancer',
disassociate_elastic_ip(self,
self.make_request(action='DisassociateElasticIp',
get_hostname_suggestion(self,
self.make_request(action='GetHostnameSuggestion',
reboot_instance(self,
self.make_request(action='RebootInstance',
register_elastic_ip(self,
self.make_request(action='RegisterElasticIp',
register_instance(self,
private_ip=None,
rsa_public_key=None,
rsa_public_key_fingerprint=None,
params['PrivateIp']
params['RsaPublicKey']
params['RsaPublicKeyFingerprint']
params['InstanceIdentity']
self.make_request(action='RegisterInstance',
register_rds_db_instance(self,
db_password):
'RdsDbInstanceArn':
'DbUser':
'DbPassword':
db_password,
self.make_request(action='RegisterRdsDbInstance',
register_volume(self,
ec_2_volume_id=None):
params['Ec2VolumeId']
self.make_request(action='RegisterVolume',
set_load_based_auto_scaling(self,
enable=None,
up_scaling=None,
down_scaling=None):
params['Enable']
params['UpScaling']
params['DownScaling']
self.make_request(action='SetLoadBasedAutoScaling',
set_permission(self,
allow_ssh=None,
allow_sudo=None,
level=None):
'IamUserArn':
params['AllowSsh']
params['AllowSudo']
params['Level']
self.make_request(action='SetPermission',
set_time_based_auto_scaling(self,
auto_scaling_schedule=None):
params['AutoScalingSchedule']
self.make_request(action='SetTimeBasedAutoScaling',
start_instance(self,
self.make_request(action='StartInstance',
start_stack(self,
self.make_request(action='StartStack',
stop_instance(self,
self.make_request(action='StopInstance',
stop_stack(self,
self.make_request(action='StopStack',
unassign_instance(self,
self.make_request(action='UnassignInstance',
unassign_volume(self,
self.make_request(action='UnassignVolume',
update_app(self,
params['Type']
self.make_request(action='UpdateApp',
update_elastic_ip(self,
self.make_request(action='UpdateElasticIp',
update_instance(self,
layer_ids=None,
self.make_request(action='UpdateInstance',
update_layer(self,
self.make_request(action='UpdateLayer',
update_my_user_profile(self,
ssh_public_key=None):
self.make_request(action='UpdateMyUserProfile',
update_rds_db_instance(self,
db_user=None,
db_password=None):
params['DbUser']
params['DbPassword']
self.make_request(action='UpdateRdsDbInstance',
service_role_arn=None,
default_root_device_type=None,
use_opsworks_security_groups=None):
params['ServiceRoleArn']
self.make_request(action='UpdateStack',
update_user_profile(self,
self.make_request(action='UpdateUserProfile',
update_volume(self,
mount_point=None):
params['MountPoint']
self.make_request(action='UpdateVolume',
get_instance_metadata,
Bootstrap(ScriptBase):
'/mnt/pyami'
self.write_metadata()
super(Bootstrap,
write_metadata(self):
open(os.path.expanduser(BotoConfigPath),
fp.write('[Instance]\n')
inst_data
get_instance_metadata()
inst_data:
inst_data[key]))
fp.write('\n%s\n'
user_data)
fp.write('[Pyami]\n')
fp.write('working_dir
boto.config
boto.init_logging()
create_working_dir(self):
boto.log.info('Working
os.path.exists(self.working_dir):
os.mkdir(self.working_dir)
load_boto(self):
'boto_update',
'svn:HEAD')
update.startswith('svn'):
'-r%s'
'-rHEAD'
'/usr/local/boto')
self.run('svn
(version,
location))
update.startswith('git'):
'/usr/share/python-support/python-boto/boto')
pull',
boto.log.info('git
exception.
bit.
'master'
checkout
self.run('rm
/usr/local/lib/python2.5/site-packages/boto')
update)
fetch_s3_file(self,
s3_file):
fetch_file
fetch_file(s3_file)
s3_file.split("/")[-1])
"w").write(f.read())
s3_file)
load_packages(self):
package_str
'packages')
package_str:
package_str.split(',')
packages:
package.strip()
package.startswith('s3:'):
self.fetch_s3_file(package)
package:
package.endswith('.py'):
-Z
package,
self.create_working_dir()
self.load_boto()
self.load_packages()
self.notify('Bootstrap
boto.config.get_instance('instance-id'))
boto.set_file_logger('bootstrap',
Bootstrap()
expanduser,
NoSectionError,
'/etc/boto.cfg'
[BotoConfigPath]
UserConfigPath
'.boto')
BotoConfigLocations.append(UserConfigPath)
'BOTO_CONFIG'
[expanduser(os.environ['BOTO_CONFIG'])]
'BOTO_PATH'
os.environ['BOTO_PATH'].split(os.pathsep):
BotoConfigLocations.append(expanduser(path))
Config(object):
do_load=True):
ConfigParser({'working_dir':
'/mnt/pyami',
'debug':
'0'})
do_load:
self.load_from_path(path)
self.readfp(fp)
self.read(BotoConfigLocations)
"AWS_CREDENTIAL_FILE"
expanduser(os.environ['AWS_CREDENTIAL_FILE'])
self.load_credential_file(full_path)
warnings.warn('Unable
AWS_CREDENTIAL_FILE
full_path)
state['_parser']
getattr(self._parser,
self._parser.has_option(*args,
load_credential_file(self,
c_data
c_data.write("[Credentials]\n")
"r").readlines():
c_data.write(line.replace("AWSAccessKeyId",
"aws_access_key_id").replace("AWSSecretKey",
"aws_secret_access_key"))
c_data.seek(0)
self.readfp(c_data)
load_from_path(self,
file.readlines():
re.match("^#import[\s\t]*([^\s^\t]*)[\s\t]*$",
extended_file
(dir,
os.path.split(path)
self.load_from_path(os.path.join(dir,
extended_file))
self.read(path)
save_option(self,
config.read(path)
config.has_section(section):
config.add_section(section)
config.set(section,
save_user_option(self,
self.save_option(UserConfigPath,
save_system_option(self,
self.save_option(BotoConfigPath,
get_instance(self,
self.get('Instance',
self.get('User',
getint_user(self,
self.getint('User',
self._parser.get(section,
self._parser.getint(section,
self._parser.getfloat(section,
float(default)
self.has_option(section,
setbool(self,
dump(self):
self.write(s)
print(s.getvalue())
dump_safe(self,
fp=None):
fp.write('[%s]\n'
section)
xxxxxxxxxxxxxxxxxx\n'
(option,
option)))
dump_to_sdb(self,
sdb.create_domain(domain_name)
domain.new_item(item_name)
item.active
d[option]
item[section]
json.dumps(d)
load_from_sdb(self,
domain.get_item(item_name)
item.keys():
json.loads(item[section])
d.keys():
'None'
isinstance(attr_value,
self.setbool(section,
CopyBot(ScriptBase):
super(CopyBot,
self.wdir
self.src_name
'src_bucket')
self.dst_name
'dst_bucket')
self.replace
'replace_dst',
s3.lookup(self.src_name)
boto.log.error('Source
exist:
self.src_name)
dest_access_key
'dest_aws_access_key_id',
dest_access_key:
dest_secret_key
'dest_aws_secret_access_key',
boto.connect(dest_access_key,
dest_secret_key)
s3.lookup(self.dst_name)
s3.create_bucket(self.dst_name)
copy_bucket_acl(self):
self.src.get_xml_acl()
self.dst.set_xml_acl(acl)
copy_key_acl(self,
src.get_xml_acl()
dst.set_xml_acl(acl)
copy_keys(self):
boto.log.info('src=%s'
self.src.name)
boto.log.info('dst=%s'
self.dst.name)
self.replace:
self.dst.lookup(key.name)
exists:
boto.log.info('key=%s
skipping'
self.dst.name))
boto.log.info('copying
key=%s'
(key.size,
os.path.split(key.name)
base)
key.get_contents_to_filename(path)
self.dst.new_key(key.name)
new_key.set_contents_from_filename(path)
self.copy_key_acl(key,
new_key)
os.unlink(path)
boto.log.exception('Error
copy_log(self):
self.dst.new_key(self.log_file)
StringIO.StringIO()
boto.config.dump_safe(fp)
fp.getvalue())
self.copy_keys()
self.copy_log()
Stopping'
'Copy
Operation
Complete')
'exit_on_completion',
ec2.terminate_instances([self.instance_id])
HelloWorld(ScriptBase):
self.log('Hello
World!!!')
getopt
usage_string
=def
usage():
print(usage_string)
getopt.getopt(sys.argv[1:],
'a:b:c:g:hi:k:m:n:o:rs:w',
['ami',
'class',
'group',
'help',
'inputqueue',
'keypair',
'module',
'numinstances',
'outputqueue',
'reload',
'script_name',
'wait'])
{'module_name':
'script_name':
'class_name':
'script_bucket':
'keypair':
'ami':
'num_instances':
'input_queue_name':
'output_queue_name':
o,
opts:
('-a',
'--ami'):
('-b',
'--bucket'):
params['script_bucket']
('-c',
'--class'):
params['class_name']
('-g',
'--group'):
('-h',
'--help'):
('-i',
'--inputqueue'):
params['input_queue_name']
('-k',
'--keypair'):
('-m',
'--module'):
params['module_name']
('-n',
'--num_instances'):
params['num_instances']
int(a)
('-o',
'--outputqueue'):
params['output_queue_name']
('-r',
'--reload'):
('-s',
'--script'):
params['script_name']
('-w',
'--wait'):
['ami']
params.get(pname,
pname)
params['script_name']:
print('Reloading
print('Copying
imp.find_module(params['script_name'])
c.get_bucket(params['script_bucket'])
bucket.new_key(params['script_name']
key.set_contents_from_file(l[0])
params['script_md5']
l.append('aws_access_key_id=%s'
c.aws_access_key_id)
l.append('aws_secret_access_key=%s'
c.aws_secret_access_key)
l.append(kv)
'|'.join(l)
c.get_all_images([params['ami']])
img.run(user_data=s,
key_name=params['keypair'],
security_groups=[params['group']],
max_count=params.get('num_instances',
print('AMI:
(Started)'
(params['ami'],
img.location))
running:
print(status)
status.count('running')
len(r.instances):
print('Instance:
i.ami_launch_index)
print('Public
i.public_dns_name)
print('Private
i.private_dns_name)
ShellCommand,
ScriptBase(object):
self.ts
boto.config.read(config_file)
notify(self,
boto.utils.notify(subject,
mkdir(self,
umount(self,
os.path.ismount(path):
exit_on_error=False,
self.last_command
ShellCommand(command,
Output:
notify:
self.notify('Error
encountered',
command:\n\t%s\n\nCommand
output:\n\t%s'
exit_on_error:
sys.exit(-1)
Startup(ScriptBase):
run_scripts(self):
config.get('Pyami',
scripts:
scripts.split(','):
script.strip("
script.rfind('.')
script[0:pos]
script[pos
find_class(mod_name,
boto.log.info('Running
s.main()
boto.log.warning('Trouble
Running
Startup
halting.'
self.run_scripts()
self.notify('Startup
config.get('Instance',
'instance-id'))
config.has_section('loggers'):
boto.set_file_logger('startup',
sys.path.append(config.get('Pyami',
'working_dir'))
su
Startup()
su.main()
Installer(ScriptBase):
minute,
Apache(Installer):
self.run("apt-get
update")
apache2',
libapache2-mod-python',
rewrite',
ssl',
proxy',
proxy_ajp',
self.stop("apache2")
self.start("apache2")
BackupScriptTemplate
=BackupCleanupScript=TagBasedBackupCleanupScript=class
EBSInstaller(Installer):
super(EBSInstaller,
'instance-id')
'device',
'/dev/sdp')
'volume_id')
self.logical_volume_name
'logical_volume_name')
'mount_point',
'/ebs')
attach(self):
self.logical_volume_name:
next(Volume.find(name=self.logical_volume_name))
logical_volume._volume_id
volume.update()
boto.log.info('Volume
yet
Current
%s.'
(volume.id,
volume.status))
ec2.get_only_instances([self.instance_id])[0]
attempt_attach:
e.error_code
'IncorrectState':
boto.log.info('Attempt
bit.'
e.errors))
boto.log.info('Attached
self.device))
make_fs(self):
self.run('fsck
self.run('mkfs
create_backup_script(self):
Template(BackupScriptTemplate)
t.substitute(volume_id=self.volume_id,
mount_point=self.mount_point)
open('/usr/local/bin/ebs_backup',
fp.write(s)
/usr/local/bin/ebs_backup')
create_backup_cleanup_script(self,
use_tag_based_cleanup=False):
open('/usr/local/bin/ebs_backup_cleanup',
use_tag_based_cleanup:
fp.write(TagBasedBackupCleanupScript)
fp.write(BackupCleanupScript)
/usr/local/bin/ebs_backup_cleanup')
handle_mount_point(self):
os.path.isdir(self.mount_point):
self.run("mkdir
self.last_command.output.split('\n')
self.run("mount
self.run('xfs_growfs
update_fstab(self):
open("/etc/fstab",
"a")
f.write('%s\t%s\txfs\tdefaults
0\n'
xfsprogs
self.make_fs()
self.handle_mount_point()
self.create_backup_script()
'backup_cron_minute',
'backup_cron_hour',
'4,16')
self.add_cron("ebs_backup",
"/usr/local/bin/ebs_backup",
'backup_cleanup_cron_minute')
'backup_cleanup_cron_hour')
(minute
(hour
use_tag_based_cleanup
boto.config.has_option('EBS',
'use_tag_based_snapshot_cleanup')
self.create_backup_cleanup_script(use_tag_based_cleanup)
self.add_cron("ebs_backup_cleanup",
"/usr/local/bin/ebs_backup_cleanup",
self.update_fstab()
boto.log.info("Device
attached,
Installer"
boto.pyami.installers
getpwnam
Installer(boto.pyami.installers.Installer):
minute="*",
hour="*",
mday="*",
month="*",
wday="*",
who="root",
str(random.randrange(60))
str(random.randrange(24))
open('/etc/cron.d/%s'
env.items():
fp.write('%s=%s\n'
(minute,
command))
f_path
os.path.join("/etc/init.d",
open(f_path,
f.write(file)
os.chmod(f_path,
stat.S_IREAD
stat.S_IWRITE
stat.S_IEXEC)
self.run("/usr/sbin/update-rc.d
defaults"
boto.log.info('Adding
variable:
%s=%s'
os.path.exists("/etc/environment.orig"):
/etc/environment
/etc/environment.orig',
notify=False,
open('/etc/environment',
fp.write('\n%s="%s"'
os.environ[key]
stop'
start'
user):
self.run("useradd
user)
getpwnam(user)
ShellCommand
ConfigSection
MySQL(Installer):
update')
mysql-server',
change_data_dir(self,
boto.config.get('MySQL',
'data_dir',
'/mnt')
is_mysql_running_command
ShellCommand('mysqladmin
ping')
is_mysql_running_command.run()
is_mysql_running_command.getStatus()
#trying
immediately
installing
fails
self.run("echo
'quit'
root")
self.run('/etc/init.d/mysql
stop')
self.run("pkill
-9
mysql")
mysql_path
os.path.join(data_dir,
os.path.exists(mysql_path):
self.run('mkdir
self.run('chown
mysql:mysql
open('/etc/mysql/conf.d/use_mnt.cnf',
pyami\n')
data\n'
data_dir)
fp.write('[mysqld]\n')
fp.write('datadir
fp.write('log_bin
os.path.join(mysql_path,
'mysql-bin.log'))
fresh_install:
-pr
/var/lib/mysql/*
%s/'
config_parser
config_parser.read('/etc/mysql/debian.cnf')
config_parser.get('client',
'password')
#time
grant_command
"echo
\"GRANT
ALL
PRIVILEGES
ON
*.*
TO
'debian-sys-maint'@'localhost'
IDENTIFIED
WITH
GRANT
OPTION;\"
mysql"
self.run(grant_command)
self.change_data_dir()
Trac(Installer):
trac',
libapache2-svn',
ssl")
mod_python")
dav_svn")
rewrite")
self.run("touch
self.run("chmod
a+w
setup_vhost(self):
"hostname").strip()
domain_info
domain.split('.')
cnf
open("/etc/apache2/sites-available/%s"
domain_info[0],
*:80\n")
"SSLCertificateFile"):
*:443\n\n")
cnf.write("\tRewriteEngine
cnf.write("\tRewriteRule
^(.*)$
https://%s$1\n"
cnf.write("</VirtualHost>\n\n")
*:443>\n")
cnf.write("\tDocumentRoot
cnf.write("\t<Directory
%s>\n"
cnf.write("\t\tOptions
FollowSymLinks
Indexes
MultiViews\n")
cnf.write("\t\tAllowOverride
All\n")
cnf.write("\t\tOrder
allow,deny\n")
cnf.write("\t\tallow
all\n")
cnf.write("\t</Directory>\n")
/>\n")
cnf.write("\t\tAuthType
Basic\n")
cnf.write("\t\tAuthName
\"%s\"\n"
"name"))
cnf.write("\t\tRequire
valid-user\n")
cnf.write("\t\tAuthUserFile
/mnt/apache/passwd/passwords\n")
"data_dir")
os.listdir(data_dir):
/trac/%s>\n"
cnf.write("\t\tSetHandler
mod_python\n")
cnf.write("\t\tPythonInterpreter
main_interpreter\n")
cnf.write("\t\tPythonHandler
trac.web.modpython_frontend\n")
TracEnv
(data_dir,
TracUriRoot
/trac/%s\n"
svn_dir
"svn_dir")
os.listdir(svn_dir):
/svn/%s>\n"
cnf.write("\t\tDAV
svn\n")
cnf.write("\t\tSVNPath
(svn_dir,
cnf.write("\tErrorLog
/var/log/apache2/error.log\n")
cnf.write("\tLogLevel
warn\n")
cnf.write("\tCustomLog
/var/log/apache2/access.log
combined\n")
cnf.write("\tServerSignature
SSLCertificateFile
"SSLCertificateFile")
SSLCertificateFile:
cnf.write("\tSSLEngine
cnf.write("\tSSLCertificateFile
SSLCertificateFile)
SSLCertificateKeyFile
"SSLCertificateKeyFile")
SSLCertificateKeyFile:
cnf.write("\tSSLCertificateKeyFile
SSLCertificateKeyFile)
SSLCertificateChainFile
"SSLCertificateChainFile")
SSLCertificateChainFile:
cnf.write("\tSSLCertificateChainFile
SSLCertificateChainFile)
cnf.write("</VirtualHost>\n")
cnf.close()
self.run("a2ensite
domain_info[0])
self.run("/etc/init.d/apache2
force-reload")
self.setup_vhost()
boto.rds.dbinstance
boto.rds.optiongroup
OptionGroup,
OptionGroupOption
boto.rds.event
Event
boto.rds.regioninfo
RDSRegionInfo
region_cls=RDSRegionInfo,
connection_cls=RDSConnection
'rds.amazonaws.com'
'2013-05-15'
RDSRegionInfo(self,
get_all_dbinstances(self,
self.get_list('DescribeDBInstances',
[('DBInstance',
DBInstance)])
create_dbinstance(self,
auto_minor_version_upgrade=True,
character_set_name,
'DBName':
param_group),
engine_version,
license_model,
list(params.items()):
del(params[k])
self.get_object('CreateDBInstance',
create_dbinstance_read_replica(self,
source_id,
source_id}
self.get_object('CreateDBInstanceReadReplica',
self.get_object('PromoteReadReplica',
modify_dbinstance(self,
new_instance_id=None,
param_group:
param_group)
preferred_maintenance_window:
master_password:
master_password
allocated_storage:
multi_az:
apply_immediately:
new_instance_id:
new_instance_id
self.get_object('ModifyDBInstance',
delete_dbinstance(self,
skip_final_snapshot:
final_snapshot_id
self.get_object('DeleteDBInstance',
reboot_dbinstance(self,
self.get_object('RebootDBInstance',
get_all_dbparameter_groups(self,
self.get_list('DescribeDBParameterGroups',
[('DBParameterGroup',
get_all_dbparameters(self,
groupname,
groupname}
self.get_object('DescribeDBParameters',
pg.name
create_parameter_group(self,
self.get_object('CreateDBParameterGroup',
modify_parameter_group(self,
self.get_list('ModifyDBParameterGroup',
ParameterGroup,
reset_parameter_group(self,
reset_all_params=False,
reset_all_params:
self.get_status('ResetDBParameterGroup',
delete_parameter_group(self,
self.get_status('DeleteDBParameterGroup',
get_all_dbsecurity_groups(self,
self.get_list('DescribeDBSecurityGroups',
[('DBSecurityGroup',
create_dbsecurity_group(self,
params['DBSecurityGroupDescription']
self.get_object('CreateDBSecurityGroup',
delete_dbsecurity_group(self,
self.get_status('DeleteDBSecurityGroup',
authorize_dbsecurity_group(self,
urllib.quote(cidr_ip)
self.get_object('AuthorizeDBSecurityGroupIngress',
revoke_dbsecurity_group(self,
ec2_security_group_owner_id=None,
cidr_ip=None):
self.get_object('RevokeDBSecurityGroupIngress',
revoke_security_group
revoke_dbsecurity_group
get_all_dbsnapshots(self,
self.get_list('DescribeDBSnapshots',
[('DBSnapshot',
DBSnapshot)])
get_all_logs(self,
file_last_written=None):
file_size:
filename_contains:
file_last_written:
self.get_list('DescribeDBLogFiles',
[('DescribeDBLogFilesDetails',LogFile)])
get_log_file(self,
number_of_lines=None,
max_records=None):
number_of_lines:
self.get_object('DownloadDBLogFilePortion',
LogFileObject)
logfile:
logfile.log_filename
log_file_name
logfile.dbinstance_id
dbinstance_id
create_dbsnapshot(self,
dbinstance_id):
self.get_object('CreateDBSnapshot',
copy_dbsnapshot(self,
target_snapshot_id):
{'SourceDBSnapshotIdentifier':
target_snapshot_id}
self.get_object('CopyDBSnapshot',
delete_dbsnapshot(self,
identifier):
identifier}
self.get_object('DeleteDBSnapshot',
restore_dbinstance_from_dbsnapshot(self,
instance_class}
self.get_object('RestoreDBInstanceFromDBSnapshot',
restore_dbinstance_from_point_in_time(self,
target_instance_id,
use_latest=False,
dbinstance_class=None,
{'SourceDBInstanceIdentifier':
target_instance_id}
use_latest:
restore_time:
restore_time.isoformat()
dbinstance_class:
dbinstance_class
self.get_object('RestoreDBInstanceToPointInTime',
get_all_events(self,
source_type:
self.get_list('DescribeEvents',
[('Event',
Event)])
desc,
desc}
self.get_object('CreateDBSubnetGroup',
self.get_object('DeleteDBSubnetGroup',
get_all_db_subnet_groups(self,
self.get_list('DescribeDBSubnetGroups',
[('DBSubnetGroup',DBSubnetGroup)])
self.get_object('ModifyDBSubnetGroup',
self.get_object('CreateOptionGroup',
group.engine_name
group.major_engine_version
self.get_status('DeleteOptionGroup',
self.get_list('DescribeOptionGroups',
('OptionGroup',
self.get_list('DescribeOptionGroupOptions',
('OptionGroupOptions',
OptionGroupOption)
boto.rds.statusinfo
StatusInfo
DBInstance(object):
'DBInstance:%s'
'DBParameterGroups':
ResultSet([('DBParameterGroup',
'DBSecurityGroups':
ResultSet([('DBSecurityGroup',
'VpcSecurityGroups':
ResultSet([('VpcSecurityGroupMembership',
VPCSecurityGroupMembership)])
PendingModifiedValues()
'ReadReplicaDBInstanceIdentifiers':
ReadReplicaDBInstanceIdentifiers()
'StatusInfos':
('DBInstanceStatusInfo',
StatusInfo)
'DBSubnetGroup':
DBSubnetGroup()
'Address':
(self._address,
'LatestRestorableTime':
security_group(self):
len(self.security_groups)
self.security_groups[-1]
parameter_group(self):
len(self.parameter_groups)
self.parameter_groups[-1]
snapshot(self,
snapshot_id):
self.connection.create_dbsnapshot(snapshot_id,
self.connection.reboot_dbinstance(self.id)
self.connection.get_all_dbinstances(self.id)
self.connection.delete_dbinstance(self.id,
skip_final_snapshot,
final_snapshot_id)
modify(self,
new_instance_id=None):
self.connection.modify_dbinstance(self.id,
param_group,
apply_immediately,
vpc_security_groups,
new_instance_id)
PendingModifiedValues(dict):
ReadReplicaDBInstanceIdentifiers(list):
'ReadReplicaDBInstanceIdentifier':
DBSecurityGroup(object):
self.ec2_groups
self.ip_ranges
'DBSecurityGroup:%s'
'IPRange':
IPRange(self)
self.ip_ranges.append(cidr)
'EC2SecurityGroup':
EC2SecurityGroup(self)
self.ec2_groups.append(ec2_grp)
'OwnerId':
'IPRanges':
self.connection.delete_dbsecurity_group(self.name)
self.connection.authorize_dbsecurity_group(self.name,
group_owner_id)
ec2_security_group_name=group_name,
ec2_security_group_owner_id=group_owner_id)
cidr_ip=cidr_ip)
IPRange(object):
'IPRange:%s'
'CIDRIP':
EC2SecurityGroup(object):
'EC2SecurityGroup:%s'
DBSnapshot(object):
'DBSnapshot:%s'
'SnapshotCreateTime':
'SnapshotTime':
'PercentProgress':
'SnapshotType':
self.connection.get_all_dbsnapshots(self.id)
DBSubnetGroup(object):
'DBSubnetGroup:%s'
'SubnetIdentifier':
self.subnet_ids.append(value)
'SubnetGroupStatus':
'SourceType':
LogFile(object):
(self.log_filename)
'LastWritten':
LogFileObject(object):
"LogFileObject:
%s/%s"
(self.dbinstance_id,
self.log_filename)
'LogFileData':
'AdditionalDataPending':
self.additional_data_pending
OptionGroup(object):
allow_both_vpc_and_nonvpc=False,
allow_both_vpc_and_nonvpc
'OptionGroup:%s'
('Options',
Option)
'AllowsVpcAndNonVpcInstanceMemberships':
self.connection.delete_option_group(self.name)
Option(object):
vpc_security_groups=None):
'Option:%s'
'OptionSettings':
('OptionSettings',
OptionSetting)
'DBSecurityGroupMemberships':
('DBSecurityGroupMemberships',
'VpcSecurityGroupMemberships':
('VpcSecurityGroupMemberships',
VpcSecurityGroup)
'OptionName':
'OptionDescription':
OptionSetting(object):
data_type=None,
is_modifiable=False,
is_collection=False):
is_collection
'OptionSetting:%s'
'IsCollection':
VpcSecurityGroup(object):
'VpcSecurityGroup:%s'
OptionGroupOption(object):
min_minor_engine_version=None,
port_required=False,
default_port=None,
depends_on=None):
min_minor_engine_version
port_required
depends_on
'OptionGroupOption:%s'
'OptionGroupOptionSettings':
('OptionGroupOptionSettings',
OptionGroupOptionSetting)
'OptionsDependedOn':
'MinimumRequiredMinorEngineVersion':
'PortRequired':
'DefaultPort':
OptionGroupOptionSetting(object):
is_modifiable=False):
'OptionGroupOptionSetting:%s'
'SettingName':
'SettingDescription':
ParameterGroup(dict):
'ParameterGroup:%s'
'Parameter':
self._current_param:
self[self._current_param.name]
Parameter(self)
modifiable(self):
p.is_modifiable:
mod.append(p)
self.connection.get_all_dbparameters(self.name)
self.update(pg)
add_param(self,
apply_method):
Parameter()
param.value
param.apply_method
apply_method
self.params.append(param)
ValidTypes
{'integer'
bool}
ValidSources
['user',
'system',
'engine-default']
ValidApplyTypes
['static',
'dynamic']
ValidApplyMethods
['immediate',
'pending-reboot']
group=None,
self.group
'Parameter:%s'
'ParameterName':
'ParameterValue':
self.ValidTypes:
self.ValidSources:
self.ValidApplyTypes:
merge(self,
'Parameters.member.%d.'
d[prefix+'ParameterName']
d[prefix+'ParameterValue']
self.apply_type:
d[prefix+'ApplyMethod']
_set_string_value(self,
str')
self.allowed_values.split(',')
_set_integer_value(self,
self.allowed_values.split('-')
int(min)
int(max):
ValueError('range
integer')
_set_boolean_value(self,
boolean')
self._set_string_value(value)
self._set_integer_value(value)
self._set_boolean_value(value)
get_value(self):
self._set_integer_value(self._value)
self._set_boolean_value(self._value)
property(get_value,
set_value,
parameter')
apply(self,
immediate=False):
immediate:
'immediate'
'pending-reboot'
self.group.connection.modify_parameter_group(self.group.name,
[self])
RDSRegionInfo(RegionInfo):
super(RDSRegionInfo,
RDSConnection)
StatusInfo(object):
status_type=None,
normal=None,
status_type
normal
'StatusInfo:%s'
'StatusType':
'Normal':
VPCSecurityGroupMembership(object):
vpc_group=None):
vpc_group
'VPCSecurityGroupMembership:%s'
get_regions('rds',
connection_cls=RDSConnection)
DBParameterGroupQuotaExceeded(JSONResponseError):
DBSubnetGroupAlreadyExists(JSONResponseError):
DBSubnetGroupQuotaExceeded(JSONResponseError):
InstanceQuotaExceeded(JSONResponseError):
InvalidDBParameterGroupState(JSONResponseError):
DBSecurityGroupAlreadyExists(JSONResponseError):
InsufficientDBInstanceCapacity(JSONResponseError):
ReservedDBInstanceQuotaExceeded(JSONResponseError):
DBSecurityGroupNotFound(JSONResponseError):
DBInstanceAlreadyExists(JSONResponseError):
ReservedDBInstanceNotFound(JSONResponseError):
DBSubnetGroupDoesNotCoverEnoughAZs(JSONResponseError):
InvalidDBSecurityGroupState(JSONResponseError):
ReservedDBInstancesOfferingNotFound(JSONResponseError):
SnapshotQuotaExceeded(JSONResponseError):
OptionGroupQuotaExceeded(JSONResponseError):
DBParameterGroupNotFound(JSONResponseError):
InvalidDBSubnetGroupState(JSONResponseError):
DBSubnetGroupNotFound(JSONResponseError):
InvalidOptionGroupState(JSONResponseError):
DBSecurityGroupNotSupported(JSONResponseError):
InvalidEventSubscriptionState(JSONResponseError):
InvalidDBSubnetState(JSONResponseError):
InvalidDBSnapshotState(JSONResponseError):
DBSecurityGroupQuotaExceeded(JSONResponseError):
ProvisionedIopsNotAvailableInAZ(JSONResponseError):
OptionGroupAlreadyExists(JSONResponseError):
DBUpgradeDependencyFailure(JSONResponseError):
PointInTimeRestoreNotEnabled(JSONResponseError):
DBSubnetQuotaExceeded(JSONResponseError):
OptionGroupNotFound(JSONResponseError):
DBParameterGroupAlreadyExists(JSONResponseError):
DBInstanceNotFound(JSONResponseError):
ReservedDBInstanceAlreadyExists(JSONResponseError):
InvalidDBInstanceState(JSONResponseError):
DBSnapshotNotFound(JSONResponseError):
DBSnapshotAlreadyExists(JSONResponseError):
StorageQuotaExceeded(JSONResponseError):
"2013-09-09"
"rds.us-east-1.amazonaws.com"
"DBParameterGroupQuotaExceeded":
exceptions.DBParameterGroupQuotaExceeded,
"DBSubnetGroupAlreadyExists":
exceptions.DBSubnetGroupAlreadyExists,
"DBSubnetGroupQuotaExceeded":
exceptions.DBSubnetGroupQuotaExceeded,
"InstanceQuotaExceeded":
exceptions.InstanceQuotaExceeded,
"InvalidDBParameterGroupState":
exceptions.InvalidDBParameterGroupState,
"DBSecurityGroupAlreadyExists":
exceptions.DBSecurityGroupAlreadyExists,
"InsufficientDBInstanceCapacity":
exceptions.InsufficientDBInstanceCapacity,
"ReservedDBInstanceQuotaExceeded":
exceptions.ReservedDBInstanceQuotaExceeded,
"DBSecurityGroupNotFound":
exceptions.DBSecurityGroupNotFound,
"DBInstanceAlreadyExists":
exceptions.DBInstanceAlreadyExists,
"ReservedDBInstanceNotFound":
exceptions.ReservedDBInstanceNotFound,
"DBSubnetGroupDoesNotCoverEnoughAZs":
exceptions.DBSubnetGroupDoesNotCoverEnoughAZs,
"InvalidDBSecurityGroupState":
exceptions.InvalidDBSecurityGroupState,
"ReservedDBInstancesOfferingNotFound":
exceptions.ReservedDBInstancesOfferingNotFound,
"SnapshotQuotaExceeded":
exceptions.SnapshotQuotaExceeded,
"OptionGroupQuotaExceeded":
exceptions.OptionGroupQuotaExceeded,
"DBParameterGroupNotFound":
exceptions.DBParameterGroupNotFound,
"InvalidDBSubnetGroupState":
exceptions.InvalidDBSubnetGroupState,
"DBSubnetGroupNotFound":
exceptions.DBSubnetGroupNotFound,
"InvalidOptionGroupState":
exceptions.InvalidOptionGroupState,
"DBSecurityGroupNotSupported":
exceptions.DBSecurityGroupNotSupported,
"InvalidEventSubscriptionState":
exceptions.InvalidEventSubscriptionState,
"InvalidDBSubnetState":
exceptions.InvalidDBSubnetState,
"InvalidDBSnapshotState":
exceptions.InvalidDBSnapshotState,
"DBSecurityGroupQuotaExceeded":
exceptions.DBSecurityGroupQuotaExceeded,
"ProvisionedIopsNotAvailableInAZ":
exceptions.ProvisionedIopsNotAvailableInAZ,
"OptionGroupAlreadyExists":
exceptions.OptionGroupAlreadyExists,
"DBUpgradeDependencyFailure":
exceptions.DBUpgradeDependencyFailure,
"PointInTimeRestoreNotEnabled":
exceptions.PointInTimeRestoreNotEnabled,
"DBSubnetQuotaExceeded":
exceptions.DBSubnetQuotaExceeded,
"OptionGroupNotFound":
exceptions.OptionGroupNotFound,
"DBParameterGroupAlreadyExists":
exceptions.DBParameterGroupAlreadyExists,
"DBInstanceNotFound":
exceptions.DBInstanceNotFound,
"ReservedDBInstanceAlreadyExists":
exceptions.ReservedDBInstanceAlreadyExists,
"InvalidDBInstanceState":
exceptions.InvalidDBInstanceState,
"DBSnapshotNotFound":
exceptions.DBSnapshotNotFound,
"DBSnapshotAlreadyExists":
exceptions.DBSnapshotAlreadyExists,
"StorageQuotaExceeded":
exceptions.StorageQuotaExceeded,
add_source_identifier_to_subscription(self,
action='AddSourceIdentifierToSubscription',
add_tags_to_resource(self,
action='AddTagsToResource',
authorize_db_security_group_ingress(self,
action='AuthorizeDBSecurityGroupIngress',
copy_db_snapshot(self,
action='CopyDBSnapshot',
create_db_instance(self,
character_set_name=None,
params['CharacterSetName']
action='CreateDBInstance',
create_db_instance_read_replica(self,
action='CreateDBInstanceReadReplica',
create_db_parameter_group(self,
action='CreateDBParameterGroup',
create_db_security_group(self,
action='CreateDBSecurityGroup',
create_db_snapshot(self,
action='CreateDBSnapshot',
action='CreateDBSubnetGroup',
action='CreateOptionGroup',
delete_db_instance(self,
skip_final_snapshot=None,
final_db_snapshot_identifier=None):
skip_final_snapshot
skip_final_snapshot).lower()
action='DeleteDBInstance',
delete_db_parameter_group(self,
db_parameter_group_name):
action='DeleteDBParameterGroup',
delete_db_security_group(self,
db_security_group_name):
action='DeleteDBSecurityGroup',
delete_db_snapshot(self,
db_snapshot_identifier):
action='DeleteDBSnapshot',
db_subnet_group_name):
action='DeleteDBSubnetGroup',
option_group_name):
action='DeleteOptionGroup',
describe_db_engine_versions(self,
db_parameter_group_family=None,
default_only=None,
list_supported_character_sets=None):
params['DBParameterGroupFamily']
list_supported_character_sets
params['ListSupportedCharacterSets']
list_supported_character_sets).lower()
action='DescribeDBEngineVersions',
describe_db_instances(self,
action='DescribeDBInstances',
describe_db_log_files(self,
file_last_written=None,
action='DescribeDBLogFiles',
describe_db_parameter_groups(self,
action='DescribeDBParameterGroups',
describe_db_parameters(self,
action='DescribeDBParameters',
describe_db_security_groups(self,
db_security_group_name=None,
action='DescribeDBSecurityGroups',
describe_db_snapshots(self,
db_snapshot_identifier=None,
action='DescribeDBSnapshots',
describe_db_subnet_groups(self,
action='DescribeDBSubnetGroups',
{'EngineName':
action='DescribeOptionGroupOptions',
major_engine_version=None):
action='DescribeOptionGroups',
describe_orderable_db_instance_options(self,
vpc=None,
{'Engine':
params['Vpc']
vpc).lower()
action='DescribeOrderableDBInstanceOptions',
describe_reserved_db_instances(self,
action='DescribeReservedDBInstances',
describe_reserved_db_instances_offerings(self,
action='DescribeReservedDBInstancesOfferings',
download_db_log_file_portion(self,
number_of_lines=None):
action='DownloadDBLogFilePortion',
list_tags_for_resource(self,
resource_name):
action='ListTagsForResource',
modify_db_instance(self,
allow_major_version_upgrade=None,
new_db_instance_identifier=None):
allow_major_version_upgrade
params['AllowMajorVersionUpgrade']
allow_major_version_upgrade).lower()
action='ModifyDBInstance',
modify_db_parameter_group(self,
action='ModifyDBParameterGroup',
db_subnet_group_description=None):
action='ModifyDBSubnetGroup',
modify_option_group(self,
options_to_include=None,
apply_immediately=None):
options_to_include
options_to_include,
'OptionsToInclude.member',
('OptionName',
'Port',
'DBSecurityGroupMemberships',
'VpcSecurityGroupMemberships',
'OptionSettings'))
options_to_remove
action='ModifyOptionGroup',
action='PromoteReadReplica',
purchase_reserved_db_instances_offering(self,
db_instance_count=None,
'ReservedDBInstancesOfferingId':
params['DBInstanceCount']
action='PurchaseReservedDBInstancesOffering',
reboot_db_instance(self,
force_failover=None):
force_failover
params['ForceFailover']
force_failover).lower()
action='RebootDBInstance',
remove_source_identifier_from_subscription(self,
action='RemoveSourceIdentifierFromSubscription',
remove_tags_from_resource(self,
'TagKeys.member')
action='RemoveTagsFromResource',
reset_db_parameter_group(self,
action='ResetDBParameterGroup',
restore_db_instance_from_db_snapshot(self,
action='RestoreDBInstanceFromDBSnapshot',
restore_db_instance_to_point_in_time(self,
use_latest_restorable_time=None,
use_latest_restorable_time
use_latest_restorable_time).lower()
action='RestoreDBInstanceToPointInTime',
revoke_db_security_group_ingress(self,
action='RevokeDBSecurityGroupIngress',
get_regions('redshift',
connection_cls=RedshiftConnection)
ClusterNotFoundFault(JSONResponseError):
InvalidClusterSnapshotStateFault(JSONResponseError):
ClusterSnapshotNotFoundFault(JSONResponseError):
ClusterSecurityGroupQuotaExceededFault(JSONResponseError):
ReservedNodeOfferingNotFoundFault(JSONResponseError):
ClusterSubnetGroupQuotaExceededFault(JSONResponseError):
InvalidClusterStateFault(JSONResponseError):
InvalidClusterParameterGroupStateFault(JSONResponseError):
ClusterParameterGroupAlreadyExistsFault(JSONResponseError):
InvalidClusterSecurityGroupStateFault(JSONResponseError):
InvalidRestoreFault(JSONResponseError):
AuthorizationNotFoundFault(JSONResponseError):
ResizeNotFoundFault(JSONResponseError):
NumberOfNodesQuotaExceededFault(JSONResponseError):
ClusterSnapshotAlreadyExistsFault(JSONResponseError):
AuthorizationQuotaExceededFault(JSONResponseError):
AuthorizationAlreadyExistsFault(JSONResponseError):
ClusterSnapshotQuotaExceededFault(JSONResponseError):
ReservedNodeNotFoundFault(JSONResponseError):
ReservedNodeAlreadyExistsFault(JSONResponseError):
ClusterSecurityGroupAlreadyExistsFault(JSONResponseError):
ClusterParameterGroupNotFoundFault(JSONResponseError):
ReservedNodeQuotaExceededFault(JSONResponseError):
ClusterQuotaExceededFault(JSONResponseError):
ClusterSubnetQuotaExceededFault(JSONResponseError):
UnsupportedOptionFault(JSONResponseError):
InvalidVPCNetworkStateFault(JSONResponseError):
ClusterSecurityGroupNotFoundFault(JSONResponseError):
InvalidClusterSubnetGroupStateFault(JSONResponseError):
ClusterSubnetGroupAlreadyExistsFault(JSONResponseError):
NumberOfNodesPerClusterLimitExceededFault(JSONResponseError):
ClusterSubnetGroupNotFoundFault(JSONResponseError):
ClusterParameterGroupQuotaExceededFault(JSONResponseError):
ClusterAlreadyExistsFault(JSONResponseError):
InsufficientClusterCapacityFault(JSONResponseError):
InvalidClusterSubnetStateFault(JSONResponseError):
InvalidParameterCombinationFault(JSONResponseError):
AccessToSnapshotDeniedFault(JSONResponseError):
UnauthorizedOperationFault(JSONResponseError):
SnapshotCopyAlreadyDisabled(JSONResponseError):
ClusterNotFound(JSONResponseError):
UnknownSnapshotCopyRegion(JSONResponseError):
InvalidClusterSubnetState(JSONResponseError):
ReservedNodeQuotaExceeded(JSONResponseError):
InvalidClusterState(JSONResponseError):
HsmClientCertificateQuotaExceeded(JSONResponseError):
HsmClientCertificateNotFound(JSONResponseError):
SubscriptionEventIdNotFound(JSONResponseError):
ClusterSecurityGroupAlreadyExists(JSONResponseError):
HsmConfigurationAlreadyExists(JSONResponseError):
NumberOfNodesQuotaExceeded(JSONResponseError):
ReservedNodeOfferingNotFound(JSONResponseError):
BucketNotFound(JSONResponseError):
InsufficientClusterCapacity(JSONResponseError):
UnauthorizedOperation(JSONResponseError):
ClusterQuotaExceeded(JSONResponseError):
ClusterSnapshotNotFound(JSONResponseError):
InvalidHsmClientCertificateState(JSONResponseError):
ResizeNotFound(JSONResponseError):
ClusterSubnetGroupNotFound(JSONResponseError):
ClusterSnapshotQuotaExceeded(JSONResponseError):
AccessToSnapshotDenied(JSONResponseError):
InvalidClusterSecurityGroupState(JSONResponseError):
NumberOfNodesPerClusterLimitExceeded(JSONResponseError):
ClusterSubnetQuotaExceeded(JSONResponseError):
ClusterSecurityGroupNotFound(JSONResponseError):
InvalidElasticIp(JSONResponseError):
InvalidClusterParameterGroupState(JSONResponseError):
InvalidHsmConfigurationState(JSONResponseError):
ClusterAlreadyExists(JSONResponseError):
HsmConfigurationQuotaExceeded(JSONResponseError):
ClusterSnapshotAlreadyExists(JSONResponseError):
SubscriptionSeverityNotFound(JSONResponseError):
ReservedNodeAlreadyExists(JSONResponseError):
ClusterSubnetGroupQuotaExceeded(JSONResponseError):
ClusterParameterGroupNotFound(JSONResponseError):
InvalidS3BucketName(JSONResponseError):
InvalidS3KeyPrefix(JSONResponseError):
HsmConfigurationNotFound(JSONResponseError):
ClusterSecurityGroupQuotaExceeded(JSONResponseError):
InvalidClusterSnapshotState(JSONResponseError):
ClusterParameterGroupQuotaExceeded(JSONResponseError):
SnapshotCopyDisabled(JSONResponseError):
ClusterSubnetGroupAlreadyExists(JSONResponseError):
ReservedNodeNotFound(JSONResponseError):
HsmClientCertificateAlreadyExists(JSONResponseError):
InvalidClusterSubnetGroupState(JSONResponseError):
InsufficientS3BucketPolicy(JSONResponseError):
ClusterParameterGroupAlreadyExists(JSONResponseError):
UnsupportedOption(JSONResponseError):
CopyToRegionDisabled(JSONResponseError):
SnapshotCopyAlreadyEnabled(JSONResponseError):
IncompatibleOrderableOptions(JSONResponseError):
InvalidSubscriptionState(JSONResponseError):
RedshiftConnection(AWSQueryConnection):
"2012-12-01"
"redshift.us-east-1.amazonaws.com"
"SnapshotCopyAlreadyDisabled":
exceptions.SnapshotCopyAlreadyDisabled,
"ClusterNotFound":
exceptions.ClusterNotFound,
"UnknownSnapshotCopyRegion":
exceptions.UnknownSnapshotCopyRegion,
"InvalidClusterSubnetState":
exceptions.InvalidClusterSubnetState,
"ReservedNodeQuotaExceeded":
exceptions.ReservedNodeQuotaExceeded,
"InvalidClusterState":
exceptions.InvalidClusterState,
"HsmClientCertificateQuotaExceeded":
exceptions.HsmClientCertificateQuotaExceeded,
"HsmClientCertificateNotFound":
exceptions.HsmClientCertificateNotFound,
"SubscriptionEventIdNotFound":
exceptions.SubscriptionEventIdNotFound,
"ClusterSecurityGroupAlreadyExists":
exceptions.ClusterSecurityGroupAlreadyExists,
"HsmConfigurationAlreadyExists":
exceptions.HsmConfigurationAlreadyExists,
"NumberOfNodesQuotaExceeded":
exceptions.NumberOfNodesQuotaExceeded,
"ReservedNodeOfferingNotFound":
exceptions.ReservedNodeOfferingNotFound,
"BucketNotFound":
exceptions.BucketNotFound,
"InsufficientClusterCapacity":
exceptions.InsufficientClusterCapacity,
"UnauthorizedOperation":
exceptions.UnauthorizedOperation,
"ClusterQuotaExceeded":
exceptions.ClusterQuotaExceeded,
"ClusterSnapshotNotFound":
exceptions.ClusterSnapshotNotFound,
"InvalidHsmClientCertificateState":
exceptions.InvalidHsmClientCertificateState,
"ResizeNotFound":
exceptions.ResizeNotFound,
"ClusterSubnetGroupNotFound":
exceptions.ClusterSubnetGroupNotFound,
"ClusterSnapshotQuotaExceeded":
exceptions.ClusterSnapshotQuotaExceeded,
"AccessToSnapshotDenied":
exceptions.AccessToSnapshotDenied,
"InvalidClusterSecurityGroupState":
exceptions.InvalidClusterSecurityGroupState,
"NumberOfNodesPerClusterLimitExceeded":
exceptions.NumberOfNodesPerClusterLimitExceeded,
"ClusterSubnetQuotaExceeded":
exceptions.ClusterSubnetQuotaExceeded,
"ClusterSecurityGroupNotFound":
exceptions.ClusterSecurityGroupNotFound,
"InvalidElasticIp":
exceptions.InvalidElasticIp,
"InvalidClusterParameterGroupState":
exceptions.InvalidClusterParameterGroupState,
"InvalidHsmConfigurationState":
exceptions.InvalidHsmConfigurationState,
"ClusterAlreadyExists":
exceptions.ClusterAlreadyExists,
"HsmConfigurationQuotaExceeded":
exceptions.HsmConfigurationQuotaExceeded,
"ClusterSnapshotAlreadyExists":
exceptions.ClusterSnapshotAlreadyExists,
"SubscriptionSeverityNotFound":
exceptions.SubscriptionSeverityNotFound,
"ReservedNodeAlreadyExists":
exceptions.ReservedNodeAlreadyExists,
"ClusterSubnetGroupQuotaExceeded":
exceptions.ClusterSubnetGroupQuotaExceeded,
"ClusterParameterGroupNotFound":
exceptions.ClusterParameterGroupNotFound,
"InvalidS3BucketName":
exceptions.InvalidS3BucketName,
"InvalidS3KeyPrefix":
exceptions.InvalidS3KeyPrefix,
"HsmConfigurationNotFound":
exceptions.HsmConfigurationNotFound,
"InvalidSubscriptionState":
exceptions.InvalidSubscriptionState,
"ClusterSecurityGroupQuotaExceeded":
exceptions.ClusterSecurityGroupQuotaExceeded,
"InvalidClusterSnapshotState":
exceptions.InvalidClusterSnapshotState,
"ClusterParameterGroupQuotaExceeded":
exceptions.ClusterParameterGroupQuotaExceeded,
"SnapshotCopyDisabled":
exceptions.SnapshotCopyDisabled,
"ClusterSubnetGroupAlreadyExists":
exceptions.ClusterSubnetGroupAlreadyExists,
"ReservedNodeNotFound":
exceptions.ReservedNodeNotFound,
"HsmClientCertificateAlreadyExists":
exceptions.HsmClientCertificateAlreadyExists,
"InvalidClusterSubnetGroupState":
exceptions.InvalidClusterSubnetGroupState,
"InsufficientS3BucketPolicy":
exceptions.InsufficientS3BucketPolicy,
"ClusterParameterGroupAlreadyExists":
exceptions.ClusterParameterGroupAlreadyExists,
"UnsupportedOption":
exceptions.UnsupportedOption,
"CopyToRegionDisabled":
exceptions.CopyToRegionDisabled,
"SnapshotCopyAlreadyEnabled":
exceptions.SnapshotCopyAlreadyEnabled,
"IncompatibleOrderableOptions":
exceptions.IncompatibleOrderableOptions,
super(RedshiftConnection,
authorize_cluster_security_group_ingress(self,
action='AuthorizeClusterSecurityGroupIngress',
authorize_snapshot_access(self,
action='AuthorizeSnapshotAccess',
copy_cluster_snapshot(self,
source_snapshot_cluster_identifier=None):
'SourceSnapshotIdentifier':
'TargetSnapshotIdentifier':
params['SourceSnapshotClusterIdentifier']
action='CopyClusterSnapshot',
elastic_ip=None):
'NodeType':
encrypted).lower()
create_cluster_parameter_group(self,
'ParameterGroupName':
'ParameterGroupFamily':
action='CreateClusterParameterGroup',
create_cluster_security_group(self,
action='CreateClusterSecurityGroup',
create_cluster_snapshot(self,
action='CreateClusterSnapshot',
create_cluster_subnet_group(self,
action='CreateClusterSubnetGroup',
create_hsm_client_certificate(self,
action='CreateHsmClientCertificate',
create_hsm_configuration(self,
hsm_server_public_certificate):
'HsmIpAddress':
'HsmPartitionName':
'HsmPartitionPassword':
'HsmServerPublicCertificate':
hsm_server_public_certificate,
action='CreateHsmConfiguration',
skip_final_cluster_snapshot=None,
final_cluster_snapshot_identifier=None):
skip_final_cluster_snapshot
params['SkipFinalClusterSnapshot']
skip_final_cluster_snapshot).lower()
params['FinalClusterSnapshotIdentifier']
delete_cluster_parameter_group(self,
parameter_group_name):
action='DeleteClusterParameterGroup',
delete_cluster_security_group(self,
cluster_security_group_name):
action='DeleteClusterSecurityGroup',
delete_cluster_snapshot(self,
{'SnapshotIdentifier':
action='DeleteClusterSnapshot',
delete_cluster_subnet_group(self,
cluster_subnet_group_name):
action='DeleteClusterSubnetGroup',
delete_hsm_client_certificate(self,
action='DeleteHsmClientCertificate',
delete_hsm_configuration(self,
hsm_configuration_identifier):
action='DeleteHsmConfiguration',
describe_cluster_parameter_groups(self,
parameter_group_name=None,
params['ParameterGroupName']
action='DescribeClusterParameterGroups',
describe_cluster_parameters(self,
action='DescribeClusterParameters',
describe_cluster_security_groups(self,
cluster_security_group_name=None,
params['ClusterSecurityGroupName']
action='DescribeClusterSecurityGroups',
describe_cluster_snapshots(self,
snapshot_identifier=None,
owner_account=None):
params['SnapshotIdentifier']
action='DescribeClusterSnapshots',
describe_cluster_subnet_groups(self,
action='DescribeClusterSubnetGroups',
describe_cluster_versions(self,
cluster_parameter_group_family=None,
params['ClusterParameterGroupFamily']
action='DescribeClusterVersions',
describe_default_cluster_parameters(self,
{'ParameterGroupFamily':
action='DescribeDefaultClusterParameters',
describe_hsm_client_certificates(self,
action='DescribeHsmClientCertificates',
describe_hsm_configurations(self,
action='DescribeHsmConfigurations',
describe_logging_status(self,
action='DescribeLoggingStatus',
describe_orderable_cluster_options(self,
action='DescribeOrderableClusterOptions',
describe_reserved_node_offerings(self,
reserved_node_offering_id=None,
params['ReservedNodeOfferingId']
action='DescribeReservedNodeOfferings',
describe_reserved_nodes(self,
reserved_node_id=None,
params['ReservedNodeId']
action='DescribeReservedNodes',
describe_resize(self,
action='DescribeResize',
action='DisableLogging',
disable_snapshot_copy(self,
action='DisableSnapshotCopy',
s3_key_prefix=None):
action='EnableLogging',
enable_snapshot_copy(self,
retention_period=None):
'DestinationRegion':
params['RetentionPeriod']
action='EnableSnapshotCopy',
modify_cluster(self,
new_cluster_identifier=None):
params['NewClusterIdentifier']
action='ModifyCluster',
modify_cluster_parameter_group(self,
action='ModifyClusterParameterGroup',
modify_cluster_subnet_group(self,
action='ModifyClusterSubnetGroup',
modify_snapshot_copy_retention_period(self,
retention_period):
'RetentionPeriod':
retention_period,
action='ModifySnapshotCopyRetentionPeriod',
purchase_reserved_node_offering(self,
node_count=None):
'ReservedNodeOfferingId':
params['NodeCount']
action='PurchaseReservedNodeOffering',
reboot_cluster(self,
action='RebootCluster',
reset_cluster_parameter_group(self,
action='ResetClusterParameterGroup',
restore_from_cluster_snapshot(self,
snapshot_cluster_identifier=None,
owner_account=None,
elastic_ip=None,
automated_snapshot_retention_period=None):
action='RestoreFromClusterSnapshot',
revoke_cluster_security_group_ingress(self,
action='RevokeClusterSecurityGroupIngress',
revoke_snapshot_access(self,
action='RevokeSnapshotAccess',
rotate_encryption_key(self,
action='RotateEncryptionKey',
boto.roboto.awsqueryservice
bdb
epdb
boto_except_hook(debugger_flag,
debug_flag):
excepthook(typ,
typ
bdb.BdbQuit:
sys.__excepthook__
debugger_flag
sys.stdout.isatty()
sys.stdin.isatty():
debugger.__name__
'epdb':
debugger.post_mortem(tb,
typ,
debugger.post_mortem(tb)
debug_flag:
print(traceback.print_tb(tb))
print(value)
excepthook
Line(object):
self.fmt
append(self,
datum):
datum
print_it(self):
self.printed:
print(self.line)
RequiredParamError(boto.exception.BotoClientError):
required):
'Required
missing:
super(RequiredParamError,
EncoderError(boto.exception.BotoClientError):
error_msg):
error_msg
super(EncoderError,
FilterError(boto.exception.BotoClientError):
super(FilterError,
Encoder(object):
encode(cls,
p.name.startswith('_'):
'encode_'+p.ptype)
mthd(p,
EncoderError('Unknown
p.ptype)
encode_string(cls,
encode_file
encode_enum
encode_integer(cls,
encode_boolean(cls,
encode_datetime(cls,
encode_array(cls,
boto.utils.mklist(v)
'.%d'
enumerate(v):
rp[label%(i+1)]
AWSQueryRequest(object):
ServiceClass
Params
Args
Filters
CLITypeMap
{'string'
'integer'
'int'
'enum'
'choice',
'datetime'
'dateTime'
name(cls):
self.cli_options
self.cli_output_format
self.list_markers
self.item_markers
self.request_params
self.name()
get_connection(self,
self.ServiceClass(**args)
reason(self):
self.http_response.reason
request_id(self):
getattr(self.aws_response,
'requestId')
process_filters(self):
self.args.get('filters',
filter_names
[f['name']
self.Filters]
unknown_filters
filter_names]
unknown_filters:
FilterError('Unknown
unknown_filters)
enumerate(self.Filters):
filter['name']
self.request_params['Filter.%d.Name'
(i+1)]
enumerate(boto.utils.mklist(filters[name])):
Encoder.encode(filter,
'Filter.%d.Value.%d'
j+1))
process_args(self,
self.args.update(args)
copy.copy(self.args)
'debug'
boto.set_stream_logger(self.name())
[p.name
self.Params+self.Args
p.optional]
boto.utils.pythonize_name(param.name,
self.args[python_name]
param.default
required.remove(param.name)
param.request_param:
param.encoder:
param.encoder(param,
Encoder.encode(param,
self.connection_args[python_name]
p.short_name
p.long_name:
l.append('(%s,
(p.optparse_short_name,
p.optparse_long_name))
p.short_name:
p.optparse_short_name)
p.optparse_long_name)
RequiredParamError(','.join(l))
boto.log.debug('request_params:
self.request_params)
self.process_markers(self.Response)
process_markers(self,
prev_name=None):
self.process_markers(prop,
fmt['name'])
self.list_markers.append(prev_name)
self.item_markers.append(fmt['name'])
self.process_args(**args)
self.process_filters()
self.get_connection(**self.connection_args)
conn.make_request(self.name(),
verb=verb)
self.http_response.read()
boto.log.debug(self.body)
boto.jsonresponse.Element(list_marker=self.list_markers,
item_marker=self.item_markers)
boto.jsonresponse.XmlHandler(self.aws_response,
h.parse(self.body)
(self.http_response.status,
self.http_response.reason))
conn.ResponseError(self.http_response.status,
self.http_response.reason,
add_standard_options(self):
optparse.OptionGroup(self.parser,
'Standard
Options')
group.add_option('-D',
help='Turn
output')
group.add_option('--debugger',
interactive
group.add_option('-U',
'--url',
group.add_option('--region',
to')
group.add_option('-I',
'--access-key-id',
group.add_option('-S',
'--secret-key',
group.add_option('--version',
string')
self.group.add_option('--help-filters',
filters')
self.group.add_option('--filter',
action='append',
metavar='
name=value',
help='A
limiting
results')
self.parser.add_option_group(group)
process_standard_options(self,
hasattr(options,
'help_filters')
options.help_filters:
filters:')
print('%s\t%s'
(filter.name,
filter.doc))
options.debug:
options.url:
options.url
options.region:
self.args['region']
options.region
options.access_key_id:
options.access_key_id
options.secret_key:
options.secret_key
options.version:
print('version
x.xx')
exit(0)
boto_except_hook(options.debugger,
options.debug)
'usage:
a.long_name
self.Args
'.join(l)
a.doc:
'\n\n\t%s
(a.long_name,
a.doc)
build_cli_parser(self):
optparse.OptionParser(description=self.Description,
usage=self.get_usage())
self.add_standard_options()
self.CLITypeMap:
self.CLITypeMap[param.ptype]
'store'
'store_true'
len(param.items)
param.items[0]['type']
param.cardinality
'store_true':
param.short_name:
self.parser.add_option(param.optparse_short_name,
param.optparse_long_name,
self.parser.add_option(param.optparse_long_name,
do_cli(self):
self.build_cli_parser()
self.cli_options,
self.process_standard_options(self.cli_options,
self.cli_args,
boto.utils.pythonize_name(param.name)
getattr(self.cli_options,
p_name)
sys.stdin.read()
os.path.expanduser(value)
self.parser.error('Unable
arg.long_name:
arg.long_name.replace('-',
boto.utils.pythonize_name(arg.name)
arg.cardinality
len(self.cli_args)
self.cli_args[0]
self.args.update(d)
hasattr(self.cli_options,
'filter')
filter.split('=')
'filters'
self.args['filters'].update(d)
self.args['filters']
self.main()
self.cli_formatter(response)
RequiredParamError
self.ServiceClass.ResponseError
print('Error(%s):
(err.error_code,
err.error_message))
boto.roboto.awsqueryservice.NoCredentialsError
credentials.')
_generic_cli_formatter(self,
label=''):
fmt:
data[fmt['name']]
self.list_markers:
label[-1]
's':
label[0:-1]
label.upper()
self._generic_cli_formatter(prop,
Line(fmt,
line.append(item[field_name])
line.append(item)
line.print_it()
cli_formatter(self,
self._generic_cli_formatter(self.Response,
boto.roboto
awsqueryrequest
NoCredentialsError(boto.exception.BotoClientError):
super(NoCredentialsError,
AWSQueryService(boto.connection.AWSQueryConnection):
Name
Authentication
'sign-v2'
Path
Port
EnvURL
'AWS_URL'
Regions
self.check_for_credential_file()
self.check_for_env_url()
self.args.get('region_name',
self.Regions[0]['name'])
region['name']
region['endpoint']
self.Path
self.Port
super(AWSQueryService,
self).__init__(**self.args)
boto.exception.NoAuthHandlerFound:
NoCredentialsError()
check_for_credential_file(self):
'AWS_CREDENTIAL_FILE'
os.environ['AWS_CREDENTIAL_FILE']
fp.readlines()
line.split('=',
'AWSAccessKeyId':
'aws_access_key_id'
'AWSSecretKey':
'aws_secret_access_key'
unable
AWS_CREDENTIAL_FILE')
check_for_env_url(self):
self.args.get('url',
self.EnvURL
os.environ[self.EnvURL]
rslt
urlparse.urlparse(url)
rslt.scheme
rslt.netloc
host.split(':')
l[0]
int(l[1])
[self.Authentication]
Converter(object):
convert_string(cls,
convert_integer(cls,
convert_boolean(cls,
convert_file(cls,
os.path.exists(value)
convert_dir(cls,
convert(cls,
'convert_'+param.ptype):
'convert_'+param.ptype)
cls.convert_string
mthd(param,
ValidationException(param,
Param(Converter):
ptype='string',
optional=True,
short_name=None,
long_name=None,
doc='',
metavar=None,
cardinality=1,
encoder=None,
request_param=True):
self.optional
short_name
long_name
self.metavar
metavar
self.cardinality
cardinality
self.request_param
request_param
optparse_long_name(self):
synopsis_long_name(self):
getopt_long_name(self):
optparse_short_name(self):
synopsis_short_name(self):
getopt_short_name(self):
convert(self,
super(Param,
self).convert(self,value)
Route53RegionInfo(RegionInfo):
region_cls=Route53RegionInfo,
Route53RegionInfo(
endpoint='route53.amazonaws.com',
=HZPXML
Route53Connection(AWSAuthConnection):
'route53.amazonaws.com'
'2013-04-01'
XMLNameSpace
'https://route53.amazonaws.com/doc/2013-04-01/'
['route53']
six.iteritems(params):
urllib.parse.quote(str(val)))
get_all_hosted_zones(self,
start_marker=None,
zone_list=None):
start_marker:
{'marker':
start_marker}
boto.jsonresponse.Element(list_marker='HostedZones',
item_marker=('HostedZone',))
zone_list:
e['ListHostedZonesResponse']['HostedZones'].extend(zone_list)
'NextMarker'
e['ListHostedZonesResponse']:
e['ListHostedZonesResponse']['NextMarker']
zone_list
e['ListHostedZonesResponse']['HostedZones']
self.get_all_hosted_zones(next_marker,
zone_list)
get_hosted_zone(self,
get_hosted_zone_by_name(self,
hosted_zone_name):
hosted_zone_name[-1]
hosted_zone_name
all_hosted_zones
all_hosted_zones['ListHostedZonesResponse']['HostedZones']:
zone['Name']
hosted_zone_name:
self.get_hosted_zone(zone['Id'].split('/')[-1])
create_hosted_zone(self,
caller_ref=None,
private_zone:
'vpc_id':
'vpc_region':
vpc_region,
HZPXML
delete_hosted_zone(self,
POSTHCXMLBody
create_health_check(self,
health_check,
caller_ref=None):
{'xmlns':
self.XMLNameSpace,
'health_check':
health_check.to_xml()
self.POSTHCXMLBody
get_list_health_checks(self,
maxitems=None,
params['maxitems']
boto.jsonresponse.Element(list_marker='HealthChecks',
item_marker=('HealthCheck',))
get_checker_ip_ranges(self):
'/%s/checkeripranges'
boto.jsonresponse.Element(list_marker='CheckerIpRanges',
item_marker=('member',))
delete_health_check(self,
health_check_id):
'/%s/healthcheck/%s'
health_check_id)
get_all_rrsets(self,
maxitems=None):
'identifier':
'maxitems':
maxitems}
ResourceRecordSets(connection=self,
hosted_zone_id=hosted_zone_id)
change_rrsets(self,
xml_body):
get_change(self,
change_id):
'/%s/change/%s'
change_id)
create_zone(self,
self.create_hosted_zone(name,
private_zone=private_zone,
vpc_id=vpc_id,
vpc_region=vpc_region)
Zone(self,
zone['CreateHostedZoneResponse']['HostedZone'])
self._make_qualified(name)
self.get_zones():
zone.name:
get_zones(self):
[Zone(self,
zone)
zones['ListHostedZonesResponse']['HostedZones']]
_make_qualified(self,
record[-1]
new_list.append("%s."
new_list.append(record)
value[-1]
"%s."
exception.DNSServerError(
err.error_code:
'PriorRequestNotComplete',
'Throttling',
'ServiceUnavailable',
'RequestExpired'):
DNSServerError(BotoServerError):
POSTXMLBody
XMLIpAddrPart
XMLFQDNPart
XMLStringMatchPart
XMLRequestIntervalPart
valid_request_intervals
ip_addr,
hc_type,
resource_path,
fqdn=None,
string_match=None,
request_interval=30,
failure_threshold=3):
self.ip_addr
self.hc_type
hc_type
self.resource_path
resource_path
fqdn
string_match
self.failure_threshold
self.valid_request_intervals:
self.request_interval
"Valid
are:
",".join(str(i)
self.valid_request_intervals))
'Valid
10.')
'ip_addr_part':
self.hc_type,
self.resource_path,
'fqdn_part':
'string_match_part':
'request_interval':
(self.XMLRequestIntervalPart
{'request_interval':
self.request_interval}),
'failure_threshold':
self.failure_threshold,
params['fqdn_part']
self.XMLFQDNPart
{'fqdn':
self.fqdn}
self.ip_addr:
params['ip_addr_part']
self.XMLIpAddrPart
{'ip_addr':
self.ip_addr}
params['string_match_part']
self.XMLStringMatchPart
{'string_match':
self.string_match}
self.POSTXMLBody
HostedZone(object):
version=None,
owner
RECORD_TYPES
'PTR',
'SRV',
'SPF']
ResourceRecordSets(ResultSet):
ChangeResourceRecordSetsBody
ChangeXML
hosted_zone_id=None,
self.hosted_zone_id
hosted_zone_id
self.changes
self).__init__([('ResourceRecordSet',
Record)])
','.join([c.__repr__()
self.changes])
','.join([record.__repr__()
self])
'<ResourceRecordSets:%s
(self.hosted_zone_id,
record_list)
add_change(self,
Record(name,
alias_hosted_zone_id=alias_hosted_zone_id,
alias_dns_name=alias_dns_name,
alias_evaluate_target_health=alias_evaluate_target_health,
health_check=health_check,
failover=failover)
add_change_record(self,
change):
change[0],
"record":
change[1].to_xml()}
self.ChangeXML
{"comment":
self.comment,
"changes":
changesXML}
self.ChangeResourceRecordSetsBody
boto.connect_route53()
self.connection.change_rrsets(self.hosted_zone_id,
self.to_xml())
'NextRecordName':
'NextRecordType':
'NextRecordIdentifier':
self).__iter__()
self.is_truncated:
self.connection.get_all_rrsets(self.hosted_zone_id,
name=self.next_record_name,
type=self.next_record_type,
identifier=self.next_record_identifier)
Record(object):
HealthCheckBody
XMLBody
WRRBody
RRRBody
FailoverBody
ResourceRecordsBody
ResourceRecordBody
AliasBody
EvaluateTargetHealth
resource_records=None,
self.resource_records
failover
'<Record:%s:%s:%s>'
self.to_print())
set_alias(self,
alias_hosted_zone_id,
alias_dns_name,
alias_evaluate_target_health=False):
self.EvaluateTargetHealth
('true'
self.AliasBody
{"hosted_zone_id":
self.alias_hosted_zone_id,
"dns_name":
self.alias_dns_name,
"eval_target_health":
eval_target_health}
self.resource_records:
self.ResourceRecordBody
self.ResourceRecordsBody
"ttl":
self.ttl,
"records":
self.WRRBody
self.weight}
self.RRRBody
self.region}
self.FailoverBody
"failover":
self.failover}
self.HealthCheckBody
(self.health_check)
"name":
"type":
weight,
"body":
"health_check":
self.XMLBody
to_print(self):
'ALIAS
(EvalTarget
",".join(self.resource_records)
(WRR
w=%s)'
self.weight)
(LBR
region=%s)'
(FAILOVER
failover=%s)'
self.failover)
'TTL':
'HostedZoneId':
'SetIdentifier':
'EvaluateTargetHealth':
'Weight':
'Failover':
'HealthCheckId':
change_dict):
change_dict:
change_dict[key].replace('/change/',
change_dict[key])
self.route53connection.get_change(self.id)['GetChangeResponse']['ChangeInfo']['Status']
'<Status:%s>'
boto.route53.status
Zone(object):
zone_dict):
zone_dict:
zone_dict['Id'].replace('/hostedzone/',
zone_dict[key])
'<Zone:%s>'
_commit(self,
changes):
changes.commit()
response['ChangeResourceRecordSetsResponse']['ChangeInfo']
_new_record(self,
changes,
changes.add_change("CREATE",
change.add_value(record)
change.add_value(value)
add_record(self,
ttl=60,
update_record(self,
old_record,
new_ttl=None,
new_identifier=None,
copy.copy(old_record)
record.type,
record.name,
new_ttl,
new_identifier,
delete_record(self,
type(record)
add_cname(self,
self.add_record(resource_type='CNAME',
add_a(self,
self.add_record(resource_type='A',
add_mx(self,
self.route53connection._make_qualified(records)
self.add_record(resource_type='MX',
value=records,
find_records(self,
desired=1,
all=False,
identifier=None):
self.route53connection.get_all_rrsets(self.id,
type=type)
returned:
r.name
r.type
results.append(r)
(r.weight
(r.region
((not
all)
(len(results)
desired)):
"Search:
"\nFound:
".join(["%s
(r.name,
r.type,
r.to_print())
results])
TooManyRecordsException(message)
get_cname(self,
get_a(self,
get_mx(self,
update_cname(self,
self.get_cname(name)
update_a(self,
self.get_a(name)
update_mx(self,
self.get_mx(name)
delete_cname(self,
delete_a(self,
delete_mx(self,
get_records(self):
self.route53connection.get_all_rrsets(self.id)
self.route53connection.delete_hosted_zone(self.id)
get_nameservers(self):
self.find_records(self.name,
'NS')
ns.resource_records
get_regions('route53domains',
connection_cls=Route53DomainsConnection)
DuplicateRequest(BotoServerError):
DomainLimitExceeded(BotoServerError):
InvalidInput(BotoServerError):
OperationLimitExceeded(BotoServerError):
UnsupportedTLD(BotoServerError):
TLDRulesViolation(BotoServerError):
boto.route53.domains
Route53DomainsConnection(AWSQueryConnection):
"2014-05-15"
"route53domains.us-east-1.amazonaws.com"
"Route53Domains"
"Route53Domains_v20140515"
"DuplicateRequest":
exceptions.DuplicateRequest,
"DomainLimitExceeded":
exceptions.DomainLimitExceeded,
"InvalidInput":
exceptions.InvalidInput,
"OperationLimitExceeded":
exceptions.OperationLimitExceeded,
"UnsupportedTLD":
exceptions.UnsupportedTLD,
"TLDRulesViolation":
exceptions.TLDRulesViolation,
super(Route53DomainsConnection,
check_domain_availability(self,
idn_lang_code=None):
self.make_request(action='CheckDomainAvailability',
disable_domain_transfer_lock(self,
self.make_request(action='DisableDomainTransferLock',
enable_domain_transfer_lock(self,
self.make_request(action='EnableDomainTransferLock',
get_domain_detail(self,
self.make_request(action='GetDomainDetail',
get_operation_detail(self,
operation_id):
{'OperationId':
operation_id,
self.make_request(action='GetOperationDetail',
self.make_request(action='ListDomains',
list_operations(self,
self.make_request(action='ListOperations',
self.make_request(action='RegisterDomain',
retrieve_domain_auth_code(self,
self.make_request(action='RetrieveDomainAuthCode',
transfer_domain(self,
auth_code=None,
params['AuthCode']
self.make_request(action='TransferDomain',
update_domain_contact(self,
admin_contact=None,
registrant_contact=None,
tech_contact=None):
params['AdminContact']
params['RegistrantContact']
params['TechContact']
self.make_request(action='UpdateDomainContact',
update_domain_contact_privacy(self,
admin_privacy=None,
registrant_privacy=None,
tech_privacy=None):
params['AdminPrivacy']
params['RegistrantPrivacy']
params['TechPrivacy']
self.make_request(action='UpdateDomainContactPrivacy',
update_domain_nameservers(self,
nameservers):
self.make_request(action='UpdateDomainNameservers',
S3RegionInfo(RegionInfo):
region_cls=S3RegionInfo,
connection_cls=S3Connection
kw_params.keys():
['',
None]:
'bucket-owner-full-control',
'log-delivery-write']
Policy(object):
self.acl.grants:
g.id
self.owner.id:
(owner)
(g.display_name,
"<Policy:
".join(grants)
'AccessControlPolicy':
attrs.get('xmlns',
'<AccessControlPolicy
xmlns="{0}">'.format(self.namespace)
'<AccessControlPolicy>'
self.acl.to_xml()
'</AccessControlPolicy>'
type='AmazonCustomerByEmail',
email_address=email_address)
type='CanonicalUser',
id=user_id,
self.grants.append(Grant(self))
'<AccessControlList>'
'</AccessControlList>'
Grant(object):
NameSpace
'xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"'
permission=None,
display_name=None,
uri=None,
email_address=None):
attrs['xsi:type']
'URI':
'Permission':
'<Grant>'
'<Grantee
(self.NameSpace,
'<URI>%s</URI>'
'<EmailAddress>%s</EmailAddress>'
'</Grantee>'
'<Permission>%s</Permission>'
'</Grant>'
CannedACLStrings,
CompleteMultiPartUpload
MultiDeleteResult
MultiPartUploadListResultSet
Tags
website
S3WebsiteEndpointTranslate(object):
trans_region
defaultdict(lambda:
's3-website-us-east-1')
trans_region['eu-west-1']
's3-website-eu-west-1'
trans_region['eu-central-1']
's3-website.eu-central-1'
trans_region['us-west-1']
's3-website-us-west-1'
trans_region['us-west-2']
's3-website-us-west-2'
trans_region['sa-east-1']
's3-website-sa-east-1'
trans_region['ap-northeast-1']
's3-website-ap-northeast-1'
trans_region['ap-southeast-1']
's3-website-ap-southeast-1'
trans_region['ap-southeast-2']
's3-website-ap-southeast-2'
trans_region['cn-north-1']
's3-website.cn-north-1'
translate_region(self,
reg):
self.trans_region[reg]
S3Permissions
'READ_ACP',
'WRITE_ACP',
LoggingGroup
'http://acs.amazonaws.com/groups/s3/LogDelivery'
BucketPaymentBody
VersionRE
'<Status>([A-Za-z]+)</Status>'
MFADeleteRE
'<MfaDelete>([A-Za-z]+)</MfaDelete>'
key_class=Key):
key_name):
(self.get_key(key_name)
set_key_class(self,
key_class):
self.get_key(key_name,
validate
providing
'validate=False',
"are
allowed."
self.new_key(key_name)
urllib.parse.quote(rv)))
query_args_l)
_get_key_internal(self,
query_args_l):
self.connection.make_request('HEAD',
k.metadata
boto.utils.get_aws_metadata(response.msg,
k.__dict__[field.lower().replace('-',
response.getheader(field)
clen
response.getheader('content-length')
clen:
int(response.getheader('content-length'))
k.handle_restore_headers(response)
BucketListResultSet(self,
version_id_marker,
MultiPartUploadListResultSet(self,
upload_id_marker,
_get_all_query_args(self,
initial_query_string=''):
initial_query_string:
pairs.append(initial_query_string)
sorted(params.items(),
x[0]):
key.replace('_',
'max-keys'
(six.binary_type,)):
pairs.append(u'%s=%s'
urllib.parse.quote(key),
_get_all(self,
element_map,
initial_query_string='',
self._get_all_query_args(
initial_query_string=initial_query_string
ResultSet(element_map)
validate_kwarg_names(self,
kwargs,
TypeError('Invalid
"%s"!'
kwarg)
self._get_all([('Contents',
get_all_versions(self,
self.validate_get_all_versions_params(params)
self._get_all([('Version',
DeleteMarker)],
self.validate_kwarg_names(
'version_id_marker',
get_all_multipart_uploads(self,
['max_uploads',
'upload_id_marker',
'encoding_type',
'prefix'])
self._get_all([('Upload',
self.key_class(self,
expires_in_absolute=False):
self.connection.generate_url(expires_in,
expires_in_absolute=expires_in_absolute)
delete_keys(self,
quiet=False,
ikeys
iter(keys)
MultiDeleteResult(self)
'delete'
delete_keys2(hdrs):
u"<Delete>"
quiet:
u"<Quiet>true</Quiet>"
1000:
next(ikeys)
len(key)
(isinstance(key,
Key)
key.name:
Prefix):
'PrefixSkipped'
Don't
repr(key)
'InvalidArgument'
'Invalid.
taken
object.'
Error(key_name,
code=code,
message=message)
result.errors.append(error)
u"<Object><Key>%s</Key>"
xml.sax.saxutils.escape(key_name)
u"<VersionId>%s</VersionId>"
u"</Object>"
u"</Delete>"
hdrs['Content-MD5']
hdrs['Content-Type']
hdrs[provider.mfa_header]
headers=hdrs,
handler.XmlHandler(result,
more?
delete_keys2(headers):
query_args_l=None)
_delete_key_internal(self,
query_args_l=None):
src_key_name
boto.utils.get_utf8_value(src_key_name)
src_bucket_name:
src_bucket.get_xml_acl(src_key_name)
(src_bucket_name,
urllib.parse.quote(src_key_name))
src_version_id:
'?versionId=%s'
src_version_id
headers[provider.copy_source_header]
str(src)
'REPLACE'
Can't
multi-part
copy.
'COPY'
self.new_key(new_key_name)
handler.XmlHandler(key,
'Error'):
provider.storage_copy_error(key.Code,
key.Message,
key.handle_version_headers(response)
key.handle_addl_headers(response.getheaders())
self.set_xml_acl(acl,
new_key_name)
{self.connection.provider.acl_header:
acl_str}
query_args='acl'):
isinstance(acl_str,
acl_str.encode('utf-8')
data=acl_str,
Policy(self)
handler.XmlHandler(policy,
TypeError('set_subresource
data=value,
TypeError('get_subresource
policy.acl.grants
get_location(self):
query_args='location')
rs.LocationConstraint
set_xml_logging(self,
logging_str,
target_prefix='',
grants=None,
BucketLogging(target=target_bucket,
prefix=target_prefix,
grants=grants)
get_logging_status(self,
handler.XmlHandler(blogging,
set_as_logging_target(self,
Grant(permission='WRITE',
Grant(permission='READ_ACP',
policy.acl.add_grant(g1)
policy.acl.add_grant(g2)
get_request_payment(self,
set_request_payment(self,
payer='BucketOwner',
self.BucketPaymentBody
payer
versioning,
versioning:
mfa_delete:
'Disabled'
(ver,
mfa)
re.search(self.VersionRE,
ver:
ver.group(1)
re.search(self.MFADeleteRE,
mfa:
mfa.group(1)
StringIO(xml)
handler.XmlHandler(lifecycle,
delete_lifecycle_configuration(self,
routing_rules=None,
website.WebsiteConfiguration(
redirect_all_requests_to,
routing_rules)
self.set_website_configuration(config,
set_website_configuration(self,
self.set_website_configuration_xml(config.to_xml(),
set_website_configuration_xml(self,
data=xml,
get_website_configuration_obj(self,
config_xml
website.WebsiteConfiguration()
handler.XmlHandler(config,
xml.sax.parseString(config_xml,
get_website_configuration_xml(self,
get_website_endpoint(self):
[self.name]
l.append(S3WebsiteEndpointTranslate.translate_region(self.get_location()))
l.append('.'.join(self.connection.host.split('.')[-2:]))
data=policy,
data='/?policy',
set_cors_xml(self,
cors_xml,
StringIO(cors_xml)
cors_config,
self.set_cors_xml(cors_config.to_xml())
get_cors_xml(self,
self.get_cors_xml(headers)
delete_cors(self,
'uploads'
storage_class_header
storage_class_header:
headers[storage_class_header]
self.connection.provider)
MultiPartUpload(self)
xml_body,
data=xml_body)
body.find('<Error>')
contains_error:
CompleteMultiPartUpload(self)
resp.version_id
resp.encrypted
k.encrypted
cancel_multipart_upload(self,
self.connection.delete_bucket(self.name,
get_tags(self):
self.get_xml_tags()
handler.XmlHandler(tags,
response.encode('utf-8')
xml.sax.parseString(response,
get_xml_tags(self):
headers=None)
set_xml_tags(self,
tag_str,
query_args='tagging'):
boto.utils.compute_md5(StringIO(tag_str))
isinstance(tag_str,
tag_str
tag_str.encode('utf-8')
data=tag_str,
set_tags(self,
self.set_xml_tags(tags.to_xml(),
bucket_lister(bucket,
bucket.get_all_keys(prefix=prefix,
unquote_str(marker)
BucketListResultSet(object):
bucket_lister(self.bucket,
key_marker=key_marker,
version_id_marker=version_id_marker,
max_keys=999,
rs.next_version_id_marker
version_id_marker=self.version_id_marker,
multipart_upload_lister(bucket,
bucket.get_all_multipart_uploads(key_marker=key_marker,
upload_id_marker=upload_id_marker,
rs.next_upload_id_marker
MultiPartUploadListResultSet(object):
multipart_upload_lister(self.bucket,
upload_id_marker=self.upload_id_marker,
BucketLogging(object):
grants=None):
Disabled>"
(%s)>"
(self.target,
self.prefix,
".join(grants))
self.grants.append(Grant())
'TargetBucket':
'TargetPrefix':
u'<BucketLoggingStatus
xmlns="http://doc.s3.amazonaws.com/2006-03-01">'
u'<LoggingEnabled>'
u'<TargetBucket>%s</TargetBucket>'
u'<TargetPrefix>%s</TargetPrefix>'
xml.sax.saxutils.escape(prefix)
'<TargetGrants>'
'</TargetGrants>'
u'</LoggingEnabled>'
u'</BucketLoggingStatus>'
BotoClientError,
check_lowercase_bucketname(n):
(n
'a').islower():
BotoClientError("Bucket
upper-case
"characters
sub-domain
virtual
"hosting
calling
format.")
assert_case_insensitive(f):
check_lowercase_bucketname(args[2]):
_CallingFormat(object):
build_host(self,
self.get_bucket_server(server,
build_auth_path(self,
SubdomainCallingFormat(_CallingFormat):
VHostCallingFormat(_CallingFormat):
OrdinaryCallingFormat(_CallingFormat):
"%s/"
ProtocolIndependentOrdinaryCallingFormat(OrdinaryCallingFormat):
US
Classic
Ireland
EUCentral1
'eu-central-1'
Frankfurt
USWest
'us-west-1'
USWest2
SAEast
'sa-east-1'
APNortheast
'ap-northeast-1'
APSoutheast
'ap-southeast-1'
APSoutheast2
'ap-southeast-2'
CNNorth1
'cn-north-1'
NoHostProvided(object):
HostRequiredError(BotoClientError):
S3Connection(AWSAuthConnection):
DefaultCallingFormat
'calling_format',
'boto.s3.connection.SubdomainCallingFormat')
'Signature=%s&Expires=%d&AWSAccessKeyId=%s'
host=NoHostProvided,
calling_format=DefaultCallingFormat,
bucket_class=Bucket,
validate_certs=None,
NoHostProvided:
self.DefaultHost
isinstance(calling_format,
calling_format=boto.utils.find_class(calling_format)()
self.calling_format
calling_format
provider=provider,
no_host_provided:
HostRequiredError(
SigV4,
parameter."
self.get_all_buckets():
bucket_name):
(self.lookup(bucket_name)
set_bucket_class(self,
bucket_class):
build_post_policy(self,
expiration_time,
isinstance(expiration_time,
time.struct_time),
'Policy
Time
'{"expiration":
"%s",\n"conditions":
[%s]}'
(time.strftime(boto.utils.ISO8601,
expiration_time),
",".join(conditions))
build_post_form_args(self,
expires_in=6000,
acl=None,
success_action_redirect=None,
max_content_length=None,
http_method='http',
fields=None,
conditions=None,
server_side_encryption=None):
time.gmtime(int(time.time()
expires_in))
conditions.append('{"bucket":
key.endswith("${filename}"):
conditions.append('["starts-with",
"$key",
"%s"]'
key[:-len("${filename}")])
conditions.append('{"key":
acl:
conditions.append('{"acl":
acl)
"acl",
acl})
success_action_redirect:
conditions.append('{"success_action_redirect":
success_action_redirect)
"success_action_redirect",
success_action_redirect})
max_content_length:
conditions.append('["content-length-range",
%i]'
max_content_length)
'x-amz-security-token',
self.provider.security_token})
conditions.append('{"x-amz-security-token":
'x-amz-storage-class',
storage_class})
conditions.append('{"x-amz-storage-class":
server_side_encryption:
'x-amz-server-side-encryption',
server_side_encryption})
conditions.append('{"x-amz-server-side-encryption":
server_side_encryption)
self.build_post_policy(expiration,
policy_b64
base64.b64encode(policy)
"policy",
policy_b64})
"AWSAccessKeyId",
self.aws_access_key_id})
self._auth_handler.sign_string(policy_b64)
"signature",
signature})
"key",
key})
'%s://%s/'
(http_method,
bucket_name))
"fields":
generate_url_sigv4(self,
host.endswith(':443'):
host[:-4]
params['VersionId']
params.update(response_headers)
self._auth_handler.presign(http_request,
iso_date=iso_date)
self._auth_handler.capability[0]
self.generate_url_sigv4(expires_in,
key=key,
expires_in_absolute:
int(expires_in)
int(time.time()
expires_in)
extra_qp
extra_qp.append("versionId=%s"
headers['x-amz-security-token']
c_string
self._auth_handler.sign_string(c_string)
encoded_canonical
urllib.parse.quote(b64_hmac,
self.QueryString
(encoded_canonical,
hdr_prefix
self.provider.header_prefix
k.startswith(hdr_prefix):
force_http:
self.calling_format.build_url_base(self,
self.server_name(port),
ResultSet([('Bucket',
self.bucket_class)])
get_canonical_user_id(self,
self.get_all_buckets(headers=headers)
rs.owner.id
self.head_bucket(bucket_name,
head_bucket(self,
self.make_request('HEAD',
'AccessDenied'
Denied'
'NoSuchBucket'
exist'
self.get_bucket(bucket_name,
{self.provider.acl_header:
Location.DEFAULT:
'<CreateBucketConfiguration><LocationConstraint>'
'</LocationConstraint></CreateBucketConfiguration>'
isinstance(bucket,
self.bucket_class):
Key):
retry_handler=retry_handler
CORSRule(object):
allowed_method=None,
allowed_origin=None,
self.allowed_method
self.allowed_origin
self.allowed_header
max_age_seconds
self.expose_header
'AllowedMethod':
self.allowed_method.append(value)
'AllowedOrigin':
self.allowed_origin.append(value)
'AllowedHeader':
self.allowed_header.append(value)
'MaxAgeSeconds':
'ExposeHeader':
self.expose_header.append(value)
self.allowed_method:
'<AllowedMethod>%s</AllowedMethod>'
self.allowed_origin:
'<AllowedOrigin>%s</AllowedOrigin>'
self.allowed_header:
'<AllowedHeader>%s</AllowedHeader>'
self.expose_header:
'<ExposeHeader>%s</ExposeHeader>'
self.max_age_seconds:
'<MaxAgeSeconds>%d</MaxAgeSeconds>'
CORSConfiguration(list):
'CORSRule':
CORSRule()
'</CORSConfiguration>'
allowed_method,
isinstance(allowed_method,
[allowed_method]
isinstance(expose_header,
[expose_header]
CORSRule(allowed_method,
allowed_header,
max_age_seconds,
expose_header)
DeleteMarker(object):
StorageDataError
compute_md5,
RestoreBody
BufferSize
boto.config.getint('Boto',
'key_buffer_size',
8192)
base_user_settable_fields
set(["cache-control",
"content-disposition",
"content-encoding",
"content-language",
"content-md5",
"content-type",
"x-robots-tag",
"expires"])
_underscore_base_user_settable_fields
base_user_settable_fields:
_underscore_base_user_settable_fields.add(f.replace('-',
'_'))
base_fields
(base_user_settable_fields
set(["last-modified",
"content-length",
"date",
"etag"]))
self.cache_control
self.content_disposition
None,%s>'
name.encode('utf-8')
_get_key(self):
_set_key(self,
property(_get_key,
_set_key);
_get_md5(self):
binascii.b2a_hex(self.local_hashes['md5'])
_set_md5(self,
binascii.a2b_hex(value)
self.local_hashes.pop('md5',
property(_get_md5,
_set_md5);
_get_base64md5(self):
md5.encode('utf-8')
binascii.b2a_base64(md5).decode('utf-8').rstrip('\n')
_set_base64md5(self,
binascii.a2b_base64(value)
property(_get_base64md5,
_set_base64md5);
_get_storage_class(self):
list_items
list(self.bucket.list(self.name.encode('utf-8')))
len(list_items)
getattr(list_items[0],
'_storage_class',
list_items[0]._storage_class
'STANDARD'
_set_storage_class(self,
property(_get_storage_class,
_set_storage_class)
get_md5_from_hexdigest(self,
md5_hexdigest):
binascii.unhexlify(md5_hexdigest)
encodebytes(digest)
base64md5[-1]
base64md5[0:-1]
(md5_hexdigest,
base64md5)
handle_encryption_headers(self,
provider.server_side_encryption_header:
resp.getheader(
provider.server_side_encryption_header,
resp.getheader(provider.version_id,
resp.getheader(provider.copy_source_version_id,
resp.getheader(provider.delete_marker,
response.getheader(provider.restore_header)
header.split(',',
[i.strip()
part.split('=')]
val.replace('"',
'ongoing-request':
'expiry-date':
199
provider.storage_response_error(self.resp.status,
self.resp.reason,
self.resp.msg
boto.utils.get_aws_metadata(response_headers,
(name.lower()
'content-length'
'Content-Range'
'content-range':
end_range
re.sub('.*/(.*)',
'\\1',
int(end_range)
self.handle_version_headers(self.resp)
self.handle_encryption_headers(self.resp)
self.handle_restore_headers(self.resp)
self.handle_addl_headers(self.resp.getheaders())
open_write(self,
BotoClientError('Not
Implemented')
'r':
self.open_read(headers=headers,
'w':
self.open_write(headers=headers,
mode:
fast=False):
fast:
self.resp.read(self.BufferSize)
self.resp.read(size)
change_storage_class(self,
new_storage_class,
dst_bucket=None,
self.bucket.name
'STANDARD':
'REDUCED_REDUNDANCY':
new_storage_class)
dst_bucket,
self.bucket.connection.lookup(dst_bucket,
validate_dst_bucket)
src_version_id=self.version_id)
bool(self.bucket.lookup(self.name,
headers=headers))
get_metadata(self,
self.metadata.get(name)
self.metadata['Content-Type']
'content-md5':
self.metadata['Content-MD5']
self.metadata[name]
Key.base_user_settable_fields:
update_metadata(self,
self.metadata.update(d)
self.bucket.set_acl(acl_str,
self.bucket.set_canned_acl(acl_str,
get_redirect(self):
response.getheader('x-amz-website-redirect-location')
set_redirect(self,
redirect_location,
headers['x-amz-website-redirect-location']
redirect_location
self.bucket.connection.make_request('PUT',
self.bucket.set_canned_acl('public-read',
self.bucket.connection.generate_url(expires_in,
query_auth,
force_http,
expires_in_absolute,
_send_file_internal(self,
self.md5:
sender(http_conn,
fp.tell():
self.read_from_stream:
seeking.')
skips
skips['skip_host']
boto.utils.find_matching_headers('accept-encoding',
skips['skip_accept_encoding']
http_conn.putrequest(method,
**skips)
http_conn.putheader(key,
headers[key])
getattr(http_conn,
'debuglevel',
math.ceil(cb_size
(num_cb
1.0)))
http_conn.send('%x;\r\n'
chunk_len)
digesters[alg].update(chunk)
bytes_togo:
http_conn.send('0\r\n')
http_conn.set_debuglevel(save_debug)
self.should_retry(response,
chunked_transfer):
find_matching_headers('User-Agent',
headers[header]
'STANDARD']:
'Content-Encoding',
'Content-Language',
content_type_headers
content_type_headers:
(len(content_type_headers)
'Content-Type',
self.path:
mimetypes.guess_type(self.path)[0]
self.base64md5:
str(self.size)
self.bucket.connection._required_auth_capability():
{'fp':
'hash_algorithm':
hashlib.sha256}
kwargs['size']
headers['_sha256']
compute_hash(**kwargs)[0]
headers['Expect']
'100-Continue'
query_args=query_args
self.handle_version_headers(resp,
force=True)
self.handle_addl_headers(resp.getheaders())
should_retry(self,
chunked_transfer=False):
response.getheader('location'):
response.getheader('etag')
md5.decode('utf-8')
response.getheader(
'x-amz-server-side-encryption-customer-algorithm',
'ETag
computed
MD5.
vs.
(self.etag,
self.md5))
['RequestTimeout']:
PleaseRetryException(
"Saw
retrying"
response=response
chunked
transfer'
provider.get_provider_name())
BotoClientError('Cannot
determine
'object
stream')
chunked_transfer=True,
rewind=False):
string_data,
isinstance(string_data,
string_data
string_data.encode("utf-8")
BytesIO(string_data)
query_args=None)
_get_file_internal(self,
query_args.append('torrent')
query_args.append('versionId=%s'
query_args.append('%s=%s'
urllib.parse.quote(response_headers[key])))
'&'.join(query_args)
self.open('r',
int(math.ceil(cb_size/self.BufferSize/(num_cb-1.0)))
fp.write(bytes)
digesters[alg].update(bytes)
cb_size:
errno.ENOSPC:
StorageDataError('Out
fp.name)
torrent
"Range"
get_torrent_file(self,
num_cb=10):
torrent=True)
get_contents_to_filename(self,
res_download_handler=res_download_handler,
os.remove(filename)
email.utils.parsedate_tz(self.last_modified)
int(email.utils.mktime_tz(modified_tuple))
os.utime(fp.name,
(modified_stamp,
modified_stamp))
value.decode(encoding)
_normalize_metadata(self,
type(metadata)
set:
norm_metadata.add(k.lower())
norm_metadata[k.lower()]
_get_remote_metadata(self,
self._underscore_base_user_settable_fields:
underscore_name):
underscore_name)
metadata[field_name.lower()]
self.provider.metadata_prefix
self.metadata:
metadata['%s%s'
field_name.lower())]
self.metadata[underscore_name])
set_remote_metadata(self,
metadata_plus
self._normalize_metadata(metadata_plus)
metadata_minus
self._normalize_metadata(metadata_minus)
self._get_remote_metadata()
metadata.update(metadata_plus)
metadata_minus:
(h.startswith('x-goog-meta-')
h.startswith('x-amz-meta-')):
(h.replace('x-goog-meta-',
.replace('x-amz-meta-',
rewritten_metadata[rewritten_h]
src_bucket.copy_key(self.name,
restore(self,
data=self.RestoreBody
query_args='restore')
202):
KeyFile():
self.key.open_read()
self.softspace
implemented.
self.newlines
tell(self):
seek(self,
pos,
whence=os.SEEK_SET):
self.key.close(fast=True)
os.SEEK_END:
self.key.read(1)
os.SEEK_SET:
os.SEEK_CUR:
IOError('Invalid
seek'
whence)
416:
self.key.read(size)
self.key.close()
isatty(self):
getkey(self):
NotImplementedError('write
fileno(self):
NotImplementedError('fileno
NotImplementedError('flush
NotImplementedError('next
readinto(self):
NotImplementedError('readinto
NotImplementedError('readline
readlines(self):
NotImplementedError('readlines
truncate(self):
NotImplementedError('truncate
writelines(self):
NotImplementedError('writelines
xreadlines(self):
NotImplementedError('xreadlines
isinstance(expiration,
Expiration(days=expiration)
isinstance(transition,
Transition):
self.transition.append(transition)
transition:
'Transition':
Expiration()
'<Rule>'
'<Prefix>%s</Prefix>'
'<Status>%s</Status>'
self.expiration.to_xml()
self.transition.to_xml()
'</Rule>'
Expiration(object):
date=None):
'<Expiration:
'<Expiration>'
'</Expiration>'
Transition(object):
'<Transition:
(how_long,
self.storage_class)
'<Transition>'
'<StorageClass>%s</StorageClass>'
'</Transition>'
Transitions(list):
self.transition_properties
self.transition_properties:
self.append(Transition(self.temp_days,
self.temp_date,
self.temp_storage_class))
transition.to_xml()
add_transition(self,
Transition(days,
self.append(transition)
__first_or_default(self,
prop):
getattr(transition,
days(self):
self.__first_or_default('days')
date(self):
self.__first_or_default('date')
storage_class(self):
self.__first_or_default('storage_class')
Lifecycle(list):
'Rule':
'<LifecycleConfiguration>'
'</LifecycleConfiguration>'
Rule(id,
expiration,
transition)
Deleted(object):
delete_marker=False,
delete_marker_version_id=None):
delete_marker
delete_marker_version_id
'DeleteMarker':
'DeleteMarkerVersionId':
Error(object):
%s.%s(%s)>'
self.version_id,
%s(%s)>'
MultiDeleteResult(object):
'Deleted':
Deleted()
self.deleted.append(d)
Error()
self.errors.append(e)
CompleteMultiPartUpload(object):
'<CompleteMultiPartUpload:
(self.bucket_name,
Part(object):
isinstance(self.part_number,
%d>'
'PartNumber':
part_lister(mpupload,
part_number_marker=None):
mpupload.get_all_parts(None,
part_number_marker)
mpupload.next_part_number_marker
mpupload.is_truncated
MultiPartUpload(object):
'<MultiPartUpload
part_lister(self)
'<CompleteMultipartUpload>\n'
<Part>\n'
<PartNumber>%d</PartNumber>\n'
part.part_number
<ETag>%s</ETag>\n'
</Part>\n'
'</CompleteMultipartUpload>'
'Initiator':
'Part':
Part(self.bucket)
self._parts.append(part)
'UploadId':
'PartNumberMarker':
'NextPartNumberMarker':
'MaxParts':
'Initiated':
get_all_parts(self,
max_parts=None,
part_number_marker=None,
max_parts:
'&max-parts=%d'
max_parts
part_number_marker:
'&part-number-marker=%s'
encoding_type:
'&encoding-type=%s'
self.bucket.connection.make_request('GET',
self.key_name,
handler.XmlHandler(self,
upload_part_from_file(self,
self.bucket.new_key(self.key_name)
replace=replace,
md5=md5,
copy_part_from_key(self,
start=None,
end=None,
'bytes=%s-%s'
headers[provider.copy_source_range_header]
self.bucket.copy_key(self.key_name,
storage_class=None,
complete_upload(self):
self.to_xml()
self.bucket.complete_multipart_upload(self.key_name,
cancel_upload(self):
self.bucket.cancel_multipart_upload(self.key_name,
Prefix(object):
storage_uri_for_key
ByteTranslatingCallbackHandler(object):
proxied_cb,
download_start_point):
self.proxied_cb
proxied_cb
download_start_point
self.proxied_cb(self.download_start_point
position_to_eof=False):
KeyFile)
fp.seek(cur_pos,
ResumableDownloadHandler(object):
MIN_ETAG_LEN
self._load_tracker_file_etag()
_load_tracker_file_etag(self):
f.readline().rstrip('\n')
len(self.etag_value_for_current_download)
self.MIN_ETAG_LEN:
(%s).
self.tracker_file_name)
_save_tracker_info(self,
key.etag.strip('"\'')
self.etag_value_for_current_download)
_attempt_resumable_download(self,
hash_algs):
position_to_eof=True)
(cur_file_size
key.etag.strip('"\'')):
(%d).\nDeleting
re-try
scratch'
(fp.name,
cur_file_size,
str(storage_uri_for_key(key)),
key.size),
print('Download
headers['Range']
(cur_file_size,
ByteTranslatingCallbackHandler(cb,
cur_file_size).call
self._save_tracker_info(key)
fp.truncate(0)
had_file_bytes_before_attempt
self._attempt_resumable_download(key,
hash_algs)
'(%s)'
'(%s);
aborting
'retry'
had_file_bytes_before_attempt:
httplib.IncompleteRead:
2**progress_less_iterations
(progress_less_iterations,
'<Tag><Key>%s</Key><Value>%s</Value></Tag>'
self.key,
(self.key
other.key
other.value)
TagSet(list):
'Tag':
Tag(key,
'<TagSet>'
tag.to_xml()
'</TagSet>'
Tags(list):
'TagSet':
'<Tagging>'
tag_set.to_xml()
+='</Tagging>'
add_tag_set(self,
tag_set):
display_name=''):
tag(key,
'%s%s%s'
WebsiteConfiguration(object):
routing_rules=None):
redirect_all_requests_to
'RoutingRules':
'IndexDocument':
_XMLKeyValue([('Suffix',
'suffix')],
'ErrorDocument':
_XMLKeyValue([('Key',
'error_key')],
['<?xml
'<WebsiteConfiguration
xmlns="http://s3.amazonaws.com/doc/2006-03-01/">']
parts.append(tag('IndexDocument',
tag('Suffix',
self.suffix)))
parts.append(tag('ErrorDocument',
tag('Key',
self.error_key)))
parts.append(self.redirect_all_requests_to.to_xml())
self.routing_rules:
parts.append(self.routing_rules.to_xml())
parts.append('</WebsiteConfiguration>')
_XMLKeyValue(object):
translator,
container=None):
self.translator
translator
container:
xml_key:
setattr(self.container,
getattr(self.container,
attr_name)
parts.append(tag(xml_key,
content))
RedirectLocation(_XMLKeyValue):
[('HostName',
protocol=None):
tag('RedirectAllRequestsTo',
RoutingRules(list):
rule):
'RoutingRule':
RoutingRule(Condition(),
Redirect())
self.add_rule(rule)
"RoutingRules(%s)"
super(RoutingRules,
inner_text
inner_text.append(rule.to_xml())
tag('RoutingRules',
'\n'.join(inner_text))
RoutingRule(object):
condition=None,
redirect=None):
'Redirect':
self.condition:
parts.append(self.condition.to_xml())
self.redirect:
parts.append(self.redirect.to_xml())
tag('RoutingRule',
'\n'.join(parts))
when(cls,
cls(Condition(key_prefix=key_prefix,
http_error_code=http_error_code),
then_redirect(self,
Redirect(
hostname=hostname,
protocol=protocol,
replace_key=replace_key,
replace_key_prefix=replace_key_prefix,
http_redirect_code=http_redirect_code)
Condition(_XMLKeyValue):
('KeyPrefixEquals',
'key_prefix'),
('HttpErrorCodeReturnedEquals',
'http_error_code'),
self.key_prefix
key_prefix
self.http_error_code
http_error_code
tag('Condition',
Redirect(_XMLKeyValue):
('HostName',
('ReplaceKeyWith',
'replace_key'),
('ReplaceKeyPrefixWith',
'replace_key_prefix'),
('HttpRedirectCode',
'http_redirect_code'),
self.replace_key
replace_key
self.replace_key_prefix
replace_key_prefix
self.http_redirect_code
http_redirect_code
tag('Redirect',
region_cls=SDBRegionInfo
boto.sdb.domain
DomainMetaData
boto.sdb.item
ItemThread(threading.Thread):
item_names):
super(ItemThread,
self).__init__(name=name)
self.item_names
self.item_names:
self.conn.get_attributes(self.domain_name,
SDBConnection(AWSQueryConnection):
'sdb.us-east-1.amazonaws.com'
'2009-04-15'
boto.config.get('SDB',
boto.sdb.regions():
super(SDBConnection,
converter
['sdb']
set_item_cls(self,
_build_name_value_list(self,
label='Attribute'):
sorted(attributes.keys())
attributes[key]
_build_expected_value(self,
params['Expected.1.Name']
expected_value[0]
params['Expected.1.Value']
_build_batch_list(self,
items.keys()
item_names:
params['Item.%d.ItemName'
items[item_name]
item.keys()
item[attr_name]
_build_name_list(self,
attribute_names):
attribute_names.sort()
params['Attribute.%d.Name'
print_usage(self):
print('Total
Usage:
compute
self.box_usage)
cost
0.14
print('Approximate
Cost:
$%f'
cost)
get_domain(self,
self.select(domain,
self.get_domain(domain_name,
validate)
get_all_domains(self,
max_domains=None,
max_domains:
params['MaxNumberOfDomains']
max_domains
self.get_list('ListDomains',
[('DomainName',
Domain)])
self.get_object('CreateDomain',
Domain)
d.name
get_domain_and_name(self,
(isinstance(domain_or_name,
Domain)):
(domain_or_name,
domain_or_name.name)
(self.get_domain(domain_or_name),
domain_or_name)
self.get_status('DeleteDomain',
domain_metadata(self,
self.get_object('DomainMetadata',
DomainMetaData)
d.domain
self.get_status('PutAttributes',
self.get_status('BatchPutAttributes',
isinstance(attribute_names,
[attribute_names]
self.make_request('GetAttributes',
self.item_cls(domain,
handler.XmlHandler(item,
SDBResponseError(response.status,
attr_names=None,
self._build_name_list(params,
self.item_cls):
self.get_status('DeleteAttributes',
self.get_status('BatchDeleteAttributes',
{'SelectExpression':
query}
self.get_list('Select',
[('Item',
self.item_cls)],
parent=domain)
e.body
"Query:
%s\n%s"
(query,
boto.sdb.queryresultset
SelectResultSet
'Domain:%s'
iter(self.select("SELECT
FROM
`%s`"
get_metadata(self):
self._metadata:
self.connection.domain_metadata(self)
self.connection.put_attributes(self,
self.connection.batch_put_attributes(self,
attribute_name=None,
self.connection.get_attributes(self,
attribute_name,
expected_values=None):
self.connection.delete_attributes(self,
expected_values)
self.connection.batch_delete_attributes(self,
SelectResultSet(self,
next_token=next_token,
self.get_attributes(item_name,
item.domain
self.connection.item_cls(self,
self.delete_attributes(item.name)
f=None):
print('<?xml
print('<Domain
print('\t<Item
print('\t\t<attribute
item[k]
[values]
print('\t\t\t<value><![CDATA[',
value.encode('utf-8',
errors='replace').encode('utf-8',
f.write(value)
print(']]></value>',
print('\t\t</attribute>',
print('\t</Item>',
print('</Domain>',
from_xml(self,
DomainDumpParser(self)
xml.sax.parse(doc,
self.connection.delete_domain(self)
DomainMetaData(object):
'ItemNamesSizeBytes':
'AttributeNameCount':
'AttributeNamesSizeBytes':
'AttributeValueCount':
'AttributeValuesSizeBytes':
xml.sax.handler
ContentHandler
DomainDumpParser(ContentHandler):
UploaderThread(domain)
"attribute":
ch):
self.attribute:
self.value.strip()
self.attribute.strip()
self.attrs[attr_name].append(value)
self.attrs[attr_name]
self.uploader.items[self.item_id]
len(self.uploader.items)
20:
UploaderThread(self.domain)
"Domain":
UploaderThread(Thread):
super(UploaderThread,
self.db.batch_put_attributes(self.items)
print("Exception
put,
regular
self.db.put_attributes(item_name,
self.items[item_name])
print(".",
name='',
active=False):
self.domain.connection.converter
attrs.get('encoding',
'base64':
base64.decodestring(value)
self.in_attribute:
isinstance(self[self.last_key],
[self[self.last_key]]
self[self.last_key].append(value)
self.domain.get_attributes(self.name,
item=self)
self.domain.put_attributes(self.name,
del_attrs.append(name)
self.domain.delete_attributes(self.name,
isinstance(self[key],
[self[key]]
self[key].append(value)
self.domain.delete_item(self)
query_lister(domain,
domain.connection.query_with_attributes(domain,
attr_names,
QueryResultSet(object):
self.attr_names
query_lister(self.domain,
self.max_items,
self.attr_names)
select_lister(domain,
domain.connection.select(domain,
SelectResultSet(object):
self.domain.connection.select(self.domain,
next_token=self.next_token,
consistent_read=self.consistent_read)
next(self.__iter__())
SDBRegionInfo(RegionInfo):
super(SDBRegionInfo,
SDBConnection)
Blob(object):
file(self):
StringIO(self.value)
six.text_type(self).encode('utf-8')
__unicode__(self):
self.file.getvalue()
self.file.read()
self.file.readline()
next(self.file)
iter(self.file)
self._file.size
self.value:
len(self.value)
from_path(cls,
NotImplementedError("Paths
encoded=None,
obj=None):
obj.kind()
app(self):
NotImplementedError("Applications
kind(self):
id_or_name(self):
has_id_or_name(self):
parent(self):
self.id_or_name()
Property
ModelMeta(type):
"Metaclass
Models"
super(ModelMeta,
cls.__sub_classes__
boto.sdb.db.manager
get_manager
issubclass(b,
Model),
bases):
bases:
base.__sub_classes__.append(cls)
get_manager(cls)
dict.keys():
isinstance(dict[key],
dict[key]
property.__property_config__(cls,
prop_names
cls.properties()
props:
prop_names.append(prop.name)
setattr(cls,
'_prop_names',
prop_names)
Model(object):
ModelMeta
__consistent__
Consistent
get_lineage(cls):
[c.__name__
cls.mro()]
l.reverse()
kind(cls):
_get_by_id(cls,
manager.get_object(cls,
get_by_id(cls,
ids=None,
isinstance(ids,
[cls._get_by_id(id)
ids]
cls._get_by_id(ids)
get_by_ids
get_by_id
get_by_key_name(cls,
key_names,
find(cls,
Query(cls,
q.filter('%s
='
all(cls,
cls.find(limit=limit,
get_or_insert(key_name,
NotImplementedError("get_or_insert
properties(cls,
hidden=True):
hidden
properties.append(prop)
find_property(cls,
prop_name):
prop.__class__.__name__.startswith('_')
prop.name:
get_xmlmanager(cls):
'_xmlmanager'):
from_xml(cls,
cls.get_xmlmanager()
xmlmanager.unmarshal_object(fp)
prop.default_value())
'manager'
kw['manager']
'manager':
kw[key])
'%s<%s>'
str(self.id)
Model)
other.id
_get_raw_item(self):
self._manager.get_raw_item(self)
reload(self):
self._manager.save_object(self,
dict)),
key->values
save"
attrs[prop_name]
self.find_property(prop_name)
assert(prop),
"Property
self._manager.set_property(prop,
prop_name,
list)),
delete."
self._manager.domain.delete_attributes(self.id,
save_attributes
put_attributes
self._manager.delete_object(self)
key(self):
Key(obj=self)
set_manager(self,
{'properties':
self.id}
{self.__class__.__name__:
obj}
self.get_xmlmanager()
xmlmanager.marshal_object(self,
find_subclass(cls,
sc.find_subclass(name)
Expando(Model):
self._prop_names:
self._manager.set_key_value(self,
self._manager.get_key_value(self,
Property(object):
self.verbose_name
self.validator
validator
obj.load()
self.validate(value)
self.name):
fnc(value)
on_set_%s"
property_name
default_validator(self,
(self.model_class.__name__,
self.choices:
self.model_class.__name__,
self.default_validator(value)
getattr(model_instance,
get_choices(self):
callable(self.choices):
self.choices()
validate_string(value):
StringProperty(Property):
'String'
validator=validate_string,
super(StringProperty,
TextProperty(Property):
'Text'
Text,
self.max_length:
self.max_length)
PasswordProperty(StringProperty):
'Password'
self.data_type(value,
len(value):
self.data_type(hashfunc=self.hashfunc)
p.set(value)
self.data_type(super(PasswordProperty,
objtype),
(type(self.data_type),
BlobProperty(Property):
"blob"
Blob):
oldb
self.__get__(obj,
type(obj))
oldb:
oldb.id
Blob(value=value,
id=id)
super(BlobProperty,
S3KeyProperty(Property):
boto.s3.key.Key
'S3Key'
validate_regex
"^s3:\/\/([^\/]*)\/(.*)$"
str(self.default_value()):
objtype)
obj._manager.get_s3_connection()
bucket.new_key(match.group(2))
k.set_contents_from_string("")
(value.bucket.name,
value.name)
IntegerProperty(Property):
'Integer'
max=2147483647,
min=-2147483648):
self.max
self.min
self.max:
self.max)
self.min:
self.min)
LongProperty(Property):
'Long'
-9223372036854775808
9223372036854775807
max:
max)
min:
min)
BooleanProperty(Property):
'Boolean'
super(BooleanProperty,
FloatProperty(Property):
float
'Float'
default=0.0,
DateTimeProperty(Property):
datetime.datetime
'DateTime'
DateProperty(Property):
datetime.date
val.date()
datetime.date.today()
TimeProperty(Property):
datetime.time
'Time'
ReferenceProperty(Property):
'Reference'
reference_class=None,
collection_name=None,
self.reference_class
reference_class
collection_name
self.reference_class(value)
(obj.id
(hasattr(value,
value.id)):
ValueError("Can
associate
itself!")
self).__property_config__(model_class,
property_name)
'%s_%s_set'
(model_class.__name__.lower(),
hasattr(self.reference_class,
self.collection_name):
ValueError('duplicate
property:
self.collection_name)
setattr(self.reference_class,
self.collection_name,
_ReverseReferenceProperty(model_class,
property_name,
self.collection_name))
check_uuid(self,
value.split('-')
check_instance(self,
obj_lineage
value.get_lineage()
cls_lineage
self.reference_class.get_lineage()
obj_lineage.startswith(cls_lineage):
(obj_lineage,
cls_lineage))
Model'
self.check_instance(value)
_ReverseReferenceProperty(Property):
'query'
model,
self.__model
self.__property
model_class):
model_instance
Query(self.__model)
isinstance(self.__property,
self.__property:
props.append("%s
="
query.filter(props,
query.filter(self.__property
=',
ValueError('Virtual
read-only')
CalculatedProperty(Property):
use_method=False):
super(CalculatedProperty,
calculated_type
self.use_method
use_method
value()
_set_direct(self,
[str,
bool]:
self.__get__(model_instance,
model_instance.__class__)
ListProperty(Property):
list(super(ListProperty,
self).default_value())
Override
"None"
MapProperty(Property):
'Map'
item_type=str,
ValueError('Value
dict')
isinstance(value[key],
__local_iter__
self.model_class._manager
self.select
self.rs
iter(self.manager.query(self))
self.__iter__()
next(self.__local_iter__)
property_operator,
self.filters.append((property_operator,
offset=0):
quick=True):
self.manager.count(self.model_class,
quick,
get_query(self):
self.manager._build_filter_part(self.model_class,
order(self,
self.model_class.get_xmlmanager()
xmlmanager.new_doc()
obj.to_xml(doc)
get_next_token(self):
self.rs:
self.rs.next_token
self._next_token:
set_next_token(self,
property(get_next_token,
set_next_token)
SequenceGenerator(object):
"SequenceGenerator"
sequence_string,
rollover=False):
self.sequence_string
sequence_string
self.sequence_length
len(sequence_string[0])
self.rollover
rollover
self.last_item
sequence_string[-1]
"%s('%s')"
sequence_string)
last=None):
len(val)
self.sequence_length:
self.sequence_string[0]
last_value
val[-self.sequence_length:]
self.rollover)
(last_value
self.last_item):
(self(val[:-self.sequence_length]),
(val[:-self.sequence_length],
_inc(self,
assert(len(val)
self.sequence_length)
self.sequence_string[(self.sequence_string.index(val)
len(self.sequence_string)]
increment_by_one(cv=None,
double(cv=None,
fib(cv=1,
lv=0):
SequenceGenerator("ABCDEFGHIJKLMNOPQRSTUVWXYZ")
Sequence(object):
fnc=increment_by_one,
init_val=None):
fnc(init_val)
type(fnc(None))
isinstance(fnc,
find_class(fnc)
self.fnc
self.val:
new_val
new_val['last_value']
['current_value',
str(self._value)]
new_val['current_value']
self.db.put_attributes(self.id,
new_val,
ValueError("Sequence
sync")
self.db.get_attributes(self.id,
'timestamp'
val['timestamp']
'current_value'
self.item_type(val['current_value'])
"last_value"
val['last_value']
self.item_type(val['last_value'])
property(get,
set)
"%s('%s',
'%s.%s',
'%s')"
self.domain_name,
self.fnc.__module__,
self.fnc.__name__,
self._db:
self.domain_name:
"sequence_db",
"db_name",
"default"))
sdb.get_domain(self.domain_name)
sdb.create_domain(self.domain_name)
property(_connect)
self.fnc(self.val,
self.last_value)
self.db.delete_attributes(self.id)
BooleanProperty
FloatProperty,
ReferenceProperty
PasswordProperty,
SDBPersistenceError
logging.getLogger('test_db')
TestBasic(Model):
BooleanProperty()
TestFloat(Model):
FloatProperty()
TestRequired(Model):
default='foo')
TestReference(Model):
ReferenceProperty(reference_class=TestBasic,
collection_name='refs')
TestSubClass(TestBasic):
TestPassword(Model):
PasswordProperty()
TestList(Model):
TestMap(Model):
MapProperty()
TestListReference(Model):
basics
ListProperty(TestBasic)
TestAutoNow(Model):
DateTimeProperty(auto_now_add=True)
modified_date
DateTimeProperty(auto_now=True)
TestUnique(Model):
StringProperty(unique=True)
test_basic():
-42
t.date
_objects['test_basic_t']
TestBasic.get_by_id(t.id)
_objects['test_basic_tt']
TestBasic.get_by_id([t.id])
l[0].id
tt.size
tt.foo
test_float():
TestFloat()
'float
98.6
_objects['test_float_t']
TestFloat.get_by_id(t.id)
_objects['test_float_tt']
tt.value
test_required():
TestRequired()
_objects['test_required_t']
test_reference(t=None):
TestReference()
tt.ref
TestReference.get_by_id(tt.id)
_objects['test_reference_tt']
tt.ref.id
t.refs:
log.debug(o)
test_subclass():
TestSubClass()
_objects['test_subclass_t']
subclass'
-489
test_password():
TestPassword()
_objects['test_password_t']
t.password
TestPassword.get_by_id(t.id)
_objects['test_password_tt']
tt.password
str(tt.password)
test_list():
TestList()
_objects['test_list_t']
ints'
TestList.get_by_id(t.id)
_objects['test_list_tt']
tt.nums:
test_list_reference():
_objects['test_list_ref_t']
TestListReference()
tt.basics
[t]
_objects['test_list_ref_tt']
ttt
TestListReference.get_by_id(tt.id)
ttt.basics[0].id
test_unique():
_objects['test_unique_t']
_objects['test_unique_tt']
except(SDBPersistenceError):
test_datetime():
TestAutoNow()
_objects['test_datetime_t']
TestAutoNow.get_by_id(t.id)
tt.create_date.timetuple()
t.create_date.timetuple()
log.info('test_basic')
log.info('test_required')
test_required()
log.info('test_reference')
test_reference(t1)
log.info('test_subclass')
test_subclass()
log.info('test_password')
test_password()
log.info('test_list')
test_list()
log.info('test_list_reference')
test_list_reference()
log.info("test_datetime")
test_datetime()
log.info('test_unique')
test_unique()
get_manager(cls):
"sdb.amazonaws.com")
boto.config.getbool('DB',
sql_dir
'sql_dir',
cls.__module__.replace('.',
db_user)
db_passwd)
db_name)
db_table)
db_host)
db_port)
"_db_name")
hasattr(cls.__bases__[0],
"_manager"):
cls.__bases__[0]._manager
'SimpleDB':
boto.sdb.db.manager.sdbmanager
SDBManager
SDBManager(cls,
'XML':
db_type:
SDBPersistenceError,
TimeDecodeError(Exception):
SDBConverter(object):
{bool:
int:
float:
(self.encode_float,
self.decode_float),
self.model_class:
self.encode_reference,
self.decode_reference
Key:
datetime:
self.decode_datetime),
(self.encode_date,
self.decode_date),
(self.encode_time,
self.decode_time),
Blob:
(self.encode_blob,
self.decode_blob),
(self.encode_string,
self.decode_string),
encode_list(self,
values["%03d"
encode_map(self,
ValueError('Expected
value[key])
new_value.append('%s:%s'
(urllib.quote(key),
encoded_value))
self.encode_list(prop,
decode_list(self,
dec_val
int(k)
dec_val[k]
dec_val.values()
decode_map(self,
ret_value[k]
decode_map_element(self,
":"
value.split(':',
urllib.unquote(key)
item_type(id=value)
self.decode_list(prop,
self.decode_map(prop,
'%010d'
boto.log.error("Error,
'%020d'
("true",
"yes"):
encode_float(self,
'%e'
s.split('e')
l[0].ljust(18,
l[1]
0.0:
'3'
'000'
'+':
'5'
'4'
(case,
exponent,
mantissa)
decode_float(self,
value[2:5]
value[6:]
'3':
'4':
(int(exponent)
999)
'2':
abs((int(exponent)
999))
float(mantissa
'e'
exponent)
"T"
datetime.strptime(value.split(".")[0],
"%Y-%m-%dT%H:%M:%S")
encode_date(self,
decode_date(self,
encode_time
encode_date
decode_time(self,
'+'
TimeDecodeError("Can't
handle
timezone
aware
objects:
value.split('.')
tmp[0].split(':'))
len(tmp)
arg.append(int(tmp[1]))
time(*arg)
'None',
"None":
encode_blob(self,
value.id:
self.manager.get_blob_bucket()
bucket.new_key(str(uuid.uuid4()))
SDBPersistenceError("Invalid
value.value
key.set_contents_from_string(value.value)
decode_blob(self,
"Forbidden":
Blob(file=key,
id="s3://%s/%s"
encode_string(self,
arr
arr.append(six.unichr(ord(ch)))
u"".join(arr)
decode_string(self,
SDBManager(object):
enable_ssl,
consistent=None):
SDBConverter(self)
"__consistent__"):
cls.__consistent__
self.consistent
sdb(self):
domain(self):
dict(aws_access_key_id=self.db_user,
aws_secret_access_key=self.db_passwd,
is_secure=self.enable_ssl)
x.endpoint
self.db_host][0]
args['region']
boto.connect_sdb(**args)
self._sdb.lookup(self.db_name,
self._sdb.create_domain(self.db_name)
query_lister):
query_lister:
self.get_object(cls,
boto.connect_s3(self.db_user,
self.db_passwd)
get_blob_bucket(self,
self.get_s3_connection()
"%s-%s"
(s3.aws_access_key_id,
bucket_name.lower()
a=None):
self.domain.get_attributes(id,
a['__type__']
find_class(a['__module__'],
cls.properties(hidden=False):
params[prop.name]
cls(id,
'(%s)
(id,
a['__module__'],
boto.log.info('sdbmanager:
get_object_from_id(self,
self.get_object(None,
self._build_filter_part(query.model_class,
query.filters,
query.sort_by,
query.select))
query.limit:
query.limit
self.domain.select(query_str,
max_items=query.limit,
next_token=query.next_token)
query.rs
self._object_lister(query.model_class,
quick=True,
count(*)
self._build_filter_part(cls,
select))
self.domain.select(query):
int(row['Count'])
quick:
_build_filter(self,
"__id__":
'itemName()'
"itemName()":
'`%s`'
('is',
'='):
"%(name)s
('is
'!='):
property.__class__
ListProperty:
("is",
"="):
"like"
("!=",
"not"):
like"
not(op
["like",
like"]
val.startswith("%")):
"%%:%s"
val.replace("'",
"''"))
_build_filter_part(self,
order_by=None,
query_parts
order_by[0]
"-":
"DESC"
"ASC"
select)
(filters,
filter[0]
isinstance(filter_props,
[filter_props]
filter_prop
filter_props:
op)
filter_prop.strip().split("
filter[1]
cls.find_property(name)
filter_parts_sub
filter_parts.append("(%s)"
".join(filter_parts_sub)))
".join(filter_parts)))
"(`__type__`
self._get_all_decendents(cls).keys():
")"
query_parts.append(type_query)
order_by_filtered:
query_parts.append("`%s`
LIKE
'%%'"
order_by)
len(query_parts)
".join(query_parts),
order_by_query)
_get_all_decendents(self,
decendents[sc.__name__]
decendents.update(self._get_all_decendents(sc))
SimpleDB")
{'__type__':
obj.__class__.__name__,
'__module__':
obj.__class__.__module__,
'__lineage__':
obj.get_lineage()}
del_attrs.append(property.name)
attrs[property.name]
property.unique:
{property.name:
obj.find_property(expected_value[0])
self.domain.delete_attributes(obj.id)
prop.get_value_for_datastore(obj)
prop.unique:
{prop.name:
a[name])
AttributeError('%s
find_class,
getDOMImplementation,
parse,
parseString,
Node
XMLConverter(object):
(self.encode_password,
self.decode_password),
self.decode_datetime)}
get_text_value(self,
parent_node):
parent_node.childNodes:
node.nodeType
node.TEXT_NODE:
node.data
new_value.append(self.encode(item_type,
prop.data_type
value.getElementsByTagName('item'):
self.manager.doc.createElement("object")
val_node.setAttribute('id',
val_node.setAttribute('class',
(value.__class__.__module__,
value.__class__.__name__))
value.childNodes[0]
value.getAttribute("class")
value.getAttribute("id")
cls.get_by_ids(id)
encode_password(self,
decode_password(self,
Password(value)
XMLManager(object):
enable_ssl):
db_name:
cls.__name__.lower()
XMLConverter(self)
self.impl
getDOMImplementation()
self.db_user:
encodebytes('%s:%s'
(self.db_user,
self.db_passwd))[:-1]
"Basic
self.db_host:
self.enable_ssl:
HTTPSConnection
HTTPConnection
Connection(self.db_host,
self.db_port)
post_data=None,
self.connection.close()
self.connection.connect()
self.auth_header:
headers["Authorization"]
self.connection.request(method,
self.connection.getresponse()
new_doc(self):
doc.getElementsByTagName('object'):
self.get_list(prop_node,
prop.item_type)
get_doc(self):
boto.connect_s3(self.aws_access_key_id,
self.aws_secret_access_key)
prop_node,
prop_node.getElementsByTagName('items')[0]
items_node.getElementsByTagName('item'):
self.converter.decode(item_type,
get_object_from_doc(self,
get_props_from_doc(self,
cls.find_property(prop_name)
(cls,
order_by=None):
str(self._build_query(cls,
order_by))
"/%s?%s"
urlencode({"query":
query}))
self._object_lister(cls,
_build_query(self,
order_by):
len(filters)
Exception('Too
4')
cls.properties(hidden=False)
filter.strip().split()
properties:
property.name
filter_parts.append("'%s'
parts.append("[%s]"
".join(filter_parts))
'%s']"
Exception('%s
field'
order_by.startswith("-"):
"asc"
starts-with
'']
intersection
'.join(parts)
XML")
save_list(self,
doc,
prop_node):
doc.createElement('items')
prop_node.appendChild(items_node)
doc.createElement('item')
items_node.appendChild(item_node)
item_node.appendChild(item)
doc.createTextNode(item)
item_node.appendChild(text_node)
self.marshal_object(obj)
(self.db_name)
self._make_request("PUT",
body=doc.toxml())
new_obj
self.get_object_from_doc(obj.__class__,
parse(resp))
new_obj.id
obj.properties():
propname:
getattr(new_obj,
marshal_object(self,
self.new_doc()
doc.createElement('object')
obj_node.setAttribute('id',
obj_node.setAttribute('class',
(obj.__class__.__module__,
obj.__class__.__name__))
doc.documentElement
root.appendChild(obj_node)
doc.createElement('property')
prop_node.setAttribute('name',
prop_node.setAttribute('type',
property.type_name)
self.save_list(doc,
prop_node.appendChild(value)
doc.createTextNode(six.text_type(value).encode("ascii",
"ignore"))
prop_node.appendChild(text_node)
obj_node.appendChild(prop_node)
unmarshal_object(self,
unmarshal_props(self,
self.get_props_from_doc(cls,
self._make_request("DELETE",
obj.get_by_id(obj.id)
boto.services.submit
Submitter
boto.services.result
ResultProcessor
BS(object):
Usage
"usage:
config_file
command"
Commands
{'reset'
'Clear
bucket',
'submit'
'Submit
'Start
'status'
'Report
queues',
'retrieve'
batch',
'batches'
'List
batches
output_domain'}
OptionParser(usage=self.Usage)
self.parser.add_option("--help-commands",
dest="help_commands",
help="provides
commands")
self.parser.add_option("-a",
"--access-key",
self.parser.add_option("-s",
"--secret-key",
Secret
self.parser.add_option("-p",
"--path",
dest="path",
retrieve")
self.parser.add_option("-k",
"--keypair",
dest="keypair",
self.parser.add_option("-l",
"--leave",
dest="leave",
help="leave
(don't
retrieve)
self.parser.set_defaults(leave=False)
self.parser.add_option("-n",
"--num-instances",
dest="num_instances",
self.parser.set_defaults(num_instances=1)
self.parser.add_option("-i",
"--ignore-dirs",
dest="ignore",
help="directories
self.parser.add_option("-b",
"--batch-id",
dest="batch",
help="batch
print_command_help(self):
print('\nCommands:')
self.Commands.keys():
%s\t\t%s'
self.Commands[key]))
do_reset(self):
print('clearing
queue')
iq.delete_message(m)
print('delete
do_submit(self):
Submitter(self.sd)
s.submit_path(self.options.path,
self.options.ignore,
submitted'
print('Batch
Identifier:
do_start(self):
self.sd.get('ami_id')
self.sd.get('instance_type',
self.sd.get('security_group',
ami_id:
self.parser.error('ami_id
self.sd.has_section('Credentials'):
self.sd.add_section('Credentials')
self.sd.write(s)
ec2.get_all_images([ami_id])
img.run(user_data=s.getvalue(),
key_name=self.options.keypair,
max_count=self.options.num_instances,
instance_type=instance_type,
security_groups=[security_group])
AMI:
ami_id)
do_status(self):
input_queue
approximately
(iq.id,
iq.count()))
(ob.name,
total))
do_retrieve(self):
self.options.batch:
self.parser.error('batch
ResultProcessor(self.options.batch,
self.sd)
s.get_results(self.options.path,
get_file=(not
self.options.leave))
do_batches(self):
Batches:')
d.query("['type'='Batch']")
item.name)
output_domain
self.options,
self.options.help_commands:
self.print_command_help()
self.parser.error("config_file
self.config_file
ServiceDef(self.config_file)
self.args[1]
self.command):
method()
self.parser.error('command
recognized'
BS()
gethostname
mimetypes,
ServiceMessage(MHMessage):
for_key(self,
self.update(params)
key.path:
os.path.split(key.path)
mimetypes.guess_type(t[1])[0]
self['Content-Type']
os.stat(key.path)
time.gmtime(s[7])
self['FileAccessedDate']
time.gmtime(s[8])
self['FileModifiedDate']
time.gmtime(s[9])
self['FileCreateDate']
self['ContentType']
key.content_type
self['Host']
gethostname()
bucket_name:
self['InputKey']
self['Size']
ResultProcessor(object):
LogFileName
'log.csv'
batch_name,
sd,
self.batch
batch_name
timedelta.max
timedelta.min
datetime.max
datetime.min
calculate_stats(self,
parse_ts(msg['Service-Read'])
parse_ts(msg['Service-Write'])
self.max_time:
self.min_time:
elapsed_time.seconds
self.earliest_time:
self.latest_time:
log_message(self,
sorted(msg.keys())
self.LogFileName),
','.join(keys)
msg[key]
value.find(',')
','.join(values)
process_record(self,
self.log_message(record,
self.calculate_stats(record)
record['OutputKey'].split(',')
record['OutputBucket'])
record['Bucket'])
outputs:
get_file:
output.split(';')[0]
(key_name,
get_results_from_queue(self,
'Batch'
m['Batch']
self.batch:
self.process_record(m,
delete_msg:
self.queue.delete_message(m)
get_results_from_domain(self,
self.domain.query("['Batch'='%s']"
self.batch)
self.process_record(item,
get_results_from_bucket(self,
output_bucket')
get_results(self,
self.get_results_from_queue(path,
self.get_results_from_domain(path,
self.get_results_from_bucket(path)
self.log_fp.close()
print('%d
retrieved.'
self.avg_time
float(self.total_time)/self.num_files
print('Minimum
self.min_time.seconds)
print('Maximum
self.max_time.seconds)
print('Average
%f'
self.avg_time)
self.elapsed_time
self.latest_time-self.earliest_time
print('Elapsed
self.elapsed_time.seconds)
tput
((self.elapsed_time.seconds/60.0)
print('Throughput:
minute'
tput)
Service(ScriptBase):
ProcessingTime
super(Service,
ServiceDef(config_file)
self.sd.getint('retry_count',
self.loop_delay
self.sd.getint('loop_delay',
self.processing_time
self.sd.getint('processing_time',
self.input_queue
self.output_queue
mimetype_files:
mimetypes.init(mimetype_files)
split_key(key):
key.find(';')
key.split(';')
mtype
type.split('=')
mtype)
read_message(self):
boto.log.info('read_message')
self.input_queue.read(self.processing_time)
boto.log.info(message.get_body())
'Service-Read'
message[key]
message['Bucket']
message['InputKey']
message.get('OriginalFileName',
'in_file'))
boto.log.info('get_file:
key.get_contents_to_filename(os.path.join(self.working_dir,
file_path,
boto.log.info('putting
(file_path,
key.set_contents_from_filename(file_path)
save_results(self,
output_message):
output_keys
input_message['OutputBucket']
input_message['Bucket']
os.path.split(file)[1]
self.put_file(output_bucket,
output_keys.append('%s;type=%s'
output_message['OutputKey']
','.join(output_keys)
message['Service-Write']
message['Server']
'HOSTNAME'
os.environ['HOSTNAME']
'unknown'
message['Instance-ID']
self.output_queue:
SQS
self.output_queue.id)
self.output_queue.write(message)
self.output_domain.name)
'/'.join([message['Service-Write'],
message['Bucket'],
message['InputKey']])
self.output_domain.put_attributes(item_name,
boto.log.info('deleting
self.input_queue.id)
self.input_queue.delete_message(message)
cleanup(self):
self.sd.get('on_completion',
'shutdown')
'shutdown':
c.terminate_instances([self.instance_id])
notify=False):
self.retry_count:
input_message
self.read_message()
output_message
ServiceMessage(None,
input_message.get_body())
input_file
self.get_file(input_message)
self.process_file(input_file,
self.save_results(results,
self.write_message(output_message)
self.delete_message(input_message)
self.cleanup()
time.sleep(self.loop_delay)
boto.log.exception('Service
Failed')
Down'
self.shutdown()
ServiceDef(Config):
config_file,
aws_secret_access_key=None):
self.aws_secret_access_key
Config.get(self,
'Pyami',
script.split('.')[-1]
self).has_option(self.name,
get_obj(self,
name.find('queue')
obj.set_message_class(ServiceMessage)
name.find('bucket')
name.find('domain')
boto.lookup('sdb',
boto.services.service
SonOfMMM(Service):
self.sd.has_option('ffmpeg_args'):
self.sd.get('ffmpeg_args')
-i
self.output_mimetype
self.sd.get('output_mimetype')
self.sd.has_option('output_ext'):
self.sd.get('output_ext')
mimetypes.guess_extension(self.output_mimetype)
self.input_queue.read(1)
self.queue_files()
queue_files(self):
self.input_bucket.name)
self.input_bucket:
ServiceMessage()
{'OutputBucket'
self.output_bucket.name}
self.input_queue.write(m)
os.path.splitext(in_file_name)
out_file_name
base+self.output_ext)
(in_file_name,
out_file_name)
boto.log.info('running:\n%s'
self.run(command)
[(out_file_name,
self.output_mimetype)]
os.path.isfile(self.log_path):
self.output_bucket.new_key(self.log_file)
self).shutdown()
Submitter(object):
sd):
get_key_name(self,
fullpath,
fullpath[len(prefix):]
key_name.split(os.sep)
'/'.join(l)
self.queue.new_message()
m['OutputBucket']
self.output_bucket.name
self.queue.write(m)
submit_file(self,
self.get_key_name(path,
self.input_bucket.new_key(key_name)
k.update_metadata(metadata)
k.set_contents_from_filename(path,
num_cb=num_cb)
self.write_message(k,
submit_path(self,
ignore_dirs=None,
status=False,
metadata['Tags']
time.gmtime():
l.append(str(t))
metadata['Batch']
'_'.join(l)
self.output_domain.put_attributes(metadata['Batch'],
{'type'
'Batch'})
root,
dirs,
dirs:
dirs.remove(ignore)
files:
print('Submitting
self.submit_file(fullpath,
self.submit_file(path,
print('problem
(metadata['Batch'],
get_regions('ses',
connection_cls=SESConnection)
ses_exceptions
SESConnection(AWSAuthConnection):
'email.us-east-1.amazonaws.com'
['ses']
ct
ct}
UTF-8
v.encode('utf-8')
data=urllib.parse.urlencode(params)
list_markers
('VerifiedEmailAddresses',
'Identities',
'DkimTokens',
'DkimAttributes',
'VerificationAttributes',
'SendDataPoints')
item_markers
'item',
'entry')
boto.jsonresponse.Element(list_marker=list_markers,
item_marker=item_markers)
self._handle_error(response,
_handle_error(self,
ses_exceptions.SESAddressBlacklistedError
ses_exceptions.SESAddressNotVerifiedError
ses_exceptions.SESDailyQuotaExceededError
ses_exceptions.SESMaxSendingRateExceededError
ses_exceptions.SESDomainEndsWithDotError
whitespace"
ses_exceptions.SESLocalAddressCharacterError
whitespace."
ses_exceptions.SESIllegalAddressError
re.search('Identity.*is
verified',
ses_exceptions.SESIdentityNotVerifiedError
"Identity
"ownership
confirmed"
ses_exceptions.SESDomainNotConfirmedError
ownership
confirmed."
ExceptionToRaise(response.status,
exc_reason,
send_email(self,
cc_addresses=None,
bcc_addresses=None,
format='text',
reply_addresses=None,
return_path=None,
text_body=None,
html_body=None):
format.lower().strip()
"text":
"text_body;
"html_body;
'Message.Subject.Data':
return_path:
params['ReturnPath']
return_path
params['Message.Body.Html.Data']
params['Message.Body.Text.Data']
if(format
("text",
"html")):
ValueError("'format'
'html'")
(html_body
text_body)):
mail")
'Destination.ToAddresses.member')
cc_addresses:
cc_addresses,
'Destination.CcAddresses.member')
bcc_addresses:
bcc_addresses,
'Destination.BccAddresses.member')
reply_addresses:
reply_addresses,
'ReplyToAddresses.member')
self._make_request('SendEmail',
send_raw_email(self,
raw_message,
destinations=None):
isinstance(raw_message,
raw_message
raw_message.encode('utf-8')
'RawMessage.Data':
base64.b64encode(raw_message),
destinations:
destinations,
'Destinations.member')
self._make_request('SendRawEmail',
list_verified_email_addresses(self):
self._make_request('ListVerifiedEmailAddresses')
get_send_quota(self):
self._make_request('GetSendQuota')
get_send_statistics(self):
self._make_request('GetSendStatistics')
delete_verified_email_address(self,
self._make_request('DeleteVerifiedEmailAddress',
verify_email_address(self,
self._make_request('VerifyEmailAddress',
verify_domain_dkim(self,
self._make_request('VerifyDomainDkim',
set_identity_dkim_enabled(self,
dkim_enabled):
self._make_request('SetIdentityDkimEnabled',
'DkimEnabled':
get_identity_dkim_attributes(self,
self._make_request('GetIdentityDkimAttributes',
list_identities(self):
self._make_request('ListIdentities')
get_identity_verification_attributes(self,
self._make_request('GetIdentityVerificationAttributes',
verify_domain_identity(self,
self._make_request('VerifyDomainIdentity',
verify_email_identity(self,
self._make_request('VerifyEmailIdentity',
delete_identity(self,
identity):
self._make_request('DeleteIdentity',
set_identity_notification_topic(self,
notification_type,
sns_topic=None):
'NotificationType':
notification_type
sns_topic:
params['SnsTopic']
self._make_request('SetIdentityNotificationTopic',
set_identity_feedback_forwarding_enabled(self,
forwarding_enabled=True):
self._make_request('SetIdentityFeedbackForwardingEnabled',
'ForwardingEnabled':
forwarding_enabled
SESError(BotoServerError):
SESAddressNotVerifiedError(SESError):
SESIdentityNotVerifiedError(SESError):
SESDomainNotConfirmedError(SESError):
SESAddressBlacklistedError(SESError):
SESDailyQuotaExceededError(SESError):
SESMaxSendingRateExceededError(SESError):
SESDomainEndsWithDotError(SESError):
SESLocalAddressCharacterError(SESError):
SESIllegalAddressError(SESError):
get_regions('sns',
SNSConnection(AWSQueryConnection):
'sns_region_name',
'sns_region_endpoint',
'sns.us-east-1.amazonaws.com')
'sns_version',
'2010-03-31')
super(SNSConnection,
_build_dict_as_list_params(self,
dictionary,
sorted(dictionary.items(),
x:x[0])
kv,
zip(items,
len(items)+1))):
'%s.entry.%s'
params['%s.key'
params['%s.value'
get_all_topics(self,
self._make_request('ListTopics',
get_topic_attributes(self,
self._make_request('GetTopicAttributes',
set_topic_attributes(self,
'AttributeValue':
attr_value}
self._make_request('SetTopicAttributes',
actions):
'AWSAccountId.member')
actions,
'ActionName.member')
self._make_request('AddPermission',
self._make_request('RemovePermission',
create_topic(self,
self._make_request('CreateTopic',
delete_topic(self,
self._make_request('DeleteTopic',
publish(self,
topic=None,
message=None,
subject=None,
target_arn=None,
message_structure=None,
TypeError("'message'
parameter")
message}
params['Subject']
params['TopicArn']
params['TargetArn']
params['MessageStructure']
params['MessageAttributes.entry.{0}.Name'.format(i)]
params['MessageAttributes.entry.{0}.Value.DataType'.format(i)]
params['MessageAttributes.entry.{0}.Value.StringValue'.format(i)]
params['MessageAttributes.entry.{0}.Value.BinaryValue'.format(i)]
self._make_request('Publish',
subscribe(self,
endpoint}
self._make_request('Subscribe',
subscribe_sqs_queue(self,
queue.id.split('/')
q_arn
sid
hashlib.md5((topic
q_arn).encode('utf-8')).hexdigest()
self.subscribe(topic,
q_arn)
'Policy'
json.loads(attr['Policy'])
policy['Version']
'2008-10-17'
policy['Statement']
policy['Statement']:
s['Sid']
sid:
sid_exists:
'SQS:SendMessage',
{'AWS':
'*'},
q_arn,
'Sid':
sid,
{'StringLike':
{'aws:SourceArn':
topic}}}
policy['Statement'].append(statement)
queue.set_attribute('Policy',
json.dumps(policy))
confirm_subscription(self,
authenticate_on_unsubscribe=False):
token}
authenticate_on_unsubscribe:
params['AuthenticateOnUnsubscribe']
self._make_request('ConfirmSubscription',
unsubscribe(self,
subscription):
{'SubscriptionArn':
subscription}
self._make_request('Unsubscribe',
get_all_subscriptions(self,
self._make_request('ListSubscriptions',
get_all_subscriptions_by_topic(self,
self._make_request('ListSubscriptionsByTopic',
create_platform_application(self,
params['Platform']
self._make_request(action='CreatePlatformApplication',
set_platform_application_attributes(self,
self._make_request(action='SetPlatformApplicationAttributes',
get_platform_application_attributes(self,
self._make_request(action='GetPlatformApplicationAttributes',
list_platform_applications(self,
self._make_request(action='ListPlatformApplications',
list_endpoints_by_platform_application(self,
self._make_request(action='ListEndpointsByPlatformApplication',
delete_platform_application(self,
self._make_request(action='DeletePlatformApplication',
create_platform_endpoint(self,
token=None,
custom_user_data=None,
params['Token']
params['CustomUserData']
self._make_request(action='CreatePlatformEndpoint',
delete_endpoint(self,
self._make_request(action='DeleteEndpoint',
set_endpoint_attributes(self,
endpoint_arn=None,
self._make_request(action='SetEndpointAttributes',
get_endpoint_attributes(self,
self._make_request(action='GetEndpointAttributes',
verb=verb,
region_cls=SQSRegionInfo,
connection_cls=SQSConnection
Attributes(dict):
ResultEntry(dict):
self['id']
self['message_id']
'MD5OfMessageBody':
self['message_md5']
'SenderFault':
self['sender_fault']
self['error_code']
self['error_message']
BatchResults(object):
name.endswith('MessageBatchResultEntry'):
self.results.append(entry)
'BatchResultErrorEntry':
self.errors.append(entry)
BigMessage(RawMessage):
s3_url=None):
_get_bucket_key(self,
s3_url:
s3_url.startswith('s3://'):
s3_components
s3_url[5:].split('/',
s3_components[0]
len(s3_components)
s3_components[1]:
s3_components[1]
's3_url
s3://'
self._get_bucket_key(self.s3_url)
s3_bucket.new_key(key_name)
key.set_contents_from_file(value)
's3://%s/%s'
_get_s3_object(self,
self._get_bucket_key(s3_url)
s3_bucket.get_key(key_name)
URL:
self._get_s3_object(value)
self.s3_url:
self._get_s3_object(self.s3_url)
boto.sqs.batchresults
BatchResults
SQSError,
SQSConnection(AWSQueryConnection):
'sqs_region_name',
'sqs_region_endpoint',
'queue.amazonaws.com')
'sqs_version',
'2012-11-05')
SQSRegionInfo(self,
super(SQSConnection,
create_queue(self,
visibility_timeout=None):
visibility_timeout:
params['Attribute.1.Name']
params['Attribute.1.Value']
int(visibility_timeout)
self.get_object('CreateQueue',
delete_queue(self,
force_deletion=False):
self.get_status('DeleteQueue',
purge_queue(self,
self.get_status('PurgeQueue',
get_queue_attributes(self,
attribute='All'):
{'AttributeName'
self.get_object('GetQueueAttributes',
Attributes,
set_queue_attribute(self,
{'Attribute.Name'
'Attribute.Value'
self.get_status('SetQueueAttributes',
receive_message(self,
{'MaxNumberOfMessages'
number_messages}
params['VisibilityTimeout']
params['WaitTimeSeconds']
message_attributes,
'MessageAttributeName')
self.get_list('ReceiveMessage',
[('Message',
queue.message_class)],
message.receipt_handle}
'DeleteMessageBatchRequestEntry'
msg.receipt_handle
self.get_object('DeleteMessageBatch',
delete_message_from_handle(self,
receipt_handle):
receipt_handle}
send_message(self,
message_content,
delay_seconds=None,
{'MessageBody'
message_content}
delay_seconds:
params['DelaySeconds']
int(delay_seconds)
params['MessageAttribute.%s.Name'
params['MessageAttribute.%s.Value.DataType'
params['MessageAttribute.%s.Value.StringValue'
params['MessageAttribute.%s.Value.BinaryValue'
params['MessageAttribute.%s.Value.StringListValue'
params['MessageAttribute.%s.Value.BinaryListValue'
self.get_object('SendMessage',
Message,
send_message_batch(self,
'SendMessageBatchRequestEntry.%i'
params['%s.Id'
msg[0]
params['%s.MessageBody'
msg[1]
params['%s.DelaySeconds'
msg[2]
len(msg)
'.MessageAttribute'
sorted(msg[3].keys())
enumerate(keys):
msg[3][name]
'%s.%i.Name'
'%s.%i.Value.DataType'
'%s.%i.Value.StringValue'
'%s.%i.Value.BinaryValue'
'%s.%i.Value.StringListValue'
'%s.%i.Value.BinaryListValue'
self.get_object('SendMessageBatch',
change_message_visibility(self,
visibility_timeout}
self.get_status('ChangeMessageVisibility',
'ChangeMessageVisibilityBatchRequestEntry'
t[0].id
t[0].receipt_handle
'%s.%i.VisibilityTimeout'
self.get_object('ChangeMessageVisibilityBatch',
get_all_queues(self,
params['QueueNamePrefix']
self.get_list('ListQueues',
get_queue(self,
owner_acct_id=None):
owner_acct_id:
params['QueueOwnerAWSAccountId']=owner_acct_id
self.get_object('GetQueueUrl',
get_queue
get_dead_letter_source_queues(self,
{'QueueUrl':
queue.url}
self.get_list('ListDeadLetterSourceQueues',
'AWSAccountId'
'ActionName'
action_name}
self.get_status('AddPermission',
self.get_status('RemovePermission',
JSONMessage(MHMessage):
json.dumps(value)
boto.sqs.messageattributes
MessageAttributes
RawMessage(object):
self.set_body(body)
Attributes(self)
MessageAttributes(self)
len(self.encode(self._body))
'Body':
self.set_body(value)
'ReceiptHandle':
'MD5OfBody':
'MD5OfMessageAttributes':
endNode(self,
self.set_body(self.decode(self.get_body()))
get_body_encoded(self):
self.encode(self.get_body())
self.queue.delete_message(self)
change_visibility(self,
self.queue.connection.change_message_visibility(self.queue,
self.receipt_handle,
Message(RawMessage):
base64.b64encode(value).decode('utf-8')
boto.log.warning('Unable
MHMessage(Message):
super(MHMessage,
StringIO(value)
line.find(':')
line[0:delim]
line[delim+1:].strip()
msg[key.strip()]
(item[0],
item[1])
self._body:
KeyError(key)
self._body.keys()
self._body.values()
self._body.items()
self._body.update(d)
self._body.get(key,
EncodedMHMessage(MHMessage):
self).decode(value)
self).encode(value)
MessageAttributes(dict):
MessageAttributeValue(self)
MessageAttributeValue(dict):
self['data_type']
'StringValue':
self['string_value']
'BinaryValue':
self['binary_value']
'StringListValue':
self['string_list_value']
'BinaryListValue':
self['binary_list_value']
Queue(object):
message_class=Message):
'Queue(%s)'
_id(self):
urllib.parse.urlparse(self.url)[2]
property(_id)
_name(self):
urllib.parse.urlparse(self.url)[2].split('/')[2]
property(_name)
_arn(self):
self.id.split('/')
self.connection.region.name
'cn-north-1':
'aws-cn'
'arn:%s:sqs:%s:%s:%s'
partition,
self.connection.region.name,
parts[1],
parts[2])
property(_arn)
'QueueUrl':
'VisibilityTimeout':
set_message_class(self,
message_class):
attributes='All'):
self.connection.get_queue_attributes(self,
attributes)
set_attribute(self,
self.connection.set_queue_attribute(self,
get_timeout(self):
self.get_attributes('VisibilityTimeout')
int(a['VisibilityTimeout'])
set_timeout(self,
self.set_attribute('VisibilityTimeout',
retval:
self.connection.add_permission(self,
action_name)
self.connection.remove_permission(self,
self.get_messages(1,
visibility_timeout,
delay_seconds=None):
self.connection.send_message(self,
message.get_body_encoded(),
delay_seconds=delay_seconds,
message_attributes=message.message_attributes)
message.id
message.md5
new_msg.md5
write_batch(self,
self.connection.send_message_batch(self,
new_message(self,
self.message_class(self,
m.queue
get_messages(self,
self.connection.receive_message(
number_messages=num_messages,
visibility_timeout=visibility_timeout,
self.connection.delete_message(self,
self.connection.delete_message_batch(self,
self.connection.change_message_visibility_batch(self,
self.connection.delete_queue(self)
purge(self):
self.connection.purge_queue(self)
clear(self,
self.purge()
self.get_attributes('ApproximateNumberOfMessages')
int(a['ApproximateNumberOfMessages'])
count_slow(self,
dump(self,
vtimeout=10,
save_to_file(self,
save_to_filename(self,
self.save_to_file(fp,
save_to_filename
save_to_s3(self,
bucket.new_key('%s/%s'
key.set_contents_from_string(m.get_body())
load_from_s3(self,
self.id[1:]
bucket.list(prefix=prefix)
self.new_message(key.get_contents_as_string())
load_from_file(self,
Message(self,
print('writing
load_from_filename(self,
self.load_from_file(fp,
load_from_filename
SQSRegionInfo(RegionInfo):
super(SQSRegionInfo,
SQSConnection)
get_regions('sts',
Provider,
AssumedRole
DecodeAuthorizationMessage
_session_token_cache
STSConnection(AWSQueryConnection):
'sts.amazonaws.com'
'2011-06-15'
self._mutex
threading.Semaphore()
NO_CREDENTIALS_PROVIDED)
super(STSConnection,
_check_token_cache(self,
token_key,
window_seconds=60):
_session_token_cache.get(token_key,
boto.utils.parse_ts(token.expiration)
datetime.timedelta(seconds=window_seconds):
'Cached
expired'
_get_session_token(self,
mfa_serial_number:
self.get_object('GetSessionToken',
get_session_token(self,
force_new=False,
(self.region.name,
self.provider.access_key)
self._check_token_cache(token_key,
duration)
force_new
boto.log.debug('fetching
token_key)
self._mutex.acquire()
self._get_session_token(duration,
mfa_serial_number,
_session_token_cache[token_key]
self._mutex.release()
get_federation_token(self,
self.get_object('GetFederationToken',
assume_role(self,
duration_seconds=None,
role_session_name
self.get_object('AssumeRole',
assume_role_with_saml(self,
self.get_object('AssumeRoleWithSAML',
assume_role_with_web_identity(self,
provider_id=None,
params['ProviderId']
decode_authorization_message(self,
encoded_message):
'EncodedMessage':
encoded_message,
'DecodeAuthorizationMessage',
DecodeAuthorizationMessage,
Credentials(object):
from_json(cls,
json_doc):
json.loads(json_doc)
token.__dict__.update(d)
load(cls,
open(file_path)
json_doc
cls.from_json(json_doc)
'AccessKeyId':
'SecretAccessKey':
'SessionToken':
{'access_key':
self.access_key,
self.secret_key,
self.session_token,
self.expiration,
self.request_id}
json.dump(self.to_dict(),
is_expired(self,
time_offset_seconds=0):
time_offset_seconds:
datetime.timedelta(seconds=time_offset_seconds)
boto.utils.parse_ts(self.expiration)
delta.total_seconds()
FederationToken(object):
'FederatedUserId':
'PackedPolicySize':
AssumedRole(object):
credentials=None,
user=None):
'AssumedRoleUser':
arn=None,
assume_role_id=None):
assume_role_id
'AssumedRoleId':
DecodeAuthorizationMessage(object):
decoded_message=None):
decoded_message
'DecodedMessage':
get_regions('support',
connection_cls=SupportConnection)
CaseIdNotFound(JSONResponseError):
CaseCreationLimitExceeded(JSONResponseError):
AttachmentLimitExceeded(JSONResponseError):
DescribeAttachmentLimitExceeded(JSONResponseError):
AttachmentSetIdNotFound(JSONResponseError):
AttachmentSetExpired(JSONResponseError):
AttachmentIdNotFound(JSONResponseError):
AttachmentSetSizeLimitExceeded(JSONResponseError):
SupportConnection(AWSQueryConnection):
"2013-04-15"
"support.us-east-1.amazonaws.com"
"Support"
"AWSSupport_20130415"
"CaseCreationLimitExceeded":
exceptions.CaseCreationLimitExceeded,
"AttachmentLimitExceeded":
exceptions.AttachmentLimitExceeded,
"CaseIdNotFound":
exceptions.CaseIdNotFound,
"DescribeAttachmentLimitExceeded":
exceptions.DescribeAttachmentLimitExceeded,
"AttachmentSetIdNotFound":
exceptions.AttachmentSetIdNotFound,
"AttachmentSetExpired":
exceptions.AttachmentSetExpired,
"AttachmentIdNotFound":
exceptions.AttachmentIdNotFound,
"AttachmentSetSizeLimitExceeded":
exceptions.AttachmentSetSizeLimitExceeded,
super(SupportConnection,
add_attachments_to_set(self,
{'attachments':
self.make_request(action='AddAttachmentsToSet',
add_communication_to_case(self,
case_id=None,
{'communicationBody':
self.make_request(action='AddCommunicationToCase',
create_case(self,
service_code=None,
severity_code=None,
category_code=None,
issue_type=None,
'subject':
'communicationBody':
params['serviceCode']
params['severityCode']
params['categoryCode']
params['issueType']
self.make_request(action='CreateCase',
describe_attachment(self,
attachment_id):
{'attachmentId':
self.make_request(action='DescribeAttachment',
describe_cases(self,
case_id_list=None,
display_id=None,
include_resolved_cases=None,
include_communications=None):
params['caseIdList']
params['displayId']
params['includeResolvedCases']
params['includeCommunications']
self.make_request(action='DescribeCases',
describe_communications(self,
{'caseId':
self.make_request(action='DescribeCommunications',
describe_services(self,
service_code_list=None,
params['serviceCodeList']
self.make_request(action='DescribeServices',
describe_severity_levels(self,
self.make_request(action='DescribeSeverityLevels',
describe_trusted_advisor_check_refresh_statuses(self,
self.make_request(action='DescribeTrustedAdvisorCheckRefreshStatuses',
describe_trusted_advisor_check_result(self,
self.make_request(action='DescribeTrustedAdvisorCheckResult',
describe_trusted_advisor_check_summaries(self,
self.make_request(action='DescribeTrustedAdvisorCheckSummaries',
describe_trusted_advisor_checks(self,
language):
{'language':
language,
self.make_request(action='DescribeTrustedAdvisorChecks',
refresh_trusted_advisor_check(self,
check_id):
self.make_request(action='RefreshTrustedAdvisorCheck',
resolve_case(self,
case_id=None):
self.make_request(action='ResolveCase',
REGION_ENDPOINTS
load_regions().get('swf',
get_regions('swf',
connection_cls=boto.swf.layer1.Layer1)
SWFDomainAlreadyExistsError(SWFResponseError):
SWFLimitExceededError(SWFResponseError):
SWFOperationNotPermittedError(SWFResponseError):
SWFTypeAlreadyExistsError(SWFResponseError):
SWFWorkflowExecutionAlreadyStartedError(SWFResponseError):
Debug
'com.amazonaws.swf.service.model.SimpleWorkflowService'
_fault_excp
'com.amazonaws.swf.base.model#DomainAlreadyExistsFault':
swf_exceptions.SWFDomainAlreadyExistsError,
'com.amazonaws.swf.base.model#LimitExceededFault':
swf_exceptions.SWFLimitExceededError,
'com.amazonaws.swf.base.model#OperationNotPermittedFault':
swf_exceptions.SWFOperationNotPermittedError,
'com.amazonaws.swf.base.model#TypeAlreadyExistsFault':
swf_exceptions.SWFTypeAlreadyExistsError,
'com.amazonaws.swf.base.model#WorkflowExecutionAlreadyStartedFault':
swf_exceptions.SWFWorkflowExecutionAlreadyStartedError,
boto.config.get('SWF',
boto.swf.regions():
session_token,
_normalize_request_dict(cls,
list(data.keys()):
isinstance(data[item],
cls._normalize_request_dict(data[item])
{}):
json_request(self,
self._normalize_request_dict(data)
object_hook)
'application/json;
'amz-1.0',
excp_cls
self._fault_excp.get(fault_name,
excp_cls(response.status,
poll_for_activity_task(self,
identity=None):
self.json_request('PollForActivityTask',
respond_activity_task_completed(self,
self.json_request('RespondActivityTaskCompleted',
'result':
respond_activity_task_failed(self,
self.json_request('RespondActivityTaskFailed',
respond_activity_task_canceled(self,
self.json_request('RespondActivityTaskCanceled',
record_activity_task_heartbeat(self,
self.json_request('RecordActivityTaskHeartbeat',
poll_for_decision_task(self,
identity=None,
self.json_request('PollForDecisionTask',
respond_decision_task_completed(self,
execution_context=None):
self.json_request('RespondDecisionTaskCompleted',
'decisions':
'executionContext':
execution_context,
request_cancel_workflow_execution(self,
self.json_request('RequestCancelWorkflowExecution',
start_workflow_execution(self,
workflow_version,
self.json_request('StartWorkflowExecution',
execution_start_to_close_timeout,
tag_list,
task_start_to_close_timeout,
signal_workflow_execution(self,
self.json_request('SignalWorkflowExecution',
'signalName':
terminate_workflow_execution(self,
self.json_request('TerminateWorkflowExecution',
register_activity_type(self,
default_task_heartbeat_timeout=None,
default_task_schedule_to_close_timeout=None,
default_task_schedule_to_start_timeout=None,
self.json_request('RegisterActivityType',
'defaultTaskHeartbeatTimeout':
default_task_heartbeat_timeout,
'defaultTaskScheduleToCloseTimeout':
default_task_schedule_to_close_timeout,
'defaultTaskScheduleToStartTimeout':
default_task_schedule_to_start_timeout,
deprecate_activity_type(self,
self.json_request('DeprecateActivityType',
register_workflow_type(self,
default_child_policy=None,
default_execution_start_to_close_timeout=None,
self.json_request('RegisterWorkflowType',
'defaultChildPolicy':
default_child_policy,
'defaultExecutionStartToCloseTimeout':
default_execution_start_to_close_timeout,
deprecate_workflow_type(self,
self.json_request('DeprecateWorkflowType',
self.json_request('RegisterDomain',
'workflowExecutionRetentionPeriodInDays':
deprecate_domain(self,
self.json_request('DeprecateDomain',
list_activity_types(self,
self.json_request('ListActivityTypes',
describe_activity_type(self,
self.json_request('DescribeActivityType',
list_workflow_types(self,
self.json_request('ListWorkflowTypes',
describe_workflow_type(self,
self.json_request('DescribeWorkflowType',
workflow_version}
describe_workflow_execution(self,
workflow_id):
self.json_request('DescribeWorkflowExecution',
get_workflow_execution_history(self,
self.json_request('GetWorkflowExecutionHistory',
count_open_workflow_executions(self,
self.json_request('CountOpenWorkflowExecutions',
list_open_workflow_executions(self,
latest_date=None,
self.json_request('ListOpenWorkflowExecutions',
count_closed_workflow_executions(self,
self.json_request('CountClosedWorkflowExecutions',
workflow_id}
list_closed_workflow_executions(self,
self.json_request('ListClosedWorkflowExecutions',
self.json_request('ListDomains',
describe_domain(self,
self.json_request('DescribeDomain',
self.json_request('CountPendingDecisionTasks',
self.json_request('CountPendingActivityTasks',
Layer1Decisions(object):
schedule_activity_task(self,
activity_id,
heartbeat_timeout=None,
schedule_to_close_timeout=None,
schedule_to_start_timeout=None,
'ScheduleActivityTask'
o['scheduleActivityTaskDecisionAttributes']
attrs['activityType']
attrs['heartbeatTimeout']
attrs['scheduleToCloseTimeout']
attrs['scheduleToStartTimeout']
attrs['startToCloseTimeout']
request_cancel_activity_task(self,
activity_id):
'RequestCancelActivityTask'
o['requestCancelActivityTaskDecisionAttributes']
record_marker(self,
marker_name,
'RecordMarker'
o['recordMarkerDecisionAttributes']
attrs['markerName']
marker_name
complete_workflow_execution(self,
'CompleteWorkflowExecution'
o['completeWorkflowExecutionDecisionAttributes']
attrs['result']
fail_workflow_execution(self,
'FailWorkflowExecution'
o['failWorkflowExecutionDecisionAttributes']
attrs['reason']
cancel_workflow_executions(self,
'CancelWorkflowExecution'
o['cancelWorkflowExecutionsDecisionAttributes']
continue_as_new_workflow_execution(self,
workflow_type_version=None):
'ContinueAsNewWorkflowExecution'
o['continueAsNewWorkflowExecutionDecisionAttributes']
attrs['workflowTypeVersion']
start_timer(self,
start_to_fire_timeout,
timer_id,
control=None):
'StartTimer'
o['startTimerDecisionAttributes']
attrs['startToFireTimeout']
start_to_fire_timeout
cancel_timer(self,
timer_id):
'CancelTimer'
o['cancelTimerDecisionAttributes']
signal_external_workflow_execution(self,
run_id=None,
'SignalExternalWorkflowExecution'
o['signalExternalWorkflowExecutionDecisionAttributes']
attrs['signalName']
signal_name
request_cancel_external_workflow_execution(self,
'RequestCancelExternalWorkflowExecution'
o['requestCancelExternalWorkflowExecutionDecisionAttributes']
start_child_workflow_execution(self,
'StartChildWorkflowExecution'
o['startChildWorkflowExecutionDecisionAttributes']
attrs['workflowType']
DEFAULT_CREDENTIALS
set_default_credentials(aws_access_key_id,
DEFAULT_CREDENTIALS.update({
SWFBase(object):
credkey
'aws_secret_access_key'):
DEFAULT_CREDENTIALS.get(credkey):
credkey,
DEFAULT_CREDENTIALS[credkey])
kwarg,
kwargs[kwarg])
self._swf
Layer1(self.aws_access_key_id,
region=self.region)
str(self.name)
'version'):
'version'))
0x%x>'
rep_str,
Domain(SWFBase):
retention
@wraps(Layer1.describe_domain)
self._swf.describe_domain(self.name)
@wraps(Layer1.deprecate_domain)
self._swf.deprecate_domain(self.name)
@wraps(Layer1.register_domain)
register(self):
self._swf.register_domain(self.name,
str(self.retention),
@wraps(Layer1.list_activity_types)
activities(self,
act_types
self._swf.list_activity_types(self.name,
act_args
act_types['typeInfos']:
act_ident
act_args.update(act_ident)
act_args.update({
act_objects.append(ActivityType(**act_args))
@wraps(Layer1.list_workflow_types)
workflows(self,
wf_types
self._swf.list_workflow_types(self.name,
wf_args
wf_types['typeInfos']:
wf_ident
wf_args.update(wf_ident)
wf_args.update({
wf_objects.append(WorkflowType(**wf_args))
executions(self,
closed=False,
closed:
self._swf.list_closed_workflow_executions(self.name,
'oldest_date'
kwargs['oldest_date']
(3600
self._swf.list_open_workflow_executions(self.name,
exe_args
executions['executionInfos']:
nested_key
('execution',
'workflowType'):
nested_dict
exe_args.update(nested_dict)
exe_args.update({
exe_objects.append(WorkflowExecution(**exe_args))
@wraps(Layer1.count_pending_activity_tasks)
self._swf.count_pending_activity_tasks(self.name,
@wraps(Layer1.count_pending_decision_tasks)
self._swf.count_pending_decision_tasks(self.name,
Actor(SWFBase):
last_tasktoken
ActivityWorker(Actor):
@wraps(Layer1.respond_activity_task_canceled)
self._swf.respond_activity_task_canceled(task_token,
@wraps(Layer1.respond_activity_task_completed)
self._swf.respond_activity_task_completed(task_token,
@wraps(Layer1.respond_activity_task_failed)
fail(self,
self._swf.respond_activity_task_failed(task_token,
@wraps(Layer1.record_activity_task_heartbeat)
heartbeat(self,
self._swf.record_activity_task_heartbeat(task_token,
@wraps(Layer1.poll_for_activity_task)
self._swf.poll_for_activity_task(self.domain,
task.get('taskToken')
Decider(Actor):
@wraps(Layer1.respond_decision_task_completed)
isinstance(decisions,
Layer1Decisions):
decisions._data
self._swf.respond_decision_task_completed(task_token,
@wraps(Layer1.poll_for_decision_task)
self._swf.poll_for_decision_task(self.domain,
decision_task.get('taskToken')
WorkflowType(SWFBase):
'TERMINATE'
@wraps(Layer1.describe_workflow_type)
self._swf.describe_workflow_type(self.domain,
@wraps(Layer1.register_workflow_type)
'default_execution_start_to_close_timeout':
'default_child_policy':
self._swf.register_workflow_type(self.domain,
@wraps(Layer1.deprecate_workflow_type)
self._swf.deprecate_workflow_type(self.domain,
@wraps(Layer1.start_workflow_execution)
'workflow_id'
'%s-%s-%i'
time.time())
def_attr
('task_list',
'child_policy'):
kwargs[def_attr]
kwargs.get(def_attr,
def_attr))
self._swf.start_workflow_execution(self.domain,
**kwargs)['runId']
WorkflowExecution(name=self.name,
version=self.version,
runId=run_id,
domain=self.domain,
workflowId=workflow_id,
aws_access_key_id=self.aws_access_key_id,
aws_secret_access_key=self.aws_secret_access_key)
WorkflowExecution(SWFBase):
workflowId
runId
@wraps(Layer1.signal_workflow_execution)
self._swf.signal_workflow_execution(self.domain,
@wraps(Layer1.terminate_workflow_execution)
self._swf.terminate_workflow_execution(self.domain,
@wraps(Layer1.get_workflow_execution_history)
history(self,
self._swf.get_workflow_execution_history(self.domain,
**kwargs)['events']
@wraps(Layer1.describe_workflow_execution)
self._swf.describe_workflow_execution(self.domain,
self.workflowId)
@wraps(Layer1.request_cancel_workflow_execution)
request_cancel(self):
self._swf.request_cancel_workflow_execution(self.domain,
self.runId)
ActivityType(SWFBase):
@wraps(Layer1.deprecate_activity_type)
self._swf.deprecate_activity_type(self.domain,
@wraps(Layer1.describe_activity_type)
self._swf.describe_activity_type(self.domain,
@wraps(Layer1.register_activity_type)
'default_task_heartbeat_timeout':
'600',
'default_task_schedule_to_close_timeout':
'3900',
'default_task_schedule_to_start_timeout':
self._swf.register_activity_type(self.domain,
__author__
"Benjamin
Peterson
<benjamin@python.org>"
"1.9.0"
sys.maxsize
basestring,
types.ClassType)
sys.platform.startswith("java"):
len(X())
OverflowError:
63)
X
_add_doc(func,
_import_module(name):
__import__(name)
sys.modules[name]
_LazyDescr(object):
tp):
Invokes
__set__.
delattr(obj.__class__,
MovedModule(_LazyDescr):
new=None):
super(MovedModule,
_module
getattr(_module,
_LazyModule(types.ModuleType):
super(_LazyModule,
self.__class__.__doc__
["__doc__",
"__name__"]
[attr.name
self._moved_attributes]
MovedAttribute(_LazyDescr):
old_mod,
new_mod,
old_attr=None,
new_attr=None):
super(MovedAttribute,
old_mod
self.attr)
_SixMetaPathImporter(object):
six_module_name):
six_module_name
self.known_modules
_add_module(self,
mod,
*fullnames):
fullnames:
_get_module(self,
self.known_modules:
__get_module(self,
self.known_modules[fullname]
ImportError("This
know
isinstance(mod,
mod._resolve()
mod.__loader__
is_package(self,
hasattr(self.__get_module(fullname),
"__path__")
get_code(self,
eventually
raises
get_source
_importer
_SixMetaPathImporter(__name__)
_MovedItems(_LazyModule):
MovedAttribute("cStringIO",
"cStringIO",
"io",
"StringIO"),
MovedAttribute("filter",
"ifilter",
"filter"),
MovedAttribute("filterfalse",
"ifilterfalse",
"filterfalse"),
MovedAttribute("input",
"raw_input",
"input"),
MovedAttribute("intern",
"sys"),
MovedAttribute("map",
"imap",
"map"),
MovedAttribute("getcwd",
"getcwdu",
"getcwd"),
MovedAttribute("getcwdb",
"getcwd",
"getcwdb"),
MovedAttribute("range",
MovedAttribute("reload_module",
"importlib"
"imp",
"reload"),
MovedAttribute("reduce",
"functools"),
MovedAttribute("shlex_quote",
"pipes",
"shlex",
"quote"),
MovedAttribute("StringIO",
"StringIO",
"io"),
MovedAttribute("UserDict",
"UserDict",
MovedAttribute("UserList",
"UserList",
MovedAttribute("UserString",
"UserString",
MovedAttribute("xrange",
MovedAttribute("zip",
"izip",
"zip"),
MovedAttribute("zip_longest",
"izip_longest",
"zip_longest"),
MovedModule("builtins",
"__builtin__"),
MovedModule("configparser",
"ConfigParser"),
MovedModule("copyreg",
"copy_reg"),
MovedModule("dbm_gnu",
"gdbm",
"dbm.gnu"),
MovedModule("_dummy_thread",
"dummy_thread",
"_dummy_thread"),
MovedModule("http_cookiejar",
"cookielib",
"http.cookiejar"),
MovedModule("http_cookies",
"Cookie",
"http.cookies"),
MovedModule("html_entities",
"htmlentitydefs",
"html.entities"),
MovedModule("html_parser",
"HTMLParser",
"html.parser"),
MovedModule("http_client",
"httplib",
"http.client"),
MovedModule("email_mime_multipart",
"email.MIMEMultipart",
"email.mime.multipart"),
MovedModule("email_mime_nonmultipart",
"email.MIMENonMultipart",
"email.mime.nonmultipart"),
MovedModule("email_mime_text",
"email.MIMEText",
"email.mime.text"),
MovedModule("email_mime_base",
"email.MIMEBase",
"email.mime.base"),
MovedModule("BaseHTTPServer",
"BaseHTTPServer",
MovedModule("CGIHTTPServer",
"CGIHTTPServer",
MovedModule("SimpleHTTPServer",
"SimpleHTTPServer",
MovedModule("cPickle",
"cPickle",
"pickle"),
MovedModule("queue",
"Queue"),
MovedModule("reprlib",
"repr"),
MovedModule("socketserver",
"SocketServer"),
MovedModule("_thread",
"thread",
"_thread"),
MovedModule("tkinter",
"Tkinter"),
MovedModule("tkinter_dialog",
"Dialog",
"tkinter.dialog"),
MovedModule("tkinter_filedialog",
"FileDialog",
MovedModule("tkinter_scrolledtext",
"ScrolledText",
"tkinter.scrolledtext"),
MovedModule("tkinter_simpledialog",
"SimpleDialog",
MovedModule("tkinter_tix",
"Tix",
"tkinter.tix"),
MovedModule("tkinter_ttk",
"ttk",
"tkinter.ttk"),
MovedModule("tkinter_constants",
"Tkconstants",
"tkinter.constants"),
MovedModule("tkinter_dnd",
"Tkdnd",
"tkinter.dnd"),
MovedModule("tkinter_colorchooser",
"tkColorChooser",
"tkinter.colorchooser"),
MovedModule("tkinter_commondialog",
"tkCommonDialog",
"tkinter.commondialog"),
MovedModule("tkinter_tkfiledialog",
"tkFileDialog",
MovedModule("tkinter_font",
"tkFont",
"tkinter.font"),
MovedModule("tkinter_messagebox",
"tkMessageBox",
"tkinter.messagebox"),
MovedModule("tkinter_tksimpledialog",
"tkSimpleDialog",
MovedModule("urllib_parse",
".moves.urllib_parse",
MovedModule("urllib_error",
".moves.urllib_error",
MovedModule("urllib",
".moves.urllib",
MovedModule("urllib_robotparser",
MovedModule("xmlrpc_client",
"xmlrpclib",
"xmlrpc.client"),
MovedModule("xmlrpc_server",
"SimpleXMLRPCServer",
"xmlrpc.server"),
"win32":
MovedModule("winreg",
"_winreg"),
_moved_attributes:
_importer._add_module(attr,
"moves."
attr.name)
_MovedItems._moved_attributes
moves
_MovedItems(__name__
".moves")
_importer._add_module(moves,
"moves")
Module_six_moves_urllib_parse(_LazyModule):
MovedAttribute("ParseResult",
MovedAttribute("SplitResult",
MovedAttribute("parse_qs",
MovedAttribute("parse_qsl",
MovedAttribute("urldefrag",
MovedAttribute("urljoin",
MovedAttribute("urlparse",
MovedAttribute("urlsplit",
MovedAttribute("urlunparse",
MovedAttribute("urlunsplit",
MovedAttribute("quote",
MovedAttribute("quote_plus",
MovedAttribute("unquote",
MovedAttribute("unquote_plus",
MovedAttribute("urlencode",
MovedAttribute("splitquery",
MovedAttribute("splittag",
MovedAttribute("splituser",
MovedAttribute("uses_fragment",
MovedAttribute("uses_netloc",
MovedAttribute("uses_params",
MovedAttribute("uses_query",
MovedAttribute("uses_relative",
_urllib_parse_moved_attributes:
setattr(Module_six_moves_urllib_parse,
Module_six_moves_urllib_parse._moved_attributes
_importer._add_module(Module_six_moves_urllib_parse(__name__
".moves.urllib_parse"),
"moves.urllib_parse",
"moves.urllib.parse")
Module_six_moves_urllib_error(_LazyModule):
MovedAttribute("URLError",
MovedAttribute("HTTPError",
MovedAttribute("ContentTooShortError",
_urllib_error_moved_attributes:
setattr(Module_six_moves_urllib_error,
Module_six_moves_urllib_error._moved_attributes
_importer._add_module(Module_six_moves_urllib_error(__name__
".moves.urllib.error"),
"moves.urllib_error",
"moves.urllib.error")
Module_six_moves_urllib_request(_LazyModule):
MovedAttribute("urlopen",
MovedAttribute("install_opener",
MovedAttribute("build_opener",
MovedAttribute("pathname2url",
MovedAttribute("url2pathname",
MovedAttribute("getproxies",
MovedAttribute("Request",
MovedAttribute("OpenerDirector",
MovedAttribute("HTTPDefaultErrorHandler",
MovedAttribute("HTTPRedirectHandler",
MovedAttribute("HTTPCookieProcessor",
MovedAttribute("ProxyHandler",
MovedAttribute("BaseHandler",
MovedAttribute("HTTPPasswordMgr",
MovedAttribute("HTTPPasswordMgrWithDefaultRealm",
MovedAttribute("AbstractBasicAuthHandler",
MovedAttribute("HTTPBasicAuthHandler",
MovedAttribute("ProxyBasicAuthHandler",
MovedAttribute("AbstractDigestAuthHandler",
MovedAttribute("HTTPDigestAuthHandler",
MovedAttribute("ProxyDigestAuthHandler",
MovedAttribute("HTTPHandler",
MovedAttribute("HTTPSHandler",
MovedAttribute("FileHandler",
MovedAttribute("FTPHandler",
MovedAttribute("CacheFTPHandler",
MovedAttribute("UnknownHandler",
MovedAttribute("HTTPErrorProcessor",
MovedAttribute("urlretrieve",
MovedAttribute("urlcleanup",
MovedAttribute("URLopener",
MovedAttribute("FancyURLopener",
MovedAttribute("proxy_bypass",
_urllib_request_moved_attributes:
setattr(Module_six_moves_urllib_request,
Module_six_moves_urllib_request._moved_attributes
_importer._add_module(Module_six_moves_urllib_request(__name__
".moves.urllib.request"),
"moves.urllib_request",
"moves.urllib.request")
Module_six_moves_urllib_response(_LazyModule):
MovedAttribute("addbase",
MovedAttribute("addclosehook",
MovedAttribute("addinfo",
MovedAttribute("addinfourl",
_urllib_response_moved_attributes:
setattr(Module_six_moves_urllib_response,
Module_six_moves_urllib_response._moved_attributes
_importer._add_module(Module_six_moves_urllib_response(__name__
".moves.urllib.response"),
"moves.urllib_response",
"moves.urllib.response")
Module_six_moves_urllib_robotparser(_LazyModule):
MovedAttribute("RobotFileParser",
_urllib_robotparser_moved_attributes:
setattr(Module_six_moves_urllib_robotparser,
Module_six_moves_urllib_robotparser._moved_attributes
_importer._add_module(Module_six_moves_urllib_robotparser(__name__
".moves.urllib.robotparser"),
"moves.urllib_robotparser",
"moves.urllib.robotparser")
Module_six_moves_urllib(types.ModuleType):
_importer._get_module("moves.urllib_parse")
_importer._get_module("moves.urllib_error")
_importer._get_module("moves.urllib_request")
_importer._get_module("moves.urllib_response")
_importer._get_module("moves.urllib_robotparser")
['parse',
'robotparser']
_importer._add_module(Module_six_moves_urllib(__name__
"moves.urllib")
add_move(move):
move.name,
move)
remove_move(name):
delattr(_MovedItems,
moves.__dict__[name]
AttributeError("no
(name,))
"__func__"
"__self__"
"__closure__"
"__code__"
"__defaults__"
"__globals__"
"im_func"
"im_self"
"func_closure"
"func_code"
"func_defaults"
"func_globals"
advance_iterator(it):
it.next()
callable(obj):
any("__call__"
klass.__dict__
type(obj).__mro__)
unbound
create_bound_method
types.MethodType
Iterator
unbound.im_func
create_bound_method(func,
obj.__class__)
Iterator(object):
type(self).__next__(self)
_add_doc(get_unbound_function,
get_method_function
operator.attrgetter(_meth_func)
get_method_self
operator.attrgetter(_meth_self)
get_function_closure
operator.attrgetter(_func_closure)
get_function_code
operator.attrgetter(_func_code)
get_function_defaults
operator.attrgetter(_func_defaults)
get_function_globals
operator.attrgetter(_func_globals)
iter(d.keys(**kw))
iter(d.values(**kw))
iter(d.items(**kw))
iter(d.lists(**kw))
operator.methodcaller("keys")
operator.methodcaller("values")
operator.methodcaller("items")
d.iterkeys(**kw)
d.itervalues(**kw)
d.iteritems(**kw)
d.iterlists(**kw)
operator.methodcaller("viewkeys")
operator.methodcaller("viewvalues")
operator.methodcaller("viewitems")
_add_doc(iterkeys,
_add_doc(itervalues,
_add_doc(iteritems,
_add_doc(iterlists,
[values])
s.encode("latin-1")
struct.Struct(">B").pack
byte2int
operator.itemgetter(0)
indexbytes
operator.getitem
io.StringIO
io.BytesIO
"assertCountEqual"
sys.version_info[1]
"assertRaisesRegex"
"assertRegex"
unicode(s.replace(r'\\',
r'\\\\'),
"unicode_escape")
byte2int(bs):
ord(bs[0])
indexbytes(buf,
ord(buf[i])
functools.partial(itertools.imap,
StringIO.StringIO
"assertItemsEqual"
_add_doc(b,
_add_doc(u,
assertCountEqual(self,
_assertCountEqual)(*args,
assertRaisesRegex(self,
_assertRaisesRegex)(*args,
assertRegex(self,
_assertRegex)(*args,
exec_
"exec")
tp()
exec_(_code_,
_globs_=None,
_locs_=None):
sys._getframe(1)
frame.f_globals
frame.f_locals
exec()
raise_from(value,
from_value):
"print",
kwargs.pop("file",
str(data)
(isinstance(fp,
fp.encoding
getattr(fp,
"errors",
"strict"
data.encode(fp.encoding,
fp.write(data)
kwargs.pop("sep",
TypeError("sep
kwargs.pop("end",
TypeError("end
TypeError("invalid
print()")
unicode("\n")
unicode("
write(sep)
write(arg)
write(end)
3):
_print
kwargs.get("file",
kwargs.pop("flush",
_print(*args,
_add_doc(reraise,
wraps(wrapped,
assigned=functools.WRAPPER_ASSIGNMENTS,
updated=functools.WRAPPER_UPDATES):
wrapper(f):
functools.wraps(wrapped,
assigned,
updated)(f)
f.__wrapped__
functools.wraps
metaclass(meta):
add_metaclass(metaclass):
wrapper(cls):
orig_vars
cls.__dict__.copy()
orig_vars.get('__slots__')
isinstance(slots,
[slots]
slots_var
slots:
orig_vars.pop(slots_var)
orig_vars.pop('__dict__',
orig_vars.pop('__weakref__',
metaclass(cls.__name__,
cls.__bases__,
orig_vars)
python_2_unicode_compatible(klass):
'__str__'
klass.__dict__:
ValueError("@python_2_unicode_compatible
applied
__str__()."
klass.__name__)
klass.__unicode__
self.__unicode__().encode('utf-8')
366
@ReservedAssignment
globals().get("__spec__")
__spec__.submodule_search_locations
@UndefinedVariable
enumerate(sys.meta_path):
(type(importer).__name__
"_SixMetaPathImporter"
importer.name
__name__):
sys.meta_path[i]
sys.meta_path.append(_importer)
boto.vpc.vpc
boto.vpc.customergateway
boto.vpc.networkacl
NetworkAcl
boto.vpc.routetable
boto.vpc.internetgateway
boto.vpc.vpngateway
boto.vpc.dhcpoptions
boto.vpc.subnet
boto.vpc.vpnconnection
boto.vpc.vpc_peering_connection
connection_cls=VPCConnection)
VPCConnection(EC2Connection):
get_all_vpcs(self,
self.get_list('DescribeVpcs',
VPC)])
create_vpc(self,
{'CidrBlock':
instance_tenancy:
self.get_object('CreateVpc',
delete_vpc(self,
self.get_status('DeleteVpc',
enable_dns_support:
enable_dns_hostnames:
get_all_route_tables(self,
route_table_ids=None,
route_table_ids:
route_table_ids,
"RouteTableId")
self.get_list('DescribeRouteTables',
RouteTable)])
associate_route_table(self,
self.get_object('AssociateRouteTable',
result.associationId
disassociate_route_table(self,
{'AssociationId':
association_id}
self.get_status('DisassociateRouteTable',
create_route_table(self,
self.get_object('CreateRouteTable',
delete_route_table(self,
{'RouteTableId':
route_table_id}
self.get_status('DeleteRouteTable',
_replace_route_table_association(self,
route_table_id
self.get_object('ReplaceRouteTableAssociation',
replace_route_table_assocation(self,
dry_run=dry_run).status
replace_route_table_association_with_assoc(self,
dry_run=dry_run).newAssociationId
create_route(self,
self.get_status('CreateRoute',
replace_route(self,
self.get_status('ReplaceRoute',
delete_route(self,
self.get_status('DeleteRoute',
get_all_network_acls(self,
network_acl_ids=None,
network_acl_ids:
network_acl_ids,
"NetworkAclId")
self.get_list('DescribeNetworkAcls',
NetworkAcl)])
associate_network_acl(self,
subnet_id):
self.get_all_network_acls(filters=[('association.subnet-id',
subnet_id)])[0]
acl.associations
association.subnet_id
][0]
association.id,
network_acl_id
self.get_object('ReplaceNetworkAclAssociation',
result.newAssociationId
disassociate_network_acl(self,
self.get_all_subnets([subnet_id])[0].vpc_id
acls
self.get_all_network_acls(filters=[('vpc-id',
vpc_id),
('default',
'true')])
default_acl_id
acls[0].id
self.associate_network_acl(default_acl_id,
subnet_id)
create_network_acl(self,
vpc_id):
self.get_object('CreateNetworkAcl',
NetworkAcl)
delete_network_acl(self,
network_acl_id):
{'NetworkAclId':
network_acl_id}
self.get_status('DeleteNetworkAcl',
create_network_acl_entry(self,
self.get_status('CreateNetworkAclEntry',
replace_network_acl_entry(self,
self.get_status('ReplaceNetworkAclEntry',
delete_network_acl_entry(self,
egress=None):
rule_number
self.get_status('DeleteNetworkAclEntry',
get_all_internet_gateways(self,
internet_gateway_ids=None,
internet_gateway_ids:
internet_gateway_ids,
'InternetGatewayId')
self.get_list('DescribeInternetGateways',
InternetGateway)])
create_internet_gateway(self,
self.get_object('CreateInternetGateway',
delete_internet_gateway(self,
{'InternetGatewayId':
internet_gateway_id}
self.get_status('DeleteInternetGateway',
attach_internet_gateway(self,
self.get_status('AttachInternetGateway',
detach_internet_gateway(self,
self.get_status('DetachInternetGateway',
get_all_customer_gateways(self,
customer_gateway_ids=None,
customer_gateway_ids:
customer_gateway_ids,
'CustomerGatewayId')
self.get_list('DescribeCustomerGateways',
CustomerGateway)])
create_customer_gateway(self,
bgp_asn,
bgp_asn}
self.get_object('CreateCustomerGateway',
delete_customer_gateway(self,
{'CustomerGatewayId':
customer_gateway_id}
self.get_status('DeleteCustomerGateway',
get_all_vpn_gateways(self,
vpn_gateway_ids=None,
vpn_gateway_ids:
vpn_gateway_ids,
'VpnGatewayId')
self.get_list('DescribeVpnGateways',
VpnGateway)])
create_vpn_gateway(self,
type}
self.get_object('CreateVpnGateway',
delete_vpn_gateway(self,
self.get_status('DeleteVpnGateway',
attach_vpn_gateway(self,
self.get_object('AttachVpnGateway',
detach_vpn_gateway(self,
self.get_status('DetachVpnGateway',
get_all_subnets(self,
subnet_ids=None,
subnet_ids:
'SubnetId')
self.get_list('DescribeSubnets',
Subnet)])
create_subnet(self,
self.get_object('CreateSubnet',
delete_subnet(self,
self.get_status('DeleteSubnet',
get_all_dhcp_options(self,
dhcp_options_ids=None,
dhcp_options_ids:
dhcp_options_ids,
'DhcpOptionsId')
self.get_list('DescribeDhcpOptions',
DhcpOptions)])
create_dhcp_options(self,
domain_name_servers=None,
ntp_servers=None,
netbios_name_servers=None,
netbios_node_type=None,
params['DhcpConfiguration.%d.Key'
(key_counter,)]
enumerate(value,
'DhcpConfiguration.%d.Value.%d'
key_counter,
'DhcpConfiguration.%d.Value.1'
(key_counter,)
domain_name:
domain_name_servers:
domain_name_servers)
ntp_servers:
ntp_servers)
netbios_name_servers:
netbios_name_servers)
netbios_node_type:
netbios_node_type)
self.get_object('CreateDhcpOptions',
delete_dhcp_options(self,
dhcp_options_id}
self.get_status('DeleteDhcpOptions',
associate_dhcp_options(self,
self.get_status('AssociateDhcpOptions',
get_all_vpn_connections(self,
vpn_connection_ids=None,
vpn_connection_ids:
vpn_connection_ids,
'VpnConnectionId')
self.get_list('DescribeVpnConnections',
VpnConnection)])
create_vpn_connection(self,
static_routes_only=None,
isinstance(static_routes_only,
str(static_routes_only).lower()
params['Options.StaticRoutesOnly']
self.get_object('CreateVpnConnection',
delete_vpn_connection(self,
{'VpnConnectionId':
vpn_connection_id}
self.get_status('DeleteVpnConnection',
disable_vgw_route_propagation(self,
self.get_status('DisableVgwRoutePropagation',
enable_vgw_route_propagation(self,
self.get_status('EnableVgwRoutePropagation',
create_vpn_connection_route(self,
self.get_status('CreateVpnConnectionRoute',
delete_vpn_connection_route(self,
self.get_status('DeleteVpnConnectionRoute',
get_all_vpc_peering_connections(self,
vpc_peering_connection_ids=None,
vpc_peering_connection_ids:
vpc_peering_connection_ids,
'VpcPeeringConnectionId')
dict(filters))
self.get_list('DescribeVpcPeeringConnections',
VpcPeeringConnection)])
create_vpc_peering_connection(self,
peer_vpc_id,
peer_owner_id=None,
'PeerVpcId':
peer_vpc_id
params['PeerOwnerId']
self.get_object('CreateVpcPeeringConnection',
delete_vpc_peering_connection(self,
self.get_status('DeleteVpcPeeringConnection',
reject_vpc_peering_connection(self,
self.get_status('RejectVpcPeeringConnection',
accept_vpc_peering_connection(self,
self.get_object('AcceptVpcPeeringConnection',
get_all_classic_link_vpcs(self,
self.get_list('DescribeVpcClassicLink',
VPC)],
attach_classic_link_vpc(self,
hasattr(group,
'id'):
self.get_status('AttachClassicLinkVpc',
detach_classic_link_vpc(self,
self.get_status('DetachClassicLinkVpc',
disable_vpc_classic_link(self,
self.get_status('DisableVpcClassicLink',
enable_vpc_classic_link(self,
self.get_status('EnableVpcClassicLink',
CustomerGateway(TaggedEC2Object):
super(CustomerGateway,
'CustomerGateway:%s'
'bgpAsn':
DhcpValueSet(list):
DhcpConfigSet(dict):
'valueSet':
DhcpValueSet()
DhcpOptions(TaggedEC2Object):
'DhcpOptions:%s'
'dhcpConfigurationSet':
DhcpConfigSet()
InternetGateway(TaggedEC2Object):
'InternetGateway:%s'
InternetGatewayAttachment)])
'internetGatewayId':
InternetGatewayAttachment(object):
'InternetGatewayAttachment:%s'
Icmp(object):
'Icmp::code:%s,
type:%s)'
NetworkAcl(TaggedEC2Object):
'NetworkAcl:%s'
'entrySet':
NetworkAclEntry)])
NetworkAclAssociation)])
NetworkAclEntry(object):
PortRange()
Icmp()
'Acl:%s'
'portRange':
'icmpTypeCode':
'egress':
'protocol':
'ruleAction':
'ruleNumber':
NetworkAclAssociation(object):
'NetworkAclAssociation:%s'
'networkAclAssociationId':
PortRange(object):
'PortRange:(%s-%s)'
'from':
'to':
RouteTable(TaggedEC2Object):
'RouteTable:%s'
'routeSet':
Route)])
RouteAssociation)])
Route(object):
'Route:%s'
'gatewayId':
'origin':
RouteAssociation(object):
'RouteAssociation:%s'
'routeTableAssociationId':
'main':
Subnet(TaggedEC2Object):
super(Subnet,
'Subnet:%s'
'availableIpAddressCount':
VPC(TaggedEC2Object):
super(VPC,
'VPC:%s'
'isDefault':
'classicLinkEnabled':
self.connection.delete_vpc(self.id)
_get_status_then_update_vpc(self,
get_status_method,
vpc_list
get_status_method(
len(vpc_list):
updated_vpc
vpc_list[0]
self._update(updated_vpc)
self.connection.get_all_vpcs,
update_classic_link_enabled(self,
self.connection.get_all_classic_link_vpcs,
disable_classic_link(self,
self.connection.disable_vpc_classic_link(self.id,
enable_classic_link(self,
self.connection.enable_vpc_classic_link(self.id,
attach_classic_instance(self,
self.connection.attach_classic_link_vpc(
detach_classic_instance(self,
self.connection.detach_classic_link_vpc(
VpcInfo(object):
'VpcInfo:%s'
VpcPeeringConnectionStatus(object):
VpcPeeringConnection(TaggedEC2Object):
VpcPeeringConnectionStatus()
status_code(self):
self._status.code
status_message(self):
self._status.message
'VpcPeeringConnection:%s'
'requesterVpcInfo':
'accepterVpcInfo':
'expirationTime':
self.connection.delete_vpc_peering_connection(self.id)
vpc_peering_connection_list
self.connection.get_all_vpc_peering_connections(
len(vpc_peering_connection_list):
updated_vpc_peering_connection
vpc_peering_connection_list[0]
self._update(updated_vpc_peering_connection)
VpnConnectionOptions(object):
static_routes_only=None):
'VpnConnectionOptions'
'staticRoutesOnly':
VpnStaticRoute(object):
destination_cidr_block=None,
'VpnStaticRoute:
'source':
VpnTunnel(object):
outside_ip_address=None,
last_status_change=None,
accepted_route_count=None):
outside_ip_address
last_status_change
accepted_route_count
'VpnTunnel:
'outsideIpAddress':
'lastStatusChange':
'acceptedRouteCount':
boto.log.warning('Error
converting
int'
VpnConnection(TaggedEC2Object):
'VpnConnection:%s'
'vgwTelemetry':
VpnTunnel)])
'routes':
VpnStaticRoute)])
'options':
VpnConnectionOptions()
'vpnConnectionId':
'customerGatewayConfiguration':
self.connection.delete_vpn_connection(
VpnGateway(TaggedEC2Object):
'VpnGateway:%s'
self.attachments.append(att)
'attachments':
self.connection.attach_vpn_gateway(
'extensions'))
['sphinx.ext.autodoc',
'sphinx.ext.todo',
'githublinks']
autoclass_content
"both"
u'boto'
u'2009,2010,
Mitch
Garnaat'
boto.__version__
'boto_theme'
["."]
'botodoc'
('index',
'boto.tex',
u'boto
u'Mitch
Garnaat',
{'http://docs.python.org/':
github_project_url
'https://github.com/boto/boto/'
os.environ.get('SVN_REVISION',
html_title
"boto
v%s"
nodes,
slug,
app.config.github_project_url
"Configuration
'github_project_url'
(type_,
slug)
full_ref
relative)
'issues':
'issue'
utils.unescape(slug),
refuri=full_ref,
github_sha(name,
github_issue(name,
int(text)
inliner.reporter.error(
Github
Issue
line=lineno)
problem
inliner.problematic(rawtext,
[problem],
[msg]
'issues',
str(issue),
app.info('Adding
github
roles')
app.add_role('sha',
github_sha)
github_issue)
app.add_config_value('github_project_url',
'env')
RELEASE
re.compile(r'[0-9]+\.[0-9]+\.[0-9]+')
ISSUE
re.compile(r'#([0-9]+)')
REVLIST
'git
rev-list
develop
--abbrev-commit
--format="parents
%p%n%B%n~~~"
--max-count=200
develop'
TEMPLATE
=revisions
subprocess.check_output(REVLIST,
stderr=subprocess.STDOUT)
commit_list
revisions.split('~~~')[:-1]:
hunk.strip().splitlines()
commit
lines[0].split('
lines[1].split('
1)[1].split('
'.join(lines[2:])
RELEASE.search(message):
print('Found
stopping:')
print(message)
len(parents)
commit_list.append([commit,
message])
removals
re.compile(r'merge
#[0-9]+
[a-z0-9/_-]+',
re.compile(r"merge
branch
'[a-z0-9/_-]+'
[a-z0-9/_-]+",
re.compile(r'fix(es)?
[#0-9,
]+.?',
re.I)
commit_list:
issues
ISSUE.findall(message):
issues:
append.append(':issue:`{issue}`'.format(issue=issue))
issues.add(issue)
append.append(':sha:`{commit}`'.format(commit=commit))
'.join(append)
removal
removals:
removal.sub('',
message.strip()
original.strip()
'*
print(TEMPLATE.format(
version='?.?.?',
date=datetime.datetime.now().strftime('%Y/%m/%d'),
changes=changes
botocore.regions
EndpointResolver
print("Couldn't
botocore,
regen
data.")
EXISTING_ENDPOINTS_FILE
os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
'boto',
_load_endpoint_services(filename):
list(json.load(f))
StrictEndpointResolver(object):
SERVICE_RENAMES
'awslambda':
'lambda',
'cloudwatch':
'monitoring',
'ses':
'email',
'ec2containerservice':
'configservice':
'config',
resolver,
endpoint_data,
service_name_map=None):
self._endpoint_data
self.SERVICE_RENAMES
self._service_map
regions_for_service(self,
self._resolver.get_available_endpoints(
regions_for_partition(self,
list(partition_data['regions'])
'global'
r]
partitions_for_service(self,
get_available_partitions(self):
self._resolver.get_available_partitions()
get_hostname(self,
self._partition_for_region(region_name)
partition):
partition))
endpoint_config
self._resolver.construct_endpoint(
endpoint_config.get('sslCommonName',
endpoint_config.get('hostname'))
is_service_in_partition(self,
partition_data['services']
_partition_for_region(self,
partition['regions']:
_get_partition_data(self,
partition_name:
ValueError("Could
partition_name)
_endpoint_prefix(self,
self._service_map.get(
is_global_service(self,
'partitionEndpoint'
self._endpoint_data['partitions'][0]['services'].get(
StaticEndpointBuilder(object):
resolver):
build_static_endpoints(self,
service_names):
service_names:
self._build_endpoints_for_service(name)
endpoints_for_service:
static_endpoints[name]
self._deal_with_special_cases(static_endpoints)
_build_endpoints_for_service(self,
self._resolver.is_global_service(service_name):
self._special_case_global_service(service_name)
self._resolver.regions_for_service(service_name):
self._resolver.get_hostname(service_name,
_special_case_global_service(self,
self._resolver.partitions_for_service(service_name):
region_names
self._resolver.regions_for_partition(
partition)
self._resolver.get_hostname(
_deal_with_special_cases(self,
static_endpoints):
static_endpoints:
static_endpoints['cloudsearchdomain']
static_endpoints['cloudsearch']
parser.add_argument('--overwrite',
parser.add_argument('--endpoints-file',
help=('Path
endpoints.json.
given,
endpoints.json
'bundled
used.'))
known_services_in_existing_endpoints
_load_endpoint_services(
EXISTING_ENDPOINTS_FILE)
args.endpoints_file:
open(args.endpoints_file)
session.get_data('endpoints')
EndpointResolver(endpoint_data)
strict_resolver
StrictEndpointResolver(resolver,
endpoint_data)
StaticEndpointBuilder(strict_resolver)
builder.build_static_endpoints(
known_services_in_existing_endpoints)
json_data
json.dumps(static_endpoints,
indent=4,
sort_keys=True)
args.overwrite:
open(EXISTING_ENDPOINTS_FILE,
f.write(json_data)
print(json_data)
ordereddict
nose.core
("Runs
and/or
integration
"Arguments
nosetests.
"See
nosetests
--help
information.")
argparse.ArgumentParser(description=description)
'--service-tests',
help="Run
service.
"run
tagged
"e.g
ec2")
known_args,
parser.parse_known_args()
service_attribute
known_args.service_tests:
attribute_args.extend(['-a',
'!notdefault,'
service_attribute])
attribute_args:
['-a',
'!notdefault']
remaining_args:
os.chdir(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
enumerate(remaining_args):
remaining_args[i]
'tests/unit'
all_args
[__file__]
print("nose
command:",
'.join(all_args))
run(argv=all_args):
sys.exit(main())
ListProperty
SimpleListModel(Model):
TestLists(object):
test_list_order(self):
assert(t.nums
assert(t.strs
"Foo"])
test_old_compat(self):
t._get_raw_item()
item['strs']
["A",
"B",
"C"]
sorted(item['strs'])
i2.sort()
assert(i1
test_query_equals(self):
["Bizzle",
"Bar"]
assert(SimpleListModel.find(strs="Bizzle").count()
assert(SimpleListModel.find(strs="Bar").count()
assert(SimpleListModel.find(strs=["Bar",
"Bizzle"]).count()
test_query_not_equals(self):
["Fizzle"]
"Fizzle").get_query()
"Fizzle"):
tt.strs
assert("Fizzle"
tt.strs)
log=
logging.getLogger('password_property_test')
PasswordPropertyTest(unittest.TestCase):
cls=self.test_model()
obj.delete()
hmac_hashfunc(self):
hashfunc(msg):
test_model(self,hashfunc=None):
password=PasswordProperty(hashfunc=hashfunc)
MyModel
test_custom_password_class(self):
hmac,
hashlib.md5
MyPassword(Password):
#hashlib.md5
#lambda
cls,msg:
hmac.new('mysecret',msg)
MyPasswordProperty(PasswordProperty):
data_type=MyPassword
type_name=MyPassword.__name__
password=MyPasswordProperty()#hashfunc=hashlib.md5)
MyModel()
myhashfunc('bar').hexdigest()
#hmac.new('mysecret','bar').hexdigest()
log.debug("\npassword=%s\nexpected=%s"
(obj.password,
id=
MyModel.get_by_id(id)
test_aaa_default_password_property(self):
self.test_model()
cls(id='passwordtest')
cls.get_by_id('passwordtest')
test_password_constructor_hashfunc(self):
myhashfunc=lambda
self.test_model(hashfunc=myhashfunc)
obj.password='hello'
myhashfunc('hello').hexdigest()
cls.get_by_id(id)
log.debug("\npassword=%s"
os.path.dirname(
os.path.abspath(__file__)
"/../.."
log.setLevel(logging.INFO)
suite
unittest.TestLoader().loadTestsFromTestCase(PasswordPropertyTest)
unittest.TextTestRunner(verbosity=2).run(suite)
SimpleModel(Model):
SubModel(SimpleModel):
ReferenceProperty(SimpleModel,
collection_name="reverse_ref")
TestQuerying(object):
o.name
o.strs
o.num
o.put()
cls.objs.append(o)
o2.name
"Referenced
o2.num
o2.put()
cls.objs.append(o2)
SubModel()
o3.name
o3.num
o3.ref
o3.put()
cls.objs.append(o3)
test_find(self):
assert(SimpleModel.find(name="Simple
assert(SimpleModel.find(name="Referenced
self.objs[1].id)
assert(SimpleModel.find(name="Sub
self.objs[2].id)
test_like_filter(self):
test_equals_filter(self):
test_or_filter(self):
["Simple
Object",
Object"])
test_and_filter(self):
%")
test_none_filter(self):
query.filter("ref
test_greater_filter(self):
>",
>=",
test_less_filter(self):
<",
<=",
test_query_on_list(self):
assert(SimpleModel.find(strs="A").next().id
assert(SimpleModel.find(strs="B").next().id
assert(SimpleModel.find(strs="C").next().id
test_like(self):
query.filter("strs
"%oo%")
query.get_query()
TestDBHandler(object):
cls.sequences
cls.sequences:
s.delete()
test_sequence_generator_no_rollover(self):
SequenceGenerator("ABC")
assert(gen("AC")
"BA")
test_sequence_generator_with_rollover(self):
SequenceGenerator("ABC",
rollover=True)
test_sequence_simple_int(self):
Sequence
Sequence()
test_sequence_simple_string(self):
test_fib(self):
assert(fib(v,
lv+v)
fib(v,
test_sequence_fib(self):
Sequence(fnc=fib)
matter
reference
garunteed
test_sequence_string(self):
s.val
"Z")
S3PermissionsError
'{UserToken}...your
here...'
DEVPAY_HEADERS
'x-amz-security-token':
(DevPay)
c.create_bucket(bucket_name,
c.get_bucket(bucket_name,
'-log',
logging_bucket.set_as_logging_target(headers=DEVPAY_HEADERS)
target_prefix=bucket.name,
bucket.disable_logging(headers=DEVPAY_HEADERS)
c.delete_bucket(logging_bucket,
k.get_contents_to_file(fp,
headers.update(DEVPAY_HEADERS)
k.set_contents_from_filename('foobar',
bucket.get_all_keys(prefix='foo',
delimiter='/',
bucket.get_all_keys(maxkeys=5,
bucket.lookup('notthere',
bucket.lookup('has_metadata',
k.get_contents_as_string(headers=DEVPAY_HEADERS)
bucket.list(headers=DEVPAY_HEADERS)
'testnewline\n'
bucket.set_acl('public-read',
bucket.set_acl('private',
k.set_acl('public-read',
k.set_acl('private',
'foo@bar.com',
all:
c.delete_bucket(bucket,
'>>>
FPS
sources'
ComplexAmount
FPSTestCase(unittest.TestCase):
self.fps
FPSConnection(host='fps.sandbox.amazonaws.com')
self.activity
self.fps.get_account_activity(\
StartDate='2012-01-01')
self.activity.GetAccountActivityResult
self.transactions
result.Transaction
test_get_account_balance(self):
'GetAccountBalanceResult'))
self.assertTrue(hasattr(response.GetAccountBalanceResult,
'AccountBalance'))
'TotalBalance'))
self.assertIsInstance(accountbalance.TotalBalance,
ComplexAmount)
'AvailableBalances'))
availablebalances
accountbalance.AvailableBalances
self.assertTrue(hasattr(availablebalances,
'RefundBalance'))
test_complex_amount(self):
asfloat
float(accountbalance.TotalBalance.Value)
self.assertIn('.',
str(asfloat))
test_required_arguments(self):
self.fps.write_off_debt(AdjustmentAmount=123.45)
test_cbui_url(self):
'transactionAmount':
'pipelineName':
'SingleUse',
'returnURL':
'https://localhost/',
'paymentReason':
payment',
'callerReference':
self.fps.cbui_url(**inputs)
"cbui_url()
yields
{0}".format(result)
test_get_account_activity(self):
self.fps.get_account_activity(StartDate='2012-01-01')
'GetAccountActivityResult'))
response.GetAccountActivityResult
self.assertTrue(hasattr(result,
'BatchSize'))
int(result.BatchSize)
@unittest.skipUnless(advanced,
test_get_transaction(self):
len(self.transactions)
transactionid
self.transactions[0].TransactionId
self.fps.get_transaction(TransactionId=transactionid)
self.assertTrue(hasattr(result.GetTransactionResult,
'Transaction'))
test_bad_request(self):
self.fps.write_off_debt(CreditInstrumentId='foo',
AdjustmentAmount=123.45)
FPSConnection()
'expiry=08%2F2015&signature=ynDukZ9%2FG77uSJVb5YM0cadwHVwYKPMKOO3PNvgADbv6VtymgBxeOWEhED6KGHsGSvSJnMWDN%2FZl639AkRe9Ry%2F7zmn9CmiM%2FZkp1XtshERGTqi2YL10GwQpaH17MQqOX3u1cW4LlyFoLy4celUFBPq1WM2ZJnaNZRJIEY%2FvpeVnCVK8VIPdY3HMxPAkNi5zeF2BbqH%2BL2vAWef6vfHkNcJPlOuOl6jP4E%2B58F24ni%2B9ek%2FQH18O4kw%2FUJ7ZfKwjCCI13%2BcFybpofcKqddq8CuUJj5Ii7Pdw1fje7ktzHeeNhF0r9siWcYmd4JaxTP3NmLJdHFRq2T%2FgsF3vK9m3gw%3D%3D&signatureVersion=2&signatureMethod=RSA-SHA1&certificateUrl=https%3A%2F%2Ffps.sandbox.amazonaws.com%2Fcerts%2F090909%2FPKICert.pem&tokenID=A5BB3HUNAZFJ5CRXIPH72LIODZUNAUZIVP7UB74QNFQDSQ9MN4HPIKISQZWPLJXF&status=SC&callerReference=callerReferenceMultiUse1'
'http://vamsik.desktop.amazon.com:8080/ipn.jsp'
conn.verify_signature(endpoint,
ServiceCertVerificationTest(object):
test_certs(self):
self.assertTrue(len(self.regions)
self.regions:
('gov',
'cn-'):
region.name:
self.sample_service_call(c)
special_access_required:
boto.awslambda.exceptions
TestAWSLambda(unittest.TestCase):
self.awslambda
boto.connect_awslambda()
test_list_functions(self):
self.awslambda.list_functions()
self.assertIn('Functions',
test_resource_not_found_exceptions(self):
self.awslambda.get_function(function_name='non-existant-function')
boto.beanstalk.wrapper
Layer1Wrapper
BasicSuite(unittest.TestCase):
self.app_name
self.app_version
self.environment
self.beanstalk
MiscSuite(BasicSuite):
test_check_dns_availability(self):
self.beanstalk.check_dns_availability('amazon')
response.CheckDNSAvailabilityResponse,
'incorrect
returned')
self.assertFalse(result.available)
TestApplicationObjects(BasicSuite):
create_application(self):
self.addCleanup(partial(self.beanstalk.delete_application,
application_name=self.app_name))
test_create_delete_application_version(self):
app_result
self.assertIsInstance(app_result,
response.CreateApplicationResponse)
self.assertEqual(app_result.application.application_name,
self.app_name)
version_result
self.assertIsInstance(version_result,
response.CreateApplicationVersionResponse)
self.assertEqual(version_result.application_version.version_label,
self.app_version)
self.beanstalk.delete_application_version(
response.DeleteApplicationVersionResponse)
self.beanstalk.delete_application(
application_name=self.app_name
response.DeleteApplicationResponse)
test_create_configuration_template(self):
self.beanstalk.create_configuration_template(
template_name=self.template,
response.CreateConfigurationTemplateResponse)
self.assertEqual(result.solution_stack_name,
test_create_storage_location(self):
self.beanstalk.create_storage_location()
response.CreateStorageLocationResponse)
test_update_application(self):
self.beanstalk.update_application(application_name=self.app_name)
response.UpdateApplicationResponse)
test_update_application_version(self):
self.beanstalk.update_application_version(
response.UpdateApplicationVersionResponse)
GetSuite(BasicSuite):
test_describe_applications(self):
self.beanstalk.describe_applications()
response.DescribeApplicationsResponse)
test_describe_application_versions(self):
self.beanstalk.describe_application_versions()
response.DescribeApplicationVersionsResponse)
test_describe_configuration_options(self):
self.beanstalk.describe_configuration_options()
response.DescribeConfigurationOptionsResponse)
test_12_describe_environments(self):
self.beanstalk.describe_environments()
response.DescribeEnvironmentsResponse)
test_14_describe_events(self):
self.beanstalk.describe_events()
response.DescribeEventsResponse)
test_15_list_available_solution_stacks(self):
self.beanstalk.list_available_solution_stacks()
response.ListAvailableSolutionStacksResponse)
self.assertIn('32bit
result.solution_stacks)
TestsWithEnvironment(unittest.TestCase):
setUpClass(cls):
cls.app_name
cls.environment
cls.template
cls.beanstalk
cls.beanstalk.create_application(application_name=cls.app_name)
cls.beanstalk.create_configuration_template(
template_name=cls.template,
cls.app_version
cls.beanstalk.create_application_version(
version_label=cls.app_version)
cls.beanstalk.create_environment(cls.app_name,
cls.environment,
template_name=cls.template)
cls.wait_for_env(cls.environment)
tearDownClass(cls):
cls.beanstalk.delete_application(application_name=cls.app_name,
terminate_env_by_force=True)
cls.wait_for_env(cls.environment,
'Terminated')
wait_for_env(cls,
status='Ready'):
cls.env_ready(env_name,
env_ready(cls,
desired_status):
cls.beanstalk.describe_environments(
environment_names=env_name)
result.environments[0].status
desired_status
test_describe_environment_resources(self):
self.beanstalk.describe_environment_resources(
response.DescribeEnvironmentResourcesResponse)
test_describe_configuration_settings(self):
self.beanstalk.describe_configuration_settings(
response.DescribeConfigurationSettingsResponse)
test_request_environment_info(self):
self.beanstalk.request_environment_info(
response.RequestEnvironmentInfoResponse)
self.beanstalk.retrieve_environment_info(
response.RetrieveEnvironmentInfoResponse)
test_rebuild_environment(self):
self.beanstalk.rebuild_environment(
response.RebuildEnvironmentResponse)
test_restart_app_server(self):
self.beanstalk.restart_app_server(
response.RestartAppServerResponse)
test_update_configuration_template(self):
self.beanstalk.update_configuration_template(
template_name=self.template)
response.UpdateConfigurationTemplateResponse)
test_update_environment(self):
self.beanstalk.update_environment(
response.UpdateEnvironmentResponse)
CloudFormationCertVerificationTest(unittest.TestCase,
cloudformation
boto.cloudformation.regions()
conn.describe_stacks()
BASIC_EC2_TEMPLATE
"AWSTemplateFormatVersion":
"2010-09-09",
"AWS
CloudFormation
Sample
EC2InstanceSample",
"Parameter1":
"Parameter2":
"Mappings":
"RegionMap":
"us-east-1":
"AMI":
"ami-7f418316"
"Resources":
"Ec2Instance":
"AWS::EC2::Instance",
"Properties":
"ImageId":
"Fn::FindInMap":
"RegionMap",
"AWS::Region"
"AMI"
"UserData":
"Fn::Base64":
"Fn::Join":[
[{"Ref":
"Parameter1"},
{"Ref":
"Parameter2"}]
"InstanceId":
"InstanceId
"Ec2Instance"
"AZ":
"Availability
"AvailabilityZone"
"PublicIP":
"PublicIp"
"PrivateIP":
"PrivateIp"
"PublicDNS":
"PublicDnsName"
"PrivateDNS":
"PrivateDnsName"
TestCloudformationConnection(unittest.TestCase):
CloudFormationConnection()
'testcfnstack'
test_large_template_stack_size(self):
self.connection.create_stack(
'initial_value')])
self.addCleanup(self.connection.delete_stack,
self.stack_name)
self.connection.describe_stack_events(self.stack_name)
self.assertTrue(events)
self.connection.get_stack_policy(self.stack_name)
self.assertEqual(self.stack_name,
stack.stack_name)
stack.parameters]
'initial_value')],
self.connection.update_stack(
'updated_value')])
stacks[0].parameters]
'updated_value')],
boto.cloudhsm.exceptions
InvalidRequestException
TestCloudHSM(unittest.TestCase):
self.cloudhsm
boto.connect_cloudhsm()
test_hapgs(self):
'my-hapg'
self.cloudhsm.create_hapg(label=label)
hapg_arn
response['HapgArn']
self.addCleanup(self.cloudhsm.delete_hapg,
hapg_arn)
self.cloudhsm.list_hapgs()
self.assertIn(hapg_arn,
response['HapgList'])
test_validation_exception(self):
invalid_arn
'arn:aws:cloudhsm:us-east-1:123456789012:hapg-55214b8d'
self.assertRaises(InvalidRequestException):
self.cloudhsm.describe_hapg(invalid_arn)
boto.cloudsearch
boto.cloudsearch.regions()
self.assertTrue(resp.get('created',
self.assertEqual(domain.num_searchable_docs,
boto.cloudsearch2.regions()
CloudSearchConnection()
(resp['CreateDomainResponse']
self.assertTrue(resp.get('Created',
CloudTrailCertVerificationTest(unittest.TestCase,
boto.cloudtrail.regions()
conn.describe_trails()
DEFAULT_S3_POLICY
TestCloudTrail(unittest.TestCase):
test_cloudtrail(self):
boto.connect_cloudtrail()
len(res['trailList']):
self.fail('A
trail
account!')
iam.get_user()
response['get_user_response']['get_user_result']
['user']['user_id']
'cloudtrail-integ-{0}'.format(time())
DEFAULT_S3_POLICY.replace('<BucketName>',
bucket_name)\
.replace('<CustomerAccountID>',
account_id)\
.replace('<Prefix>/',
b.set_policy(policy)
cloudtrail.create_trail(trail={'Name':
bucket_name})
cloudtrail.update_trail(trail={'Name':
'IncludeGlobalServiceEvents':
trails
trails['trailList'][0]['Name'])
self.assertFalse(trails['trailList'][0]['IncludeGlobalServiceEvents'])
cloudtrail.start_logging(name='test')
self.assertTrue(status['IsLogging'])
cloudtrail.stop_logging(name='test')
self.assertFalse(status['IsLogging'])
cloudtrail.delete_trail(name='test')
b.list():
s3.delete_bucket(bucket_name)
boto.codedeploy.exceptions
ApplicationDoesNotExistException
TestCodeDeploy(unittest.TestCase):
self.codedeploy
boto.connect_codedeploy()
test_applications(self):
'my-boto-application'
self.codedeploy.create_application(application_name=application_name)
self.addCleanup(self.codedeploy.delete_application,
application_name)
self.codedeploy.list_applications()
self.assertIn(application_name,
response['applications'])
test_exception(self):
self.assertRaises(ApplicationDoesNotExistException):
self.codedeploy.get_application('some-non-existant-app')
CognitoTest(unittest.TestCase):
self.cognito_identity
boto.connect_cognito_identity()
self.cognito_sync
boto.connect_cognito_sync()
self.identity_pool_name
'myIdentityPool'
self.cognito_identity.create_identity_pool(
identity_pool_name=self.identity_pool_name,
allow_unauthenticated_identities=False
self.identity_pool_id
response['IdentityPoolId']
self.cognito_identity.delete_identity_pool(
boto.cognito.identity.exceptions
TestCognitoIdentity(CognitoTest):
test_cognito_identity(self):
self.cognito_identity.list_identity_pools(max_results=5)
expected_identity
self.identity_pool_id,
self.identity_pool_name}
self.assertIn(expected_identity,
response['IdentityPools'])
self.assertEqual(response['IdentityPoolName'],
self.identity_pool_name)
self.assertEqual(response['IdentityPoolId'],
self.identity_pool_id)
self.assertFalse(response['AllowUnauthenticatedIdentities'])
boto.cognito.sync.exceptions
TestCognitoSync(CognitoTest):
test_cognito_sync(self):
identity_pool_usage
response['IdentityPoolUsage']
self.assertEqual(identity_pool_usage['SyncSessionsCount'],
self.assertEqual(identity_pool_usage['DataStorage'],
boto.configservice.exceptions
NoSuchConfigurationRecorderException
TestConfigService(unittest.TestCase):
boto.connect_configservice()
test_describe_configuration_recorders(self):
test_handle_no_such_configuration_recorder(self):
self.assertRaises(NoSuchConfigurationRecorderException):
self.configservice.describe_configuration_recorders(
configuration_recorder_names=['non-existant-recorder'])
test_connect_to_non_us_east_1(self):
boto.configservice.connect_to_region('us-west-2')
DatapipelineCertVerificationTest(unittest.TestCase,
boto.datapipeline.regions()
TestDataPipeline(unittest.TestCase):
layer1.DataPipelineConnection()
self.sample_pipeline_objects
'MyworkerGroup'}],
'startDateTime',
'2012-09-25T17:00:00'},
'period',
hour'},
'endDateTime',
'2012-09-25T18:00:00'}],
'Schedule',
'ShellCommandActivity'},
'echo
hello'},
'parent',
'schedule',
'Schedule'}],
'SayHello',
'SayHello'}
self.connection.auth_service_name
'datapipeline'
self.connection.create_pipeline(name,
response['pipelineId']
self.addCleanup(self.connection.delete_pipeline,
get_pipeline_state(self,
self.connection.describe_pipelines([pipeline_id])
response['pipelineDescriptionList'][0]['fields']:
attr['key']
'@pipelineState':
attr['stringValue']
test_can_create_and_delete_a_pipeline(self):
self.connection.create_pipeline('name',
'unique_id',
self.connection.delete_pipeline(response['pipelineId'])
test_validate_pipeline(self):
self.create_pipeline('name2',
'unique_id2')
self.connection.validate_pipeline_definition(
self.sample_pipeline_objects,
test_put_pipeline_definition(self):
self.create_pipeline('name3',
'unique_id3')
self.connection.get_pipeline_definition(pipeline_id)
response['pipelineObjects']
self.assertEqual(len(objects),
self.assertEqual(objects[0]['id'],
self.assertEqual(objects[0]['name'],
self.assertEqual(objects[0]['fields'],
'MyworkerGroup'}])
test_activate_pipeline(self):
self.create_pipeline('name4',
'unique_id4')
self.connection.activate_pipeline(pipeline_id)
'SCHEDULED'
self.fail("Pipeline
scheduled
"after
attempts.")
self.connection.describe_objects(['Default'],
objects['pipelineObjects'][0]['fields'][0]
self.assertDictEqual(field,
{'stringValue':
'COMPONENT',
'@sphere'})
test_list_pipelines(self):
self.create_pipeline('name5',
'unique_id5')
pipeline_id_list
[p['id']
self.connection.list_pipelines()['pipelineIdList']]
self.assertTrue(pipeline_id
pipeline_id_list)
DirectConnectTest(unittest.TestCase):
boto.connect_directconnect()
conn.describe_connections()
self.assertTrue('connections'
self.assertIsInstance(response['connections'],
DynamoDBCertVerificationTest(unittest.TestCase,
boto.dynamodb.regions()
conn.layer1.list_tables()
DynamoDBValidationError
DynamoDBLayer1Test(unittest.TestCase):
self.hash_key_type},
self.range_key_type}}
self.write_units}
test_layer1_basic(self):
result['TableDescription']['TableName']
result_schema
result['TableDescription']['KeySchema']
result_schema['HashKeyElement']['AttributeName']
result_schema['HashKeyElement']['AttributeType']
result_schema['RangeKeyElement']['AttributeName']
result_schema['RangeKeyElement']['AttributeType']
result['TableDescription']['ProvisionedThroughput']
result['TableNames']
new_provisioned_throughput
new_read_units,
new_write_units}
c.update_table(table_name,
new_provisioned_throughput)
'UPDATING':
item1_data:
result['Item']
invalid_key
'bogus_key'},
c.get_item,
key=invalid_key)
attributes_to_get=attributes)
result['Item']:
'1'}}}
c.delete_item,
expected=expected)
'5'},
'PUT'},
['foobar']},
'ADD'}}
c.update_item(table_name,
item_size_overflow_text
'Text
padded'.zfill(64
32)
item_size_overflow_text},
'PUT'}}
self.assertRaises(DynamoDBValidationError,
c.update_item,
item2_data
item2_range},
item2_data)
item2_range}}
item3_data
item3_range},
['largeobject',
upload']},
item3_data)
key3
item3_range}}
c.query(table_name,
DynamoDB'},
'DynamoDB'}],
'BEGINS_WITH'})
c.scan(table_name,
{'Tags':
'table'}],
'CONTAINS'}})
key=key1)
key=key2)
key=key3)
test_binary_attributes(self):
self.provisioned_throughput)
self.hash_key_name:
base64.b64encode(b'\x01\x02\x03\x04').decode('utf-8')},
c.put_item(self.table_name,
c.get_item(self.table_name,
self.assertEqual(result['Item']['BinaryData'],
base64.b64encode(b'\x01\x02\x03\x04').decode('utf-8')})
boto.dynamodb.condition
BEGINS_WITH,
CONTAINS,
GT
DynamoDBLayer2Test(unittest.TestCase):
self.hash_key_proto_value
self.range_key_proto_value
'sample_data_%s'
create_sample_table(self):
self.dynamodb.create_schema(
test_layer2_basic(self):
schema2
c.create_schema('post_id',
table.name
table.schema.hash_key_name
table.schema.hash_key_type
get_dynamodb_type(self.hash_key_proto_value)
table.schema.range_key_name
table.schema.range_key_type
get_dynamodb_type(self.range_key_proto_value)
table.item_count
table.size_bytes
table2
self.create_table(table2_name,
schema2,
table2.refresh(wait_for_active=True)
table.update_throughput(new_read_units,
new_write_units)
'Public':
set(['index',
'primarykey',
'table']),
table.new_item(attrs=item1_attrs)
c.layer1.ResponseError
Exception("Item
table.get_item,
'bogus_key',
item1_range)
item1_copy
item1_copy.hash_key
item1.hash_key
item1_copy.range_key
item1.range_key
item1_attrs:
item1_copy[attr_name]
six.string_types)):
item1[attr_name]
item1_small
item1_small:
(item1_small.hash_key_name,
item1_small.range_key_name):
self.assertTrue(table.has_item(item1_key,
item1.delete,
expected_value=expected)
{'FooBar':
c.layer1.ResponseError:
item1.add_attribute('Replies',
removed_attr
'Public'
item1.delete_attribute(removed_attr)
removed_tag
item1_attrs['Tags'].copy().pop()
item1.delete_attribute('Tags',
set([removed_tag]))
replies_by_set
set(['Adam',
'Arnie'])
item1.put_attribute('RepliesBy',
item1.save(return_values='ALL_OLD')
'Attributes'
item1_updated
item1_updated['Replies']
item1_attrs['Replies']
self.assertFalse(removed_attr
self.assertTrue(removed_tag
item1_updated['Tags'])
self.assertTrue('RepliesBy'
self.assertTrue(item1_updated['RepliesBy']
item2_attrs
set(["index",
"table"]),
'LastPost2DateTime':
item2
table.new_item(item2_key,
item2_range,
item2_attrs)
item2.put()
item3_attrs
table.new_item(item3_key,
item3_attrs)
table2_item1_key
uuid.uuid4().hex
table2_item1_attrs
'DateTimePosted':
'25/1/2011
12:34:56
PM',
'Text':
think
rocks
table2_item1
table2.new_item(table2_item1_key,
attrs=table2_item1_attrs)
table2_item1.put()
range_key_condition=BEGINS_WITH('DynamoDB'),
request_limit=1,
max_results=1)
table.scan()
table.scan(scan_filter={'Replies':
GT(0)})
345.678
item3['IntAttr']
item3['FloatAttr']
item3['TrueBoolean']
item3['FalseBoolean']
5])
4.4,
5.5])
5.555])
'fie',
'baz'])
item3['IntSetAttr']
item3['FloatSetAttr']
item3['MixedSetAttr']
item3['StrSetAttr']
table.get_item(item3_key,
item4['IntAttr']
item4['FloatAttr']
bool(item4['TrueBoolean'])
bool(item4['FalseBoolean'])
item4['IntSetAttr']:
item4['FloatSetAttr']:
item4['MixedSetAttr']:
item4['StrSetAttr']:
[(item2_key,
item2_range),
(item3_key,
item3_range)])
len(response['Responses'][table.name]['Items'])
item4_key
item4_range
item4_attrs
item5_key
item5_range
3'
item5_attrs
table.new_item(item4_key,
item4_range,
item4_attrs)
item5
table.new_item(item5_key,
item5_range,
item5_attrs)
puts=[item4,
item5])
table.scan(scan_filter={'Tags':
CONTAINS('table')})
results.scanned_count
table.scan(request_limit=2,
max_results=5)
results.next_response()
table.scan(request_limit=6,
max_results=4)
len(list(results))
deletes=[(item4_key,
item4_range),
(item5_key,
item5_range)])
self.assertFalse(table.has_item(item1_key,
ret_vals
item2.delete(return_values='ALL_OLD')
ret_vals['Attributes'][self.hash_key_name]
ret_vals['Attributes'][self.range_key_name]
item3.delete()
table2_item1.delete()
test_binary_attrs(self):
Binary(b'\x01\x02\x03\x04'),
'BinarySequence':
Binary(b'\x03\x04')]),
self.assertEqual(retrieved['Message'],
text')
self.assertEqual(retrieved['Views'],
self.assertEqual(retrieved['Tags'],
upload']))
Binary(b'\x01\x02\x03\x04'))
b'\x01\x02\x03\x04')
self.assertEqual(retrieved['BinarySequence'],
Binary(b'\x03\x04')]))
test_put_decimal_attrs(self):
Decimal('1.12345678912345')
Decimal('1.12345678912345'))
@unittest.skipIf(six.PY3,
lossy_float_conversion
3.x")
test_lossy_float_conversion(self):
item['floatvalue']
1.12345678912345
'bar')['floatvalue']
self.assertNotEqual(1.12345678912345,
self.assertEqual(1.12345678912,
test_large_integers(self):
Decimal('129271300103398600')
Decimal('129271300103398600'))
129271300103398600)
test_put_single_letter_attr(self):
'foo1')
item.put_attribute('b',
item.save(return_values='UPDATED_NEW')
self.assertEqual(stored['Attributes'],
{'b':
TestDynamoDBTable(unittest.TestCase):
Schema.create(('foo',
('bar',
'testtable%s'
assertAllEqual(self,
*items):
items[0]
items[1:]:
self.assertEqual(first,
test_table_retrieval_parity(self):
created_table
created_table.refresh(wait_for_active=True)
retrieved_table
self.dynamodb.get_table(self.table_name)
constructed_table
self.dynamodb.table_from_schema(self.table_name,
self.schema)
self.assertAllEqual(created_table.name,
retrieved_table.name,
constructed_table.name)
self.assertAllEqual(created_table.schema,
retrieved_table.schema,
constructed_table.schema)
self.assertEqual(created_table.create_time,
retrieved_table.create_time)
self.assertEqual(created_table.status,
retrieved_table.status)
self.assertEqual(created_table.read_units,
retrieved_table.read_units)
self.assertEqual(created_table.write_units,
retrieved_table.write_units)
self.assertIsNone(constructed_table.create_time)
self.assertIsNone(constructed_table.status)
self.assertIsNone(constructed_table.read_units)
self.assertIsNone(constructed_table.write_units)
DynamoDB2CertVerificationTest(unittest.TestCase,
dynamodb2
boto.dynamodb2.regions()
conn.list_tables()
GlobalIncludeIndex,
GlobalAllIndex)
DynamoDBv2Test(unittest.TestCase):
RangeKey('friend_count',
KeysOnlyIndex('LastNameIndex',
RangeKey('last_name')
Table('users')
len(users_hit_api.schema))
self.assertEqual(users.throughput,
users_hit_api.throughput)
self.assertEqual(len(users.indexes),
len(users_hit_api.indexes))
batch.delete_item(username='alice',
friend_count=2)
'Smith',
self.assertTrue(users.has_item(username='jane',
friend_count=3))
self.assertFalse(users.has_item(
username='mrcarmichaeljones',
friend_count=72948
jane
users.get_item(username='jane',
friend_count=3)
'Doh'
self.assertTrue(jane.save())
client_1_jane
client_2_jane
client_1_jane['first_name']
self.assertTrue(client_1_jane.save())
check_name
self.assertEqual(check_name['first_name'],
client_2_jane['first_name']
'Joan'
self.assertRaises(exceptions.JSONResponseError,
client_2_jane.save)
self.assertTrue(client_2_jane.save(overwrite=True))
check_name_again
self.assertEqual(check_name_again['first_name'],
'Joan')
client_3_jane
client_4_jane
client_3_jane['favorite_band']
Me'
self.assertTrue(client_3_jane.save())
client_4_jane['first_name']
'Jacqueline'
self.assertTrue(client_4_jane.partial_save())
partial_jane
self.assertEqual(partial_jane['favorite_band'],
Me')
self.assertEqual(partial_jane['first_name'],
'Jacqueline')
sadie
Item(users,
'sadie',
'Sadie',
'favorite_band':
'Zedd',
self.assertTrue(sadie.partial_save())
serverside_sadie
username='sadie',
friend_count=7,
self.assertEqual(serverside_sadie['first_name'],
'Sadie')
attributes=('username',),
self.assertTrue(res['username']
['johndoe',])
friend_count__eq=4,
self.assertEqual(res['first_name'],
['first_name'])
self.assertEqual(res['username'],
query_filter={
'first_name__beginswith':
all_users
users.scan(limit=7)
'bob')
filtered_users
users.scan(limit=2,
username__beginswith='j')
users.get_item(username='johndoe',
friend_count=4)
johndoe.delete()
users.max_batch_get
'noone',
'nothere',
batch_users
batch_users.append(res)
self.assertIn(res['first_name'],
self.assertEqual(len(batch_users),
consistent=True)
c_batch_users
c_batch_users.append(res)
self.assertEqual(len(c_batch_users),
self.assertTrue(users.count()
users.query_count(
username__eq='bob',
self.assertEqual(count,
admins
Table.create('admins',
HashKey('username')
self.addCleanup(admins.delete)
admins.describe()
self.assertEqual(admins.throughput['read'],
self.assertEqual(admins.indexes,
exceptions.QueryError,
admins.query,
username__eq='johndoe'
users.query_2(username__eq='johndoe')
mau5_created
'dead',
set(['skrill',
'penny']),
self.assertTrue(mau5_created)
penny_created
'penny',
'Penny',
set([]),
self.assertTrue(penny_created)
mau5
username='mau5',
friend_count=2,
'first_name']
self.assertEqual(mau5['username'],
'mau5')
self.assertEqual(mau5['first_name'],
'dead')
self.assertTrue('last_name'
mau5)
test_unprocessed_batch_writes(self):
Table.create('slow_users',
range(500):
'Droid
#{0}'.format(i),
self.assertTrue(len(batch._unprocessed)
test_gsi(self):
Table.create('gsi_users',
GlobalKeysOnlyIndex('StuffIndex',
users.update(
'StuffIndex':
time.sleep(150)
test_gsi_with_just_hash_key(self):
Table.create('gsi_query_users',
GlobalIncludeIndex('UsernameIndex',
includes=['user_id',
'username'],
user_id__eq='24'
['alice'])
index='UsernameIndex'
test_query_with_limits(self):
Table.create('posts',
max_page_size=2
all_posts
list(results)
[post['posted_by']
all_posts],
['joe',
'joe']
self.assertTrue(results._fetches
test_query_with_reverse(self):
Table.create('more-posts',
reverse=False
test_query_after_describe_with_gsi(self):
Table.create('more_gsi_query_users',
'johndoe@johndoe.com',
'alice@alice.com',
'jane@jane.com',
Table('more_gsi_query_users')
users_hit_api.query_2(
test_update_table_online_indexing_support(self):
Table.create('online_indexing_support_users',
users.update_global_secondary_index(global_indexes={
users.update(global_indexes={
users.delete_global_secondary_index('EmailGSIIndex')
users.create_global_secondary_index(
'AddressGSIIndex',
HashKey('address',
data_type=STRING)
time.sleep(60*10)
DynamoDBv2Layer1Test(unittest.TestCase):
self.range_key_type,
self.write_units,
'MostRecentIndex',
'KEYS_ONLY',
lsi=None,
wait=True):
local_secondary_indexes=lsi
self.dynamodb.describe_table(table_name)
description['Table']['TableStatus'].lower()
test_integrated(self):
self.assertEqual(record_1['Item']['friends']['SS'],
'1366056789'},
lsi_results
self.dynamodb.query(
index_name='MostRecentIndex',
consistent_read=True
self.assertEqual(lsi_results['Count'],
self.dynamodb.query(self.table_name,
'1366050000'}
s_items
sorted([res['username']['S']
results['Items']])
self.assertEqual(s_items,
self.dynamodb.delete_item(self.table_name,
'Does'},
'1366058000'},
'1366056800'},
segment=0,
segment=1,
test_without_range_key(self):
self.assertEqual(johndoe['Item']['username']['S'],
self.assertEqual(johndoe['Item']['first_name']['S'],
self.assertEqual(johndoe['Item']['friends']['SS'],
test_throughput_exceeded_regression(self):
tiny_tablename
'TinyThroughput'
tiny
tiny_tablename,
'1366056669'},
'1366057000'},
self.dynamodb.scan(tiny_tablename)
test_recursive(self):
'friend_data':
'4'}}}
recursive_data
record_1['Item']['friend_data']['M']
self.assertEqual(recursive_data['username']['S'],
'alice')
self.assertEqual(recursive_data['friend_count']['N'],
'4')
EC2CertVerificationTest(unittest.TestCase,
conn.get_all_reservations()
telnetlib
EC2ConnectionTest(unittest.TestCase):
test_launch_permissions(self):
'963068290131'
c.get_all_images(owners=[user_id])
image.set_launch_permissions(group_names=['all'])
len(d['groups'])
image.remove_launch_permissions(group_names=['all'])
group1_name
c.create_security_group(group1_name,
group2_name
group2
c.create_security_group(group2_name,
group1_name:
c.get_all_security_groups([group1_name])
c.delete_security_group(group2_name)
group2_name:
c.get_all_images()
img_loc
'ec2-public-images/fedora-core4-apache.manifest.xml'
image.location
img_loc:
image.run(security_groups=[group.name])
print('\tinstance
instance.state)
telnetlib.Telnet()
group.authorize('tcp',
t.close()
group.revoke('tcp',
'shutting-down'
instance.state_code
instance.previous_state
'running'
instance.previous_state_code
c.create_key_pair(key_name)
c.get_all_key_pairs([key_name])
key_pair
c.delete_key_pair(key_name)
demo_paid_ami_id
'ami-bd9d78d4'
'A79EC0DB'
c.get_all_images([demo_paid_ami_id])
len(l[0].product_codes)
l[0].product_codes[0]
test_dry_run(self):
dry_run_msg
'Request
would
succeeded,
DryRun
set.'
c.get_all_images(dry_run=True)
instance_type='m1.small'
time.sleep(120)
c.stop_instances(
c.terminate_instances(
rs.instances[0].terminate()
test_can_get_all_instances_sigv4(self):
boto.ec2.connect_to_region('eu-central-1')
self.assertTrue(isinstance(connection.get_all_instances(),
AutoscaleCertVerificationTest(unittest.TestCase,
boto.ec2.autoscale.regions()
conn.get_all_groups()
AutoScalingGroup,
AdjustmentType,
MetricCollectionTypes,
AutoscaleConnectionTest(unittest.TestCase):
self.assertTrue(repr(c).startswith('AutoScaleConnection'))
self.assertIsInstance(group,
AutoScalingGroup)
activities
group.get_activities()
activities:
self.assertIsInstance(activity,
configs
configs:
self.assertIsInstance(config,
LaunchConfiguration)
policies
c.get_all_policies()
self.assertIsInstance(policy,
ScalingPolicy)
actions
c.get_all_scheduled_actions()
actions:
self.assertIsInstance(action,
ScheduledUpdateGroupAction)
c.get_all_autoscaling_instances()
self.assertIsInstance(instance,
Instance)
ptypes
c.get_all_scaling_process_types()
ptypes:
self.assertTrue(ptype,
ProcessType)
adjustments
c.get_all_adjustment_types()
adjustment
adjustments:
self.assertIsInstance(adjustment,
AdjustmentType)
c.get_all_metric_collection_types()
self.assertIsInstance(types,
LaunchConfiguration(name=lc_name,
instance_type='t1.micro')
lcs
lcs:
lc.name
lc_name:
'group-%s'
AutoScalingGroup(name=group_name,
launch_config=lc,
availability_zones=['us-east-1a'],
max_size=1)
c.create_auto_scaling_group(group)
Tag(key='foo',
value='bar',
resource_id=group_name,
propagate_at_launch=True)
c.create_or_update_tags([tag])
c.delete_tags([tag])
group.shutdown_instances()
group.instances:
group.delete()
lc.delete()
test_ebs_optimized_regression(self):
LaunchConfiguration(
name=lc_name,
ebs_optimized=True
self.addCleanup(c.delete_launch_configuration,
lc_name)
CloudWatchCertVerificationTest(unittest.TestCase,
cloudwatch
boto.ec2.cloudwatch.regions()
conn.describe_alarms()
CloudWatchConnectionTest(unittest.TestCase):
c.build_list_params(
test_build_put_params_one(self):
test_build_put_params_multiple_metrics(self):
'M',
test_build_put_params_multiple_dimensions(self):
dimensions=[{"D":
"V"},
{"D":
"W"}])
test_build_put_params_multiple_parameter_dimension(self):
[OrderedDict((("D1",
"W")))]
value=[1],
dimensions=dimensions)
'MetricData.member.1.Dimensions.member.2.Name':
'MetricData.member.1.Dimensions.member.2.Value':
test_build_get_params_multiple_parameter_dimension1(self):
"W")))
test_build_get_params_multiple_parameter_dimension2(self):
["V1",
"V2"]),
"W"),
("D3",
'V1',
'V2',
'Dimensions.member.3.Name':
'Dimensions.member.3.Value':
'Dimensions.member.4.Name':
'D3',
test_build_put_params_invalid(self):
accept
lists
lengths.")
test_get_metric_statistics(self):
c.list_metrics()[0]
datetime.timedelta(hours=24
14)
c.get_metric_statistics(
end,
m.name,
m.namespace,
'Sum'])
test_put_metric_data(self):
'unit-test-metric',
'boto-unit-test'
c.put_metric_data(namespace,
'Bytes')
test_describe_alarms(self):
make_request(*args,
Body(object):
Body()
c.make_request
make_request
alarms
c.describe_alarms()
self.assertEquals(alarms.next_token,
'mynexttoken')
self.assertEquals(alarms[0].name,
'FancyAlarm')
self.assertEquals(alarms[0].comparison,
'<')
self.assertEquals(alarms[0].dimensions,
[u'ANiceCronJob']})
self.assertEquals(alarms[1].name,
'SuperFancyAlarm')
self.assertEquals(alarms[1].comparison,
self.assertEquals(alarms[1].dimensions,
[u'ABadCronJob']})
ELBCertVerificationTest(unittest.TestCase,
boto.ec2.elb.regions()
conn.get_all_load_balancers()
ELBConnectionTest(unittest.TestCase):
ELBConnection()
'elb-boto-unit-test'
['us-east-1a']
self.balancer
'boto-elb-%s'
self.s3.create_bucket(self.bucket_name)
self.bucket.set_canned_acl('public-read-write')
self.addCleanup(self.cleanup_bucket,
self.bucket)
cleanup_bucket(self,
bucket.get_all_keys():
self.balancer.delete()
self.conn.build_list_params(
test_create_load_balancer(self):
self.assertEqual(self.balancer.name,
self.assertEqual(self.balancer.availability_zones,
self.availability_zones)
self.assertEqual(self.balancer.listeners,
test_create_load_balancer_listeners(self):
test_delete_load_balancer_listeners(self):
mod_listeners
(443,
"-mod"
self.mod_balancer
mod_name,
mod_listeners)
sorted([l.get_tuple()
mod_balancers[0].listeners]),
sorted(mod_listeners))
self.conn.delete_load_balancer_listeners(self.mod_balancer.name,
[443])
self.assertEqual([l.get_tuple()
mod_balancers[0].listeners],
mod_listeners[:1])
self.mod_balancer.delete()
test_create_load_balancer_listeners_with_policies(self):
app_policy_name
'app-policy'
self.conn.create_app_cookie_stickiness_policy(
'appcookie',
more_listeners[0][0],
test_create_load_balancer_backend_with_policies(self):
other_policy_name
'enable-proxy-protocol'
backend_port
self.conn.create_lb_policy(
other_policy_name,
'ProxyProtocolPolicyType',
{'ProxyProtocol':
self.conn.set_lb_policies_of_backend_server(
[other_policy_name])
self.assertEqual(balancers[0].policies.other_policies[0].policy_name,
self.assertEqual(balancers[0].backends[0].instance_port,
backend_port)
self.assertEqual(balancers[0].backends[0].policies[0].policy_name,
self.conn.set_lb_policies_of_backend_server(self.name,
test_create_load_balancer_complex_listeners(self):
complex_listeners
self.conn.create_load_balancer_listeners(
complex_listeners=complex_listeners
load_balancer_names=[self.name]
sorted(l.get_complex_tuple()
sorted([(80,
complex_listeners)
test_load_balancer_access_log(self):
attributes.access_log.enabled
attributes.access_log.s3_bucket_name
attributes.access_log.s3_bucket_prefix
'access-logs'
attributes.access_log.emit_interval
self.conn.modify_lb_attribute(self.balancer.name,
'accessLog',
attributes.access_log)
new_attributes
new_attributes.access_log.enabled)
self.assertEqual(self.bucket_name,
new_attributes.access_log.s3_bucket_name)
self.assertEqual('access-logs',
new_attributes.access_log.s3_bucket_prefix)
new_attributes.access_log.emit_interval)
test_load_balancer_get_attributes(self):
connection_draining
'ConnectionDraining')
self.assertEqual(connection_draining.enabled,
self.assertEqual(connection_draining.timeout,
access_log
'AccessLog')
self.assertEqual(access_log.enabled,
self.assertEqual(access_log.s3_bucket_name,
attributes.access_log.s3_bucket_name)
self.assertEqual(access_log.s3_bucket_prefix,
attributes.access_log.s3_bucket_prefix)
self.assertEqual(access_log.emit_interval,
attributes.access_log.emit_interval)
cross_zone_load_balancing
self.conn.get_lb_attribute(
'CrossZoneLoadBalancing')
self.assertEqual(cross_zone_load_balancing,
attributes.cross_zone_load_balancing.enabled)
change_and_verify_load_balancer_connection_draining(
attributes.connection_draining.enabled
attributes.connection_draining.timeout
self.conn.modify_lb_attribute(
'ConnectionDraining',
attributes.connection_draining)
self.assertEqual(enabled,
self.assertEqual(timeout,
test_load_balancer_connection_draining_config(self):
128)
self.change_and_verify_load_balancer_connection_draining(False)
64)
test_set_load_balancer_policies_of_listeners(self):
test_can_make_sigv4_call(self):
boto.ec2.elb.connect_to_region('eu-central-1')
connection.get_all_load_balancers()
self.assertTrue(isinstance(lbs,
TestVPCConnection(unittest.TestCase):
self.post_terminate_cleanups
boto.connect_vpc()
self.api.create_vpc('10.0.0.0/16')
self.subnet
self.api.create_subnet(self.vpc.id,
'10.0.0.0/24')
self.post_terminate_cleanups.append((self.api.delete_subnet,
(self.subnet.id,)))
post_terminate_cleanup(self):
fn,
self.post_terminate_cleanups:
fn(*args)
self.vpc:
self.api.delete_vpc(self.vpc.id)
terminate_instances(self):
self.terminate_instance(instance)
self.post_terminate_cleanup()
instance):
six.moves.range(300):
'terminated':
delete_elastic_ip(self,
eip):
new_eip
self.api.get_all_addresses([eip.public_ip])[0]
new_eip.association_id:
new_eip.disassociate()
new_eip.release()
test_multi_ip_create(self):
private_ip_address='10.0.0.21',
description="This
boto.",
private_ip_addresses=[
PrivateIPAddress(private_ip_address='10.0.0.22',
PrivateIPAddress(private_ip_address='10.0.0.23',
PrivateIPAddress(private_ip_address='10.0.0.24',
primary=False)])
self.api.run_instances(image_id='ami-a0cd60c9',
network_interfaces=interfaces)
self.addCleanup(self.terminate_instance,
self.api.get_all_reservations(instance_ids=[instance.id])
self.assertEqual(len(private_ip_addresses),
self.assertEqual(private_ip_addresses[0].private_ip_address,
'10.0.0.21')
self.assertEqual(private_ip_addresses[0].primary,
self.assertEqual(private_ip_addresses[1].private_ip_address,
'10.0.0.22')
self.assertEqual(private_ip_addresses[2].private_ip_address,
'10.0.0.23')
self.assertEqual(private_ip_addresses[3].private_ip_address,
'10.0.0.24')
test_associate_public_ip(self):
self.api.get_all_reservations(
instance_ids=[
self.assertTrue(interface.publicIp.count('.')
test_associate_elastic_ip(self):
associate_public_ip_address=False,
igw
self.api.create_internet_gateway()
self.api.attach_internet_gateway(igw.id,
self.vpc.id)
self.post_terminate_cleanups.append((self.api.detach_internet_gateway,
(igw.id,
self.vpc.id)))
self.post_terminate_cleanups.append((self.api.delete_internet_gateway,
(igw.id,)))
eip
self.api.allocate_address('vpc')
self.post_terminate_cleanups.append((self.delete_elastic_ip,
(eip,)))
eip.associate(instance.id)
boto.ec2containerservice.exceptions
ClientException
TestEC2ContainerService(unittest.TestCase):
self.ecs
boto.connect_ec2containerservice()
self.ecs.list_clusters()
self.assertIn('clusterArns',
response['ListClustersResponse']['ListClustersResult'])
self.assertRaises(ClientException):
self.ecs.stop_task(task='foo')
boto.elasticache
TestElastiCacheConnection(unittest.TestCase):
self.elasticache
layer1.ElastiCacheConnection()
wait_until_cluster_available(self,
(response['DescribeCacheClustersResponse']
['DescribeCacheClustersResult']
['CacheClusters'][0]['CacheClusterStatus'])
available.'
test_create_delete_cache_cluster(self):
'cluster-id2'
self.elasticache.create_cache_cluster(
'cache.t1.micro',
'memcached')
self.wait_until_cluster_available(cluster_id)
self.elasticache.delete_cache_cluster(cluster_id)
BotoServerError:
deleted.'
ElasticTranscoderCertVerificationTest(unittest.TestCase,
elastictranscoder
boto.elastictranscoder.regions()
boto.elastictranscoder.exceptions
ValidationException
TestETSLayer1PipelineManagement(unittest.TestCase):
ElasticTranscoderConnection()
self.iam
'boto-pipeline-out-%s'
self.role_name
'boto-ets-role-%s'
self.pipeline_name
self.s3.create_bucket(self.input_bucket)
self.s3.create_bucket(self.output_bucket)
self.input_bucket)
self.output_bucket)
self.iam.create_role(self.role_name)
self.role_arn
self.role['create_role_response']['create_role_result']\
['role']['arn']
self.addCleanup(self.iam.delete_role,
self.role_name)
create_pipeline(self):
self.addCleanup(self.api.delete_pipeline,
test_create_delete_pipeline(self):
self.api.delete_pipeline(pipeline_id)
test_can_retrieve_pipeline_information(self):
pipelines
self.api.list_pipelines()['Pipelines']
pipeline_names
[p['Name']
pipelines]
self.assertIn(self.pipeline_name,
pipeline_names)
self.assertEqual(response['Pipeline']['Id'],
test_update_pipeline(self):
self.api.update_pipeline_status(pipeline_id,
self.assertEqual(response['Pipeline']['Status'],
test_update_pipeline_notification(self):
self.sns.create_topic('pipeline-errors')
response['CreateTopicResponse']['CreateTopicResult']\
self.addCleanup(self.sns.delete_topic,
self.api.update_pipeline_notifications(
topic_arn})
self.assertEqual(response['Pipeline']['Notifications']['Error'],
test_list_jobs_by_pipeline(self):
self.api.list_jobs_by_pipeline(pipeline_id)
self.assertEqual(response['Jobs'],
test_proper_error_when_pipeline_does_not_exist(self):
self.assertRaises(ValidationException):
self.api.read_pipeline('badpipelineid')
EMRCertVerificationTest(unittest.TestCase,
emr
boto.emr.regions()
conn.describe_jobflows()
GlacierCertVerificationTest(unittest.TestCase,
boto.glacier.regions()
conn.list_vaults()
TestGlacierLayer1(unittest.TestCase):
test_initialiate_multipart_upload(self):
glacier.create_vault('l1testvault')
self.addCleanup(glacier.delete_vault,
'l1testvault')
glacier.initiate_multipart_upload('l1testvault',
'double
spaces
here')['UploadId']
self.addCleanup(glacier.abort_multipart_upload,
'l1testvault',
glacier.list_multipart_uploads('l1testvault')['UploadsList']
self.assertEqual(response[0]['MultipartUploadId'],
Layer1,
TestGlacierLayer2(unittest.TestCase):
'testvault%s'
test_create_delete_vault(self):
self.layer2.create_vault(self.vault_name)
retrieved_vault
self.layer2.get_vault(self.vault_name)
self.layer2.delete_vault(self.vault_name)
retrieved_vault.name)
self.assertEqual(vault.arn,
retrieved_vault.arn)
self.assertEqual(vault.creation_date,
retrieved_vault.creation_date)
self.assertEqual(vault.last_inventory_date,
retrieved_vault.last_inventory_date)
retrieved_vault.number_of_archives)
CallbackTestHarness(object):
fail_after_n_bytes=0,
num_times_to_fail=1,
exception=socket.error('mock
fp_to_change=None,
fp_change_pos=None,
delay_after_change=None):
fail_after_n_bytes
self.num_times_to_fail
num_times_to_fail
fp_to_change
fp_change_pos
self.delay_after_change
delay_after_change
self.transferred_seq_before_first_failure
self.transferred_seq_after_first_failure
total_bytes_transferred,
unused_total_size):
self.num_failures:
self.transferred_seq_after_first_failure.append(
self.transferred_seq_before_first_failure.append(
(total_bytes_transferred
self.num_times_to_fail):
self.fp_to_change.tell()
self.fp_to_change.seek(self.fp_change_pos)
self.fp_to_change.write('abc')
self.fp_to_change.seek(cur_pos)
self.delay_after_change:
time.sleep(self.delay_after_change)
self.called
CORS_EMPTY
'<CorsConfig></CorsConfig>'
CORS_DOC
('<CorsConfig><Cors><Origins><Origin>origin1.example.com'
'</Origin><Origin>origin2.example.com</Origin></Origins>'
'<Methods><Method>GET</Method><Method>PUT</Method>'
'<Method>POST</Method></Methods><ResponseHeaders>'
'<ResponseHeader>foo</ResponseHeader>'
'<ResponseHeader>bar</ResponseHeader></ResponseHeaders>'
'</Cors></CorsConfig>')
LIFECYCLE_EMPTY
'<LifecycleConfiguration></LifecycleConfiguration>')
LIFECYCLE_DOC
'<LifecycleConfiguration><Rule>'
'<Action><Delete/></Action>'
'<Condition><Age>365</Age>'
'<CreatedBefore>2013-01-15</CreatedBefore>'
'<NumberOfNewerVersions>3</NumberOfNewerVersions>'
'<IsLive>true</IsLive></Condition>'
'</Rule></LifecycleConfiguration>')
LIFECYCLE_CONDITIONS
'365',
'2013-01-15',
'NumberOfNewerVersions':
'3',
'IsLive':
PROJECT_PRIVATE_RE
('\s*<AccessControlList>\s*<Entries>\s*<Entry>'
'\s*<Permission>READ</Permission></Entry>\s*</Entries>'
'\s*</AccessControlList>\s*')
GSBasicTest(GSTestCase):
test_read_write(self):
self._conn.generate_url(900,
bucket=bucket.name,
key=key_name)
StringIO.StringIO('foo')
sfp2
StringIO.StringIO('foo2')
k.set_contents_from_file(sfp2)
'foo2')
test_get_all_keys(self):
'foobar1')
fpath2
f.write('test-data')
'test-contents'
'test-contents2'
k.set_contents_from_filename(fpath2)
self.assertNotEqual(k.md5,
k.base64md5
k.set_contents_from_stream(fp2)
fp2.seek(0,
self.assertEqual(fp2.read(),
fp2.close()
self.assertEqual(len(all),
test_bucket_lookup(self):
bucket.new_key('foo/bar')
k.set_contents_from_string('testdata',
self.assertIsInstance(k,
self.assertEqual(k.content_type,
phony_mimetype)
self.assertIsNone(k)
test_metadata(self):
test_list_iterator(self):
len([k
bucket.list()])
self.assertEqual(num_iter,
num_keys)
test_acl(self):
xml.sax.parseString(acl_xml,
bucket.set_acl(acl)
aclstr
k.get_xml_acl()
self.assertGreater(aclstr.count('/Entry',
empty_logging_str="<?xml
encoding='UTF-8'?><Logging/>"
"<?xml
encoding='UTF-8'?><Logging>"
"<LogBucket>log-bucket</LogBucket>"
"<LogObjectPrefix>example</LogObjectPrefix>"
"</Logging>")
bucket.set_subresource('logging',
empty_logging_str)
bucket.enable_logging('log-bucket',
'example')
test_copy_key(self):
bucket_name_1
bucket1.name
bucket_name_2
bucket2.name
self._GetConnection().get_bucket(bucket_name_1)
self._GetConnection().get_bucket(bucket_name_2)
k1
bucket1.new_key(key_name)
self.assertIsInstance(k1,
bucket1.key_class)
k1.name
test.'
k1.set_contents_from_string(s)
k1.copy(bucket_name_2,
bucket2.lookup(key_name)
self.assertIsInstance(k2,
bucket2.key_class)
k2.get_contents_to_file(fp)
bucket1.delete_key(k1)
bucket2.delete_key(k2)
test_default_object_acls(self):
bucket.set_def_acl('public-read')
bucket.set_def_acl(public_read_acl)
test_default_object_acls_storage_uri(self):
uri.set_def_acl('public-read')
uri.set_def_acl(public_read_acl)
test_cors_xml_bucket(self):
bucket.set_cors(CORS_DOC)
test_cors_xml_storage_uri(self):
cors_obj
handler.XmlHandler(cors_obj,
xml.sax.parseString(CORS_DOC,
uri.set_cors(cors_obj)
test_lifecycle_config_bucket(self):
bucket.configure_lifecycle(lifecycle_config)
test_lifecycle_config_storage_uri(self):
uri.configure_lifecycle(lifecycle_config)
VERSION_MISMATCH
"412"
GSGenerationConditionalsTest(GSTestCase):
testConditionalSetContentsFromFile(self):
testConditionalSetContentsFromString(self):
testConditionalSetContentsFromFilename(self):
f1
f2
fname1
f1.name
fname2
f2.name
f1.write(s1)
f1.close()
f2.write(s2)
f2.close()
os.remove(fname1)
os.remove(fname2)
testBucketConditionalSetAcl(self):
testConditionalSetContentsFromStream(self):
testBucketConditionalSetCannedAcl(self):
testBucketConditionalSetXmlAcl(self):
testObjectConditionalSetAcl(self):
k.set_acl("public-read")
k.set_acl("public-read",
testObjectConditionalSetCannedAcl(self):
k.set_canned_acl("public-read")
k.set_canned_acl("public-read",
testObjectConditionalSetXmlAcl(self):
k.set_xml_acl(acl)
get_cur_file_size
ResumableDownloadHandler
ResumableDownloadTests(GSTestCase):
make_small_key(self):
small_src_key_as_string
os.urandom(SMALL_KEY_SIZE)
self._MakeKey(data=small_src_key_as_string)
make_dst_fp(self,
dst_file
'dstfile')
open(dst_file,
test_non_resumable_download(self):
small_src_key.get_contents_to_file(dst_fp)
test_download_without_persistent_tracker(self):
test_failed_download_with_persistent_tracker(self):
etag_line
f.readline()
self.assertEquals(etag_line.rstrip('\n'),
small_src_key.etag.strip('"\''))
ResumableDownloadHandler.RETRYABLE_EXCEPTIONS[0]
test_failed_and_restarted_download_with_persistent_tracker(self):
ResumableDownloadHandler(num_retries=3)
test_download_with_inital_partial_download_before_failure(self):
test_zero_length_object_download(self):
self._MakeKey()
k.get_contents_to_file(dst_fp,
test_download_with_invalid_tracker_etag(self):
invalid_etag_tracker_file_name
'invalid_etag_tracker')
open(invalid_etag_tracker_file_name,
f.write('3.14159\n')
tracker_file_name=invalid_etag_tracker_file_name)
test_download_with_inconsistent_etag_in_tracker(self):
inconsistent_etag_tracker_file_name
'inconsistent_etag_tracker')
open(inconsistent_etag_tracker_file_name,
good_etag
small_src_key.etag.strip('"\'')
new_val_as_list
reversed(good_etag):
new_val_as_list.append(c)
''.join(new_val_as_list))
tracker_file_name=inconsistent_etag_tracker_file_name)
test_download_with_unwritable_tracker_file(self):
boto.gs.resumable_upload_handler
ResumableUploadHandler
LARGEST_KEY_SIZE
MB.
ResumableUploadTests(GSTestCase):
build_input_file(self,
range(size):
buf.append(str(random.randint(0,
9)))
file_as_string
(file_as_string,
StringIO.StringIO(file_as_string))
make_small_file(self):
self.build_input_file(SMALL_KEY_SIZE)
make_large_file(self):
self.build_input_file(LARGE_KEY_SIZE)
test_non_resumable_upload(self):
small_src_file.seek(0,
dst_key.set_contents_from_file(small_src_file)
filepointer")
dst_key.set_contents_from_file(small_src_file,
test_upload_without_persistent_tracker(self):
test_failed_upload_with_persistent_tracker(self):
uri_from_file
self.assertEqual(uri_from_file,
ResumableUploadHandler.RETRYABLE_EXCEPTIONS[0]
test_failed_and_restarted_upload_with_persistent_tracker(self):
ResumableUploadHandler(num_retries=3)
test_upload_with_inital_partial_upload_before_failure(self):
test_empty_file_upload(self):
empty_src_file
StringIO.StringIO('')
empty_src_file.seek(0)
empty_src_file,
test_upload_retains_metadata(self):
{'Content-Type'
'x-goog-meta-abc'
'x-goog-acl'
'public-read'}
dst_key.open_read()
self.assertEqual('text/plain',
dst_key.content_type)
self.assertTrue('abc'
dst_key.metadata)
self.assertEqual('my
str(dst_key.metadata['abc']))
dst_key.get_acl()
acl.entries.entry_list:
str(entry.scope)
'<AllUsers>':
self.assertEqual('READ',
str(acl.entries.entry_list[1].permission))
self.fail('No
<AllUsers>
test_upload_with_file_size_change_between_starts(self):
largest_src_file
self.build_input_file(LARGEST_KEY_SIZE)[1]
largest_src_file.seek(0)
largest_src_file,
self.assertNotEqual(e.message.find('file
changed'),
test_upload_with_file_size_change_during_upload(self):
CallbackTestHarness(fp_to_change=test_file,
fp_change_pos=test_file_size)
e.message.find('File
upload'),
test_upload_with_file_content_change_during_upload(self):
dst_key.bucket.name)
dst_key_uri
bucket_uri.clone_replace_name(dst_key.name)
test_file.seek(0,
self.assertEqual(test_file_size,
test_file.tell())
e.message.find('md5
etag'),
dst_key_uri.get_key()
InvalidUriError')
n_bytes
fail_after_n_bytes=n_bytes,
fp_to_change=test_file,
fp_change_pos=1,
delay_after_change=delay)
(attempt
harness.transferred_seq_after_first_failure):
test_upload_with_content_length_header_set(self):
res_upload_handler=res_upload_handler,
headers={'Content-Length'
SMALL_KEY_SIZE})
e.message.find('Attempt
header'),
test_upload_with_syntactically_invalid_tracker_uri(self):
syntactically_invalid_tracker_file_name
'synt_invalid_uri_tracker')
open(syntactically_invalid_tracker_file_name,
f.write('ftp://example.com')
tracker_file_name=syntactically_invalid_tracker_file_name)
test_upload_with_invalid_upload_id_in_tracker_file(self):
invalid_upload_id
('http://pub.storage.googleapis.com/?upload_id='
'AyzB2Uo74W4EYxyi5dp_-r68jz8rtbvshsv4TX7srJVkJ57CxTY5Dw2')
invalid_upload_id_tracker_file_name
'invalid_upload_id_tracker')
open(invalid_upload_id_tracker_file_name,
f.write(invalid_upload_id)
tracker_file_name=invalid_upload_id_tracker_file_name)
self.assertNotEqual(invalid_upload_id,
test_upload_with_unwritable_tracker_file(self):
self.make_tracker_file(tmp_dir)
perms
GSStorageUriTest(GSTestCase):
testHasVersion(self):
uri.version_id
"versionid"
12345
testCloneReplaceKey(self):
orig_uri
orig_uri.clone_replace_key(k)
self.assertRegexpMatches(str(uri.generation),
testSetAclXml(self):
"obj"
bucket_acl
bucket_uri.get_acl()
("<Entry><Scope
type='AllUsers'/>"
"<Permission>READ</Permission></Entry>")
acl_string
re.sub(r"</Entries>",
"</Entries>",
bucket_acl.to_xml())
acl_no_owner_string
re.sub(r"<Owner>.*</Owner>",
acl_string)
bucket_uri.set_xml_acl(acl_string,
"obj")
bucket_uri.set_xml_acl(acl_no_owner_string)
bucket_uri.set_def_xml_acl(acl_no_owner_string)
new_obj_acl_string
k.get_acl().to_xml()
new_bucket_acl_string
bucket_uri.get_acl().to_xml()
new_bucket_def_acl_string
bucket_uri.get_def_acl().to_xml()
self.assertRegexpMatches(new_obj_acl_string,
self.assertRegexpMatches(new_bucket_acl_string,
self.assertRegexpMatches(new_bucket_def_acl_string,
testPropertiesUpdated(self):
key_uri
bucket_uri.clone_replace_name("obj")
key_uri.set_contents_from_string("data1")
"data1")
key_uri.set_contents_from_stream(StringIO.StringIO("data2"))
"data2")
key_uri.set_contents_from_file(StringIO.StringIO("data3"))
"data3")
testCompose(self):
data2
'world!'
expected_crc
1238062967
key_uri1
bucket_uri.clone_replace_name("component1")
key_uri1.set_contents_from_string(data1)
key_uri2
bucket_uri.clone_replace_name("component2")
key_uri2.set_contents_from_string(data2)
key_uri_composite
bucket_uri.clone_replace_name("composite")
components
[key_uri1,
key_uri2]
key_uri_composite.compose(components,
content_type='text/plain')
self.assertEquals(key_uri_composite.get_contents_as_string(),
data2)
composite_key
key_uri_composite.get_key()
cloud_crc32c
binascii.hexlify(
composite_key.cloud_hashes['crc32c'])
self.assertEquals(cloud_crc32c,
hex(expected_crc)[2:])
self.assertEquals(composite_key.content_type,
key_uri1.bucket_name
key_uri_composite.compose(components)
self.fail('Composing
didn\'t
expected.')
err.reason,
boto.gs
GSVersioningTest(GSTestCase):
testVersioningToggle(self):
self.assertTrue(b.get_versioning_status())
b.configure_versioning(False)
testDeleteVersionedKey(self):
generation=None)
self.assertIsNone(b.get_key("foo"))
self.assertEqual(versions[0].generation,
testGetVersionedKey(self):
self.assertEqual(o1,
self.assertNotEqual(g2,
self.assertEqual(o2,
testVersionedBucketCannedAcl(self):
testVersionedBucketXmlAcl(self):
aclo
acl.ACL()
handler.XmlHandler(aclo,
b.set_acl(aclo,
testVersionedObjectCannedAcl(self):
testCopyVersionedKey(self):
b2
b2.copy_key("foo2",
b.name,
src_generation=g1)
b2.get_key("foo2")
k2.get_contents_as_string()
self.assertEqual(s3,
testKeyGenerationUpdatesOnSet(self):
self.assertIsNone(k.generation)
self.assertRegexpMatches(g1,
k.set_contents_from_string("test2")
self.assertNotEqual(g1,
self.assertRegexpMatches(g2,
self.assertGreater(int(g2),
int(g1))
tests.integration.gs
tests.integration.gs.util
@unittest.skipUnless(util.has_google_credentials(),
"Google
Google
"Cloud
Storage
Update
boto.cfg
"these
tests.")
GSTestCase(unittest.TestCase):
gs
GSConnection()
self._buckets
self._tempdirs
@retry(GSResponseError)
len(self._tempdirs):
self._tempdirs.pop()
while(len(self._buckets)):
self._buckets[-1]
self._conn.get_bucket(b)
len(list(bucket.list_versions()))
bucket.list_versions():
bucket.delete_key(k.name,
generation=k.generation)
self._buckets.pop()
_GetConnection(self):
_MakeTempName(self):
"boto-gs-test-%s"
repr(time.time()).replace(".",
"-")
_MakeBucketName(self):
self._buckets.append(b)
_MakeBucket(self):
self._conn.create_bucket(self._MakeBucketName())
_MakeKey(self,
set_contents=True):
ValueError('MakeKey
set_contents
False.')
k.set_contents_from_string(data)
_MakeVersionedBucket(self):
_MakeTempDir(self):
tempfile.mkdtemp(prefix=self._MakeTempName())
self._tempdirs.append(tmpdir)
has_google_credentials():
Provider('google')
(provider.get_access_key()
provider.get_secret_key()
retry(ExceptionToCheck,
tries=4,
delay=3,
backoff=2,
logger=None):
deco_retry(f):
f_retry(*args,
mtries,
tries,
ExceptionToCheck
Retrying
seconds..."
(str(e),
mdelay)
logger:
logger.warning(msg)
time.sleep(mdelay)
backoff
try_one_last_time:
f_retry
true
deco_retry
IAMCertVerificationTest(unittest.TestCase,
boto.iam.regions()
conn.get_all_users()
TestIAM(unittest.TestCase):
test_group_users(self):
'boto-test-%d'
iam.create_group(name)
iam.add_user_to_group(name,
iam.remove_user_from_group(name,
iam.delete_group(name)
IAMAccountPasswordPolicy(unittest.TestCase):
test_password_policy(self):
initial_policy_result
initial_policy
iam.update_account_password_policy(minimum_password_length=test_min_length)
new_policy
new_min_length
new_policy['get_account_password_policy_response']\
['get_account_password_policy_result']['password_policy']\
['minimum_password_length']
int(new_min_length):
iam.delete_account_password_policy()
initial_policy:
initial_policy['get_account_password_policy_response']\
iam.update_account_password_policy(minimum_password_length=int(p['minimum_password_length']),
allow_users_to_change_password=bool(p['allow_users_to_change_password']),
hard_expiry=bool(p['hard_expiry']),
max_password_age=int(p['max_password_age']),
password_reuse_prevention=int(p['password_reuse_prevention']),
require_lowercase_characters=bool(p['require_lowercase_characters']),
require_numbers=bool(p['require_numbers']),
require_symbols=bool(p['require_symbols']),
require_uppercase_characters=bool(p['require_uppercase_characters']))
TestIAMPolicy(unittest.TestCase):
test_policy_actions(self):
'boto-test-role-%d'
'boto-test-group-%d'
policyname
'TestPolicyName-%d'
iam.create_role(rolename)
iam.create_group(groupname)
"Version":
"2012-10-17",
"TestPermission",
"Sid":
"TestSid",
"s3:*",
"Deny",
"arn:aws:s3:::*"
policy_json
json.dumps(policy_doc)
iam.create_policy(policyname,
policy_json)
policy_copy
iam.get_policy(policy.arn)
policy_copy.arn
policy.arn:
Exception("Policies
equal.")
iam.attach_role_policy(policy.arn,
iam.attach_group_policy(policy.arn,
iam.attach_user_policy(policy.arn,
iam.detach_role_policy(policy.arn,
iam.detach_group_policy(policy.arn,
iam.detach_user_policy(policy.arn,
iam.delete_policy(policy.arn)
iam.delete_role(rolename)
iam.delete_group(groupname)
KinesisCertVerificationTest(unittest.TestCase,
boto.kinesis.regions()
conn.list_streams()
boto.kinesis.exceptions
TimeoutError(Exception):
TestKinesis(unittest.TestCase):
boto.connect_kinesis()
test_kinesis(self):
kinesis.create_stream('test',
self.addCleanup(self.kinesis.delete_stream,
kinesis.describe_stream('test')
response['StreamDescription']['StreamStatus']
shard_id
response['StreamDescription']['Shards'][0]['ShardId']
TimeoutError('Stream
active,
kinesis.add_tags_to_stream(stream_name='test',
tags={'foo':
self.assertEqual(response['Tags'][0],
{'Key':'foo',
kinesis.remove_tags_from_stream(stream_name='test',
tag_keys=['foo'])
kinesis.get_shard_iterator('test',
'TRIM_HORIZON')
response['ShardIterator']
'Some
kinesis.put_record('test',
kinesis.put_records([record,
record.copy()],
num_expected_records
collected_records
100:
kinesis.get_records(shard_iterator)
response['NextShardIterator']
response['Records']:
'Data'
collected_records.append(record['Data'])
num_expected_records:
self.assertEqual(num_expected_records,
num_collected)
TimeoutError('No
found,
collected_records:
test_describe_non_existent_stream(self):
self.assertRaises(ResourceNotFoundException)
self.kinesis.describe_stream('this-stream-shouldnt-exist')
self.assertEqual(cm.exception.error_code,
self.assertTrue('not
cm.exception.message)
boto.kms.exceptions
NotFoundException
TestKMS(unittest.TestCase):
self.kms
boto.connect_kms()
test_list_keys(self):
self.kms.list_keys()
self.assertIn('Keys',
self.assertRaises(NotFoundException):
self.kms.describe_key(
key_id='nonexistant_key',
CloudWatchLogsCertVerificationTest(unittest.TestCase,
boto.logs.regions()
conn.describe_log_groups()
TestCloudWatchLogs(unittest.TestCase):
boto.connect_logs()
test_logs(self):
logs.describe_log_groups(log_group_name_prefix='test')
self.assertIsInstance(response['logGroups'],
mfilter
'[ip,
...,
status_code=500,
size]'
1534',
5324',
logs.test_metric_filter(mfilter,
sample)
self.assertEqual(len(response['matches']),
os.environ.get('MWS_MERCHANT',
simple:
isolator
print('>>>
MWS
sources')
MWSTestCase(unittest.TestCase):
self.mws
MWSConnection(Merchant=simple,
debug=0)
test_feedlist(self):
self.mws.get_feed_submission_list()
test_inbound_status(self):
self.mws.get_inbound_service_status()
response.GetServiceStatusResult.Status
self.assertIn(status,
('GREEN',
'GREEN_I',
'YELLOW',
'RED'))
marketplace(self):
result.ListMarketplaces.Marketplace[0]
marketplace_id(self):
self.marketplace.MarketplaceId
test_marketplace_participations(self):
self.assertTrue(result.ListMarketplaces.Marketplace[0].MarketplaceId)
test_get_product_categories_for_asin(self):
self.mws.get_product_categories_for_asin(
ASIN=asin)
self.assertEqual(len(response._result.Self),
categoryids
[x.ProductCategoryId
response._result.Self]
self.assertSequenceEqual(categoryids,
['285856',
'21',
'491314'])
test_list_matching_products(self):
self.mws.list_matching_products(
Query='boto')
products
response._result.Products
self.assertTrue(len(products))
test_get_matching_product(self):
'B001UDRNHO'
self.mws.get_matching_product(
response._result[0].Product.AttributeSets.ItemAttributes
self.assertEqual(attributes[0].Label,
'Serengeti')
test_get_matching_product_for_id(self):
asins
['B001UDRNHO',
'144930544X']
self.mws.get_matching_product_for_id(
IdType='ASIN',
IdList=asins)
self.assertEqual(len(response._result),
response._result:
self.assertEqual(len(result.Products.Product),
test_get_lowest_offer_listings_for_asin(self):
self.mws.get_lowest_offer_listings_for_asin(
ItemCondition='New',
listings
response._result[0].Product.LowestOfferListings
self.assertTrue(len(listings.LowestOfferListing))
test_list_inventory_supply(self):
asof
(datetime.today()
timedelta(days=30)).isoformat()
self.mws.list_inventory_supply(QueryStartDateTime=asof,
ResponseGroup='Basic')
self.assertTrue(hasattr(response._result,
'InventorySupplyList'))
connect_to_region,
regions,
TestOpsWorksConnection(unittest.TestCase):
OpsWorksConnection()
self.api.describe_stacks()
self.assertIn('Stacks',
test_validation_errors(self):
self.assertRaises(JSONResponseError):
self.api.create_stack('testbotostack',
'badarn',
'badarn2')
TestOpsWorksHelpers(unittest.TestCase):
test_regions(self):
regions()
RegionInfo)
test_connect_to_region(self):
OpsWorksConnection)
boto.rds.regions()
conn.get_all_dbinstances()
subnet_group.vpc_id
'vpc_id
',subnet_group.vpc_id,
subnet_group.description
"description
'"+subnet_group.description+"'
'"+description+"'"
set(subnet_group.subnet_ids)
set(subnets):
subnets_are
','.join(subnet_group.subnet_ids)
should_be
','.join(subnets)
"subnets
"+subnets_are+"
"+should_be
DbSubnetGroupTest(unittest.TestCase):
test_db_subnet_group(self):
vpc_api
rds_api
vpc_api.create_vpc('10.0.0.0/16')
az_list
vpc_api.get_all_zones(filters={'state':'available'})
list()
az
az_list:
subnet.append(vpc_api.create_subnet(vpc.id,
'10.0.'+str(n)+'.0/24',availability_zone=az.name))
n+1
grp_name
'db_subnet_group'+str(int(time.time()))
subnet_group
rds_api.create_db_subnet_group(grp_name,
[subnet[0].id,subnet[1].id])
Exception("create_db_subnet_group
description='new
desciption
subnet_ids=[subnet[1].id,subnet[2].id])
[subnet[1].id,subnet[2].id]):
rds_api.delete_db_subnet_group(subnet_group.name)
Exception(subnet_group.name+"
accessible
delete_db_subnet_group")
n-1
vpc_api.delete_subnet(subnet[n].id)
vpc_api.delete_vpc(vpc.id)
PromoteReadReplicaTest(unittest.TestCase):
"boto-db-%s"
"replica-%s"
"renamed-replica-%s"
[self.masterDB_name,
self.renamedDB_name]:
self.conn.delete_dbinstance(db,
skip_final_snapshot=True)
test_promote(self):
RDS
promotion
self.masterDB
self.conn.create_dbinstance(self.masterDB_name,
'db.t1.micro',
'root',
'bototestpw')
self.conn.create_dbinstance_read_replica(self.replicaDB_name,
self.masterDB_name)
self.conn.promote_read_replica(self.replicaDB_name)
promoted
self.assertTrue(inst)
self.assertFalse(inst.status_infos)
self.assertFalse(inst.read_replica_dbinstance_identifiers)
self.renamedDB
self.conn.modify_dbinstance(self.replicaDB_name,
new_instance_id=self.renamedDB_name,
apply_immediately=True)
time.time():
self.renamedDB_name:
boto.rds2.regions()
conn.describe_db_instances()
TestRDS2Connection(unittest.TestCase):
"test-db-%s"
test_connect_rds(self):
boto.connect_rds2()
self.conn.create_db_instance(
db_instance_identifier=self.db_name,
allocated_storage=5,
db_instance_class='db.t1.micro',
engine='postgres',
master_username='bototestuser',
master_user_password='testtestt3st',
backup_retention_period=0
self.addCleanup(
self.conn.delete_db_instance,
skip_final_snapshot=True
self.conn.modify_db_instance(
allocated_storage=10,
apply_immediately=True
'modifying')
RedshiftCertVerificationTest(unittest.TestCase,
boto.redshift.regions()
conn.describe_cluster_versions()
ClusterNotFoundFault
ResizeNotFoundFault
TestRedshiftLayer1Management(unittest.TestCase):
RedshiftConnection()
'boto-redshift-cluster-%s'
self.node_type
'dw.hs1.xlarge'
'mrtest'
self.master_password
'P4ssword'
'simon'
cluster_id(self):
create_cluster(self):
self.addCleanup(self.delete_cluster_the_slow_way,
delete_cluster_the_slow_way(self,
test_create_delete_cluster(self):
self.assertRaises(ClusterNotFoundFault):
self.api.describe_clusters('badpipelineid')
self.create_cluster()
self.assertRaises(ResizeNotFoundFault):
self.api.describe_resize(cluster_id)
self.api.describe_clusters()['DescribeClustersResponse']\
['DescribeClustersResult']\
['Clusters']
cluster_ids
[c['ClusterIdentifier']
clusters]
self.assertIn(cluster_id,
cluster_ids)
self.api.describe_clusters(cluster_id)
self.assertEqual(response['DescribeClustersResponse']\
['DescribeClustersResult']['Clusters'][0]\
['ClusterIdentifier'],
"snap-%s"
self.api.create_cluster_snapshot(snapshot_id,
['SnapshotIdentifier'],
['Status'],
self.addCleanup(self.api.delete_cluster_snapshot,
self.api.describe_cluster_snapshots(
cluster_identifier=cluster_id
response['DescribeClusterSnapshotsResponse']\
['DescribeClusterSnapshotsResult']['Snapshots'][-1]
self.assertEqual(snap['SnapshotType'],
self.assertEqual(snap['DBName'],
self.db_name)
Route53TestCase(unittest.TestCase):
TestRoute53AliasResourceRecordSets(unittest.TestCase):
self.zone.add_a('target.%s'
'102.11.23.1')
self.zone.delete_a('target.%s'
test_incomplete_add_alias_failure(self):
self.assertRaises(DNSServerError,
rrs.commit)
test_add_alias(self):
alias_evaluate_target_health=False,
test_set_alias(self):
test_set_alias_backwards_compatability(self):
Route53CertVerificationTest(unittest.TestCase,
boto.route53.regions()
conn.get_all_hosted_zones()
TestRoute53HealthCheck(Route53TestCase):
test_create_health_check(self):
test_create_https_health_check(self):
self.assertFalse('FullyQualifiedDomainName'
test_create_https_health_check_fqdn(self):
HealthCheck(ip_addr=None,
resource_path="/",
fqdn="google.com")
u'HealthCheck'][u'HealthCheckConfig'][u'FullyQualifiedDomainName'],
'google.com')
self.assertFalse('IPAddress'
test_create_and_list_health_check(self):
result1
HealthCheck(ip_addr="54.217.7.119",
result2
self.assertTrue(len(result['ListHealthChecksResponse']['HealthChecks'])
self.conn.delete_health_check(result1['CreateHealthCheckResponse']['HealthCheck']['Id'])
self.conn.delete_health_check(result2['CreateHealthCheckResponse']['HealthCheck']['Id'])
test_delete_health_check(self):
hc_id
result['CreateHealthCheckResponse']['HealthCheck']['Id']
hc['Id']
hc_id:
self.conn.delete_health_check(hc_id)
self.assertFalse(hc['Id']
hc_id)
test_create_health_check_string_match(self):
hc_type="HTTP_STR_MATCH",
'HTTP_STR_MATCH')
test_create_health_check_https_string_match(self):
hc_type="HTTPS_STR_MATCH",
test_create_resource_record_set(self):
ResourceRecordSets(
connection=self.conn,
hosted_zone_id=self.zone.id,
comment='Create
records.add_change('CREATE',
'unittest.%s.'
change.add_value("54.217.7.118")
records.add_change('DELETE',
"unittest.%s."
deleted.add_value('54.217.7.118')
test_create_health_check_invalid_request_interval(self):
HealthCheck(**self.health_check_params(request_interval=5)))
test_create_health_check_invalid_failure_threshold(self):
HealthCheck(**self.health_check_params(failure_threshold=0)))
HealthCheck(**self.health_check_params(failure_threshold=11)))
test_create_health_check_request_interval(self):
self.health_check_params(request_interval=10)
self.assertEquals(hc_config[u'RequestInterval'],
six.text_type(hc_params['request_interval']))
test_create_health_check_failure_threshold(self):
self.health_check_params(failure_threshold=1)
self.assertEquals(hc_config[u'FailureThreshold'],
six.text_type(hc_params['failure_threshold']))
health_check_params(self,
'ip_addr':
"54.217.7.118",
'hc_type':
'/testing',
params.update(kwargs)
TestRoute53ResourceRecordSets(Route53TestCase):
test_add_change(self):
created.add_value('192.168.0.25')
rrs.add_change('DELETE',
deleted.add_value('192.168.0.25')
test_record_count(self):
created.add_value(ip)
all_records
self.conn.get_all_rrsets(self.zone.id)
rrs.add_change("DELETE",
deleted.add_value(ip)
records)
TestRoute53Zone(unittest.TestCase):
route53.get_zone(self.base_domain)
zone.delete()
route53.create_zone(self.base_domain)
test_nameservers(self):
test_a(self):
self.zone.add_a(self.base_domain,
'102.11.23.1',
[u'102.11.23.1'])
u'80')
self.zone.update_a(self.base_domain,
'186.143.32.2',
'800')
[u'186.143.32.2'])
u'800')
test_cname(self):
self.zone.add_cname(
'webserver.%s'
u'webserver.%s.'
u'200')
self.zone.update_cname(
'web.%s'
u'web.%s.'
u'45')
test_mx(self):
self.zone.add_mx(
mx1.%s'
mx2.%s'
mx1.%s.'
u'20
mx2.%s.'
u'1000')
self.zone.update_mx(
mail1.%s'
mail2.%s'
mail1.%s.'
mail2.%s.'
u'50')
test_get_records(self):
self.zone.get_records()
test_get_nameservers(self):
test_get_zones(self):
route53.get_zones()
test_identifiers_wrrs(self):
'1.2.3.4',
identifier=('foo',
'20'))
'5.6.7.8',
identifier=('bar',
'10'))
wrrs
'wrr.%s'
self.assertEquals(len(wrrs),
self.zone.delete_a('wrr.%s'
test_identifiers_lbrs(self):
lbrs
'lbr.%s'
self.assertEquals(len(lbrs),
test_toomany_exception(self):
self.assertRaises(TooManyRecordsException,
self.zone.get_a('exception.%s'
self.base_domain))
self.zone.delete_a('exception.%s'
self.zone.delete_a(self.base_domain)
self.zone.delete_cname('www.%s'
self.zone.delete_mx(self.base_domain)
TestRoute53PrivateZone(unittest.TestCase):
self.route53
'boto-private-zone-test-%s.com'
self.test_vpc
self.vpc.create_vpc(cidr_block='10.11.0.0/16')
self.test_vpc.add_tag("Name",
self.route53.get_zone(self.base_domain)
self.route53.create_hosted_zone(self.base_domain,
vpc_id=self.test_vpc.id,
vpc_region='us-east-1')
self.test_vpc.delete()
unittest.main(verbosity=3)
boto.route53.domains.exceptions
InvalidInput
TestRoute53Domains(unittest.TestCase):
self.route53domains
boto.connect_route53domains()
test_check_domain_availability(self):
domain_name='amazon.com',
idn_lang_code='eng'
{'Availability':
'UNAVAILABLE'})
test_handle_invalid_input_error(self):
self.assertRaises(InvalidInput):
domain_name='!amazon.com',
compute_md5
NOT_IMPL
MockAcl(object):
parent=NOT_IMPL):
'<mock_ACL_XML/>'
MockKey(object):
'Wed,
06
Oct
2010
05:11:54
res_download_handler=NOT_IMPL):
_handle_headers(self,
merge_headers_by_name('Content-Encoding',
merge_headers_by_name('Content-Type',
merge_headers_by_name('Content-Language',
'Range'
re.match('bytes=([0-9]+)-$',
headers['Range'])
int(match.group(1))
fast=NOT_IMPL):
self.data[self.read_pos:]
self.data[self.read_pos:self.read_pos+size]
copy.copy(s)
res_upload_handler)
dst_bucket_name,
preserve_acl=NOT_IMPL):
self.bucket.connection.get_bucket(dst_bucket_name)
set_etag(self):
md5()
isinstance(self.data,
m.update(self.data.encode('utf-8'))
m.update(self.data)
tup
compute_md5(fp)
tup[2]
tup[0:2]
MockBucket(object):
key_class=NOT_IMPL):
self.acls
MockAcl()}
self.subresources
'MockBucket:
query_args=NOT_IMPL):
self.new_key(key_name=new_key_name)
src_key
src_bucket_name).get_key(src_key_name)
new_key.data
copy.copy(src_key.data)
new_key.size
len(new_key.data)
disable_logging(self):
target_bucket_prefix):
get_logging_config(self):
{"Logging":
{}}
self.subresources:
'<Subresource/>'
MockKey(self,
six.itervalues(self.keys)
marker=NOT_IMPL,
Turn
match.
key_name_set
six.itervalues(self.keys):
k.name.startswith(prefix):
k_name_past_prefix
k.name[len(prefix):]
k_name_past_prefix.find(delimiter)
(pos
-1):
Prefix(
bucket=self,
name=k.name[:len(prefix)+pos+1])
MockKey(bucket=self,
name=k.name)
key_or_prefix.name
key_name_set:
key_name_set.add(key_or_prefix.name)
result.append(key_or_prefix)
MockProvider(object):
MockConnection(object):
aws_access_key_id=NOT_IMPL,
aws_secret_access_key=NOT_IMPL,
is_secure=NOT_IMPL,
port=NOT_IMPL,
proxy=NOT_IMPL,
proxy_port=NOT_IMPL,
proxy_user=NOT_IMPL,
proxy_pass=NOT_IMPL,
host=NOT_IMPL,
https_connection_factory=NOT_IMPL,
calling_format=NOT_IMPL,
path=NOT_IMPL,
provider='s3',
bucket_class=NOT_IMPL):
self.buckets
MockProvider(provider)
boto.exception.StorageCreateError(
409,
'BucketAlreadyOwnedByYou',
"<Message>Your
"succeeded
it.</Message>")
MockBucket(name=bucket_name,
boto.exception.StorageResponseError(
'<Message>no
bucket</Message>')
self.buckets[bucket]
six.itervalues(self.buckets)
MockBucketStorageUri(object):
suppress_consec_slashes=NOT_IMPL,
('%s://%s/%s'
self.object_name))
(bool(self.generation)
bool(self.version_id))
object_name:
object_name)
MockAcl
boto.provider.Provider('aws').canned_acls
self.__class__(self.scheme,
new_name)
version_id=getattr(key,
generation=getattr(key,
is_latest=getattr(key,
access_key_id=NOT_IMPL,
secret_access_key=NOT_IMPL):
self.connect().create_bucket(self.bucket_name)
self.connect().delete_bucket(self.bucket_name)
self.get_bucket().get_versioning_status(headers)
MockBucketStorageUri)
self.get_bucket().delete_key(self.object_name)
self.get_bucket().disable_logging()
self.get_bucket().enable_logging(target_bucket)
self.get_bucket().get_logging_config()
self.get_bucket().get_acl(self.object_name)
self.get_bucket().get_def_acl(self.object_name)
self.get_bucket().get_subresource(subresource,
self.connect().get_all_buckets()
self.get_bucket().get_all_keys(self)
all_versions=NOT_IMPL):
self.get_bucket().list(prefix=prefix,
delimiter=delimiter)
self.connect().get_bucket(self.bucket_name)
self.get_bucket().get_key(self.object_name)
self.get_bucket().set_acl(acl_or_str,
self.get_bucket().set_def_acl(acl_or_str)
self.get_bucket().set_subresource(subresource,
src_generation=NOT_IMPL):
dst_bucket.copy_key(new_key_name=self.object_name,
src_key_name=src_key_name)
key.set_contents_from_string(s)
size=NOT_IMPL,
rewind=NOT_IMPL,
dst_key.set_contents_from_stream(fp)
res_download_handler=NOT_IMPL,
response_headers=NOT_IMPL):
Tags,
S3BucketTest
'bucket-%d'
test_next_marker(self):
["a/",
"b",
"c"]
self.bucket.get_all_keys(max_keys=2)
self.bucket.get_all_keys(max_keys=2,
delimiter="/")
self.bucket.list()
test_list_with_url_encoding(self):
[u"α",
u"β",
u"γ"]
orig_getall
self.bucket._get_all
getall
**k:
orig_getall(*a,
max_keys=2,
**k)
patch.object(self.bucket,
'_get_all',
getall):
self.bucket.list(encoding_type="url")
unquote_str(element.name)
sb_name
"src-"
sb
self.conn.create_bucket(sb_name)
self.bucket.set_acl("log-delivery-write")
target_prefix
u"jp/ログ/"
authuri
"http://acs.amazonaws.com/groups/global/AuthenticatedUsers"
authr
Grant(permission="READ",
type="Group",
uri=authuri)
sb.enable_logging(target_bucket,
target_prefix=target_prefix,
grants=[authr])
self.assertEqual(bls.prefix,
self.assertEqual(len(bls.grants),
self.assertEqual(bls.grants[0].type,
"Group")
self.assertEqual(bls.grants[0].uri,
authuri)
sb.delete()
test_tagging(self):
tagging
self.bucket.set_xml_tags(tagging)
'tagkey')
'tagvalue')
self.bucket.delete_tags()
self.assertEqual(e.code,
'NoSuchTagSet')
self.fail("Wrong
(expected
S3ResponseError):
self.fail("Expected
test_tagging_from_objects(self):
tag_set.add_tag('akey',
tag_set.add_tag('anotherkey',
t.add_tag_set(tag_set)
self.bucket.set_tags(t)
'akey')
self.assertEqual(response[0][1].key,
'anotherkey')
self.assertEqual(response[0][1].value,
test_website_configuration(self):
{'WebsiteConfiguration':
{'IndexDocument':
{'Suffix':
'index.html'}}})
config2,
self.bucket.get_website_configuration_with_xml()
config2)
self.assertTrue('<Suffix>index.html</Suffix>'
test_website_redirect_all_requests(self):
redirect_all_requests_to=RedirectLocation('example.com'))
'RedirectAllRequestsTo':
'example.com'}}})
redirect_all_requests_to=RedirectLocation('example.com',
'https'))
{'RedirectAllRequestsTo':
}}}
test_lifecycle(self):
self.assertEqual(actual_lifecycle.id,
'myid')
self.assertEqual(actual_lifecycle.status,
self.assertEqual(actual_lifecycle.transition,
test_lifecycle_with_glacier_transition(self):
transition=transition)
lifecycle.append(rule)
response[0].transition
self.assertEqual(transition.days,
self.assertEqual(transition.date,
test_lifecycle_multi(self):
'2022-10-12T00:00:00.000Z'
lifecycle.add_rule("1",
"1/",
lifecycle.add_rule("2",
"2/",
Expiration(days=2))
lifecycle.add_rule("3",
"3/",
Expiration(date=date))
lifecycle.add_rule("4",
"4/",
Transition(days=4,
lifecycle.add_rule("5",
"5/",
Transition(date=date,
"1":
"1/")
"2":
"2/")
"3":
"3/")
"4":
"4/")
self.assertEqual(rule.transition.days,
"5":
"5/")
self.assertEqual(rule.transition.date,
self.fail("unexpected
rule.id)
test_lifecycle_jp(self):
"Japanese
files"
"日本語/"
lifecycle.add_rule(name,
test_lifecycle_with_defaults(self):
lifecycle.add_rule(expiration=30)
self.assertNotEqual(len(actual_lifecycle.id),
test_lifecycle_rule_xml(self):
Rule(status='Enabled',
expiration=30)
self.assertEqual(s.find("<ID>"),
self.assertNotEqual(s.find("<Prefix></Prefix>"),
S3CertVerificationTest(unittest.TestCase,
boto.s3.regions()
conn.get_all_buckets()
S3SpecifyHost(unittest.TestCase):
testWithNonAWSHost(self):
dict({'host':'www.not-a-website.com'})
self.assertEquals('www.not-a-website.com',
testSuccessWithHostOverrideRegion(self):
dict({'host':'s3.amazonaws.com'})
testSuccessWithDefaultUSWest1(self):
testSuccessWithDefaultUSEast1(self):
testSuccessWithDefaultEUCentral1(self):
connect_to_region('eu-central-1')
self.assertEquals('s3.eu-central-1.amazonaws.com',
testDefaultWithInvalidHost(self):
dict({'host':''})
testDefaultWithInvalidHostNone(self):
dict({'host':None})
S3PermissionsError,
urlopen,
S3ConnectionTest
'-log')
logging_bucket.set_as_logging_target()
target_prefix=bucket.name)
c.delete_bucket(logging_bucket)
k.generate_url(3600)
headers={'x-amz-x-token'
'XYZ'})
filename="foo.txt"'}
response_headers=rh)
filename="foo&z%20ar&ar&zar&bar.txt"'}
response_headers=rh,
policy='private',
urlsplit(url)
http_client.HTTPConnection(up.hostname,
up.port)
con.request("PUT",
up.path
up.query,
body="hello
there")
con.getresponse()
b"hello
there"
k.set_contents_from_filename('foobar')
bucket.lookup('has_metadata')
bucket.list()
bucket.new_key('testnewline\n')
bucket.add_user_grant('FULL_CONTROL',
'c1e724fbfa0979a4448393c59a8c055011f739b6d102fb37a65f26414653cd67')
'foo@bar.com')
bucket.new_key('reduced_redundancy')
reduced
redundancy',
k.get_contents_as_string(response_headers={'response-content-type'
'foo/bar'})
'reduced_redundancy':
c.delete_bucket(bucket)
test_basic_anon(self):
auth_con
auth_bucket
auth_con.create_bucket(bucket_name)
anon_con
S3Connection(anon=True)
anon_bucket
Bucket(anon_con,
self.fail("anon
fail")
auth_bucket.set_acl('public-read')
Was
secondes,
turns
enough
self.fail("not
contents")
self.fail("We
public-read
access,
auth_con.delete_bucket(auth_bucket)
test_error_code_populated(self):
c.create_bucket('bad$bucket$name')
self.assertEqual(e.error_code,
'InvalidBucketName')
socket.gaierror:
possible
self.fail("S3ResponseError
S3CORSTest
'cors-%d'
test_cors(self):
self.cfg
self.cfg.add_rule(['PUT',
self.bucket.set_cors(self.cfg)
enumerate(cfg):
self.cfg[i].id)
self.assertEqual(rule.max_age_seconds,
self.cfg[i].max_age_seconds)
zip(rule.allowed_method,
self.cfg[i].allowed_method)
origins
zip(rule.allowed_origin,
self.cfg[i].allowed_origin)
origins:
zip(rule.allowed_header,
self.cfg[i].allowed_header)
zip(rule.expose_header,
self.cfg[i].expose_header)
self.bucket.delete_cors()
self.fail('CORS
there')
json_policy
S3EncryptionTest
S3Encryption
'encryption-%d'
unencrypted
bucket.set_policy(json_policy
bucket.name)
os.path.dirname(os.path.abspath(__file__
)),
'other_cacerts.txt')
PROXY_HOST
os.environ.get('PROXY_HOST',
'cache')
PROXY_PORT
os.environ.get('PROXY_PORT',
'3128')
INVALID_HOSTNAME_HOST
os.environ.get('INVALID_HOSTNAME_HOST',
'www')
'ssl')
CertValidationTest(unittest.TestCase):
boto.config.sections():
boto.config.remove_section(section)
boto.config.add_section('Boto')
boto.config.setbool('Boto',
boto.config.add_section('Credentials')
'gs_access_key_id',
enableProxy(self):
PROXY_HOST)
PROXY_PORT)
assertConnectionThrows(self,
connection_class,
connection_class('fake_id',
'fake_secret')
self.assertRaises(error,
conn.get_all_buckets)
do_test_valid_cert(self):
exception.S3ResponseError)
exception.GSResponseError)
test_valid_cert(self):
test_valid_cert_with_proxy(self):
do_test_invalid_signature(self):
test_invalid_signature(self):
test_invalid_signature_with_proxy(self):
GSConnection,
test_invalid_host(self):
test_invalid_host_with_proxy(self):
S3KeyTest(unittest.TestCase):
random.seed()
'keytest-%d-%d'
time.time(),
99999999))
test_set_contents_from_file_dataloss(self):
"abcde"
sfp.write(content)
self.fail("forgot
fail.")
test_set_contents_as_file(self):
content[5:])
test_set_contents_with_md5(self):
k.compute_md5(sfp,
size=5,
hexdig,
bad_md5
(hexdig,
base64[3:])
md5=bad_md5)
md5")
test_get_contents_with_md5(self):
self.assertEqual(kn.md5,
k.md5)
test_file_callback(self):
callback(wrote,
total):
self.assertNotEqual(wrote,
self.my_cb_last,
"called
twice
wrote
StringIO("")
k.get_contents_as_string(cb=callback)
k.get_contents_as_string(cb=callback).decode('utf-8')
num_cb=-1)
num_cb=-1).decode('utf-8')
num_cb=1)
num_cb=1).decode('utf-8')
num_cb=2)
num_cb=2).decode('utf-8')
num_cb=3)
num_cb=3).decode('utf-8')
num_cb=4)
num_cb=4).decode('utf-8')
num_cb=6)
num_cb=6).decode('utf-8')
num_cb=10).decode('utf-8')
num_cb=1000)
num_cb=1000).decode('utf-8')
test_website_redirects(self):
self.assertTrue(key.set_redirect('http://www.amazon.com/'))
'http://www.amazon.com/')
self.assertTrue(key.set_redirect('http://aws.amazon.com/'))
'http://aws.amazon.com/')
test_website_redirect_none_configured(self):
key.set_contents_from_string('')
test_website_redirect_with_bad_value(self):
key.set_redirect('ftp://ftp.example.org')
key.set_redirect('')
test_setting_date(self):
self.bucket.new_key('test_date')
key.set_metadata('date',
'20130524T155935Z')
key.set_contents_from_string('Some
here.')
self.bucket.get_key('test_date')
self.assertEqual(check.get_metadata('date'),
u'20130524T155935Z')
self.assertTrue('x-amz-meta-date'
check._get_remote_metadata())
test_header_casing(self):
self.bucket.new_key('test_header_case')
key.set_metadata('Content-type',
key.set_metadata('Content-md5',
'XmUKnus7svY1frWsVskxXg==')
key.set_contents_from_string('{"abc":
123}')
self.bucket.get_key('test_header_case')
test_header_encoding(self):
self.bucket.new_key('test_header_encoding')
key.set_metadata('Cache-control',
u'public,
max-age=500')
key.set_metadata('Test-Plus',
u'A
key.set_metadata('Content-disposition',
Zeit.txt')
key.set_metadata('Content-Encoding',
key.set_metadata('Content-Language',
key.set_metadata('Content-Type',
key.set_metadata('X-Robots-Tag',
key.set_metadata('Expires',
u'Thu,
1994
16:00:00
key.set_contents_from_string('foo')
self.bucket.get_key('test_header_encoding')
remote_metadata
check._get_remote_metadata()
self.assertEqual(check.cache_control,
self.assertEqual(remote_metadata['cache-control'],
self.assertEqual(check.get_metadata('test-plus'),
self.assertEqual(check.content_disposition,
self.assertEqual(remote_metadata['content-disposition'],
self.assertEqual(check.content_encoding,
self.assertEqual(remote_metadata['content-encoding'],
self.assertEqual(check.content_language,
self.assertEqual(remote_metadata['content-language'],
self.assertEqual(remote_metadata['content-type'],
self.assertEqual(check.x_robots_tag,
self.assertEqual(remote_metadata['x-robots-tag'],
self.assertEqual(check.expires,
self.assertEqual(remote_metadata['expires'],
Zeit.txt'
expected.encode('utf-8')
urllib.parse.unquote(check.content_disposition),
test_set_contents_with_sse_c(self):
"x-amz-server-side-encryption-customer-algorithm"
"AES256",
"x-amz-server-side-encryption-customer-key"
"MAAxAHQAZQBzAHQASwBlAHkAVABvAFMAUwBFAEMAIQA=",
"x-amz-server-side-encryption-customer-key-MD5"
"fUgCZDDh6bfEMuP2bN38mg=="
k.set_contents_from_string(content,
headers=header)
kn.get_contents_as_string(headers=header)
content.encode('utf-8'))
S3KeySigV4Test(unittest.TestCase):
boto.s3.connect_to_region('eu-central-1')
'boto-sigv4-key-%d'
location='eu-central-1')
test_put_get_with_non_string_headers_key(self):
{'Content-Length':
self.bucket.get_key('foobar',
test_head_put_get_with_non_ascii_key(self):
u'''pt-Olá_ch-你好_ko-안녕_ru-Здравствуйте%20,.<>~`!@#$%^&()_-+='"'''
self.bucket.get_key(k.key,
self.bucket.get_all_keys(prefix=k.key,
max_keys=1)
len(keys))
S3KeyVersionCopyTest(unittest.TestCase):
'boto-key-version-copy-%d'
test_key_overwrite_and_copy(self):
first_content
b"abcdefghijklm"
second_content
b"nopqrstuvwxyz"
k.set_contents_from_string(first_content)
first_key
first_version_id
first_key.version_id
k.set_contents_from_string(second_content)
second_key.version_id
first_version_id:
source_key
self.bucket.get_key('testkey',
version_id=first_version_id)
source_key.copy(self.bucket,
'copiedkey')
copied_key
copied_key_contents
copied_key.get_contents_as_string()
self.assertEqual(first_content,
copied_key_contents)
's3mfa')
S3MFATest
'mfa-%d'
test_mfadel(self):
mfa_sn
S/N:
self.bucket.configure_versioning(True,
mfa_delete=True,
self.bucket.new_key('foobar')
self.fail("Must
MFA
token")
version_id=v1,
self.bucket.configure_versioning(False,
self.assertNotEqual('Enabled',
S3MultiDeleteTest(unittest.TestCase):
'multidelete-%d'
test_delete_nothing(self):
self.bucket.delete_keys([])
test_delete_illegal(self):
self.bucket.delete_keys([{"dict":"notallowed"}])
test_delete_mix(self):
self.bucket.delete_keys(["king",
("mice",
Key(name="regular"),
Key(),
Prefix(name="folder/"),
DeleteMarker(name="deleted"),
{"bad":"type"}])
test_delete_quietly(self):
self.bucket.delete_keys(["king"],
quiet=True)
test_delete_must_escape(self):
self.bucket.delete_keys([Key(name=">_<;")])
test_delete_unknown_version(self):
no_ver
Key(name="no")
no_ver.version_id
"version"
self.bucket.delete_keys([no_ver])
test_delete_kanji(self):
self.bucket.delete_keys([u"漢字",
Key(name=u"日本語")])
test_delete_empty_by_list(self):
test_delete_kanji_by_list(self):
[u"漢字",
u"日本語",
u"テスト"]:
test_delete_with_prefixes(self):
["a",
"a/b",
"b"]:
self.bucket.delete_keys(self.bucket.list(delimiter="/"))
self.assertEqual(result.errors[0].key,
self.assertEqual(result.deleted[0].key,
"a/b")
test_delete_too_many_versions(self):
1000)]
test_1(self):
nkeys
nkeys)]
key_names:
S3MultiPartUploadTest(unittest.TestCase):
S3Connection(is_secure=False)
test_abort(self):
test_complete_ascii(self):
test_complete_japanese(self):
test_list_japanese(self):
next(iter(rs))
mpu.id)
lmpu.cancel_upload()
test_list_multipart_uploads(self):
mpus
ompu
mpus.pop(0)
ompu.key_name)
ompu.id)
len(mpus))
test_get_all_multipart_uploads(self):
'b/c'
mpu1
self.bucket.initiate_multipart_upload(key1)
mpu2
self.bucket.initiate_multipart_upload(key2)
self.bucket.get_all_multipart_uploads(prefix='b/',
mpu2.key_name)
mpu2.id)
test_four_part_file(self):
test_etag_of_parts(self):
"etagtest"
uparts
size=5))
part_num=2))
lpart
self.assertEqual(uparts[pn].etag,
lpart.etag)
S3MultiPartUploadSigV4Test(unittest.TestCase):
self.env_patch
{'S3_USE_SIGV4':
self.env_patch.start()
boto.s3.connect_to_region('us-west-2')
location='us-west-2')
self.env_patch.stop()
test_initiate_multipart(self):
"multipart"
multipart_upload
multipart_uploads
self.bucket.get_all_multipart_uploads()
multipart_uploads:
self.assertEqual(upload.key_name,
multipart_upload.key_name)
self.assertEqual(upload.id,
multipart_upload.id)
multipart_upload.cancel_upload()
test_upload_part_by_size(self):
spawn(function,
Thread(target
function,
put_object(bucket,
bucket.new_key(name).set_contents_from_string(name)
get_object(bucket,
bucket.get_key(name).get_contents_as_string().decode('utf-8')
test_close_connections():
test_close_connections")
s3.get_all_buckets():
b.name.startswith('test-'):
b.get_all_keys():
b.delete()
[str(uuid.uuid4)
range(30)]
spawn(put_object,
spawn(get_object,
WriteAndCount(object):
time.sleep(0)
read_big_object(s3,
count):
range(count):
bucket.get_key(name)
WriteAndCount()
key.get_contents_to_file(out)
BIG_SIZE:
print(out.size,
BIG_SIZE)
size:",
s3._pool.size())
LittleQuerier(object):
small_names):
self.small_names
self.thread
spawn(self.run)
self.thread.join()
self.bucket.get_key(self.small_names[i])
'response-content-type'
'small/'
key.get_contents_as_string(response_headers
rh).decode('utf-8')
actual:
print("AHA:",
repr(expected),
repr(actual))
test_reuse_connections():
test_reuse_connections")
[str(uuid.uuid4())
range(4)]
enumerate(small_names):
bucket.new_key(name).set_contents_from_string(str(i))
stale")
time.sleep(s3._pool.STALE_DURATION
s3._pool.clean()
s3._pool.size()
empty")
big_name
"-"
bucket.new_key(big_name).set_contents_from_string(contents)
spawn(read_big_object,
s3,
big_name,
queriers
LittleQuerier(bucket,
small_names)
queriers:
q.stop()
test_close_connections()
test_reuse_connections()
S3VersionTest
'version-%d'
self.assertFalse('Versioning'
v2'
k2.get_contents_as_string().decode('utf-8')
k.get_contents_as_string(version_id=v1).decode('utf-8')
k.get_contents_as_string(version_id=v2).decode('utf-8')
self.assertEqual(v2,
rs[0].version_id)
rs[1].version_id)
self.bucket.get_all_keys()
self.bucket.delete_key('foobar')
self.assertTrue(isinstance(rs[0],
[k.version_id
rs]
self.assertTrue(v1
self.assertTrue(v2
self.bucket.configure_versioning(False)
test_latest_version(self):
"key"
kv1
kv1.set_contents_from_string("v1")
next(iter(self.bucket.get_all_versions()))
self.assertEqual(listed_kv1.name,
kv2
kv2.set_contents_from_string("v2")
self.bucket.delete_key(key_name)
listed_kv3
self.assertNotEqual(listed_kv3.version_id,
self.assertEqual(listed_kv3.is_latest,
boto.sdb
SDBCertVerificationTest(unittest.TestCase,
conn.get_all_domains()
SDBConnectionTest
c.create_domain('bad:domain:name')
SDBResponseError:
c.create_domain(domain_name)
item_1
'item1'
same_value
'same_value'
attrs_1
'diff_value_1'}
domain.put_attributes(item_1,
attrs_1)
item_2
'item2'
attrs_2
'diff_value_2'}
domain.put_attributes(item_2,
attrs_2)
domain.get_attributes(item_1,
len(item.keys())
len(attrs_1.keys())
item['name1']
attrs_1['name1']
item['name2']
attrs_1['name2']
name1="%s"'
(domain_name,
same_value)
name2="diff_value_2"'
domain.delete_attributes(item_1)
{'name3_1':
'value3_1',
'name3_2':
'value3_2',
'name3_3':
['value3_3_1',
'value3_3_2']}
{'name4_1':
'value4_1',
'name4_2':
['value4_2_1',
'value4_2_2'],
'name4_3':
'value4_3'}
item3,
'item4':
item4}
domain.batch_put_attributes(items)
item['name3_2']
'value3_2'
item3}
domain.batch_delete_attributes(items)
domain.batch_delete_attributes({'item4':
domain.get_attributes('item4',
c.delete_domain(domain)
SESCertVerificationTest(unittest.TestCase,
boto.ses.regions()
conn.list_verified_email_addresses()
SESConnectionTest(unittest.TestCase):
self.ses
SESConnection()
test_get_dkim_attributes(self):
self.ses.get_identity_dkim_attributes(['example.com'])
self.assertTrue('GetIdentityDkimAttributesResponse'
self.assertTrue('GetIdentityDkimAttributesResult'
response['GetIdentityDkimAttributesResponse'])
'DkimAttributes'
['GetIdentityDkimAttributesResult'])
test_set_identity_dkim_enabled(self):
self.assertRaises(exceptions.SESIdentityNotVerifiedError):
self.ses.set_identity_dkim_enabled('example.com',
test_verify_domain_dkim(self):
self.assertRaises(exceptions.SESDomainNotConfirmedError):
self.ses.verify_domain_dkim('example.com')
SNSCertVerificationTest(unittest.TestCase,
boto.sns.regions()
conn.get_all_topics()
StubResponse(object):
'nopenopenope'
getheaders(self):
TestSNSConnection(unittest.TestCase):
test_list_platform_applications(self):
test_forced_host(self):
https
http_client.HTTPConnection
'request')
'getresponse',
return_value=StubResponse()):
self.assertRaises(self.connection.ResponseError):
mock_request.call_args_list[0]
call[0][3]
self.assertTrue('Host'
'sns.us-west-2.amazonaws.com')
SNSSubcribeSQSTest(unittest.TestCase):
self.sqsc
self.snsc
SNSConnection()
get_policy_statements(self,
json.loads(attrs.get('Policy',
"{}"))
policy.get('Statement',
test_correct_sid(self):
"test_correct_sid%d"
expected_sid
hashlib.md5((topic_arn
queue_arn).encode('utf-8')).hexdigest()
statements:
statement['Sid']
expected_sid:
self.assertTrue(found_expected_sid)
test_idempotent_subscribe(self):
"test_idempotent_subscribe%d"
initial_statements
first_subscribe_statements
self.assertEqual(len(first_subscribe_statements),
len(initial_statements)
second_subscribe_statements
self.assertEqual(len(second_subscribe_statements),
len(first_subscribe_statements))
queue.set_message_class(BigMessage)
s3.create_bucket(queue_name)
self.addCleanup(s3.delete_bucket,
msg_body
big
StringIO(msg_body)
's3://%s'
queue.new_message(fp,
s3_url=s3_url)
queue.write(message)
s3_object_name
message.s3_url.split('/')[-1]
self.assertTrue(bucket.lookup(s3_object_name))
queue.read()
self.assertEqual(m.get_body().decode('utf-8'),
msg_body)
m.delete()
self.assertIsNone(bucket.lookup(s3_object_name))
boto.sqs
SQSCertVerificationTest(unittest.TestCase,
boto.sqs.regions()
conn.get_all_queues()
SQSConnectionTest(unittest.TestCase):
c.create_queue('bad*queue*name')
self.fail('queue
bad')
queue_1
queue_1,
q.get_attributes()
q.get_attributes('ApproximateNumberOfMessages')
q.get_attributes('VisibilityTimeout')
queue_1.set_timeout(timeout)
test\n'
queue_1.new_message(message_body)
queue_1.write(message)
queue_1.read(visibility_timeout=10)
message.get_body()
queue_1.delete_message(message)
num_msgs:
queue_1.get_messages(num_msgs)
queue_1.delete_message_batch(msgs)
len(br.results)
'Hello,
queue_2
queue_2,
queue_2.set_message_class(MHMessage)
queue_2.new_message()
message['foo']
{'fie':
queue_2.new_message(body=message_body)
queue_2.read()
m['foo']
test_sqs_timeout(self):
'test_sqs_timeout_%s'
queue.read(visibility_timeout=None,
wait_time_seconds=poll_seconds)
block
self.assertIsNone(response)
c.send_message(queue,
c.receive_message(
wait_time_seconds=poll_seconds)[0]
blocked
c.get_queue_attributes(queue,
'ReceiveMessageWaitTimeSeconds')
self.assertEqual(attrs['ReceiveMessageWaitTimeSeconds'],
test_sqs_longpoll(self):
'test_sqs_longpoll_%s'
send_message():
messages.append(
queue.write(queue.new_message('this
message')))
Timer(5.0,
send_message)
self.addCleanup(t.join)
queue.read(wait_time_seconds=10)
messages[0].id)
self.assertEqual(response.get_body(),
messages[0].get_body())
self.assertTrue(4.0
(end
6.0)
test_queue_deletion_affects_full_queues(self):
len(conn.get_all_queues())
conn.create_queue('empty%d'
conn.create_queue('full%d'
full.write(m1)
self.assertEqual(full.count(),
self.assertTrue(conn.delete_queue(empty))
self.assertTrue(conn.delete_queue(full))
initial_count)
test_get_messages_attributes(self):
attributes='All'
int(first_rec)
self.assertTrue(first_rec
current_timestamp)
attributes='ApproximateReceiveCount'
test_queue_purge(self):
conn.purge_queue(test)
create_temp_queue(self,
conn.create_queue(queue_name)
self.addCleanup(conn.delete_queue,
put_queue_message(self,
queue.write(m1)
Location
StorageUriTest(unittest.TestCase):
nuke_bucket(self,
test_storage_uri_regionless(self):
S3Connection(
host='s3-us-west-2.amazonaws.com'
'keytest-%d'
conn.create_bucket(bucket_name,
location=Location.USWest2)
self.addCleanup(self.nuke_bucket,
boto.storage_uri('s3://%s/test'
suri.new_key()
the_key.key
'Test301'
the_key.set_contents_from_string(
region.'
alt_conn
boto.connect_s3(host='s3-us-west-2.amazonaws.com')
alt_bucket
alt_conn.get_bucket(bucket_name)
alt_bucket.get_key('Test301')
STSCertVerificationTest(unittest.TestCase,
boto.sts.regions()
conn.get_session_token()
SessionTokenTest(unittest.TestCase):
test_session_token(self):
Token
c.get_session_token()
token.save('token.json')
token_copy
Credentials.load('token.json')
token_copy.access_key
token.access_key
token_copy.secret_key
token.secret_key
token_copy.session_token
token.session_token
token_copy.expiration
token.expiration
token_copy.request_id
token.request_id
os.unlink('token.json')
token.is_expired()
S3Connection(aws_access_key_id=token.access_key,
aws_secret_access_key=token.secret_key,
security_token=token.session_token)
s3.get_all_buckets()
STSConnection(anon=True)
c.assume_role_with_web_identity(
self.assertTrue('Not
authorized'
test_decode_authorization_message(self):
c.decode_authorization_message('b94d27b9934')
self.assertIn('InvalidAuthorizationMessageException',
SupportCertVerificationTest(unittest.TestCase,
boto.support.regions()
conn.describe_services()
TestSupportLayer1Management(unittest.TestCase):
SupportConnection()
cases
preexisting_count
len(cases.get('cases',
services
self.api.describe_services()
self.assertTrue('services'
services)
service_codes
[serv['code']
serv
services['services']]
self.assertTrue('amazon-cloudsearch'
service_codes)
self.api.describe_severity_levels()
self.assertTrue('severityLevels'
severity)
severity_codes
[sev['code']
sev
severity['severityLevels']]
self.assertTrue('low'
severity_codes)
case_1
self.api.create_case(
subject='TEST:
I
am
case.',
service_code='amazon-cloudsearch',
category_code='other',
problem",
severity_code='low',
language='en'
case_1['caseId']
new_cases
self.assertTrue(len(new_cases['cases'])
preexisting_count)
self.api.add_communication_to_case(
solution.",
case_id=case_id
self.assertTrue(result.get('result',
final_cases
self.api.describe_cases(case_id_list=[case_id])
comms
final_cases['cases'][0]['recentCommunications']\
['communications']
self.assertEqual(len(comms),
close_result
self.api.resolve_case(case_id=case_id)
SWFCertVerificationTest(unittest.TestCase,
boto.swf.regions()
conn.list_domains('REGISTERED')
os.environ.get("BOTO_SWF_UNITTEST_DOMAIN",
"boto-swf-unittest-domain")
PAUSE_SECONDS
SimpleWorkflowLayer1TestBase(unittest.TestCase):
_domain
_workflow_execution_retention_period_in_days
'NONE'
_domain_description
domain'
_task_list
'tasklist1'
_workflow_type_name
'wft1'
_workflow_type_version
_workflow_type_description
'wft1
_default_child_policy
'REQUEST_CANCEL'
_default_execution_start_to_close_timeout
'600'
'60'
_activity_type_name
'at1'
_activity_type_version
_activity_type_description
'at1
_default_task_heartbeat_timeout
_default_task_schedule_to_close_timeout
'90'
_default_task_schedule_to_start_timeout
'10'
self.conn.register_domain(self._domain,
self._workflow_execution_retention_period_in_days,
description=self._domain_description)
swf_exceptions.SWFDomainAlreadyExistsError:
self.conn.register_workflow_type(self._domain,
default_child_policy=self._default_child_policy,
default_execution_start_to_close_timeout=
self._default_execution_start_to_close_timeout,
description=self._workflow_type_description)
self.conn.register_activity_type(self._domain,
default_task_heartbeat_timeout=
self._default_task_heartbeat_timeout,
default_task_schedule_to_close_timeout=
self._default_task_schedule_to_close_timeout,
default_task_schedule_to_start_timeout=
self._default_task_schedule_to_start_timeout,
description=self._activity_type_description)
SimpleWorkflowLayer1Test(SimpleWorkflowLayer1TestBase):
test_list_domains(self):
self.conn.list_domains('REGISTERED')
r['domainInfos']:
info['name']
self._domain_description,
test_list_workflow_types(self):
self.conn.list_workflow_types(self._domain,
info['workflowType']['name']
self._workflow_type_name
info['workflowType']['version']
self._workflow_type_version
self._workflow_type_description,
test_list_activity_types(self):
self.conn.list_activity_types(self._domain,
info['activityType']['name']
self._activity_type_name:
self._activity_type_description,
test_list_closed_workflow_executions(self):
start_latest_date=latest_date,
start_oldest_date=oldest_date)
close_oldest_date=oldest_date)
close_status='COMPLETED')
test_list_open_workflow_executions(self):
oldest_date)
tests.integration.swf.test_layer1
SimpleWorkflowLayer1TestBase
SwfL1WorkflowExecutionTest(SimpleWorkflowLayer1TestBase):
run_decider(self):
dtask
self.conn.poll_for_decision_task(self._domain,
dtask.get('taskToken')
ignorable
'DecisionTaskScheduled',
'DecisionTaskStarted',
'DecisionTaskTimedOut',
dtask['events']:
tevent['eventType']
ignorable:
Layer1Decisions()
'WorkflowExecutionStarted':
str(uuid.uuid1())
decisions.schedule_activity_task(activity_id,
input=event['workflowExecutionStartedEventAttributes']['input'])
'ActivityTaskCompleted':
decisions.complete_workflow_execution(
result=event['activityTaskCompletedEventAttributes']['result'])
'ActivityTaskFailed':
reason=event['activityTaskFailedEventAttributes']['reason'],
details=event['activityTaskFailedEventAttributes']['details'])
reason='unhandled
type;
(event['eventType'],))
self.conn.respond_decision_task_completed(dtask['taskToken'],
decisions=decisions._data,
execution_context=None)
run_worker(self):
atask
self.conn.poll_for_activity_task(self._domain,
identity='test
worker')
atask.get('activityId')
json.dumps(sum(json.loads(atask['input'])))
traceback.format_exc()
self.conn.respond_activity_task_completed(
self.conn.respond_activity_task_failed(
reason=reason,
details=details)
test_workflow_execution(self):
15]')
r['workflowExecutionCompletedEventAttributes']['result']
json.loads(result)
615
test_failed_workflow_execution(self):
"s"]')
r['workflowExecutionFailedEventAttributes']['reason']
live_connection
mturk_host
'http://www.example.com/'
config_environment():
'local.py')
execfile(local)
live_connection:
os.environ.setdefault('AWS_ACCESS_KEY_ID',
os.environ.setdefault('AWS_SECRET_ACCESS_KEY',
mocks
functools.partial(MTurkConnection,
host=mturk_host)
create_hit_test
create_hit_external
hit_persistence
doctest_suite
doctest.DocFileSuite(
*glob('*.doctest'),
**{'optionflags':
doctest.REPORT_ONLY_FIRST_FAILURE}
Program(unittest.TestProgram):
runTests(self,
self.test
unittest.TestSuite([self.test,
doctest_suite])
super(Program,
self).runTests(*args,
Program()
description_filter(substring):
substring
hit.Title
disable_hit(hit):
conn.disable_hit(hit.HITId)
dispose_hit(hit):
assignment
conn.get_assignments(hit.HITId):
assignment.AssignmentStatus
'Submitted':
conn.approve_assignment(assignment.AssignmentId)
conn.dispose_hit(hit.HITId)
cleanup():
is_boto
description_filter('Boto')
'getting
hits...'
all_hits
list(conn.get_all_hits())
is_reviewable
hit.HITStatus
'Reviewable'
is_not_reviewable
is_reviewable(hit)
hits_to_process
filter(is_boto,
all_hits)
hits_to_disable
filter(is_not_reviewable,
hits_to_dispose
filter(is_reviewable,
'disabling/disposing
%d/%d
hits'
(len(hits_to_disable),
len(hits_to_dispose))
map(disable_hit,
hits_to_disable)
map(dispose_hit,
hits_to_dispose)
len(all_hits)
len(hits_to_process)
skipped
'Processed:
%(total_hits)d
HITs,
disabled/disposed:
%(hits_processed)d,
skipped:
%(skipped)d'
cleanup()
Question,
QuestionContent,
AnswerSpecification,
FreeTextAnswer,
MTurkCommon(unittest.TestCase):
get_question():
qn_content
QuestionContent()
qn_content.append_field('Title',
'Boto
content')
qn_content.append_field('Text',
'What
type?')
Question(identifier=str(uuid.uuid4()),
content=qn_content,
answer_spec=AnswerSpecification(FreeTextAnswer()))
get_hit_params():
lifetime=datetime.timedelta(minutes=65),
title='Boto
title',
description='Boto
'test'],
reward=0.23,
duration=datetime.timedelta(minutes=6),
'HITAssignmentSummary',],
Test(unittest.TestCase):
test_create_hit_external(self):
ExternalQuestion(external_url=external_url,
'HITAssignmentSummary',])
TestHITCreation(MTurkCommon):
testCallCreateHitWithOneQuestion(self):
testCallCreateHitWithQuestionForm(self):
questions=QuestionForm([self.get_question()]),
PercentAssignmentsApprovedRequirement
ExternalQuestion(external_url="http://websort.net/s/F3481C",
MTurkConnection(host='mechanicalturk.sandbox.amazonaws.com')
qualifications.add(PercentAssignmentsApprovedRequirement(comparator="GreaterThan",
integer_value="95"))
create_hit_rs.HITTypeId
TestHITPersistence(MTurkCommon):
create_hit_result(self):
test_pickle_hit_result(self):
test_pickle_deserialized_version(self):
pickle.dumps(new_result)
RealMTurkConnection
MTurkConnection(RealMTurkConnection):
saved_args
self.__dict__.setdefault('_mock_saved_args',
dict())
saved_args['_process_request']
(args,
argparse.ArgumentParser(
description="Run
parser.add_argument('test_name')
doctest.testfile(
args.test_name,
optionflags=doctest.REPORT_ONLY_FIRST_FAILURE
boto.mturk.test.support
sel_args
4444,
'*chrome',
'https://workersandbox.mturk.com')
SeleniumFailed(object):
has_selenium():
globals().update(selenium=selenium)
sel.do_command('shutdown',
Exception'
SeleniumFailed('selenium
RC
initializing
selenium:
SeleniumFailed(msg)
globals().update(has_selenium=lambda:
skip_unless_has_selenium():
has_selenium()
unittest.skip(res.message)
complete_hit(hit_type_id,
response='Some
Response'):
verificationErrors
sel.start()
sel.open("/mturk/welcome")
sel.click("lnkWorkerSignin")
sel.type("email",
"boto.tester@example.com")
sel.type("password",
"BotoTest")
sel.click("Continue")
sel.open("/mturk/preview?groupId={hit_type_id}".format(**vars()))
sel.click("/accept")
sel.type("Answer_1_FreeText",
sel.click("//div[5]/table/tbody/tr[2]/td[1]/input")
sel.click("link=Sign
Out")
sel.stop()
tests.mturk.support
MTurkRequestError
TestDisableHITs(MTurkCommon):
test_disable_invalid_hit(self):
self.assertRaises(MTurkRequestError,
self.conn.disable_hit,
AWSMockServiceTestCase(unittest.TestCase):
mock.Mock(spec=http_client.HTTPSConnection)
self.https_connection.debuglevel
mock.Mock(return_value=self.https_connection),
self.create_service_connection(
initialize_service_connection(self):
self.original_mexe
self._mexe_spy
self.connection_class
"set
non-None
value.")
self.connection_class(**kwargs)
_mexe_spy(self,
self.original_mexe(request,
create_response(self,
mock.Mock(spec=http_client.HTTPResponse)
response.read.return_value
response.getheaders.return_value
overwrite_header(arg,
header_dict
header_dict:
header_dict[arg]
response.getheader.side_effect
overwrite_header
assert_request_parameters(self,
ignore_params_values=None):
request_params
self.actual_request.params.copy()
ignore_params_values
ignore_params_values:
request_params[param]
self.assertDictEqual(request_params,
set_http_response(self,
self.create_response(status_code,
self.https_connection.getresponse.return_value
MockServiceWithConfigTestCase(AWSMockServiceTestCase):
super(MockServiceWithConfigTestCase,
AWSAuthConnection,
TestListParamsSerialization(unittest.TestCase):
AWSQueryConnection('access_key',
test_complex_list_serialization(self):
self.connection.build_complex_list_params(
'baz'),
('foo2',
'baz2')],
'ParamName.member',
('One',
'Three'))
'ParamName.member.1.One':
'ParamName.member.1.Two':
'ParamName.member.1.Three':
'ParamName.member.2.One':
'foo2',
'ParamName.member.2.Two':
'ParamName.member.2.Three':
'baz2',
test_simple_list_serialization(self):
self.connection.build_list_params(
'ParamName.member')
'ParamName.member.1':
'ParamName.member.2':
'ParamName.member.3':
MockAWSService(AWSQueryConnection):
'2012-01-01'
['sign-v2']
AWSQueryConnection.__init__(self,
TestAWSAuthConnection(unittest.TestCase):
test_get_path(self):
self.assertEqual(conn.get_path('/'),
self.assertEqual(conn.get_path('image.jpg'),
'/image.jpg')
self.assertEqual(conn.get_path('folder/image.jpg'),
'/folder/image.jpg')
self.assertEqual(conn.get_path('folder//image.jpg'),
self.assertEqual(conn.get_path('/folder//image.jpg'),
self.assertEqual(conn.get_path('/folder////image.jpg'),
'/folder////image.jpg')
self.assertEqual(conn.get_path('///folder////image.jpg'),
'///folder////image.jpg')
test_connection_behind_proxy(self):
"http://john.doe:p4ssw0rd@127.0.0.1:8180"
self.assertEqual(conn.proxy_user,
'john.doe')
self.assertEqual(conn.proxy_pass,
'p4ssw0rd')
'8180')
test_get_proxy_url_with_auth(self):
self.assertEqual(conn.get_proxy_url_with_auth(),
'http://john.doe:p4ssw0rd@127.0.0.1:8180')
test_build_base_http_request_noproxy(self):
'mockservice.cc-zone-1.amazonaws.com'
conn.build_base_http_request('GET',
self.assertEqual(request.path,
test_connection_behind_proxy_without_explicit_port(self):
"http://127.0.0.1"
port=8180
8180)
@mock.patch.object(socket,
'create_connection')
@mock.patch('boto.compat.http_client.HTTPResponse')
@mock.patch('boto.compat.http_client.ssl')
test_proxy_ssl(self,
ssl_mock,
http_response_mock,
create_connection_mock):
type(http_response_mock.return_value).status
mock.PropertyMock(
return_value=200)
proxy_port=80
conn.https_validate_certificates
conn.proxy_ssl('mockservice.cc-zone-1.amazonaws.com',
test_host_header_with_nonstandard_port(self):
'testhost')
port=8773)
'testhost:8773')
V4AuthConnection(AWSAuthConnection):
port=443):
AWSAuthConnection.__init__(
port=port)
TestAWSQueryConnection(unittest.TestCase):
name='cc-zone-1',
endpoint='mockservice.cc-zone-1.amazonaws.com',
connection_cls=MockAWSService)
TestAWSQueryConnectionSimple(TestAWSQueryConnection):
test_query_connection_basis(self):
self.assertEqual(conn.host,
'mockservice.cc-zone-1.amazonaws.com')
test_query_connection_noproxy(self):
proxy_port="3128")
test_query_connection_noproxy_nosecure(self):
'insecure'}),
proxy_port="3128",
test_single_command(self):
self.assertEqual(args[b'SignatureMethod'],
[b'HmacSHA256'])
self.assertEqual(args[b'Version'],
[conn.APIVersion.encode('utf-8')])
self.assertEqual(args[b'par1'],
self.assertEqual(args[b'par2'],
test_multi_commands(self):
conn.make_request('myCmd2',
{'par3':
'par4':
'narf'},
self.assertEqual(body1[b'par1'],
self.assertEqual(body1[b'par2'],
body1[b'par3']
self.assertEqual(body2[b'par3'],
[b'bar'])
self.assertEqual(body2[b'par4'],
[b'narf'])
body2['par1']
self.assertEqual(resp1.read(),
self.assertEqual(resp2.read(),
test_non_secure(self):
'normal'}),
"normal"}')
test_alternate_port(self):
'http://%s:8080/'
'alternate'}),
port=8080,
"alternate"}')
test_temp_failure(self):
[HTTPretty.Response(body="{'test':
'fail'}",
status=500),
HTTPretty.Response(body="{'test':
'success'}",
status=200)]
'https://%s/temp_fail/'
responses=responses)
'/temp_fail/',
b"{'test':
'success'}")
test_unhandled_exception(self):
'https://%s/temp_exception/'
responses=[])
fake_connection(address,
timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
source_address=None):
socket.timeout('fake
socket.create_connection
fake_connection
conn.num_retries
self.assertRaises(socket.error):
'/temp_exception/',
test_connection_close(self):
content_type='application/json',
connection='close')
mock_put_conn(*args,
Exception('put_http_connection
called!')
conn.put_http_connection
mock_put_conn
self.assertEqual(resp1.getheader('connection'),
'close')
test_port_pooling(self):
port=8080)
con1
con1)
con2
self.assertEqual(con1,
conn.port
con3
self.assertNotEqual(con1,
TestAWSQueryStatus(TestAWSQueryConnection):
test_get_status(self):
'<status>ok</status>',
"ok")
test_get_status_blank_error(self):
test_get_status_error(self):
'<status>error</status>',
content_type='text/xml',
TestHTTPRequest(unittest.TestCase):
test_user_agent_not_url_encoded(self):
u'should
\u2713',
UserAgent}
mock_add_auth(req,
mock_connection.headers_at_auth
req.headers.copy()
mock_connection._auth_handler.add_auth
mock_add_auth
self.assertEqual(mock_connection.headers_at_auth,
'should
%E2%9C%93',
UserAgent})
test_content_length_str(self):
self.assertIsInstance(request.headers['Content-Length'],
S3CreateError,
HTTPretty,
httprettified
TestBotoServerError(unittest.TestCase):
test_botoservererror_basics(self):
test_message_elb_xml(self):
Load
Balancer
webapp-balancer2')
self.assertEqual(bse.request_id,
'093f80d0-4473-11e1-9234-edce8ec08e2d')
'LoadBalancerNotFound')
test_message_sd_xml(self):
bse.error_message,
perform
(sdb:CreateDomain)
'resource
(arn:aws:sdb:us-east-1:xxxxxxx:domain/test_domain).
'Contact
owner.')
self.assertEqual(bse.box_usage,
'0.0055590278')
'AuthorizationFailure')
'403')
'Forbidden')
test_xmlns_not_loaded(self):
'<ErrorResponse
xmlns="http://elasticloadbalancing.amazonaws.com/doc/2011-11-15/">'
test_xml_entity_not_loaded(self):
'<!DOCTYPE
[<!ENTITY
xxe
SYSTEM
"http://aws.amazon.com/">]><Message>error:&xxe;</Message>'
test_message_storage_create_error(self):
s3ce
S3CreateError('409',
'Conflict',
self.assertEqual(s3ce.bucket,
'cmsbk')
self.assertEqual(s3ce.error_code,
'BucketAlreadyOwnedByYou')
self.assertEqual(s3ce.status,
'409')
self.assertEqual(s3ce.reason,
'Conflict')
s3ce.error_message,
succeeded
it.')
self.assertEqual(s3ce.error_message,
s3ce.message)
self.assertEqual(s3ce.request_id,
'FF8B86A32CC3FE4F')
test_message_json_response_error(self):
'__type':
'com.amazon.coral.validate#ValidationException',
count'}
jre
JSONResponseError('400',
self.assertEqual(jre.status,
self.assertEqual(jre.reason,
body['message'])
jre.message)
'ValidationException')
jre.error_code)
test_message_not_xml(self):
XML'
test_getters(self):
self.assertEqual(bse.code,
bse.error_code)
self.assertEqual(bse.message,
bse.error_message)
load_endpoint_json,
merge_endpoints
load_regions,
TestRegionInfo(object):
FakeConn(object):
TestEndpointLoading(unittest.TestCase):
super(TestEndpointLoading,
test_load_endpoint_json(self):
self.assertTrue('ec2'
endpoints)
endpoints['ec2']['us-east-1'],
'ec2.us-east-1.amazonaws.com'
test_merge_endpoints(self):
'ec2.us-east-1.amazonaws.com',
additions
additions)
self.assertEqual(endpoints,
test_load_regions(self):
self.assertFalse('test-1'
'test_endpoints.json'
self.addCleanup(os.environ.pop,
'BOTO_ENDPOINTS')
self.assertTrue('test-1'
self.assertEqual(endpoints['ec2']['test-1'],
'ec2.test-1.amazonaws.com')
test_get_regions(self):
get_regions('ec2')
test_get_regions_overrides(self):
region_cls=TestRegionInfo,
connection_cls=FakeConn
self.assertFalse(isinstance(west_2,
TestRegionInfo))
FakeConn)
HmacAuthV4Handler
S3HmacAuthV4Handler
TestSigV4Handler(unittest.TestCase):
test_not_adding_empty_qs(self):
self.assertEqual(req.path,
'/-/vaults/foo/archives')
test_inner_whitespace_is_collapsed(self):
self.request.headers['x-amz-archive-description']
spaces'
self.request.headers['x-amz-quoted-string']
auth.headers_to_sign(self.request)
'x-amz-archive-description':
spaces',
'x-amz-glacier-version':
'x-amz-quoted-string':
'})
self.assertEqual(auth.canonical_headers(headers),
'host:glacier.us-east-1.amazonaws.com\n'
'x-amz-archive-description:two
spaces\n'
'x-amz-glacier-version:2012-06-01\n'
'x-amz-quoted-string:"a
c"')
request.params['Foo.1']
'aaa'
request.params['Foo.10']
'zzz'
auth.canonical_query_string(request)
'Foo.1=aaa&Foo.10=zzz')
test_query_string(self):
HmacAuthV4Handler('sns.us-east-1.amazonaws.com',
utf-8'.encode('utf-8'),
'sns.us-east-1.amazonaws.com',
auth.query_string(request)
'Message=We%20%E2%99%A5%20utf-8')
'x/x%20.html')
'x/./././x/html/',
'x/x/html/')
'\\x\\x.html',
'/x/x.html')
test_credential_scope(self):
HmacAuthV4Handler('iam.amazonaws.com',
'iam.amazonaws.com',
HmacAuthV4Handler('iam.us-gov.amazonaws.com',
'iam.us-gov.amazonaws.com',
'us-gov-west-1')
HmacAuthV4Handler('iam.us-west-1.amazonaws.com',
'iam.us-west-1.amazonaws.com',
'us-west-1')
HmacAuthV4Handler('localhost',
service_name='iam')
timestamp,
credential_scope.split('/')
self.assertEqual(region,
self.assertEqual(service,
'iam')
test_headers_to_sign(self):
'glacier.us-east-1.amazonaws.com:8080')
test_region_and_service_can_be_overriden(self):
self.request.headers['X-Amz-Date']
'20121121000000'
auth.region_name
auth.service_name
auth.credential_scope(self.request)
self.assertEqual(scope,
'20121121/us-west-2/sqs/aws4_request')
test_pickle_works(self):
access_key='access_key',
secret_key='secret_key')
pickled
pickle.dumps(auth)
auth2
pickle.loads(pickled)
self.assertEqual(auth.host,
auth2.host)
test_bytes_header(self):
'x-amz-hash':
b'f00'},
auth.canonical_request(request)
self.assertIn('f00',
canonical)
TestS3HmacAuthV4Handler(unittest.TestCase):
'sekret_tokens'
'/awesome-bucket/?max-keys=0',
region_name='s3-us-west-2'
test_clean_region_name(self):
self.auth.clean_region_name('us-west-2')
self.auth.clean_region_name('s3-us-west-2')
self.auth.clean_region_name('s3.amazonaws.com')
self.auth.clean_region_name('something-s3-us-west-2')
'something-s3-us-west-2')
test_region_stripping(self):
provider=self.provider
region_name='us-west-2'
self.assertEqual(self.auth.region_name,
test_determine_region_name(self):
self.auth.determine_region_name('s3-us-west-2.amazonaws.com')
'x/./././~x
'x/./././~x%20.html')
test_determine_service_name(self):
's3.us-west-2.amazonaws.com'
's3-us-west-2.amazonaws.com'
'bucket.s3.us-west-2.amazonaws.com'
'bucket.s3-us-west-2.amazonaws.com'
self.assertFalse('x-amz-content-sha256'
self.auth.add_auth(self.request)
self.assertTrue('x-amz-content-sha256'
the_sha
self.request.headers['x-amz-content-sha256']
the_sha,
'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
test_host_header(self):
self.auth.host_header(
self.awesome_bucket_request.host,
self.assertEqual(host,
'awesome-bucket.s3-us-west-2.amazonaws.com')
self.auth.canonical_query_string(self.awesome_bucket_request)
self.assertEqual(qs,
'max-keys=0')
test_correct_handling_of_plus_sign(self):
'hello+world.txt',
'hello%2Bworld.txt',
test_mangle_path_and_params(self):
path='/?delete&max-keys=0',
'/?delete&max-keys=0')
'delete':
test_unicode_query_string(self):
method='HEAD',
path=u'/?max-keys=1&prefix=El%20Ni%C3%B1o',
auth_path=u'/awesome-bucket/?max-keys=1&prefix=El%20Ni%C3%B1o',
u'/?max-keys=1&prefix=El%20Ni%C3%B1o')
u'/awesome-bucket/')
u'max-keys':
u'prefix':
u'El
Ni\xf1o',
test_canonical_request(self):
self.auth.canonical_request(self.awesome_bucket_request)
copy.copy(self.awesome_bucket_request)
request.auth_path
'/?max-keys=0'
request.params
self.auth.canonical_request(request)
test_non_string_headers(self):
self.awesome_bucket_request.headers['Content-Length']
self.auth.canonical_headers(
self.awesome_bucket_request.headers)
'content-length:8\n'
'user-agent:Boto\n'
'x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae'
'41e4649b934ca495991b7852b855\n'
'x-amz-date:20130605T193245Z'
FakeS3Connection(object):
kwargs.pop('host',
FakeEC2Connection(object):
@detect_potential_sigv4
TestS3SigV4OptIn(MockServiceWithConfigTestCase):
FakeS3Connection
region_groups
['.cn-north',
'-eu-central']
specific_regions
['.ap-northeast-2',
'-ap-south-1']
region_groups:
'-1.amazonaws.com')
specific_regions:
'.amazonaws.com')
FakeS3Connection()
self.environ['S3_USE_SIGV4']
TestSigV4OptIn(MockServiceWithConfigTestCase):
FakeEC2Connection
super(TestSigV4OptIn,
self.standard_region
endpoint='ec2.us-west-2.amazonaws.com'
self.sigv4_region
endpoint='ec2.cn-north-1.amazonaws.com.cn'
FakeEC2Connection(region=self.sigv4_region)
self.environ['EC2_USE_SIGV4']
STSAnonHandler
TestSTSAnonHandler(unittest.TestCase):
host='sts.amazonaws.com',
'2011-06-15',
'web-identity-federation',
'Atza|IQEBLjAsAhRkcxQ',
test_escape_value(self):
auth._escape_value('Atza|IQEBLjAsAhRkcxQ')
'Atza%7CIQEBLjAsAhRkcxQ')
test_build_query_string(self):
auth._build_query_string(self.request.params)
self.assertEqual(req.body,
TestAWSLambda(AWSMockServiceTestCase):
test_upload_function_binary(self):
function_zip=function_data,
test_upload_function_file(self):
rootdir
self.addCleanup(shutil.rmtree,
rootdir)
'test_file'
os.path.join(rootdir,
f.write(function_data)
function_zip=f,
self.assertEqual(self.actual_request.body.read(),
test_upload_function_unseekable_file_no_tell(self):
socket.socket()
function_zip=sock,
test_upload_function_unseekable_file_cannot_tell(self):
mock_file
mock_file.tell.side_effect
function_zip=mock_file,
FakeError(object):
TestExceptions(unittest.TestCase):
test_exception_class_names(self):
FakeError('TooManyApplications',
FakeError('TooManyApplicationsException',
self.assertEqual(exception.message,
TestListAvailableSolutionStacks(AWSMockServiceTestCase):
{u'ListAvailableSolutionStacksResponse':
{u'ListAvailableSolutionStacksResult':
{u'SolutionStackDetails':
[u'war',
u'zip'],
7'},
[u'zip'],
5.3'}],
u'SolutionStacks':
5.3']},
u'request_id'}}}).encode('utf-8')
test_list_available_solution_stacks(self):
self.service_connection.list_available_solution_stacks()
stack_details
['SolutionStackDetails']
solution_stacks
['SolutionStacks']
self.assertEqual(solution_stacks,
5.3'])
'ListAvailableSolutionStacks',
TestCreateApplicationVersion(AWSMockServiceTestCase):
'CreateApplicationVersionResponse':
{u'CreateApplicationVersionResult':
{u'ApplicationVersion':
{u'ApplicationName':
u'application1',
u'DateCreated':
u'DateUpdated':
u'SourceBundle':
{u'S3Bucket':
u'elasticbeanstalk-us-east-1',
u'S3Key':
u'resources/elasticbeanstalk-sampleapp.war'},
u'VersionLabel':
u'version1'}}}}).encode('utf-8')
test_create_application_version(self):
self.service_connection.create_application_version(
s3_bucket='mybucket',
s3_key='mykey',
auto_create_application=True)
app_version
api_response['CreateApplicationVersionResponse']\
['CreateApplicationVersionResult']\
['ApplicationVersion']
'CreateApplicationVersion',
'AutoCreateApplication':
'SourceBundle.S3Bucket':
'SourceBundle.S3Key':
self.assertEqual(app_version['ApplicationName'],
'application1')
self.assertEqual(app_version['VersionLabel'],
'version1')
TestCreateEnvironment(AWSMockServiceTestCase):
json.dumps({}).encode('utf-8')
test_create_environment(self):
'VALUE1')])
test_create_environment_with_tier(self):
'VALUE1')],
tier_name='Worker',
tier_type='SQS/HTTP',
tier_version='1.0')
'Tier.Name':
'Worker',
'Tier.Type':
'SQS/HTTP',
'Tier.Version':
SAMPLE_TEMPLATE
CloudFormationConnectionBase(AWSMockServiceTestCase):
super(CloudFormationConnectionBase,
u'arn:aws:cloudformation:us-east-1:18:stack/Name/id'
TestCloudFormationCreateStack(CloudFormationConnectionBase):
{u'CreateStackResponse':
{u'CreateStackResult':
test_create_stack_has_correct_request_params(self):
'myKeyName')],
capabilities=['CAPABILITY_IAM']
'Capabilities.member.1':
'CAPABILITY_IAM',
test_create_stack_with_minimum_args(self):
self.service_connection.create_stack('stack_name')
test_create_stack_fails(self):
body=b'{"Error":
self.assertRaisesRegexp(self.service_connection.ResponseError,
arg.'):
test_create_stack_fail_error(self):
body=b'{"RequestId":
"abc",
"Error":
self.assertEqual('abc',
e.request_id)
e.error_code)
self.assertEqual('Invalid
arg.',
TestCloudFormationUpdateStack(CloudFormationConnectionBase):
{u'UpdateStackResponse':
{u'UpdateStackResult':
test_update_stack_all_args(self):
'myKeyName'),
('KeyName2',
('KeyName3',
('KeyName4',
('KeyName5',
"Ignore
Me",
True)],
use_previous_template=True
'Parameters.member.2.ParameterKey':
'KeyName2',
'Parameters.member.2.UsePreviousValue':
'Parameters.member.3.ParameterKey':
'KeyName3',
'Parameters.member.3.ParameterValue':
'Parameters.member.4.UsePreviousValue':
'Parameters.member.4.ParameterKey':
'KeyName4',
'Parameters.member.5.UsePreviousValue':
'Parameters.member.5.ParameterKey':
'KeyName5',
'UsePreviousTemplate':
test_update_stack_with_minimum_args(self):
self.service_connection.update_stack('stack_name')
test_update_stack_fails(self):
body=b'Invalid
arg.')
TestCloudFormationDeleteStack(CloudFormationConnectionBase):
{u'DeleteStackResponse':
{u'ResponseMetadata':
test_delete_stack(self):
json.loads(self.default_body().decode('utf-8')))
'DeleteStack',
test_delete_stack_fails(self):
TestCloudFormationDescribeStackResource(CloudFormationConnectionBase):
test_describe_stack_resource(self):
'DescribeStackResource',
'resource_id',
test_describe_stack_resource_fails(self):
TestCloudFormationGetTemplate(CloudFormationConnectionBase):
test_get_template(self):
'GetTemplate',
test_get_template_fails(self):
TestCloudFormationGetStackevents(CloudFormationConnectionBase):
test_describe_stack_events(self):
self.service_connection.describe_stack_events('stack_name',
self.assertEqual(first.event_id,
'Event-1-Id')
'MyStack_One')
self.assertEqual(first.resource_properties,
initiated')
'AWS::CloudFormation::Stack')
self.assertEqual(second.event_id,
'Event-2-Id')
'MySG1')
'MyStack_SG1')
self.assertEqual(second.resource_properties,
'AWS::SecurityGroup')
'DescribeStackEvents',
TestCloudFormationDescribeStackResources(CloudFormationConnectionBase):
test_describe_stack_resources(self):
self.service_connection.describe_stack_resources(
'physical_resource_id')
self.assertEqual(first.description,
'MyDBInstance')
'MyStack_DB1')
'AWS::DBInstance')
self.assertEqual(second.description,
'MyAutoScalingGroup')
'MyStack_ASG1')
'AWS::AutoScalingGroup')
'DescribeStackResources',
'PhysicalResourceId':
'physical_resource_id',
TestCloudFormationDescribeStacks(CloudFormationConnectionBase):
self.service_connection.describe_stacks('MyStack')
self.assertEqual(stack.creation_time,
datetime(2012,
31))
self.assertEqual(stack.description,
Description')
self.assertEqual(stack.disable_rollback,
self.assertEqual(stack.stack_id,
self.assertEqual(stack.stack_status,
self.assertEqual(stack.stack_name,
self.assertEqual(stack.stack_name_reason,
self.assertEqual(stack.stack_status_reason,
self.assertEqual(stack.timeout_in_minutes,
self.assertEqual(len(stack.outputs),
self.assertEqual(stack.outputs[0].description,
self.assertEqual(stack.outputs[0].key,
'ServerURL')
self.assertEqual(stack.outputs[0].value,
'http://url/')
self.assertEqual(len(stack.parameters),
self.assertEqual(stack.parameters[0].key,
'MyKey')
self.assertEqual(stack.parameters[0].value,
'MyValue')
self.assertEqual(len(stack.capabilities),
self.assertEqual(stack.capabilities[0].value,
self.assertEqual(len(stack.notification_arns),
self.assertEqual(stack.notification_arns[0].value,
'arn:aws:sns:region-name:account-name:topic-name')
self.assertEqual(len(stack.tags),
self.assertEqual(stack.tags['MyTagKey'],
'MyTagValue')
'DescribeStacks',
TestCloudFormationListStackResources(CloudFormationConnectionBase):
test_list_stack_resources(self):
resources
self.service_connection.list_stack_resources('MyStack',
self.assertEqual(len(resources),
self.assertEqual(resources[0].last_updated_time,
57))
self.assertEqual(resources[0].logical_resource_id,
'SampleDB')
self.assertEqual(resources[0].physical_resource_id,
'My-db-ycx')
self.assertEqual(resources[0].resource_status,
self.assertEqual(resources[0].resource_status_reason,
self.assertEqual(resources[0].resource_type,
'AWS::RDS::DBInstance')
self.assertEqual(resources[1].last_updated_time,
29,
23))
self.assertEqual(resources[1].logical_resource_id,
'CPUAlarmHigh')
self.assertEqual(resources[1].physical_resource_id,
'MyStack-CPUH-PF')
self.assertEqual(resources[1].resource_status,
self.assertEqual(resources[1].resource_status_reason,
self.assertEqual(resources[1].resource_type,
'AWS::CloudWatch::Alarm')
'ListStackResources',
TestCloudFormationListStacks(CloudFormationConnectionBase):
test_list_stacks(self):
self.service_connection.list_stacks(['CREATE_IN_PROGRESS'],
self.assertEqual(stacks[0].stack_id,
'arn:aws:cfn:us-east-1:1:stack/Test1/aa')
self.assertEqual(stacks[0].stack_status,
self.assertEqual(stacks[0].stack_name,
'vpc1')
self.assertEqual(stacks[0].creation_time,
44))
self.assertEqual(stacks[0].deletion_time,
self.assertEqual(stacks[0].template_description,
'ListStacks',
'StackStatusFilter.member.1':
TestCloudFormationValidateTemplate(CloudFormationConnectionBase):
test_validate_template(self):
self.service_connection.validate_template(template_body=SAMPLE_TEMPLATE,
template_url='http://url')
self.assertEqual(template.description,
self.assertEqual(len(template.template_parameters),
param1,
param2
template.template_parameters
self.assertEqual(param1.default_value,
self.assertEqual(param1.description,
'Type
launch')
self.assertEqual(param1.no_echo,
self.assertEqual(param1.parameter_key,
'InstanceType')
self.assertEqual(param2.default_value,
self.assertEqual(param2.description,
'EC2
KeyPair')
self.assertEqual(param2.no_echo,
self.assertEqual(param2.parameter_key,
self.assertEqual(template.capabilities_reason,
'Reason')
self.assertEqual(len(template.capabilities),
self.assertEqual(template.capabilities[0].value,
'ValidateTemplate',
TestCloudFormationCancelUpdateStack(CloudFormationConnectionBase):
test_cancel_update_stack(self):
self.service_connection.cancel_update_stack('stack_name')
'CancelUpdateStack',
TestCloudFormationEstimateTemplateCost(CloudFormationConnectionBase):
test_estimate_template_cost(self):
self.service_connection.estimate_template_cost(
template_body='{}')
'http://calculator.s3.amazonaws.com/calc5.html?key=cf-2e351785-e821-450c-9d58-625e1e1ebfb6')
'EstimateTemplateCost',
TestCloudFormationGetStackPolicy(CloudFormationConnectionBase):
test_get_stack_policy(self):
self.service_connection.get_stack_policy('stack-id')
'{...}')
'GetStackPolicy',
TestCloudFormationSetStackPolicy(CloudFormationConnectionBase):
test_set_stack_policy(self):
self.service_connection.set_stack_policy('stack-id',
stack_policy_body='{}')
self.assertDictEqual(api_response,
{'SetStackPolicyResult':
{'Some':
'content'}})
'SetStackPolicy',
'StackPolicyBody':
SAMPLE_XML
DESCRIBE_STACK_RESOURCE_XML
LIST_STACKS_XML
LIST_STACK_RESOURCES_XML
TestStackParse(unittest.TestCase):
test_parse_tags(self):
rs[0].tags
self.assertEqual(tags,
{u'key0':
u'value0',
u'key1':
u'value1'})
test_event_creation_time_with_millis(self):
millis_xml
SAMPLE_XML.replace(
b"<CreationTime>2013-01-10T05:04:56Z</CreationTime>",
b"<CreationTime>2013-01-10T05:04:56.102342Z</CreationTime>"
xml.sax.parseString(millis_xml,
creation_time
creation_time,
56,
102342)
test_resource_time_with_millis(self):
boto.cloudformation.stack.StackResource)
xml.sax.parseString(DESCRIBE_STACK_RESOURCE_XML,
rs[0].timestamp
28)
rs[1].timestamp
123456)
test_list_stacks_time_with_millis(self):
boto.cloudformation.stack.StackSummary)
xml.sax.parseString(LIST_STACKS_XML,
44)
rs[1].creation_time
58,
161616)
timestamp_3
rs[1].deletion_time
timestamp_3,
51,
575757)
test_list_stacks_time_with_millis_again(self):
boto.cloudformation.stack.StackResourceSummary)
xml.sax.parseString(LIST_STACK_RESOURCES_XML,
rs[0].last_updated_time
58)
rs[1].last_updated_time
875643)
test_disable_rollback_false(self):
test_disable_rollback_false_upper(self):
b'False')
test_disable_rollback_true(self):
b'true')
test_disable_rollback_true_upper(self):
b'True')
DistributionConfig,
DistributionSummary
TestCloudFrontConnection(AWSMockServiceTestCase):
super(TestCloudFrontConnection,
test_get_all_distributions(self):
self.service_connection.get_all_distributions()
DistributionSummary))
self.assertEqual(response[0].domain_name,
self.assertEqual(response[0].cnames,
self.assertEqual(response[0].enabled,
self.assertTrue(isinstance(response[0].origin,
self.assertEqual(response[0].origin.dns_name,
self.assertEqual(response[0].origin.http_port,
self.assertEqual(response[0].origin.https_port,
self.assertEqual(response[0].origin.origin_protocol_policy,
'http-only')
test_get_distribution_config(self):
"AABBCC"})
self.service_connection.get_distribution_config('EEEEEEEEEEEEE')
self.assertTrue(isinstance(response.origin,
self.assertEqual(response.origin.dns_name,
self.assertEqual(response.origin.http_port,
self.assertEqual(response.origin.https_port,
self.assertEqual(response.origin.origin_protocol_policy,
self.assertEqual(response.cnames,
self.assertTrue(response.enabled)
self.assertEqual(response.etag,
"AABBCC")
test_set_distribution_config(self):
get_body
put_body
body=get_body,
"AA"})
self.service_connection.get_distribution_config('EEEEEEE')
body=put_body,
"AABBCCD"})
conf.comment
comment'
self.service_connection.set_distribution_config('EEEEEEE',
conf.etag,
conf)
"AABBCCD")
test_get_distribution_info(self):
self.service_connection.get_distribution_info('EEEEEEEEEEEEE')
self.assertTrue(response.config.enabled)
test_create_distribution(self):
self.set_http_response(status_code=201,
CustomOrigin("example.com",
origin_protocol_policy="match_viewer")
self.service_connection.create_distribution(origin,
comment="example.com
distribution")
"match-viewer")
response.config.enabled)
"EEEEEEEEEEEEEE")
"d2000000000000.cloudfront.net")
CloudfrontDistributionTest(unittest.TestCase):
self.assertEqual(self.dist.logging,
lo
LoggingInfo(bucket='whatever',
prefix='override_')
DistributionConfig(logging=lo)
self.assertEqual(dist.logging.bucket,
'whatever')
self.assertEqual(dist.logging.prefix,
'override_')
CFInvalidationTest(unittest.TestCase):
test_wildcard_escape(self):
cf.invalidation.InvalidationBatch()
self.assertEqual(batch.escape("/*"),
self.assertEqual(batch.escape("/foo*"),
"/foo*")
self.assertEqual(batch.escape("/foo/bar/*"),
"/foo/bar/*")
self.assertEqual(batch.escape("/nowildcard"),
"/nowildcard")
self.assertEqual(batch.escape("/other
special
characters"),
"/other%20special%20characters")
RESPONSE_TEMPLATE
CFInvalidationListTest(unittest.TestCase):
self.cf
boto.connect_cloudfront('aws.aws_access_key_id',
'aws.aws_secret_access_key')
_get_random_id(self,
length=14):
''.join([random.choice(string.ascii_letters)
range(length)])
_group_iter(self,
iterator,
iterator:
accumulator.append(item)
n:
_get_mock_responses(self,
num,
max_items):
min(max_items,
cfid_groups
list(self._group_iter([self._get_random_id()
range(num)],
max_items))
dict(status='Completed',
next_marker='')
enumerate(cfid_groups):
group[-1]
len(cfid_groups):
cfg.update(dict(next_marker=next_marker,
is_truncated=is_truncated))
cfid
cfg.update(dict(cfid=cfid))
cfg.update(dict(inval_summaries=invals))
(RESPONSE_TEMPLATE
cfg).encode('utf-8')
responses.append(mock_response)
test_manual_pagination(self,
num_invals=30,
max_items=4):
self.assertGreater(num_invals,
all_invals
ir.is_truncated:
marker=ir.next_marker,
self.assertLessEqual(len(invals),
all_invals.extend(invals)
num_invals
self.assertEqual(len(invals),
remainder)
test_auto_pagination(self,
num_invals=1024):
self.assertGreaterEqual(num_invals,
self.cf.get_invalidation_requests('dist-id-here')
self.assertEqual(len(ir._inval_cache),
self.assertEqual(len(list(ir)),
dedent
Distribution
CloudfrontSignedUrlsTest(unittest.TestCase):
notdefault
self.pk_str
dedent()
self.pk_id
"PK123456789754"
Distribution()
self.canned_policy
'{"Statement":[{"Resource":'
'"http://d604721fxaaqy9.cloudfront.net/horizon.jpg'
'?large=yes&license=yes",'
'"Condition":{"DateLessThan":{"AWS:EpochTime":1258237200}}}]}')
self.custom_policy_1
"Resource":"http://d604721fxaaqy9.cloudfront.net/training/*",
"IpAddress":{"AWS:SourceIp":"145.168.143.0/24"},
"DateLessThan":{"AWS:EpochTime":1258237200}
self.custom_policy_2
"Resource":"http://*",
"IpAddress":{"AWS:SourceIp":"216.98.35.1/32"},\n'
"DateGreaterThan":{"AWS:EpochTime":1241073790},\n'
"DateLessThan":{"AWS:EpochTime":1255674716}\n'
test_encode_custom_policy_1(self):
"OiJodHRwOi8vZDYwNDcyMWZ4YWFxeTkuY2xvdWRmcm9udC5uZXQv"
"dHJhaW5pbmcvKiIsIAogICAgICAiQ29uZGl0aW9uIjp7IAogICAg"
"ICAgICAiSXBBZGRyZXNzIjp7IkFXUzpTb3VyY2VJcCI6IjE0NS4x"
"NjguMTQzLjAvMjQifSwgCiAgICAgICAgICJEYXRlTGVzc1RoYW4i"
"OnsiQVdTOkVwb2NoVGltZSI6MTI1ODIzNzIwMH0gICAgICAKICAg"
"ICAgfSAKICAgfV0gCn0K")
self.dist._url_base64_encode(self.custom_policy_1)
test_encode_custom_policy_2(self):
"OiJodHRwOi8vKiIsIAogICAgICAiQ29uZGl0aW9uIjp7IAogICAg"
"ICAgICAiSXBBZGRyZXNzIjp7IkFXUzpTb3VyY2VJcCI6IjIxNi45"
"OC4zNS4xLzMyIn0sCiAgICAgICAgICJEYXRlR3JlYXRlclRoYW4i"
"OnsiQVdTOkVwb2NoVGltZSI6MTI0MTA3Mzc5MH0sCiAgICAgICAg"
"ICJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTI1NTY3"
"NDcxNn0KICAgICAgfSAKICAgfV0gCn0K")
self.dist._url_base64_encode(self.custom_policy_2)
test_sign_canned_policy(self):
test_sign_canned_policy_pk_file(self):
test_sign_canned_policy_pk_file_name(self):
pk_file.flush()
private_key_file=pk_file.name)
test_sign_canned_policy_pk_file_like(self):
test_sign_canned_policy_unicode(self):
unicode_policy
six.text_type(self.canned_policy)
self.dist._sign_string(unicode_policy,
test_sign_custom_policy_1(self):
("cPFtRKvUfYNYmxek6ZNs6vgKEZP6G3Cb4cyVt~FjqbHOnMdxdT7e"
"T6pYmhHYzuDsFH4Jpsctke2Ux6PCXcKxUcTIm8SO4b29~1QvhMl-"
"CIojki3Hd3~Unxjw7Cpo1qRjtvrimW0DPZBZYHFZtiZXsaPt87yB"
"P9GWnTQoaVysMxQ_")
self.dist._sign_string(self.custom_policy_1,
test_sign_custom_policy_2(self):
("rc~5Qbbm8EJXjUTQ6Cn0LAxR72g1DOPrTmdtfbWVVgQNw0q~KHUA"
"mBa2Zv1Wjj8dDET4XSL~Myh44CLQdu4dOH~N9huH7QfPSR~O4tIO"
"S1WWcP~2JmtVPoQyLlEc8YHRCuN3nVNZJ0m4EZcXXNAS-0x6Zco2"
"SYx~hywTRxWR~5Q_")
self.dist._sign_string(self.custom_policy_2,
test_create_canned_policy(self):
"http://1234567.cloudfront.com/test_resource.mp3?dog=true"
self.dist._canned_policy(url,
expires)
test_custom_policy_expires_and_policy_url(self):
expires=expires)
test_custom_policy_valid_after(self):
valid_after=valid_after)
test_custom_policy_ip_address(self):
"192.168.0.1"
self.assertEqual("%s/32"
ip_range,
test_custom_policy_ip_range(self):
test_custom_policy_all(self):
"http://1234567.cloudfront.com/test.txt"
111111
valid_after=valid_after,
test_params_canned_policy(self):
expected_sig
("Nql641NHEUkUaXQHZINK1FZ~SYeUSoBJMxjdgqrzIdzV2gyE"
"XPDNv0pYdWJkflDKJ3xIu7lbwRpSkG98NBlgPi4ZJpRRnVX4"
"kXAJK6tdNx6FucDB7OVqzcxkxHsGFd8VCG1BkC-Afh9~lOCM"
"IYHIaiOB6~5jt9w2EOwi6sIIqrg_")
self.dist._create_signing_params(url,
len(signed_url_params))
self.assertEqual(signed_url_params["Expires"],
"1258237200")
self.assertEqual(signed_url_params["Signature"],
expected_sig)
self.assertEqual(signed_url_params["Key-Pair-Id"],
"PK123456789754")
test_canned_policy(self):
expected_url
"http://d604721fxaaqy9.cloudfront.net/horizon.jpg?large=yes&license=yes&Expires=1258237200&Signature=Nql641NHEUkUaXQHZINK1FZ~SYeUSoBJMxjdgqrzIdzV2gyEXPDNv0pYdWJkflDKJ3xIu7lbwRpSkG98NBlgPi4ZJpRRnVX4kXAJK6tdNx6FucDB7OVqzcxkxHsGFd8VCG1BkC-Afh9~lOCMIYHIaiOB6~5jt9w2EOwi6sIIqrg_&Key-Pair-Id=PK123456789754"
self.dist.create_signed_url(
self.assertEqual(expected_url,
signed_url)
self.assertEqual(domain.doc_service_arn,
"arn:aws:cs:us-east-1:1234567890:doc/demo")
self.assertEqual(domain.search_service_arn,
"arn:aws:cs:us-east-1:1234567890:search/demo")
DocumentServiceConnection,
"2011-02-01/documents/batch"),
self.assertEqual(args['lang'],
'en')
self.objs[arg['id']]['version'])
document.delete("6",
"11")
'6':
'11')
CloudSearchDocumentsErrorMissingAdds(CloudSearchDocumentTest):
message'}]
self.assertRaises(SearchServiceException,
tests.unit.cloudsearch.test_search
'http://%s/2011-02-01/search'
test_cloudsearch_bqsearch(self):
search.search(bq="'Test'")
self.assertEqual(args[b'bq'],
[b"'Test'"])
test_cloudsearch_facet_single(self):
facet=["Author"])
[b"Author"])
test_cloudsearch_facet_multiple(self):
facet=["author",
"cat"])
[b"author,cat"])
self.assertEqual(args[b'facet-category-constraints'],
'alpha'})
'count'})
self.assertEqual(args[b'facet-cat-sort'],
[b'count'])
test_cloudsearch_top_n_single(self):
test_cloudsearch_top_n_multiple(self):
self.assertEqual(args[b'facet-cat-top-n'],
[b'10'])
test_cloudsearch_rank_single(self):
rank=["date"])
[b'date'])
test_cloudsearch_rank_multiple(self):
rank=["date",
"score"])
[b'date,score'])
test_cloudsearch_t_field_single(self):
'2001..2007'})
test_cloudsearch_t_field_multiple(self):
'2001..2007',
'score':
'10..50'})
self.assertEqual(args[b't-score'],
[b'10..50'])
self.assertEqual(results.rank,
"-text_relevance")
self.assertEqual(results.match_expression,
"Test")
{'constraints':
facet=['tags'])
CloudSearchConnectionTest(unittest.TestCase):
"SearchInstanceType":
"DomainId":
"1234567890/demo",
"DomainName":
"demo",
"Deleted":
"SearchInstanceCount":
"Created":
"SearchService":
"search-demo.us-east-1.cloudsearch.amazonaws.com"
"RequiresIndexDocuments":
"Processing":
"DocService":
"doc-demo.us-east-1.cloudsearch.amazonaws.com"
"ARN":
"arn:aws:cs:us-east-1:1234567890:domain/demo",
"SearchPartitionCount":
self.assertEqual(domain.service_arn,
"arn:aws:cs:us-east-1:1234567890:domain/demo")
(api_response['IndexDocumentsResponse']
['IndexDocumentsResult']
['FieldNames'])
CloudSearchDocumentConnectionTest(AWSMockServiceTestCase):
DocumentServiceConnection(domain=domain)
self.assertEqual(service.proxy,
"2013-01-01/documents/batch"),
document.delete("6")
test_attached_errors_list(self):
endpoint="doc-demo-userdomain.us-east-1.cloudsearch.amazonaws.com"
CommitMismatchError
self.assertTrue(hasattr(e,
'errors'))
self.assertIsInstance(e.errors,
self.assertEquals(e.errors[0],
self.response['errors'][0].get('message'))
tests.unit.cloudsearch2.test_search
tests.unit.cloudsearch2.test_connection
TestCloudSearchCreateDomain
'http://%s/2013-01-01/search'
options='TestOptions')
self.assertEqual(args[b'q.options'],
[b"TestOptions"])
self.assertEqual(args[b'facet.category'],
'alpha'}})
print(args)
'alpha'},
'count'}})
self.assertEqual(args[b'facet.cat'],
"count"}'])
self.assertEqual(results.hits,
self.assertEqual(results.docs[0]['fields']['rank'],
{'buckets':
facet={'tags':
{}})
CloudSearchConnectionTest(AWSMockServiceTestCase):
SearchConnection(domain=domain)
self.assertEqual(search.session.proxies,
CloudSearchDomainConnectionTest(AWSMockServiceTestCase):
domain_status
'search-demo.us-east-1.cloudsearch.amazonaws.com'
super(CloudSearchDomainConnectionTest,
self).\
create_service_connection(**kwargs)
test_get_search_service(self):
self.assertEqual(search_service.sign_request,
test_get_document_service(self):
self.assertEqual(document_service.sign_request,
test_search_with_auth(self):
search_service.domain_connection
search_service.search()
test_upload_documents_with_auth(self):
"id":
"cat_c"]
document_service.domain_connection
document_service.add("1234",
document_service.commit()
test_no_host_provided(self):
{"trailList":
self.service_connection.describe_trails()
test_describe_name_list(self):
self.service_connection.describe_trails(
trail_name_list=['test'])
self.assertEqual(json.dumps({
'trailNameList':
['test']
self.actual_request.body.decode('utf-8'))
TestCreateTrail(AWSMockServiceTestCase):
{"trail":
test_create(self):
self.service_connection.create_trail(
'cloudtrail-1',
sns_topic_name='cloudtrail-1',
include_global_service_events=False)
api_response['trail']['Name'])
api_response['trail']['S3BucketName'])
api_response['trail']['SnsTopicName'])
api_response['trail']['IncludeGlobalServiceEvents'])
self.assertTrue('CreateTrail'
"connections":
"bandwidth":
"connectionId":
"connectionName":
"connectionState":
"location":
"ownerAccount":
"partnerName":
"vlan":
self.service_connection.describe_connections()
len(api_response['connections']))
self.assertEqual('string',
api_response['connections'][0]['region'])
self.assertTrue('DescribeConnections'
Batch
DESCRIBE_TABLE_1
u'foo',
u'S'}},
'testtable',
DESCRIBE_TABLE_2
u'baz',
u'S'},
'myrange',
'N'}},
'testtable2',
TestBatchObjects(unittest.TestCase):
DESCRIBE_TABLE_1)
self.table2
DESCRIBE_TABLE_2)
test_batch_to_dict(self):
['k1',
'k2'],
attributes_to_get=['foo'],
'k1'}},
'k2'}}],
test_batch_consistent_read_defaults_to_false(self):
['k1'])
{'Keys':
test_batch_list_consistent_read(self):
BatchList(self.layer2)
b.add_batch(self.table,
['k1'],
b.add_batch(self.table2,
[('k2',
54)],
consistent_read=False)
{'testtable':
True},
'testtable2':
'k2'},
'54'}}],
False}})
Table,
"CreationDateTime":
1.353526122785E9,
"HashKeyElement":
{"AttributeName":
"N"}},
"NumberOfDecreasesToday":
5},
"footest",
"TableSizeBytes":
"ACTIVE"}
TestTableConstruction(unittest.TestCase):
self.layer2.layer1
test_get_table(self):
self.api.describe_table.return_value
self.layer2.get_table('footest')
self.assertEqual(table.create_time,
1353526122.785)
self.assertEqual(table.status,
self.assertEqual(table.item_count,
self.assertEqual(table.size_bytes,
21)
self.assertEqual(table.read_units,
self.assertEqual(table.write_units,
test_create_table_without_api_call(self):
self.layer2.table_from_schema(
name='footest',
schema=Schema.create(hash_key=('foo',
self.assertEqual(self.api.describe_table.call_count,
test_create_schema_with_hash_and_range(self):
'N')
self.assertEqual(schema.range_key_name,
self.assertEqual(schema.range_key_type,
test_create_schema_with_hash(self):
self.assertIsNone(schema.range_key_name)
self.assertIsNone(schema.range_key_type)
TestSchemaEquality(unittest.TestCase):
test_schema_equal(self):
test_schema_not_equal(self):
test_equal_with_hash_and_range(self):
test_schema_with_hash_and_range_not_equal(self):
'S'),
s4
self.assertNotEqual(s2,
self.assertNotEqual(s3,
TestDynamizer(unittest.TestCase):
test_encoding_to_dynamodb(self):
self.assertEqual(dynamizer.encode('foo'),
'foo'})
self.assertEqual(dynamizer.encode(54),
'54'})
self.assertEqual(dynamizer.encode(Decimal('1.1')),
self.assertEqual(dynamizer.encode(set([1,
'3']})
self.assertIn(dynamizer.encode(set(['foo',
'bar'])),
({'SS':
'bar']},
'foo']}))
self.assertEqual(dynamizer.encode(types.Binary(b'\x01')),
'AQ=='})
self.assertEqual(dynamizer.encode(set([types.Binary(b'\x01')])),
{'BS':
['AQ==']})
self.assertEqual(dynamizer.encode(['foo',
[1]]),
'1'}]}]})
self.assertEqual(dynamizer.encode({'foo':
1}}),
'1'}}}}})
self.assertEqual(dynamizer.encode(None),
{'NULL':
self.assertEqual(dynamizer.encode(False),
{'BOOL':
test_decoding_to_dynamodb(self):
self.assertEqual(dynamizer.decode({'S':
'54'}),
54)
Decimal('1.1'))
'3']}),
3]))
self.assertEqual(dynamizer.decode({'SS':
'bar']}),
'bar']))
self.assertEqual(dynamizer.decode({'B':
'AQ=='}),
types.Binary(b'\x01'))
self.assertEqual(dynamizer.decode({'BS':
['AQ==']}),
set([types.Binary(b'\x01')]))
self.assertEqual(dynamizer.decode({'L':
'1'}]}]}),
[1]])
self.assertEqual(dynamizer.decode({'M':
'1'}}}}}),
1}})
self.assertEqual(dynamizer.decode({'NULL':
True}),
self.assertEqual(dynamizer.decode({'BOOL':
False}),
test_float_conversion_errors(self):
self.assertEqual(dynamizer.encode(1.25),
'1.25'})
self.assertRaises(DynamoDBNumberError):
dynamizer.encode(1.1)
test_non_boolean_conversions(self):
types.NonBooleanDynamizer()
self.assertEqual(dynamizer.encode(True),
'1'})
test_lossy_float_conversions(self):
types.LossyFloatDynamizer()
self.assertEqual(dynamizer.encode(1.1),
1.1)
self.assertEqual(dynamizer.encode(set([1.1])),
['1.1']})
['1.1',
'2.2',
'3.3']}),
3.3]))
test_decoding_full_doc(self):
'''Simple
List
decoding
had
caused
errors'''
'{"__type__":{"S":"Story"},"company_tickers":{"SS":["NASDAQ-TSLA","NYSE-F","NYSE-GM"]},"modified_at":{"N":"1452525162"},"created_at":{"N":"1452525162"},"version":{"N":"1"},"categories":{"SS":["AUTOMTVE","LTRTR","MANUFCTU","PN","PRHYPE","TAXE","TJ","TL"]},"provider_categories":{"L":[{"S":"F"},{"S":"GM"},{"S":"TSLA"}]},"received_at":{"S":"2016-01-11T11:26:31Z"}}'
output_doc
{'provider_categories':
['F',
'GM',
'TSLA'],
'__type__':
'Story',
'company_tickers':
set(['NASDAQ-TSLA',
'NYSE-GM',
'NYSE-F']),
'modified_at':
Decimal('1'),
'received_at':
'2016-01-11T11:26:31Z',
'created_at':
'categories':
set(['LTRTR',
'TAXE',
'MANUFCTU',
'TL',
'TJ',
'AUTOMTVE',
'PRHYPE',
'PN'])}
self.assertEqual(json.loads(doc,
object_hook=dynamizer.decode),
output_doc)
TestBinary(unittest.TestCase):
test_good_input(self):
types.Binary(b'\x01')
test_non_ascii_good_input(self):
types.Binary(b'\x88')
test_bad_input(self):
test_bytes_input(self):
self.assertEqual(data.value,
test_unicode_py2(self):
self.assertEqual(bytes(data),
u'\x01')
self.assertEqual(type(data.value),
test_unicode_py3(self):
DynamoDBv2Layer1UnitTest(unittest.TestCase):
test_init_region(self):
region=RegionInfo(name='us-west-2',
endpoint='dynamodb.us-west-2.amazonaws.com'),
test_init_host_override(self):
host='localhost',
port=8000)
self.assertEqual(dynamodb.host,
self.assertEqual(dynamodb.port,
8000)
(STRING,
BINARY,
QUERY_OPERATORS)
FakeDynamoDBConnection
mock.create_autospec(DynamoDBConnection)
SchemaFieldsTestCase(unittest.TestCase):
test_hash_key(self):
HashKey('hello')
self.assertEqual(hash_key.name,
self.assertEqual(hash_key.data_type,
self.assertEqual(hash_key.attr_type,
self.assertEqual(hash_key.definition(),
self.assertEqual(hash_key.schema(),
test_range_key(self):
RangeKey('hello')
self.assertEqual(range_key.name,
self.assertEqual(range_key.data_type,
self.assertEqual(range_key.attr_type,
'RANGE')
self.assertEqual(range_key.definition(),
self.assertEqual(range_key.schema(),
test_alternate_type(self):
HashKey('alt',
self.assertEqual(alt_key.name,
'alt')
self.assertEqual(alt_key.data_type,
self.assertEqual(alt_key.attr_type,
self.assertEqual(alt_key.definition(),
self.assertEqual(alt_key.schema(),
IndexFieldTestCase(unittest.TestCase):
test_all_index(self):
AllIndex('AllKeys',
test_keys_only_index(self):
KeysOnlyIndex('KeysOnly',
test_include_index(self):
IncludeIndex('IncludeKeys',
test_global_all_index(self):
GlobalAllIndex('AllKeys',
test_global_keys_only_index(self):
GlobalKeysOnlyIndex('KeysOnly',
test_global_include_index(self):
test_global_include_index_throughput(self):
ItemTestCase(unittest.TestCase):
assertCountEqual
unittest.TestCase.assertItemsEqual
super(ItemTestCase,
Table('whatever',
self.create_item({
create_item(self,
self.assertEqual(empty_item.table,
self.assertEqual(empty_item._data,
full_item
self.assertEqual(full_item.table,
self.assertEqual(full_item._data,
test_keys(self):
self.assertCountEqual(self.johndoe.keys(),
test_values(self):
self.assertCountEqual(self.johndoe.values(),
[12345,
self.assertIn('username',
self.assertIn('first_name',
self.assertIn('date_joined',
self.assertNotIn('whatever',
test_iter(self):
self.assertCountEqual(self.johndoe,
12345])
self.assertEqual(self.johndoe.get('username'),
self.assertEqual(self.johndoe.get('first_name'),
self.assertEqual(self.johndoe.get('date_joined'),
self.assertEqual(self.johndoe.get('last_name'),
self.assertEqual(self.johndoe.get('last_name',
test_items(self):
self.assertCountEqual(
self.johndoe.items(),
('date_joined',
12345),
('first_name',
'John'),
('username',
'johndoe'),
test_attribute_access(self):
self.assertEqual(self.johndoe['username'],
self.assertEqual(self.johndoe['first_name'],
self.assertEqual(self.johndoe['date_joined'],
'Doe')
test_needs_save(self):
test_needs_save_set_changed(self):
self.johndoe['friends'].add('bob')
test_mark_clean(self):
empty_item.load({
self.assertEqual(empty_item['username'],
self.assertEqual(empty_item['date_joined'],
self.assertEqual(sorted(empty_item['friends']),
sorted([
test_get_keys(self):
self.assertEqual(self.johndoe.get_keys(),
test_get_raw_keys(self):
self.assertEqual(self.johndoe.get_raw_keys(),
'12345'},
test_build_expects(self):
self.assertEqual(self.johndoe.build_expects(fields=[
test_prepare_full(self):
self.johndoe.prepare_full()
self.assertEqual(data['username'],
self.assertEqual(data['first_name'],
'John'})
self.assertEqual(data['date_joined'],
'12345'})
self.assertCountEqual(data['friends']['SS'],
test_prepare_full_empty_set(self):
test_prepare_partial(self):
test_prepare_partial_empty_set(self):
test_save_no_changes(self):
self.assertFalse(self.johndoe.save())
self.assertFalse(mock_put_item.called)
test_save_with_changes(self):
self.assertTrue(self.johndoe.save())
test_save_with_changes_overwrite(self):
self.assertTrue(self.johndoe.save(overwrite=True))
expects=None)
test_partial_no_changes(self):
self.assertFalse(self.johndoe.partial_save())
self.assertFalse(mock_update_item.called)
test_partial_with_changes(self):
self.assertTrue(self.johndoe.partial_save())
self.assertTrue(mock_update_item.called)
mock_update_item.assert_called_once_with({
self.johndoe.delete()
self.assertTrue(mock_delete_item.called)
mock_delete_item.assert_called_once_with(
date_joined=12345
test_nonzero(self):
self.assertTrue(self.johndoe)
self.assertFalse(self.create_item({}))
ItemFromItemTestCase(ItemTestCase):
super(ItemFromItemTestCase,
self.create_item(self.johndoe)
fake_results(name,
greeting='hello',
Exception("Web
Returns
'400
Bad
Request'")
end_cap
range(start_key,
results.append("%s
#%s"
(greeting,
i))
len(results):
results[:limit]
retval['last_key']
ResultSetTestCase(unittest.TestCase):
super(ResultSetTestCase,
self.result_function
mock.MagicMock(side_effect=fake_results)
test_first_key(self):
self.assertEqual(self.results.first_key,
'exclusive_start_key')
test_max_page_size_fetch_more(self):
greeting='Hello')
test_max_page_size_and_smaller_limit_fetch_more(self):
test_max_page_size_and_bigger_limit_fetch_more(self):
limit=15)
limit=20,
exclusive_start_key=4)
#12',
#7')
#8')
#9')
#10')
#11')
#12')
test_limit_smaller_than_first_page(self):
limit=2)
test_limit_equals_page(self):
test_limit_greater_than_page(self):
limit=6)
test_iteration_noresults(self):
none(limit=10):
results.to_call(none,
test_iteration_sporadic_pages(self):
sporadic():
{'value':
-1}
_wrapper(limit=10,
exclusive_start_key=None):
'page-1'
'page-2'
results.to_call(sporadic(),
self.assertEqual(list(self.results),
#12'
fake_batch_results(keys):
len(keys)
keys[0]
results.append("hello
simulate_unprocessed:
retval['unprocessed_keys']
['johndoe']
BatchGetResultSetTestCase(unittest.TestCase):
super(BatchGetResultSetTestCase,
BatchGetResultSet(keys=[
self.results.to_call(fake_batch_results)
alice',
bob',
jane',
self.assertEqual(self.results._keys_left,
johndoe',
test_fetch_more_empty(self):
self.results.to_call(lambda
{'results':
alice')
bob')
jane')
johndoe')
TableTestCase(unittest.TestCase):
super(TableTestCase,
self.users
self.default_connection
aws_secret_access_key='secret_key'
test__introspect_schema(self):
raw_schema_1
raw_attributes_1
schema_1
self.users._introspect_schema(raw_schema_1,
raw_attributes_1)
self.assertEqual(len(schema_1),
self.assertTrue(isinstance(schema_1[0],
self.assertEqual(schema_1[0].name,
'username')
self.assertTrue(isinstance(schema_1[1],
self.assertEqual(schema_1[1].name,
'date_joined')
raw_schema_2
"BTREE"
exceptions.UnknownSchemaFieldError,
self.users._introspect_schema,
raw_schema_2,
raw_schema_3
"user_id",
"junk",
raw_attributes_3
'user_id',
'junk',
schema_3
self.users._introspect_schema(raw_schema_3,
raw_attributes_3)
self.assertEqual(len(schema_3),
self.assertTrue(isinstance(schema_3[0],
self.assertEqual(schema_3[0].name,
'user_id')
self.assertEqual(schema_3[0].data_type,
self.assertTrue(isinstance(schema_3[1],
self.assertEqual(schema_3[1].name,
'junk')
self.assertEqual(schema_3[1].data_type,
BINARY)
test__introspect_indexes(self):
raw_indexes_1
"EverybodyIndex",
"ALL"
"GenderIndex",
"INCLUDE",
"NonKeyAttributes":
indexes_1
self.users._introspect_indexes(raw_indexes_1)
self.assertEqual(len(indexes_1),
self.assertTrue(isinstance(indexes_1[0],
KeysOnlyIndex))
self.assertEqual(indexes_1[0].name,
'MostRecentlyJoinedIndex')
self.assertEqual(len(indexes_1[0].parts),
self.assertTrue(isinstance(indexes_1[1],
AllIndex))
self.assertEqual(indexes_1[1].name,
'EverybodyIndex')
self.assertEqual(len(indexes_1[1].parts),
self.assertTrue(isinstance(indexes_1[2],
IncludeIndex))
self.assertEqual(indexes_1[2].name,
'GenderIndex')
self.assertEqual(len(indexes_1[2].parts),
self.assertEqual(indexes_1[2].includes_fields,
['gender'])
"SOMETHING_CRAZY"
exceptions.UnknownIndexFieldError,
self.users._introspect_indexes,
connection=self.default_connection)
self.assertEqual(users.table_name,
'users')
self.assertTrue(isinstance(users.connection,
DynamoDBConnection))
self.assertEqual(users.throughput['write'],
self.assertEqual(users.schema,
self.assertEqual(users.indexes,
Table('groups',
self.assertEqual(groups.table_name,
'groups')
self.assertTrue(hasattr(groups.connection,
'assert_called_once_with'))
test_create_simple(self):
test_create_full(self):
'read':20,
KeysOnlyIndex('FriendCountIndex',
GlobalKeysOnlyIndex('FullFriendCountIndex',
global_secondary_indexes=[
'FullFriendCountIndex',
local_secondary_indexes=[
'FriendCountIndex',
self.assertEqual(self.users.indexes,
self.users.describe()
self.assertEqual(isinstance(self.users.schema[0],
HashKey),
self.assertEqual(len(self.users.indexes),
'WhateverIndex':
'AnotherIndex':
self.assertEqual(kwargs['provisioned_throughput'],
'AnotherIndex',
'WhateverIndex',
test_create_global_secondary_index(self):
self.users.create_global_secondary_index(
HashKey('requiredHashKey')
'Create':
'requiredHashKey'
attribute_definitions=[
'requiredHashKey',
test_delete_global_secondary_index(self):
self.users.delete_global_secondary_index('RandomGSIIndex')
'Delete':
'RandomGSIIndex',
test_update_global_secondary_index(self):
'B_IndexToBeUpdated':
'B_IndexToBeUpdated',
'delete_table',
mock_delete:
self.assertTrue(self.users.delete())
mock_delete.assert_called_once_with('users')
test_get_item(self):
self.users.get_item(username='johndoe')
self.assertEqual(item['username'],
self.assertEqual(item['first_name'],
attributes_to_get=None)
self.users.get_item(username='johndoe',
attributes=[
attributes_to_get=['username',
test_has_item(self):
self.users.has_item(username='johndoe')
'get_item')
mock_get_item.side_effect
JSONResponseError("Nope.",
self.users.has_item(username='mrsmith')
self.assertFalse(found)
test_lookup_hash(self):
self.users.lookup('johndoe')
test_lookup_hash_and_range(self):
self.users.lookup('johndoe',
date_joined=
test_put_item(self):
self.users.put_item(data={
test_private_put_item(self):
self.users._put_item({'some':
{'some':
test_private_update_item(self):
'update_item',
self.users._update_item({
mock_update_item.assert_called_once_with('users',
test_delete_item(self):
self.assertTrue(self.users.delete_item(username='johndoe',
test_delete_item_conditionally(self):
self.assertTrue(self.users.delete_item(expected={'balance__eq':
'balance':
'0'}]
side_effect(*args,
exceptions.ConditionalCheckFailedException(400,
side_effect=side_effect)
self.assertFalse(self.users.delete_item(expected={'balance__eq':
test_get_key_fields_no_schema_populated(self):
"N"
self.users.get_key_fields()
self.assertEqual(key_fields,
'date_joined'])
test_batch_write_no_writes(self):
test_batch_write(self):
mock_batch.assert_called_once_with({
'12342547'}
'12342888'}
test_batch_write_dont_swallow_exceptions(self):
Exception('OH
'OH
test_batch_write_flushing(self):
batch.delete_item(username='johndoe1')
batch.delete_item(username='johndoe2')
batch.delete_item(username='johndoe3')
batch.delete_item(username='johndoe4')
batch.delete_item(username='johndoe5')
batch.delete_item(username='johndoe6')
batch.delete_item(username='johndoe7')
batch.delete_item(username='johndoe8')
batch.delete_item(username='johndoe9')
batch.delete_item(username='johndoe10')
batch.delete_item(username='johndoe11')
batch.delete_item(username='johndoe12')
batch.delete_item(username='johndoe13')
batch.delete_item(username='johndoe14')
batch.delete_item(username='johndoe15')
batch.delete_item(username='johndoe16')
batch.delete_item(username='johndoe17')
batch.delete_item(username='johndoe18')
batch.delete_item(username='johndoe19')
batch.delete_item(username='johndoe20')
batch.delete_item(username='johndoe21')
batch.delete_item(username='johndoe22')
batch.delete_item(username='johndoe23')
batch.delete_item(username='johndoe24')
batch.delete_item(username='johndoe25')
test_batch_write_unprocessed_items(self):
'UnprocessedItems':
return_value=unprocessed)
batch.resend_unprocessed
batch._unprocessed
batch.flush()
test__build_filters(self):
'age__in':
[30,
31,
32,
33],
'gender__null':
self.assertEqual(filters,
'30'},
'31'},
'32'},
'33'},
q_filters
'gender__beginswith':
'm',
using=QUERY_OPERATORS)
self.assertEqual(q_filters,
'm'}],
test_private_query(self):
'mmm']
mock_query.assert_called_once_with('users',
'mmm'],
mock_query_2.assert_called_once_with('users',
test_private_scan(self):
friend_count__lte=2
mock_scan.assert_called_once_with('users',
friend_count__lte=2,
total_segments=None
'jane'})
mock_scan_2.assert_called_once_with('users',
self.users.query_2(last_name__eq='Doe')
'foodoe',
'foodoe')
self.assertEqual(mock_query_2.call_count,
test_query_with_specific_attributes(self):
self.users.query_2(last_name__eq='Doe',
attributes=['username'])
test_scan(self):
self.users.scan(last_name__eq='Doe')
self.assertEqual(mock_scan.call_count,
self.assertEqual(mock_scan_2.call_count,
test_scan_with_specific_attributes(self):
self.users.scan(attributes=['username'])
test_count(self):
'describe',
mock_count:
self.assertEqual(self.users.count(),
test_query_count_simple(self):
expected_0
expected_1
10.0,
return_value=expected_0)
self.users.query_count(username__eq='notmyname')
return_value=expected_1)
self.users.query_count(username__gt='somename',
self.assertEqual(10,
test_query_count_paginated(self):
return_side_effect(*args,
kwargs.get('exclusive_start_key'):
{'Count':
'4118642633'
side_effect=return_side_effect
self.users.query_count(username__eq='johndoe')
self.assertTrue(isinstance(count,
self.assertEqual(30,
count)
test_private_batch_get(self):
mock_batch_get.assert_called_once_with(request_items={
expected['Responses']['users'][2]
expected['UnprocessedKeys']
'jane',}},
'jane'}
mock_batch_get_2.assert_called_once_with(request_items={
test_private_batch_get_attributes(self):
mock_batch_get_attr:
first_names
[res['first_name']
self.assertEqual(first_names,
['Alice',
'Bob'])
mock_batch_get_attr.assert_called_once_with(request_items={
'alice'}
'bob'}
'AttributesToGet':
'first_name'],
test_batch_get(self):
self.users.batch_get(keys=[
'zoeydoe'},
BatchGetResultSet))
self.users._batch_get)
self.assertEqual(mock_batch_get.call_count,
['zoeydoe'])
self.assertEqual(mock_batch_get_2.call_count,
AddressTest(unittest.TestCase):
AddressWithAllocationTest(unittest.TestCase):
AddressWithNetworkInterfaceTest(unittest.TestCase):
self.address.associate(network_interface_id=1)
network_interface_id=1,
BlockDeviceTypeTests(unittest.TestCase):
self.block_device_type
self.block_device_type.endElement(name,
self.assertEqual(getattr(self.block_device_type,
"volume_id"),
("virtualName",
"ephemeral_name"),
("volumeSize",
("attachTime",
"attach_time"),
test_endElement_with_name_NoDevice_value_true(self):
test_endElement_with_name_NoDevice_value_other(self):
test_endElement_with_name_deleteOnTermination_value_true(self):
test_endElement_with_name_deleteOnTermination_value_other(self):
test_endElement_with_name_encrypted_value_true(self):
test_endElement_with_name_Encrypted_value_other(self):
BlockDeviceMappingTests(unittest.TestCase):
block_device_type_eq(self,
b1,
b2):
isinstance(b1,
BlockDeviceType)
isinstance(b2,
BlockDeviceType):
all([b1.connection
b2.connection,
b1.ephemeral_name
b2.ephemeral_name,
b1.no_device
b2.no_device,
b1.volume_id
b2.volume_id,
b1.snapshot_id
b2.snapshot_id,
b1.status
b2.status,
b1.attach_time
b2.attach_time,
b1.delete_on_termination
b2.delete_on_termination,
b1.size
b2.size,
b1.encrypted
b2.encrypted])
test_startElement_with_name_ebs_sets_and_returns_current_value(self):
self.block_device_mapping.startElement("ebs",
test_startElement_with_name_virtualName_sets_and_returns_current_value(self):
self.block_device_mapping.startElement("virtualName",
test_endElement_with_name_device_sets_current_name_dev_null(self):
self.block_device_mapping.endElement("device",
test_endElement_with_name_device_sets_current_name(self):
self.block_device_mapping.endElement("deviceName",
name")
test_endElement_with_name_item_sets_current_name_key_to_current_value(self):
self.block_device_mapping.current_name
self.block_device_mapping.current_value
self.block_device_mapping.endElement("item",
item",
self.assertEqual(self.block_device_mapping["some
name"],
test_run_instances_block_device_mapping(self):
BlockDeviceType(snapshot_id='snap-12345')
dev_sdg
BlockDeviceType(snapshot_id='snap-12346',
OrderedBlockDeviceMapping(OrderedDict,
BlockDeviceMapping):
OrderedBlockDeviceMapping()
bdm.update(OrderedDict((('/dev/sdf',
dev_sdf),
('/dev/sdg',
dev_sdg))))
block_device_map=bdm
'BlockDeviceMapping.2.DeviceName':
'/dev/sdg',
'BlockDeviceMapping.2.Ebs.DeleteOnTermination':
'BlockDeviceMapping.2.Ebs.SnapshotId':
'snap-12346',
'BlockDeviceMapping.2.Ebs.Encrypted':
MagicMock,
test_get_reserved_instance_offerings(self):
self.ec2.get_all_reserved_instances_offerings()
'2964d1bf71d8')
self.assertEqual(instance.instance_type,
'c1.medium')
self.assertEqual(instance.availability_zone,
'us-east-1c')
self.assertEqual(instance.duration,
94608000)
self.assertEqual(instance.fixed_price,
'775.0')
self.assertEqual(instance.usage_price,
'0.0')
self.assertEqual(instance.description,
'product
self.assertEqual(instance.instance_tenancy,
self.assertEqual(instance.currency_code,
self.assertEqual(instance.offering_type,
'Heavy
Utilization')
self.assertEqual(len(instance.recurring_charges),
self.assertEqual(instance.recurring_charges[0].frequency,
'Hourly')
self.assertEqual(instance.recurring_charges[0].amount,
'0.095')
self.assertEqual(len(instance.pricing_details),
self.assertEqual(instance.pricing_details[0].price,
'0.045')
self.assertEqual(instance.pricing_details[0].count,
test_get_reserved_instance_offerings_params(self):
self.ec2.get_all_reserved_instances_offerings(
reserved_instances_offering_ids=['id1',
availability_zone='us-east-1',
product_description='description',
instance_tenancy='dedicated',
offering_type='offering_type',
include_marketplace=False,
min_duration=100,
max_duration=1000,
max_instance_count=1,
'DescribeReservedInstancesOfferings',
'ReservedInstancesOfferingId.1':
'ReservedInstancesOfferingId.2':
't1.micro',
'ProductDescription':
'dedicated',
'OfferingType':
'offering_type',
'IncludeMarketplace':
'MinDuration':
'100',
'MaxDuration':
'1000',
'MaxInstanceCount':
TestPurchaseReservedInstanceOffering(TestEC2ConnectionBase):
self.ec2.purchase_reserved_instance_offering(
(100.0,
'USD'))
'PurchaseReservedInstancesOffering',
'LimitPrice.Amount':
'100.0',
'LimitPrice.CurrencyCode':
'USD',
TestCreateImage(TestEC2ConnectionBase):
test_block_device_mapping(self):
bdm['test']
block_device_mapping=bdm)
TestCancelReservedInstancesListing(TestEC2ConnectionBase):
test_reserved_instances_listing(self):
self.ec2.cancel_reserved_instances_listing()
'CANCELLED')
'166.64')
TestCreateReservedInstancesListing(TestEC2ConnectionBase):
test_create_reserved_instances_listing(self):
self.ec2.create_reserved_instances_listing(
[('2.5',
11),
('2.0',
8)],
'client_token')
'CreateReservedInstancesListing',
'client_token',
'PriceSchedules.0.Price':
'2.5',
'PriceSchedules.0.Term':
'11',
'PriceSchedules.1.Price':
'2.0',
'PriceSchedules.1.Term':
'8',
TestDescribeSpotInstanceRequests(TestEC2ConnectionBase):
test_describe_spot_instance_requets(self):
self.ec2.get_all_spot_instance_requests()
spotrequest
self.assertEqual(spotrequest.id,
'sir-id')
self.assertEqual(spotrequest.price,
0.003)
self.assertEqual(spotrequest.type,
'one-time')
self.assertEqual(spotrequest.state,
self.assertEqual(spotrequest.fault,
self.assertEqual(spotrequest.valid_from,
self.assertEqual(spotrequest.valid_until,
self.assertEqual(spotrequest.launch_group,
'mylaunchgroup')
self.assertEqual(spotrequest.launched_availability_zone,
'us-east-1d')
self.assertEqual(spotrequest.product_description,
'Linux/UNIX')
self.assertEqual(spotrequest.availability_zone_group,
self.assertEqual(spotrequest.create_time,
'2012-10-19T18:07:05.000Z')
self.assertEqual(spotrequest.instance_id,
'i-id')
launch_spec
spotrequest.launch_specification
self.assertEqual(launch_spec.key_name,
'mykeypair')
self.assertEqual(launch_spec.instance_type,
't1.micro')
self.assertEqual(launch_spec.image_id,
self.assertEqual(launch_spec.placement,
self.assertEqual(launch_spec.kernel,
self.assertEqual(launch_spec.ramdisk,
self.assertEqual(launch_spec.monitored,
self.assertEqual(launch_spec.subnet_id,
self.assertEqual(launch_spec.block_device_mapping,
self.assertEqual(launch_spec.instance_profile,
self.assertEqual(launch_spec.ebs_optimized,
spotrequest.status
self.assertEqual(status.code,
'fulfilled')
self.assertEqual(status.update_time,
'2012-10-19T18:09:26.000Z')
self.assertEqual(status.message,
Spot
fulfilled.')
TestCopySnapshot(TestEC2ConnectionBase):
test_copy_snapshot(self):
self.ec2.copy_snapshot('us-west-2',
'snap-id',
self.assertEqual(snapshot_id,
'snap-copied-id')
'CopySnapshot',
'snap-id'},
TestCopyImage(TestEC2ConnectionBase):
test_copy_image_required_params(self):
'ami-id'
test_copy_image_name_and_description(self):
'description'
test_copy_image_client_token(self):
client_token='client-token')
'client-token'
test_copy_image_encrypted(self):
test_copy_image_not_encrypted(self):
encrypted=False)
test_copy_image_encrypted_with_kms_key(self):
kms_key_id='kms-key')
'kms-key'
TestAccountAttributes(TestEC2ConnectionBase):
test_describe_account_attributes(self):
self.ec2.describe_account_attributes()
self.assertEqual(len(parsed),
self.assertEqual(parsed[0].attribute_name,
'vpc-max-security-groups-per-interface')
self.assertEqual(parsed[0].attribute_values,
['5'])
self.assertEqual(parsed[-1].attribute_name,
'default-vpc')
self.assertEqual(parsed[-1].attribute_values,
['none'])
TestDescribeVPCAttribute(TestEC2ConnectionBase):
test_describe_vpc_attribute(self):
self.ec2.describe_vpc_attribute('vpc-id',
'enableDnsHostnames')
self.assertEqual(parsed.vpc_id,
'vpc-id')
self.assertFalse(parsed.enable_dns_hostnames)
'DescribeVpcAttribute',
'enableDnsHostnames',
TestGetAllNetworkInterfaces(TestEC2ConnectionBase):
test_get_all_network_interfaces(self):
self.ec2.get_all_network_interfaces(network_interface_ids=['eni-0f62d866'])
'DescribeNetworkInterfaces',
'NetworkInterfaceId.1':
'eni-0f62d866'},
'eni-0f62d866')
test_attachment_has_device_index(self):
self.ec2.get_all_network_interfaces()
parsed[0].attachment.device_index)
TestGetAllImages(TestEC2ConnectionBase):
test_get_all_images(self):
self.ec2.get_all_images()
len(parsed))
self.assertEquals("ami-abcd1234",
parsed[0].id)
self.assertEquals("111111111111/windows2008r2-hvm-i386-20130702",
parsed[0].location)
self.assertEquals("available",
parsed[0].state)
parsed[0].ownerId)
parsed[0].owner_id)
self.assertEquals(False,
parsed[0].is_public)
self.assertEquals("i386",
parsed[0].architecture)
self.assertEquals("machine",
parsed[0].type)
parsed[0].kernel_id)
parsed[0].ramdisk_id)
parsed[0].owner_alias)
self.assertEquals("windows",
parsed[0].platform)
parsed[0].name)
Description",
parsed[0].description)
self.assertEquals("ebs",
parsed[0].root_device_type)
self.assertEquals("/dev/sda1",
parsed[0].root_device_name)
self.assertEquals("hvm",
parsed[0].virtualization_type)
self.assertEquals("xen",
parsed[0].hypervisor)
parsed[0].instance_lifecycle)
len(parsed[0].billing_products))
self.assertEquals("bp-6ba54002",
parsed[0].billing_products[0])
self.assertEquals(5,
len(parsed[0].block_device_mapping))
TestModifyInterfaceAttribute(TestEC2ConnectionBase):
test_modify_description(self):
'Description.Value':
test_modify_source_dest_check_bool(self):
test_modify_source_dest_check_str(self):
test_modify_source_dest_check_invalid(self):
test_modify_delete_on_termination_str(self):
test_modify_delete_on_termination_bool(self):
test_modify_delete_on_termination_invalid(self):
test_modify_group_set_list(self):
['sg-1',
'sg-2'])
'sg-2'},
test_modify_group_set_invalid(self):
'iterable'):
test_modify_attr_invalid(self):
attribute'):
'invalid',
list(boto.ec2.RegionData.keys())[0]
self.assertEqual(boto.ec2.RegionData[region],
test_non_aws_region(self):
region=RegionInfo(name='foo',
endpoint='https://foo.com/bar')
self.assertEqual('https://foo.com/bar',
test_missing_region(self):
self.ec2)
TestTrimSnapshots(TestEC2ConnectionBase):
_get_snapshots(self):
dates
timedelta(days=1),
timedelta(days=2),
timedelta(days=7),
timedelta(days=14),
timedelta(days=28),
timedelta(days=58),
timedelta(days=88)
dates:
Snapshot(self.ec2)
snap.tags['Name']
snap.start_time
date.strftime('%Y-%m-%dT%H:%M:%S.000Z')
snaps.append(snap)
test_trim_defaults(self):
self.ec2.trim_snapshots()
self.ec2.delete_snapshot.called)
test_trim_months(self):
self.ec2.trim_snapshots(monthly_backups=1)
self.ec2.delete_snapshot.call_count)
TestModifyReservedInstances(TestEC2ConnectionBase):
test_none_token(self):
TestDescribeReservedInstancesModifications(TestEC2ConnectionBase):
self.ec2.describe_reserved_instances_modifications(
reserved_instances_modification_ids=[
filters={
'DescribeReservedInstancesModifications',
'ReservedInstancesModificationId.1':
response[0].modification_id,
'rimod-49b9433e-fdc7-464a-a6e5-9dabcexample'
response[0].create_date,
637000)
response[0].update_date,
38,
143000)
response[0].effective_date,
response[0].status,
'fulfilled'
response[0].status_message,
response[0].client_token,
'token-f5b56c05-09b0-4d17-8d8c-c75d8a67b806'
response[0].reserved_instances[0].id,
response[0].modification_results[0].availability_zone,
'us-east-1b'
response[0].modification_results[0].platform,
'EC2-VPC'
response[0].modification_results[0].instance_count,
TestRegisterImage(TestEC2ConnectionBase):
test_vm_type_default(self):
image_location='s3://foo')
test_vm_type_hvm(self):
virtualization_type='hvm')
'VirtualizationType':
'hvm'
test_sriov_net_support_simple(self):
sriov_net_support='simple')
'SriovNetSupport':
test_volume_delete_on_termination_on(self):
snapshot_id='snap-12345678',
delete_root_volume_on_termination=True)
test_volume_delete_on_termination_default(self):
snapshot_id='snap-12345678')
TestTerminateInstances(TestEC2ConnectionBase):
test_terminate_bad_response(self):
self.ec2.terminate_instances('foo')
TestDescribeInstances(TestEC2ConnectionBase):
self.ec2.get_all_instances()
self.ec2.get_all_reservations()
self.ec2.get_only_instances()
self.ec2.get_all_instances(
self.ec2.get_all_reservations(
next_token='abcdefgh',
'abcdefgh'},
TestDescribeTags(TestEC2ConnectionBase):
self.ec2.get_all_tags()
'DescribeTags'},
self.ec2.get_all_tags(
'DescribeTags',
TestSignatureAlteration(TestEC2ConnectionBase):
endpoint='ec2.cn-north-1.amazonaws.com.cn',
connection_cls=EC2Connection
region=region
TestAssociateAddress(TestEC2ConnectionBase):
test_associate_address_object(self):
self.ec2.associate_address_object(instance_id='i-1234',
self.assertEqual('eipassoc-fc5ca095',
result.association_id)
TestAssociateAddressFail(TestEC2ConnectionBase):
TestDescribeVolumes(TestEC2ConnectionBase):
test_get_all_volumes(self):
self.ec2.get_all_volumes(volume_ids=['vol-1a2b3c4d',
'vol-5e6f7a8b'])
'DescribeVolumes',
'VolumeId.1':
'vol-1a2b3c4d',
'VolumeId.2':
'vol-5e6f7a8b'},
'vol-5e6f7a8b')
TestDescribeSnapshots(TestEC2ConnectionBase):
test_get_all_snapshots(self):
self.ec2.get_all_snapshots(snapshot_ids=['snap-1a2b3c4d',
'snap-5e6f7a8b'])
'snap-5e6f7a8b'},
'snap-5e6f7a8b')
TestCreateVolume(TestEC2ConnectionBase):
test_create_volume(self):
test_create_volume_with_specify_kms(self):
encrypted=True,kms_key_id='arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef')
'arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef'},
TestGetClassicLinkInstances(TestEC2ConnectionBase):
test_get_classic_link_instances(self):
self.ec2.get_all_classic_link_instances()
'i-31489bd8')
self.assertEqual(instance.vpc_id,
'vpc-9d24f8f8')
self.assertEqual(len(instance.groups),
self.assertEqual(instance.groups[0].id,
'sg-9b4343fe')
self.assertEqual(instance.tags,
test_get_classic_link_instances_params(self):
self.ec2.get_all_classic_link_instances(
instance_ids=['id1',
'DescribeClassicLinkInstances',
'InstanceId.1':
'InstanceId.2':
TestAddTags(AWSMockServiceTestCase):
test_add_tag(self):
taggedEC2Object.add_tag("new_key",
"new_value")
'new_key',
'new_value'},
"new_key":
"new_value"})
test_add_tags(self):
taggedEC2Object.add_tags({"key1":
"key1":
TestRemoveTags(AWSMockServiceTestCase):
test_remove_tag(self):
"value1")
'value1'},
test_remove_tag_no_value(self):
taggedEC2Object.remove_tag("key1")
'key1'},
test_remove_tag_empty_value(self):
{"key1":
test_remove_tags_wrong_values(self):
"value3"})
'value3'},
test_remove_tags_none_values(self):
'key2'},
TestRunInstanceResponseParsing(unittest.TestCase):
testIAMInstanceProfileParsedCorrectly(self):
ec2.run_instances(image_id='ami-12345')
self.assertEqual(len(reservation.instances),
self.assertEqual(instance.image_id,
'ami-ed65ba84')
'i-ff0f1299')
instance.instance_profile,
('arn:aws:iam::ownerid:'
'instance-profile/myinstanceprofile'),
'iamid'})
TestRunInstances(AWSMockServiceTestCase):
test_run_instances_user_data(self):
user_data='#!/bin/bash'
TestDescribeInstances(AWSMockServiceTestCase):
test_multiple_private_ip_addresses(self):
self.service_connection.get_all_reservations()
api_response[0].instances
self.assertEqual(len(instances),
self.assertEqual(len(instance.interfaces),
instance.interfaces[0]
self.assertEqual(len(interface.private_ip_addresses),
addresses
self.assertEqual(addresses[0].private_ip_address,
'10.0.0.67')
self.assertTrue(addresses[0].primary)
self.assertEqual(addresses[1].private_ip_address,
'10.0.0.54')
self.assertFalse(addresses[1].primary)
self.assertEqual(addresses[2].private_ip_address,
'10.0.0.55')
self.assertFalse(addresses[2].primary)
ec2.get_all_instance_status()
self.assertNotIn('IncludeAllInstances',
test_include_all_instances(self):
ec2.get_all_instance_status(include_all_instances=True)
self.assertIn('IncludeAllInstances',
self.assertEqual('true',
ec2.make_request.call_args[0][1]['IncludeAllInstances'])
test_get_instance_types(self):
self.ec2.get_all_instance_types()
18)
'256')
response[17]
'hs1.8xlarge')
'48')
'24000')
'119808')
Attachment,
NetworkInterfaceTests(unittest.TestCase):
self.attachment.id
'eni-attach-1'
self.attachment.instance_id
self.attachment.status
self.attachment.device_index
self.eni_one
self.eni_one.id
'eni-1'
self.eni_one.status
self.eni_one.attachment
self.eni_two
'eni-2'
self.eni_two.status
"^eni-1
self.eni_one.update(True)
self.eni_two.connection.get_all_network_interfaces.return_value
[self.eni_one]
self.eni_two.update()
all([self.eni_two.status
self.attachment])
[self.eni_two]
self.eni_one.update()
test_attach_calls_attach_eni(self):
self.eni_one.attach("instance_id",
self.eni_one.connection.attach_network_interface.assert_called_with(
test_detach_calls_detach_network_interface(self):
self.eni_one.detach()
self.eni_two.detach()
self.eni_two.connection.detach_network_interface.assert_called_with(
test_detach_with_force_calls_detach_network_interface_with_force(self):
self.eni_one.detach(True)
TestNetworkInterfaceCollection(unittest.TestCase):
self.private_ip_address1
private_ip_address='10.0.0.10',
self.private_ip_address2
private_ip_address='10.0.0.11',
self.network_interfaces_spec1
device_index=1,
subnet_id='subnet_id',
description='description1',
private_ip_address='10.0.0.54',
private_ip_addresses=[self.private_ip_address1,
self.private_ip_address2]
self.private_ip_address3
private_ip_address='10.0.1.10',
self.private_ip_address4
private_ip_address='10.0.1.11',
self.network_interfaces_spec2
device_index=2,
self.private_ip_address4]
self.network_interfaces_spec3
self.private_ip_address4],
associate_public_ip_address=True
test_param_serialization(self):
collection.build_list_params(params)
'NetworkInterface.0.DeviceIndex':
'NetworkInterface.0.DeleteOnTermination':
'NetworkInterface.0.Description':
'NetworkInterface.0.PrivateIpAddress':
'NetworkInterface.0.SubnetId':
'NetworkInterface.0.PrivateIpAddresses.0.Primary':
'NetworkInterface.0.PrivateIpAddresses.0.PrivateIpAddress':
'NetworkInterface.0.PrivateIpAddresses.1.Primary':
'NetworkInterface.0.PrivateIpAddresses.1.PrivateIpAddress':
'NetworkInterface.1.DeviceIndex':
'NetworkInterface.1.Description':
'NetworkInterface.1.DeleteOnTermination':
'NetworkInterface.1.PrivateIpAddress':
'NetworkInterface.1.SubnetId':
'NetworkInterface.1.SecurityGroupId.0':
'NetworkInterface.1.SecurityGroupId.1':
'NetworkInterface.1.PrivateIpAddresses.0.Primary':
'NetworkInterface.1.PrivateIpAddresses.0.PrivateIpAddress':
'NetworkInterface.1.PrivateIpAddresses.1.Primary':
'NetworkInterface.1.PrivateIpAddresses.1.PrivateIpAddress':
test_add_prefix_to_serialization(self):
'LaunchSpecification.NetworkInterface.1.DeviceIndex':
'LaunchSpecification.NetworkInterface.1.Description':
'LaunchSpecification.NetworkInterface.1.DeleteOnTermination':
'LaunchSpecification.NetworkInterface.1.PrivateIpAddress':
'LaunchSpecification.NetworkInterface.1.SubnetId':
'LaunchSpecification.NetworkInterface.1.SecurityGroupId.0':
'LaunchSpecification.NetworkInterface.1.SecurityGroupId.1':
'LaunchSpecification.NetworkInterface.1.PrivateIpAddresses.0.Primary':
'LaunchSpecification.NetworkInterface.1.PrivateIpAddresses.0.PrivateIpAddress':
'LaunchSpecification.NetworkInterface.1.PrivateIpAddresses.1.Primary':
'LaunchSpecification.NetworkInterface.1.PrivateIpAddresses.1.PrivateIpAddress':
test_cant_use_public_ip(self):
NetworkInterfaceCollection(self.network_interfaces_spec3,
self.network_interfaces_spec1)
self.network_interfaces_spec3.device_index
test_public_ip(self):
'LaunchSpecification.NetworkInterface.0.AssociatePublicIpAddress':
'LaunchSpecification.NetworkInterface.0.SecurityGroupId.0':
'LaunchSpecification.NetworkInterface.0.SecurityGroupId.1':
TestReservedInstancesSet(AWSMockServiceTestCase):
test_get_all_reserved_instaces(self):
self.service_connection.get_all_reserved_instances()
ReservedInstance))
self.assertEquals(response[0].id,
'ididididid')
self.assertEquals(response[0].instance_count,
self.assertEquals(response[0].start,
'2014-05-03T14:10:10.944Z')
self.assertEquals(response[0].end,
'2014-05-03T14:10:11.000Z')
DESCRIBE_SECURITY_GROUP
DESCRIBE_INSTANCES
TestDescribeSecurityGroups(AWSMockServiceTestCase):
test_get_instances(self):
body=DESCRIBE_SECURITY_GROUP)
self.service_connection.get_all_security_groups()
body=DESCRIBE_INSTANCES)
groups[0].instances()
len(instances))
self.assertEqual(groups[0].id,
instances[0].groups[0].id)
SecurityGroupTest(unittest.TestCase):
test_add_rule(self):
sg.add_rule(
ip_protocol='http',
from_port='80',
to_port='8080',
src_group_name='groupy',
src_group_owner_id='12345',
cidr_ip='10.0.0.1',
src_group_group_id='54321',
test_remove_rule_on_empty_group(self):
sg.remove_rule('ip',
TestDescribeSnapshots(AWSMockServiceTestCase):
test_describe_snapshots(self):
self.service_connection.get_all_snapshots(['snap-1a2b3c4d',
'snap-9f8e7d6c'],
owner=['self',
'111122223333'],
restorable_by='999988887777',
filters=OrderedDict((('status',
'pending'),
('tag-value',
'*db_*'))))
'snap-9f8e7d6c',
'Owner.1':
'self',
'Owner.2':
'111122223333',
'RestorableBy.1':
'999988887777',
'tag-value',
'*db_*'},
Snapshot)
TestCancelSpotInstanceRequests(AWSMockServiceTestCase):
test_cancel_spot_instance_requests(self):
self.service_connection.cancel_spot_instance_requests(['sir-1a2b3c4d',
'sir-9f8e7d6c'])
'CancelSpotInstanceRequests',
'SpotInstanceRequestId.1':
'sir-1a2b3c4d',
'SpotInstanceRequestId.2':
'sir-9f8e7d6c'},
'sir-1a2b3c4d')
self.assertEqual(response[0].state,
self.assertEqual(response[1].id,
'sir-9f8e7d6c')
self.assertEqual(response[1].state,
TestGetSpotPriceHistory(AWSMockServiceTestCase):
test_get_spot_price_history(self):
instance_type='c3.large')
self.assertEqual(response.next_token,
self.assertEqual(response.nextToken,
self.assertEqual(response[0].availability_zone,
'us-west-2c')
self.assertEqual(response[1].instance_type,
self.assertEqual(response[1].availability_zone,
filters={'instance-type':
'c3.large'})
'instance-type',
next_token='foobar')
'foobar'},
Tag,
AttachmentSet,
VolumeTests(unittest.TestCase):
self.attach_data.id
self.attach_data.attach_time
"/dev/null"
self.volume_one
self.volume_one.id
self.volume_one.create_time
self.volume_one.status
self.volume_one.size
"one_size"
self.volume_one.snapshot_id
self.volume_one.attach_data
self.volume_one.zone
"one_zone"
self.volume_two
self.volume_two.id
self.volume_two.create_time
"two_size"
"two_zone"
test_startElement_calls_TaggedEC2Object_startElement_with_correct_args(self,
volume.startElement("some
startElement.assert_called_with(
test_startElement_retval_not_None_returns_correct_thing(self,
mock.Mock(TagSet)
volume.startElement(None,
tag_set)
@mock.patch("boto.resultset.ResultSet")
test_startElement_with_name_tagSet_calls_ResultSet(self,
mock.Mock(ResultSet([("item",
Tag)]))
volume.tags
volume.startElement("tagSet",
volume.tags)
test_startElement_with_name_attachmentSet_returns_AttachmentSet(self,
volume.attach_data
volume.startElement("attachmentSet",
volume.attach_data)
test_startElement_else_returns_None(self,
volume.startElement("not
tagSet
attachmentSet",
obj_value=None):
volume.endElement(name,
self.assertEqual(getattr(volume,
"id"),
("createTime",
time",
"create_time"),
("size",
("availabilityZone",
zone",
"zone"),
("someName",
"someName"),
("encrypted",
"encrypted",
True)]:
self.check_that_attribute_has_been_set(*arguments)
test_endElement_with_name_status_and_empty_string_value_doesnt_set_status(self):
volume.endElement("status",
self.assertNotEqual(volume.status,
self.volume_two.connection.get_all_volumes.return_value
[self.volume_one]
self.volume_two.update()
all([self.volume_two.create_time
"one_size",
self.attach_data,
"one_zone"])
"^1
self.volume_one.update(True)
[self.volume_two]
self.volume_one.update()
test_delete_calls_delete_volume(self):
self.volume_one.delete()
self.volume_one.connection.delete_volume.assert_called_with(
test_attach_calls_attach_volume(self):
self.volume_one.attach("instance_id",
self.volume_one.connection.attach_volume.assert_called_with(
test_detach_calls_detach_volume(self):
self.volume_one.detach()
self.volume_two.detach()
self.volume_two.connection.detach_volume.assert_called_with(
test_detach_with_force_calls_detach_volume_with_force(self):
self.volume_one.detach(True)
test_create_snapshot_calls_connection_create_snapshot(self):
self.volume_one.create_snapshot()
test_create_snapshot_with_description(self):
self.volume_one.create_snapshot("some
description")
description",
test_volume_state_returns_status(self):
self.volume_one.volume_state()
"one_status")
test_attachment_state_returns_state(self):
self.volume_one.attachment_state()
status")
test_attachment_state_no_attach_data_returns_None(self):
self.volume_two.attachment_state()
test_snapshots_returns_snapshots(self):
snapshot_one
snapshot_one.volume_id
snapshot_two
snapshot_two.volume_id
[snapshot_one,
snapshot_two]
self.volume_one.snapshots()
[snapshot_one])
test_snapshots__with_owner_and_restorable_by(self):
self.volume_one.snapshots("owner",
"restorable_by")
self.volume_one.connection.get_all_snapshots.assert_called_with(
owner="owner",
restorable_by="restorable_by",
AttachmentSetTests(unittest.TestCase):
attachment_set
attachment_set.endElement(name,
self.assertEqual(getattr(attachment_set,
self.check_that_attribute_has_been_set("volumeId",
test_endElement_with_name_instanceId_sets_instance_id(self):
self.check_that_attribute_has_been_set("instanceId",
"instance_id")
test_endElement_with_name_status_sets_status(self):
self.check_that_attribute_has_been_set("status",
"status")
test_endElement_with_name_attachTime_sets_attach_time(self):
self.check_that_attribute_has_been_set("attachTime",
"attach_time")
test_endElement_with_name_device_sets_device(self):
self.check_that_attribute_has_been_set("device",
"device")
self.check_that_attribute_has_been_set("someName",
"someName")
VolumeAttributeTests(unittest.TestCase):
self.volume_attribute
VolumeAttribute()
"key_name"
self.volume_attribute.attrs
{"key_name":
test_startElement_with_name_autoEnableIO_sets_key_name(self):
self.volume_attribute.startElement("autoEnableIO",
self.assertEqual(self.volume_attribute._key_name,
"autoEnableIO")
test_startElement_without_name_autoEnableIO_returns_None(self):
self.volume_attribute.startElement("some
test_endElement_with_name_value_and_value_true_sets_attrs_key_name_True(self):
self.assertEqual(self.volume_attribute.attrs['key_name'],
test_endElement_with_name_value_and_value_false_sets_attrs_key_name_False(self):
"other_key_name"
"false",
self.assertEqual(self.volume_attribute.attrs['other_key_name'],
self.volume_attribute.endElement("volumeId",
"some_value",
self.assertEqual(self.volume_attribute.id,
"some_value")
self.volume_attribute.endElement("someName",
self.assertEqual(self.volume_attribute.someName,
EBSBlockDeviceType,
launchconfig,
TestAutoScaleGroup(AWSMockServiceTestCase):
super(TestAutoScaleGroup,
'OldestLaunchConfiguration'],
instance_id='test-id')
'lauch_config',
'TerminationPolicies.member.1':
'TerminationPolicies.member.2':
'OldestLaunchConfiguration',
'test-id',
test_autoscaling_group_single_vpc_zone_identifier(self):
vpc_zone_identifier='vpc_zone_1')
'vpc_zone_1',
test_autoscaling_group_vpc_zone_identifier_list(self):
vpc_zone_identifier=['vpc_zone_1',
'vpc_zone_2'])
test_autoscaling_group_vpc_zone_identifier_multi(self):
vpc_zone_identifier='vpc_zone_1,vpc_zone_2')
TestAutoScaleGroupHonorCooldown(AWSMockServiceTestCase):
test_honor_cooldown(self):
self.service_connection.set_desired_capacity('foo',
'SetDesiredCapacity',
'HonorCooldown':
TestScheduledGroup(AWSMockServiceTestCase):
super(TestScheduledGroup,
test_scheduled_group_creation(self):
self.service_connection.create_scheduled_group_action('foo',
desired_capacity=1,
start_time=datetime(2013,
end_time=datetime(2013,
recurrence='0
*')
'PutScheduledUpdateGroupAction',
'2013-02-01T22:55:31',
'2013-01-01T22:55:31',
'0
*',
TestParseAutoScaleGroupResponse(AWSMockServiceTestCase):
test_get_all_groups_is_parsed_correctly(self):
self.service_connection.get_all_groups(names=['test_group'])
self.assertEqual(as_group.availability_zones,
['us-east-1c',
'us-east-1a'])
self.assertEqual(as_group.default_cooldown,
300)
self.assertEqual(as_group.desired_capacity,
self.assertEqual(as_group.enabled_metrics,
self.assertEqual(as_group.health_check_period,
self.assertEqual(as_group.health_check_type,
'EC2')
self.assertEqual(as_group.launch_config_name,
'test_launchconfig')
self.assertEqual(as_group.load_balancers,
self.assertEqual(as_group.min_size,
self.assertEqual(as_group.max_size,
self.assertEqual(as_group.name,
'test_group')
self.assertEqual(as_group.suspended_processes,
self.assertEqual(as_group.tags,
self.assertEqual(as_group.termination_policies,
['OldestInstance',
self.assertEqual(as_group.instance_id,
'Something')
TestDescribeTerminationPolicies(AWSMockServiceTestCase):
self.service_connection.get_termination_policies()
self.assertListEqual(
['ClosestToNextInstanceHour',
'NewestInstance',
TestLaunchConfigurationDescribe(AWSMockServiceTestCase):
test_get_all_launch_configurations(self):
self.assertEqual(response[0].block_device_mappings,
self.assertEqual(response[0].classic_link_vpc_id,
'vpc-12345')
self.assertEqual(response[0].classic_link_vpc_security_groups,
['sg-1234'])
test_launch_config(self):
EBSBlockDeviceType(snapshot_id='snap-12345')
bdm['/dev/sdf']
launchconfig.LaunchConfiguration(
name='launch_config',
user_data='#!/bin/bash',
security_groups=['group1'],
spot_price='price',
block_device_mappings=[bdm],
volume_type='atype',
iops=3000,
classic_link_vpc_id='vpc-1234',
classic_link_vpc_security_groups=['classic_link_group']
self.service_connection.create_launch_configuration(lc)
'CreateLaunchConfiguration',
'BlockDeviceMappings.member.1.DeviceName':
'BlockDeviceMappings.member.1.Ebs.DeleteOnTermination':
'BlockDeviceMappings.member.1.Ebs.SnapshotId':
'launch_config',
'InstanceMonitoring.Enabled':
'SecurityGroups.member.1':
'price',
'atype',
3000,
'vpc-1234',
'ClassicLinkVPCSecurityGroups.member.1':
'classic_link_group'
TestCreateAutoScalePolicy(AWSMockServiceTestCase):
super(TestCreateAutoScalePolicy,
test_scaling_policy_with_min_adjustment_step(self):
test_scaling_policy_with_wrong_adjustment_type(self):
adjustment_type='ChangeInCapacity',
'ChangeInCapacity',
test_scaling_policy_without_min_adjustment_step(self):
scaling_adjustment=50)
TestPutNotificationConfiguration(AWSMockServiceTestCase):
super(TestPutNotificationConfiguration,
self.service_connection.put_notification_configuration(autoscale,
['autoscaling:EC2_INSTANCE_LAUNCH'])
'PutNotificationConfiguration',
'NotificationTypes.member.1':
'autoscaling:EC2_INSTANCE_LAUNCH',
TestDeleteNotificationConfiguration(AWSMockServiceTestCase):
super(TestDeleteNotificationConfiguration,
self.service_connection.delete_notification_configuration(autoscale,
'arn:aws:sns:us-east-1:19890506:AutoScaling-Up')
'DeleteNotificationConfiguration',
TestAutoScalingTag(AWSMockServiceTestCase):
test_create_or_update_tags(self):
key='alpha',
value='tango',
propagate_at_launch=True
key='bravo',
value='sierra',
propagate_at_launch=False
)]
self.service_connection.create_or_update_tags(tags)
'CreateOrUpdateTags',
'Tags.member.1.ResourceType':
'Tags.member.1.ResourceId':
'tango',
'Tags.member.1.PropagateAtLaunch':
'Tags.member.2.ResourceType':
'Tags.member.2.ResourceId':
'bravo',
'sierra',
'Tags.member.2.PropagateAtLaunch':
test_endElement(self):
'key'),
('Value',
'myvalue',
'value'),
('ResourceType',
'resource_type'),
('ResourceId',
'sg-01234567',
'resource_id'),
('PropagateAtLaunch',
'propagate_at_launch')]:
self.check_tag_attributes_set(i[0],
i[1],
i[2])
check_tag_attributes_set(self,
tag.endElement(name,
TestAttachInstances(AWSMockServiceTestCase):
super(TestAttachInstances,
test_attach_instances(self):
self.service_connection.attach_instances(
'AttachInstances',
TestDetachInstances(AWSMockServiceTestCase):
super(TestDetachInstances,
test_detach_instances(self):
test_detach_instances_with_decrement_desired_capacity(self):
test_detach_instances_without_decrement_desired_capacity(self):
TestGetAccountLimits(AWSMockServiceTestCase):
super(TestGetAccountLimits,
limits
self.service_connection.get_account_limits()
'DescribeAccountLimits',
self.assertEqual(limits.max_autoscaling_groups,
self.assertEqual(limits.max_launch_configurations,
TestGetAdjustmentTypes(AWSMockServiceTestCase):
super(TestGetAdjustmentTypes,
test_autoscaling_adjustment_types(self):
self.service_connection.get_all_adjustment_types()
'DescribeAdjustmentTypes'
self.assertEqual(response[0].adjustment_type,
"ChangeInCapacity")
self.assertEqual(response[1].adjustment_type,
"ExactCapacity")
self.assertEqual(response[2].adjustment_type,
"PercentChangeInCapacity")
TestLaunchConfigurationDescribeWithBlockDeviceTypes(AWSMockServiceTestCase):
test_get_all_launch_configurations_with_block_device_types(self):
self.service_connection.use_block_device_types
self.assertEqual(response[0].block_device_mappings['/dev/xvdb'].ephemeral_name,
'ephemeral0')
self.assertEqual(response[0].block_device_mappings['/dev/xvdc'].ephemeral_name,
'ephemeral1')
self.assertEqual(response[0].block_device_mappings['/dev/xvdp'].snapshot_id,
'snap-1234abcd')
self.assertEqual(response[0].block_device_mappings['/dev/xvdp'].delete_on_termination,
self.assertEqual(response[0].block_device_mappings['/dev/xvdp'].iops,
self.assertEqual(response[0].block_device_mappings['/dev/xvdp'].size,
self.assertEqual(response[0].block_device_mappings['/dev/xvdp'].volume_type,
self.assertEqual(response[0].block_device_mappings['/dev/xvdh'].delete_on_termination,
self.assertEqual(response[0].block_device_mappings['/dev/xvdh'].iops,
self.assertEqual(response[0].block_device_mappings['/dev/xvdh'].size,
self.assertEqual(response[0].block_device_mappings['/dev/xvdh'].volume_type,
TestCloudWatchConnection(AWSMockServiceTestCase):
test_build_put_params_multiple_everything(self):
['whatever',
'goeshere']
timestamp
['lbs',
'ft']
self.service_connection.build_put_params(
statistics=statistics
'whatever',
'MetricData.member.1.StatisticValues.Maximum':
'MetricData.member.1.StatisticValues.Minimum':
'MetricData.member.1.StatisticValues.SampleCount':
'MetricData.member.1.StatisticValues.Sum':
'MetricData.member.1.Timestamp':
'2013-05-13T09:02:35',
'MetricData.member.1.Unit':
'lbs',
'goeshere',
'MetricData.member.2.StatisticValues.Maximum':
'MetricData.member.2.StatisticValues.Minimum':
'MetricData.member.2.StatisticValues.SampleCount':
'MetricData.member.2.StatisticValues.Sum':
'MetricData.member.2.Timestamp':
'2013-05-12T09:02:35',
'MetricData.member.2.Unit':
'ft',
ATTRIBUTE_GET_TRUE_CZL_RESPONSE
ATTRIBUTE_GET_FALSE_CZL_RESPONSE
ATTRIBUTE_SET_CZL_TRUE_REQUEST
ATTRIBUTE_SET_CZL_FALSE_REQUEST
ATTRIBUTE_TESTS
(ATTRIBUTE_GET_TRUE_CZL_RESPONSE,
True)]),
(ATTRIBUTE_GET_FALSE_CZL_RESPONSE,
False)]),
(ATTRIBUTE_GET_CS_RESPONSE,
[('connecting_settings.idle_timeout',
30)]),
TestLbAttributes(unittest.TestCase):
_setup_mock(self):
'test_elb')
_verify_attributes(self,
attr_tests):
attr_tests:
sub_attr
attr.split('.'):
getattr(attr_result,
sub_attr,
self.assertEqual(attr_result,
test_get_all_lb_attributes(self):
test_get_lb_attribute(self):
elb.get_lb_attribute('test_elb',
self.assertEqual(status,
test_modify_lb_attribute(self):
ATTRIBUTE_SET_CZL_TRUE_REQUEST),
ATTRIBUTE_SET_CZL_FALSE_REQUEST),
elb.modify_lb_attribute('test_elb',
elb.make_request.assert_called_with(*args)
test_lb_get_attributes(self):
lb.get_attributes(force=True)
test_lb_is_cross_zone_load_balancing(self):
[True],
self.assertEqual(method(*args),
test_lb_enable_cross_zone_load_balancing(self):
self.assertTrue(lb.enable_cross_zone_load_balancing())
elb.make_request.assert_called_with(*ATTRIBUTE_SET_CZL_TRUE_REQUEST)
test_lb_disable_cross_zone_load_balancing(self):
self.assertTrue(lb.disable_cross_zone_load_balancing())
elb.make_request.assert_called_with(*ATTRIBUTE_SET_CZL_FALSE_REQUEST)
test_lb_get_connection_settings(self):
[('idle_timeout',
30),
self.assertEqual(getattr(attributes.connecting_settings,
LISTENERS_RESPONSE
TestListenerResponseParsing(unittest.TestCase):
test_parse_complex(self):
xml.sax.parseString(LISTENERS_RESPONSE,
rs[0].listeners
sorted([l.get_complex_tuple()
listeners]),
(80,
TestListenerGetItem(unittest.TestCase):
test_getitem_for_http_listener(self):
Listener(load_balancer_port=80,
protocol='HTTP',
instance_protocol='HTTP')
test_getitem_for_https_listener(self):
Listener(load_balancer_port=443,
protocol='HTTPS',
instance_protocol='HTTP',
ssl_certificate_id='look_at_me_im_an_arn')
self.assertEqual(listener[4],
'look_at_me_im_an_arn')
disabled
elb.disable_availability_zones('mine',
self.assertEqual(disabled,
TestDescribeLoadBalancers(unittest.TestCase):
test_other_policy(self):
self.assertEqual(len(load_balancers),
load_balancers[0]
self.assertEqual(len(lb.policies.other_policies),
self.assertEqual(lb.policies.other_policies[0].policy_name,
'AWSConsole-SSLNegotiationPolicy-my-test-loadbalancer')
self.assertEqual(lb.policies.other_policies[1].policy_name,
self.assertEqual(len(lb.backends),
self.assertEqual(len(lb.backends[0].policies),
self.assertEqual(lb.backends[0].policies[0].policy_name,
self.assertEqual(lb.backends[0].instance_port,
test_request_with_marker(self):
load_balancers1
self.assertEqual('1234',
load_balancers1.marker)
load_balancers2
elb.get_all_load_balancers(marker=load_balancers1.marker)
self.assertEqual(len(load_balancers2),
TestDetachSubnets(unittest.TestCase):
test_detach_subnets(self):
"mylb")
lb.detach_subnets("s-xxx")
ecs
boto.ec2containerservice.connect_to_region('us-east-1',
self.assertIsInstance(ecs,
EC2ContainerServiceConnection)
boto.ecs
TestECSConnection(AWSMockServiceTestCase):
test_item_lookup(self):
item_set
self.service_connection.item_lookup(
ItemId='0316067938',
ResponseGroup='Reviews'
{'ItemId':
'0316067938',
'Operation':
'ItemLookup',
'ResponseGroup':
'Reviews',
'AWSECommerceService'},
'Timestamp'])
list(item_set)
self.assertTrue(item_set.is_valid)
self.assertEqual(items[0].ASIN,
'B00008OE6I')
TestAPIInterface(AWSMockServiceTestCase):
test_required_launch_params(self):
'test_cache_cluster'
body=b'{}')
self.service_connection.create_cache_cluster(name)
'CreateCacheCluster',
BootstrapAction,
ClusterStateChangeReason,
ClusterStatus,
ClusterSummary,
ClusterTimeline,
InstanceInfo,
InstanceGroupInfo,
InstanceGroup,
StepSummaryList,
RunJobFlowResponse
TestListClusters(AWSMockServiceTestCase):
self.service_connection.list_clusters()
ClusterSummaryList))
self.assertEqual(len(response.clusters),
self.assertTrue(isinstance(response.clusters[0],
ClusterSummary))
self.assertEqual(response.clusters[0].name,
'analytics
self.assertEqual(response.clusters[0].normalizedinstancehours,
self.assertTrue(isinstance(response.clusters[0].status,
self.assertEqual(response.clusters[0].status.state,
isinstance(response.clusters[0].status.timeline,
ClusterTimeline))
response.clusters[0].status.timeline.creationdatetime,
response.clusters[0].status.timeline.readydatetime,
'2014-01-24T01:25:26Z')
response.clusters[0].status.timeline.enddatetime,
'2014-01-24T02:19:46Z')
self.assertTrue(isinstance(
response.clusters[0].status.statechangereason,
ClusterStateChangeReason))
response.clusters[0].status.statechangereason.code,
'USER_REQUEST')
self.assertEqual(response.clusters[
0].status.statechangereason.message,
'Terminated
request')
test_list_clusters_created_before(self):
self.service_connection.list_clusters(created_before=date)
test_list_clusters_created_after(self):
self.service_connection.list_clusters(created_after=date)
test_list_clusters_states(self):
self.service_connection.list_clusters(cluster_states=[
'ClusterStates.member.1':
'ClusterStates.member.2':
TestListInstanceGroups(AWSMockServiceTestCase):
test_list_instance_groups(self):
self.service_connection.list_instance_groups()
self.service_connection.list_instance_groups(
'ListInstanceGroups',
InstanceGroupList))
self.assertEqual(len(response.instancegroups),
isinstance(response.instancegroups[0],
InstanceGroupInfo))
self.assertEqual(response.instancegroups[0].id,
'ig-aaaaaaaaaaaaa')
response.instancegroups[0].instancegrouptype,
"MASTER")
self.assertEqual(response.instancegroups[0].instancetype,
"m1.large")
self.assertEqual(response.instancegroups[0].market,
"ON_DEMAND")
response.instancegroups[0].name,
"Master
group")
response.instancegroups[0].requestedinstancecount,
self.assertEqual(response.instancegroups[0].runninginstancecount,
isinstance(response.instancegroups[0].status,
self.assertEqual(response.instancegroups[0].status.state,
TestListInstances(AWSMockServiceTestCase):
test_list_instances(self):
self.service_connection.list_instances()
self.service_connection.list_instances(cluster_id='j-123')
InstanceList))
self.assertEqual(len(response.instances),
self.assertTrue(isinstance(response.instances[0],
InstanceInfo))
self.assertEqual(response.instances[0].ec2instanceid,
'i-aaaaaaaa')
self.assertEqual(response.instances[0].id,
'ci-123456789abc')
response.instances[0].privatednsname,
'ip-10-0-0-60.us-west-1.compute.internal')
self.assertEqual(response.instances[0].privateipaddress,
'10.0.0.60')
self.assertEqual(response.instances[
0].publicdnsname,
'ec2-54-0-0-1.us-west-1.compute.amazonaws.com')
self.assertEqual(response.instances[0].publicipaddress,
'54.0.0.1')
test_list_instances_with_group_id(self):
instance_group_id='abc')
'InstanceGroupId':
test_list_instances_with_types(self):
instance_group_types=[
'TASK'
'InstanceGroupTypes.member.1':
'InstanceGroupTypes.member.2':
'TASK',
TestListSteps(AWSMockServiceTestCase):
test_list_steps(self):
self.service_connection.list_steps()
self.service_connection.list_steps(cluster_id='j-123')
'PENDING',
'CANCELLED',
'INTERRUPTED'
response.steps:
self.assertIn(step.status.state,
valid_states)
response.steps[0]
self.assertEqual(step.config.jar,
'/home/hadoop/lib/emr-s3distcp-1.0.jar')
self.assertEqual(len(step.config.args),
self.assertEqual(step.config.args[0].value,
'--src')
self.assertEqual(step.config.args[1].value,
'hdfs:///data/test/')
response.steps[1]
self.assertEqual(step.config.mainclass,
'my.main.SomeClass')
test_list_steps_with_states(self):
self.service_connection.list_steps(
step_states=[
'FAILED'
'StepStates.member.1':
'StepStates.member.2':
TestListBootstrapActions(AWSMockServiceTestCase):
test_list_bootstrap_actions(self):
self.service_connection.list_bootstrap_actions()
self.service_connection.list_bootstrap_actions(
'ListBootstrapActions',
TestDescribeCluster(AWSMockServiceTestCase):
test_describe_cluster(self):
self.service_connection.describe_cluster()
self.service_connection.describe_cluster(cluster_id='j-123')
Cluster))
'j-aaaaaaaaa')
self.assertEqual(response.runningamiversion,
self.assertEqual(response.visibletoallusers,
self.assertEqual(response.autoterminate,
self.assertEqual(response.requestedamiversion,
self.assertEqual(response.terminationprotected,
response.ec2instanceattributes.ec2availabilityzone,
"us-west-1c")
response.ec2instanceattributes.ec2keyname,
'my_secret_key')
self.assertEqual(response.status.state,
self.assertEqual(response.applications[0].name,
'hadoop')
self.assertEqual(response.applications[0].version,
response.masterpublicdnsname,
self.assertEqual(response.normalizedinstancehours,
self.assertEqual(response.servicerole,
'my-service-role')
'DescribeCluster',
TestDescribeStep(AWSMockServiceTestCase):
test_describe_step(self):
self.service_connection.describe_step()
self.service_connection.describe_step(cluster_id='j-123')
self.service_connection.describe_step(step_id='abc')
self.service_connection.describe_step(
step_id='abc')
'DescribeStep',
TestAddJobFlowSteps(AWSMockServiceTestCase):
test_add_jobflow_steps(self):
self.service_connection.add_jobflow_steps(
jobflow_id='j-123',
steps=[])
JobFlowStepList))
self.assertEqual(response.stepids[0].value,
'Foo')
self.assertEqual(response.stepids[1].value,
TestBuildTagList(AWSMockServiceTestCase):
test_key_without_value_encoding(self):
'KeyWithNoValue':
'AnotherKeyWithNoValue':
'AnotherKeyWithNoValue',
'KeyWithNoValue'
test_key_full_key_value_encoding(self):
TestAddTag(AWSMockServiceTestCase):
test_add_mix_of_tags_with_without_values(self):
input_tags)
'AddTags',
'Tags.member.3.Key':
'ZzzNoValue',
TestRemoveTag(AWSMockServiceTestCase):
self.service_connection.remove_tags(
['FirstKey',
'SecondKey'])
'RemoveTags',
'TagKeys.member.1':
'TagKeys.member.2':
DescribeJobFlowsTestBase(AWSMockServiceTestCase):
TestDescribeJobFlows(DescribeJobFlowsTestBase):
test_describe_jobflows_response(self):
jf
self.assertTrue(isinstance(jf,
self.assertEqual(jf.amiversion,
self.assertEqual(jf.visibletoallusers,
self.assertEqual(jf.name,
self.assertEqual(jf.jobflowid,
'j-aaaaaa')
self.assertEqual(jf.ec2keyname,
'my_key')
self.assertEqual(jf.masterinstancetype,
self.assertEqual(jf.availabilityzone,
'us-west-1c')
self.assertEqual(jf.keepjobflowalivewhennosteps,
self.assertEqual(jf.slaveinstancetype,
self.assertEqual(jf.masterinstanceid,
'i-aaaaaa')
self.assertEqual(jf.hadoopversion,
self.assertEqual(jf.normalizedinstancehours,
jf.masterpublicdnsname,
self.assertEqual(jf.instancecount,
'3')
self.assertEqual(jf.terminationprotected,
self.assertTrue(isinstance(jf.steps,
jf.steps[0]
self.assertTrue(isinstance(step,
Step))
step.jar,
's3://us-west-1.elasticmapreduce/libs/script-runner/script-runner.jar')
self.assertEqual(step.name,
'Setup
hive')
self.assertEqual(step.actiononfailure,
'TERMINATE_JOB_FLOW')
self.assertTrue(isinstance(jf.instancegroups,
jf.instancegroups[0]
self.assertTrue(isinstance(ig,
InstanceGroup))
self.assertEqual(ig.creationdatetime,
self.assertEqual(ig.state,
'ENDED')
self.assertEqual(ig.instancerequestcount,
self.assertEqual(ig.instancetype,
self.assertEqual(ig.laststatechangereason,
'Job
flow
terminated')
self.assertEqual(ig.market,
'ON_DEMAND')
self.assertEqual(ig.instancegroupid,
'ig-aaaaaa')
self.assertEqual(ig.instancerole,
'MASTER')
self.assertEqual(ig.name,
'Master
test_describe_jobflows_no_args(self):
test_describe_jobflows_filtered(self):
a_bit_before
datetime.fromtimestamp(time()
self.service_connection.describe_jobflows(states=['WAITING',
'RUNNING'],
jobflow_ids=[
'j-aaaaab'],
created_after=a_bit_before,
created_before=now)
'JobFlowIds.member.2':
'j-aaaaab',
'JobFlowStates.member.1':
'JobFlowStates.member.2':
a_bit_before.strftime(boto.utils.ISO8601),
now.strftime(boto.utils.ISO8601),
TestDescribeJobFlow(DescribeJobFlowsTestBase):
test_describe_jobflow(self):
self.service_connection.describe_jobflow('j-aaaaaa')
TestRunJobFlow(AWSMockServiceTestCase):
test_run_jobflow_service_role(self):
service_role='EMR_DefaultRole')
'ServiceRole':
'EMR_DefaultRole',
'EmrCluster'},
ignore_params_values=['ActionOnFailure',
'Instances.InstanceCount',
'Instances.KeepJobFlowAliveWhenNoSteps',
'Instances.MasterInstanceType',
'Instances.SlaveInstanceType'])
test_run_jobflow_enable_debugging(self):
'ap-northeast-2'
enable_debugging=True)
actual_params
set(self.actual_request.params.copy().items())
('Steps.member.1.HadoopJarStep.Jar',
's3://ap-northeast-2.elasticmapreduce/libs/script-runner/script-runner.jar'),
('Steps.member.1.HadoopJarStep.Args.member.1',
's3://ap-northeast-2.elasticmapreduce/libs/state-pusher/0.1/fetch'),
self.assertTrue(expected_params
actual_params)
emrobject
JOB_FLOW_EXAMPLE
JOB_FLOW_COMPLETED
TestEMRResponses(unittest.TestCase):
_parse_xml(self,
markers):
_assert_fields(self,
**fields):
fields.items():
actual,
"Field
actual))
test_JobFlows_example(self):
self._parse_xml(JOB_FLOW_EXAMPLE,
creationdatetime='2009-01-28T21:49:16Z',
startdatetime='2009-01-28T21:49:16Z',
state='STARTING',
instancecount='4',
jobflowid='j-3UN6WX5RRO2AG',
loguri='mybucket/subdir/',
name='MyJobFlowName',
availabilityzone='us-east-1a',
slaveinstancetype='m1.small',
masterinstancetype='m1.small',
ec2keyname='myec2keyname',
keepjobflowalivewhennosteps='true')
test_JobFlows_completed(self):
self._parse_xml(JOB_FLOW_COMPLETED,
creationdatetime='2010-10-21T01:00:25Z',
startdatetime='2010-10-21T01:03:59Z',
enddatetime='2010-10-21T01:44:18Z',
state='COMPLETED',
instancecount='10',
jobflowid='j-3H3Q13JPFLU22',
loguri='s3n://example.emrtest.scripts/jobflow_logs/',
name='RealJobFlowName',
availabilityzone='us-east-1b',
slaveinstancetype='m1.large',
masterinstancetype='m1.large',
ec2keyname='myubersecurekey',
keepjobflowalivewhennosteps='false')
len(jobflow.steps))
len(jobflow.instancegroups))
boto.emr.instance_group
InstanceGroup
TestInstanceGroupArgs(unittest.TestCase):
test_bidprice_missing_spot(self):
'bidprice
specified'):
test_bidprice_missing_ondemand(self):
'ON_DEMAND',
test_bidprice_Decimal(self):
bidprice=Decimal(1.10))
self.assertEquals('1.10',
instance_group.bidprice[:4])
test_bidprice_float(self):
bidprice=1.1)
test_bidprice_string(self):
bidprice='1.1')
ConcurrentUploader,
ConcurrentDownloader
UploadWorkerThread
FakeThreadedConcurrentUploader(ConcurrentUploader):
hash_chunks[i]
FakeThreadedConcurrentDownloader(ConcurrentDownloader):
TestConcurrentUploader(unittest.TestCase):
super(TestConcurrentUploader,
self.stat_patch
mock.patch('os.stat')
self.addCleanup(self.stat_patch.stop)
self.stat_mock
self.stat_patch.start()
test_calculate_required_part_size(self):
test_calculate_required_part_size_too_small(self):
too_small
part_size=too_small)
test_work_queue_is_correctly_populated(self):
FakeThreadedConcurrentUploader(mock.MagicMock(),
uploader.worker_queue
test_correct_low_level_api_calls(self):
api_mock
'0898d645-ea45-4548-9a67-578f507ead49'
initiate_upload_mock
return_value={'UploadId':
upload_id})
api_mock.attach_mock(initiate_upload_mock,
'initiate_multipart_upload')
FakeThreadedConcurrentUploader(api_mock,
initiate_upload_mock.assert_called_with(
api_mock.complete_multipart_upload.assert_called_with(
test_downloader_work_queue_is_correctly_populated(self):
job.archive_size
FakeThreadedConcurrentDownloader(job)
downloader.download('foofile')
downloader.worker_queue
TestUploaderThread(unittest.TestCase):
self.fileobj
self.fileobj.name
test_fileobj_closed_when_thread_shuts_down(self):
UploadWorkerThread(mock.Mock(),
Queue(),
Queue())
thread._fileobj
self.assertFalse(fileobj.closed)
thread.run()
self.assertTrue(fileobj.closed)
test_upload_errors_have_exception_messages(self):
num_retries=1,
Exception("exception
message")
result_queue.get(timeout=1)
self.assertIn("exception
message",
str(result))
test_num_retries_is_obeyed(self):
Exception()
self.assertEqual(api.upload_part.call_count,
TestJob(unittest.TestCase):
mock.Mock(spec=Layer1)
self.vault.layer1
Job(self.vault)
test_get_job_validate_checksum_success(self):
test_get_job_validation_fails(self):
'BAD_TREE_HASH_VALUE'
self.assertRaises(TreeHashDoesNotMatchError):
validate_checksum=False)
test_download_to_fileobj(self):
mock.Mock(read=mock.Mock(return_value='xyz'))
GlacierResponse(http_response,
self.job.download_to_fileobj(fileobj)
self.assertEqual(http_response.read.return_value,
fileobj.read())
test_calc_num_chunks(self):
self.job.DefaultPartSize
GlacierLayer1ConnectionBase(AWSMockServiceTestCase):
super(GlacierLayer1ConnectionBase,
self.json_header
'application/json')]
u'examplevault'
self.vault_arn
'arn:aws:glacier:us-east-1:012345678901:vaults/'
self.vault_info
{u'CreationDate':
u'2012-03-16T22:22:47.214Z',
u'LastInventoryDate':
u'2012-03-21T22:06:51.218Z',
u'NumberOfArchives':
u'SizeInBytes':
12334,
u'VaultARN':
self.vault_arn,
u'VaultName':
self.vault_name}
GlacierVaultsOperations(GlacierLayer1ConnectionBase):
test_create_vault_parameters(self):
self.service_connection.create_vault(self.vault_name)
{u'Marker':
u'VaultList':
[self.vault_info]}
self.service_connection.list_vaults()
test_describe_vaults(self):
copy.copy(self.vault_info)
content[u'RequestId']
self.service_connection.describe_vault(self.vault_name)
self.service_connection.delete_vault(self.vault_name)
GlacierJobOperations(GlacierLayer1ConnectionBase):
super(GlacierJobOperations,
self.job_content
'abc'
test_initiate_archive_job(self):
{u'Type':
u'archive-retrieval',
u'ArchiveId':
u'AAABZpJrTyioDC_HsOmHae8EZp_uBSJr6cnGOLKp_XJCl-Q',
u'Test
Archive',
u'SNSTopic':
u'Topic',
u'JobId':
u'Location':
self.set_http_response(status_code=202,
self.service_connection.initiate_job(self.vault_name,
self.job_content)
test_get_archive_output(self):
'application/octet-stream')]
header=header,
body=self.job_content)
self.service_connection.get_job_output(self.vault_name,
'example-job-id')
self.assertEqual(self.job_content,
TestGlacierUploadPart(GlacierLayer1ConnectionBase):
test_upload_part_content_range_header(self):
self.actual_request.headers['Content-Range'],
1-2/*')
test_upload_part_with_unicode_name(self):
'/-/vaults/unicode_vault_name/multipart-uploads/upload_id')
self.assertIsInstance(self.actual_request.body,
GlacierUploadArchiveResets(GlacierLayer1ConnectionBase):
test_upload_archive(self):
fake_data.write(b'foobarbaz')
fake_data.seek(2)
self.service_connection.connection.request.side_effect
*args:
fake_data.read()
self.service_connection.upload_archive('vault_name',
fake_data,
'tree_hash')
self.assertEqual(fake_data.tell(),
sentinel
tzinfo,
"2012-02-20T17:01:45.198Z",
"LastInventoryDate":
"NumberOfArchives":
192,
"SizeInBytes":
78088912,
"arn:aws:glacier:us-east-1:012345678901:vaults/examplevault",
"VaultName":
"examplevault"
FIXTURE_PAGINATED_VAULTS
'arn:aws:glacier:us-east-1:686406519478:vaults/vault1',
'vault1',
FIXTURE_PAGINATED_VAULTS_CONT
'vault2',
"ArchiveRetrieval",
"ArchiveId":
("NkbByEejwEggmBz2fTHgJrg0XBoDfjP4q6iu87-TjhqG6eGoOY9Z8i1_AUyUs"
"uhPAdTqLHy8pTl5nfCFJmDl2yEZONi5L26Omw12vcs01MNGntHEQL8MBfGlqr"
"EXAMPLEArchiveId"),
"ArchiveSizeInBytes":
16777216,
"Completed":
"2012-05-15T17:21:39.339Z",
"CompletionDate":
"2012-05-15T17:21:43.561Z",
"InventorySizeInBytes":
"JobDescription":
"My
ArchiveRetrieval
Job",
"JobId":
("HkF9p6o7yjhFx-K3CGl6fuSm6VzW9T7esGQfco8nUXVYwS0jlb5gq1JZ55yHgt5v"
"P54ZShjoQzQVVh7vEXAMPLEjobID"),
("beb0fe31a1c7ca8c6c04d574ea906e3f97b31fdca7571defb5b44dc"
"a89b5af60"),
"SNSTopic":
"arn:aws:sns:us-east-1:012345678901:mytopic",
"StatusCode":
"InProgress",
"StatusMessage":
"Operation
progress.",
"arn:aws:glacier:us-east-1:012345678901:vaults/examplevault"
EXAMPLE_PART_LIST_RESULT_PAGE_1
"MfgsKHVjbQ6EldVl72bn3_n5h2TaGZQUO-Qb3B9j3TITf7WajQ",
EXAMPLE_PART_LIST_RESULT_PAGE_2
EXAMPLE_PART_LIST_COMPLETE
GlacierLayer2Base(unittest.TestCase):
self.mock_layer1
TestGlacierLayer2Connection(GlacierLayer2Base):
Layer2(layer1=self.mock_layer1)
test_create_vault(self):
self.layer2.create_vault("My
self.mock_layer1.create_vault.assert_called_with("My
test_get_vault(self):
self.layer2.get_vault("examplevault")
self.assertEqual(vault.layer1,
self.mock_layer1)
"examplevault")
self.assertEqual(vault.size,
78088912)
192)
self.mock_layer1.list_vaults.return_value
test_list_vaults_paginated(self):
resps
[FIXTURE_PAGINATED_VAULTS,
FIXTURE_PAGINATED_VAULTS_CONT]
return_paginated_vaults_resp(marker=None,
resps.pop(0)
self.mock_layer1.list_vaults
Mock(side_effect=return_paginated_vaults_resp)
self.assertEqual(vaults[3].name,
"vault3")
TestVault(GlacierLayer2Base):
test_create_archive_writer(self):
self.mock_layer1.initiate_multipart_upload.return_value
"UploadId":
"UPLOADID"}
self.vault.create_archive_writer(description="stuff")
self.mock_layer1.initiate_multipart_upload.assert_called_with(
self.vault.DefaultPartSize,
"stuff")
self.assertEqual(writer.vault,
self.vault)
self.assertEqual(writer.upload_id,
"UPLOADID")
self.vault.delete_archive("archive")
self.mock_layer1.delete_archive.assert_called_with("examplevault",
"archive")
test_initiate_job(self):
UTC(tzinfo):
utcoffset(self,
tzname(self,
dst(self,
self.mock_layer1.initiate_job.return_value
{'JobId':
'job-id'}
self.vault.retrieve_inventory(start_date=datetime(2014,
end_date=datetime(2014,
0o2,
limit=100)
self.mock_layer1.initiate_job.assert_called_with(
'examplevault',
'inventory-retrieval',
'InventoryRetrievalParameters':
'StartDate':
'2014-01-01T00:00:00Z',
'EndDate':
'2014-01-02T00:00:00Z',
'Limit':
test_get_job(self):
self.mock_layer1.describe_job.return_value
self.vault.get_job(
"NkbByEejwEggmBz2fTHgJrg0XBoDfjP4q6iu87-TjhqG6eGoOY9Z8i1_AUyUsuhPA"
"dTqLHy8pTl5nfCFJmDl2yEZONi5L26Omw12vcs01MNGntHEQL8MBfGlqrEXAMPLEA"
"rchiveId")
self.assertEqual(job.action,
"ArchiveRetrieval")
test_list_jobs(self):
self.mock_layer1.list_jobs.return_value
"JobList":
[FIXTURE_ARCHIVE_JOB]}
jobs
self.vault.list_jobs(False,
self.mock_layer1.list_jobs.assert_called_with("examplevault",
self.assertEqual(jobs[0].archive_id,
"NkbByEejwEggmBz2fTHgJrg0XBoDfjP4q6iu87-TjhqG6eGoOY9Z"
"8i1_AUyUsuhPAdTqLHy8pTl5nfCFJmDl2yEZONi5L26Omw12vcs0"
"1MNGntHEQL8MBfGlqrEXAMPLEArchiveId")
test_list_all_parts_one_page(self):
self.mock_layer1.list_parts.return_value
dict(EXAMPLE_PART_LIST_COMPLETE))
sentinel.upload_id)]
test_list_all_parts_two_pages(self):
self.mock_layer1.list_parts.side_effect
dict(EXAMPLE_PART_LIST_RESULT_PAGE_1),
dict(EXAMPLE_PART_LIST_RESULT_PAGE_2)
sentinel.upload_id),
call('examplevault',
marker=EXAMPLE_PART_LIST_RESULT_PAGE_1['Marker'])]
@patch('boto.glacier.vault.resume_file_upload')
test_resume_archive_from_file(self,
mock_resume_file_upload):
mock_list_parts.return_value
'PartSizeInBytes':
'Parts':
'0-3',
'12',
'4-6',
'34',
self.vault.list_all_parts
self.vault.resume_archive_from_file(
file_obj=sentinel.file_obj)
mock_resume_file_upload.assert_called_once_with(
sentinel.file_obj,
{0:
codecs.decode('12',
'hex_codec'),
codecs.decode('34',
'hex_codec')})
TestJob(GlacierLayer2Base):
Job(self.vault,
FIXTURE_ARCHIVE_JOB)
test_get_job_output(self):
self.mock_layer1.get_job_output.return_value
"TEST_OUTPUT"
self.job.get_output((0,
self.mock_layer1.get_job_output.assert_called_with(
"HkF9p6o7yjhFx-K3CGl6fuSm6VzW9T7esGQfco8nUXVYwS0jlb5gq1JZ55yHgt5vP"
"54ZShjoQzQVVh7vEXAMPLEjobID",
TestRangeStringParsing(unittest.TestCase):
test_simple_range(self):
Vault._range_string_to_part_index('0-3',
test_range_one_too_big(self):
Vault._range_string_to_part_index('0-4',
test_range_too_big(self):
'0-5',
test_range_start_mismatch(self):
'1-3',
test_range_end_mismatch(self):
Vault._range_string_to_part_index('0-2',
TestResponse(AWSMockServiceTestCase):
test_204_body_isnt_passed_to_json(self):
self.create_response(status_code=204,header=[('Content-Type','application/json')])
GlacierResponse(response,response.getheaders())
self.assertEquals(result.status,
TestPartSizeCalculations(unittest.TestCase):
test_small_values_still_use_default_part_size(self):
self.assertEqual(minimum_part_size(1),
test_under_the_maximum_value(self):
test_gigabyte_size(self):
10000),
test_terabyte_size(self):
self.assertEqual(minimum_part_size(4
512
test_file_size_too_large(self):
minimum_part_size((40000
test_default_part_size_can_be_specified(self):
default_part_size),
default_part_size)
TestChunking(unittest.TestCase):
test_chunk_hashes_exact(self):
chunk_hashes(b'a'
test_chunks_with_leftovers(self):
bytestring
chunk_hashes(bytestring)
self.assertEqual(chunks[1],
self.assertEqual(chunks[2],
20).digest())
test_less_than_one_chunk(self):
chunk_hashes(b'aaaa')
sha256(b'aaaa').digest())
TestTreeHash(unittest.TestCase):
calculate_tree_hash(self,
bytestring):
bytes_to_hex(tree_hash(chunk_hashes(bytestring)))
logging.debug("Tree
calc
len(bytestring),
test_tree_hash_calculations(self):
one_meg_bytestring
two_meg_bytestring
(4
bigger_bytestring
self.calculate_tree_hash(one_meg_bytestring),
b'9bc1b2a288b26af7257a36277ae3816a7d4f16e89c1e7e77d0a5c48bad62b360')
self.calculate_tree_hash(two_meg_bytestring),
b'560c2c9333c719cb00cfdffee3ba293db17f58743cdd1f7e4055373ae6300afa')
self.calculate_tree_hash(four_meg_bytestring),
b'9491cb2ed1d4e7cd53215f4017c23ec4ad21d7050a1e6bb636c4f67e8cddb844')
self.calculate_tree_hash(bigger_bytestring),
b'12f3cbd6101b981cde074039f6f728071da8879d6f632de8afc7cdf00661b08f')
test_empty_tree_hash(self):
self.calculate_tree_hash(''),
b'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855')
TestFileHash(unittest.TestCase):
_gen_data(self):
os.urandom(5000)
b'\xc2\x00'
test_compute_hash_tempfile(self):
"w+"
"wb+"
tempfile.TemporaryFile(mode=mode)
f.write(self._gen_data())
test_compute_hash_tempfile_py3(self):
tempfile.TemporaryFile(mode='w+')
StringIO('test
500)
test_compute_hash_stringio(self):
StringIO(self._gen_data())
test_compute_hash_bytesio(self):
BytesIO(self._gen_data())
mock.ANY
TestVault(unittest.TestCase):
self.size_patch
mock.patch('os.path.getsize')
self.getsize
self.size_patch.start()
vault.Vault(self.api,
self.vault.name
'myvault'
self.mock_open
mock.mock_open()
stringio
StringIO('content')
self.mock_open.return_value.read
stringio.read
self.size_patch.stop()
@mock.patch('boto.glacier.vault.compute_hashes_from_fileobj',
return_value=[b'abc',
b'123'])
test_upload_archive_small_file(self,
compute_hashes):
self.api.upload_archive.return_value
{'ArchiveId':
'archive_id'}
self.vault.upload_archive(
self.api.upload_archive.assert_called_with(
'myvault',
self.mock_open.return_value,
test_small_part_size_is_obeyed(self):
test_large_part_size_is_obeyed(self):
test_part_size_needs_to_be_adjusted(self):
expected_part_size
part_size=expected_part_size)
test_retrieve_inventory(self):
'x-amz-job-id':
'HkF9p6'
raw_resp
init_resp
GlacierResponse(raw_resp,
'JobId')])
raw_resp_2
desc_resp
GlacierResponse(raw_resp_2,
'initiate_job',
return_value=init_resp):
'describe_job',
return_value=desc_resp):
self.assertEqual(self.vault.retrieve_inventory(),
self.vault.retrieve_inventory_job()
self.assertTrue(isinstance(job,
Job))
self.assertEqual(job.id,
TestConcurrentUploads(unittest.TestCase):
test_concurrent_upload_file(self):
c.return_value.upload.assert_called_with('filename',
test_concurrent_upload_forwards_kwargs(self):
c.assert_called_with(None,
sentinel,
nose.tools
assert_equal
Writer,
resume_file_upload
create_mock_vault():
Mock(spec=Vault)
vault.layer1
vault.layer1.complete_multipart_upload.return_value
ArchiveId=sentinel.archive_id)
vault.name
sentinel.vault_name
partify(data,
itertools.count(0):
data[start:start
part_size]
part:
calculate_mock_vault_calls(data,
upload_part_calls
data_part
enumerate(partify(data,
len(data_part)
data_part_tree_hash_blob
chunk_hashes(data_part,
data_part_tree_hash
bytes_to_hex(data_part_tree_hash_blob)
data_part_linear_hash
sha256(data_part).hexdigest()
upload_part_calls.append(
call.layer1.upload_part(
data_part_linear_hash,
data_part_tree_hash,
data_part))
data_tree_hashes.append(data_part_tree_hash_blob)
check_mock_vault_calls(vault,
data_len):
vault.layer1.upload_part.assert_has_calls(
assert_equal(
len(upload_part_calls),
vault.layer1.upload_part.call_count)
data_tree_hash
bytes_to_hex(tree_hash(data_tree_hashes))
vault.layer1.complete_multipart_upload.assert_called_once_with(
data_tree_hash,
data_len)
TestWriter(unittest.TestCase):
super(TestWriter,
sentinel.upload_id
self.writer
Writer(
check_write(self,
write_list):
write_data
write_list:
self.writer.write(write_data)
b''.join(write_list)
test_single_byte_write(self):
self.check_write([b'1'])
test_one_part_write(self):
self.check_write([b'1234'])
test_split_write_1(self):
self.check_write([b'1',
b'234'])
test_split_write_2(self):
self.check_write([b'12',
b'34'])
test_split_write_3(self):
self.check_write([b'123',
b'4'])
test_one_part_plus_one_write(self):
self.check_write([b'12345'])
self.writer.write(b'1')
self.writer.get_archive_id())
test_current_tree_hash(self):
hash_1
self.assertEqual(hash_1,
b'\x0e\xb0\x11Z\x1d\x1f\n\x10|\xf76\xa6\xf5'
b'\x83\xd1\xd5"bU\x0c\x95\xa8<\xf5\x81\xef\x0e\x0f\x95\n\xb7k'
hash_2
self.assertEqual(hash_2,
b'\x7f\xf4\x97\x82U]\x81R\x05#^\xe8\x1c\xd19'
b'\xe8\x1f\x9e\xe0\x1aO\xaad\xe5\x06"\xa5\xc0\xa8AdL'
b';\x1a\xb8!=\xf0\x14#\x83\x11\xd5\x0b\x0f'
b'\xc7D\xe4\x8e\xd1W\x99z\x14\x06\xb9D\xd0\xf0*\x93\xa2\x8e\xf9'
self.writer.current_tree_hash)
test_current_uploaded_size(self):
size_1
self.assertEqual(size_1,
size_2
self.assertEqual(size_2,
final_size
self.writer.current_uploaded_size)
test_upload_id(self):
self.assertEquals(sentinel.upload_id,
self.writer.upload_id)
TestResume(unittest.TestCase):
super(TestResume,
check_no_resume(self,
resume_set=set()):
fobj
StringIO(data.decode('utf-8'))
resume_set:
data[start:end]
chunk_hashes(part_data,
resume_upload_part_calls
enumerate(upload_part_calls)
resume_set]
resume_upload_part_calls,
test_one_part_no_resume(self):
self.check_no_resume(b'1234')
test_two_parts_no_resume(self):
self.check_no_resume(b'12345678')
test_one_part_resume(self):
self.check_no_resume(b'1234',
resume_set=set([0]))
test_two_parts_one_resume(self):
self.check_no_resume(b'12345678',
resume_set=set([1]))
StringIO('1'),
TestCreateSamlProvider(AWSMockServiceTestCase):
test_create_saml_provider(self):
self.service_connection.create_saml_provider('document',
'CreateSAMLProvider',
'document',
self.assertEqual(response['create_saml_provider_response']
['create_saml_provider_result']
['saml_provider_arn'],
'arn')
TestListSamlProviders(AWSMockServiceTestCase):
test_list_saml_providers(self):
self.service_connection.list_saml_providers()
'ListSAMLProviders'},
self.assertEqual(response.saml_provider_list,
'arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Database',
'2032-05-09T16:27:11Z',
'2012-05-09T16:27:03Z'},
'arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Webserver',
'2015-03-11T13:11:02Z',
'2012-05-09T16:27:11Z'}])
TestGetSamlProvider(AWSMockServiceTestCase):
test_get_saml_provider(self):
self.service_connection.get_saml_provider('arn')
'GetSAMLProvider',
TestUpdateSamlProvider(AWSMockServiceTestCase):
test_update_saml_provider(self):
self.service_connection.update_saml_provider('arn',
'doc')
'UpdateSAMLProvider',
TestDeleteSamlProvider(AWSMockServiceTestCase):
test_delete_saml_provider(self):
self.service_connection.delete_saml_provider('arn')
'DeleteSAMLProvider',
TestCreateRole(AWSMockServiceTestCase):
test_create_role_default(self):
["ec2.amazonaws.com"]}}]})
test_create_role_default_cn_north(self):
["ec2.amazonaws.com.cn"]}}]})
test_create_role_string_policy(self):
assume_role_policy_document='{"hello":
"policy"}'
test_create_role_data_policy(self):
assume_role_policy_document={"hello":
"policy"}
TestGetSigninURL(AWSMockServiceTestCase):
test_get_signin_url_default(self):
'https://foocorporation.signin.aws.amazon.com/console/ec2'
test_get_signin_url_s3(self):
self.service_connection.get_signin_url(service='s3')
'https://foocorporation.signin.aws.amazon.com/console/s3'
test_get_signin_url_cn_north(self):
'https://foocorporation.signin.amazonaws.cn/console/ec2'
TestGetSigninURLNoAliases(AWSMockServiceTestCase):
test_get_signin_url_no_aliases(self):
self.assertRaises(Exception):
TestGenerateCredentialReport(AWSMockServiceTestCase):
test_generate_credential_report(self):
self.service_connection.generate_credential_report()
self.assertEquals(response['generate_credential_report_response']
['generate_credential_report_result']
['state'],
'COMPLETE')
TestGetCredentialReport(AWSMockServiceTestCase):
test_get_credential_report(self):
self.service_connection.get_credential_report()
b64decode(response['get_credential_report_response']
['get_credential_report_result']
['content'])
TestCreateVirtualMFADevice(AWSMockServiceTestCase):
test_create_virtual_mfa_device(self):
self.service_connection.create_virtual_mfa_device('/',
'ExampleName')
{'Path':
'ExampleName',
'CreateVirtualMFADevice'},
self.assertEquals(response['create_virtual_mfa_device_response']
['create_virtual_mfa_device_result']
['virtual_mfa_device']
['serial_number'],
'arn:aws:iam::123456789012:mfa/ExampleName')
TestGetAccountPasswordPolicy(AWSMockServiceTestCase):
test_get_account_password_policy(self):
self.service_connection.get_account_password_policy()
'GetAccountPasswordPolicy',
self.assertEquals(response['get_account_password_policy_response']
['minimum_password_length'],
TestUpdateAccountPasswordPolicy(AWSMockServiceTestCase):
test_update_account_password_policy(self):
self.service_connection.update_account_password_policy(minimum_password_length=88)
'UpdateAccountPasswordPolicy',
'MinimumPasswordLength':
TestDeleteAccountPasswordPolicy(AWSMockServiceTestCase):
test_delete_account_password_policy(self):
self.service_connection.delete_account_password_policy()
'DeleteAccountPasswordPolicy'
TestCreatePolicy(AWSMockServiceTestCase):
test_create_policy(self):
self.service_connection.create_policy(
'S3-read-only-example-bucket',
policy_doc)
'CreatePolicy',
'S3-read-only-example-bucket'},
self.assertEqual(response['create_policy_response']
['create_policy_result']
['policy_name'],
'S3-read-only-example-bucket')
TestCreatePolicyVersion(AWSMockServiceTestCase):
test_create_policy_version(self):
self.service_connection.create_policy_version(
set_as_default=True)
'CreatePolicyVersion',
'SetAsDefault':
self.assertEqual(response['create_policy_version_response']
['create_policy_version_result']
['is_default_version'],
TestDeletePolicy(AWSMockServiceTestCase):
test_delete_policy(self):
self.service_connection.delete_policy(
'DeletePolicy',
response['delete_policy_response']
TestDeletePolicyVersion(AWSMockServiceTestCase):
test_delete_policy_version(self):
self.service_connection.delete_policy_version(
'DeletePolicyVersion',
response['delete_policy_version_response']
TestGetPolicy(AWSMockServiceTestCase):
test_get_policy(self):
self.service_connection.get_policy(
'GetPolicy',
['arn'],
['description'],
Awesome
Policy')
TestGetPolicyVersion(AWSMockServiceTestCase):
test_get_policy_version(self):
self.service_connection.get_policy_version(
'GetPolicyVersion',
self.assertEqual(response['get_policy_version_response']
['get_policy_version_result']
['version_id'],
TestListPolicies(AWSMockServiceTestCase):
test_list_policies(self):
self.service_connection.list_policies(
max_items=4)
self.assertEqual(len(response['list_policies_response']
['policies']),
self.assertEqual(response['list_policies_response']
['is_truncated'],
TestListPolicyVersions(AWSMockServiceTestCase):
test_list_policy_versions(self):
self.service_connection.list_policy_versions(
max_items=3)
self.assertEqual(len(response['list_policy_versions_response']
['list_policy_versions_result']
['versions']),
TestSetDefaultPolicyVersion(AWSMockServiceTestCase):
test_set_default_policy_version(self):
self.service_connection.set_default_policy_version(
'SetDefaultPolicyVersion',
response['set_default_policy_version_response']
TestListEntitiesForPolicy(AWSMockServiceTestCase):
test_list_entities_for_policy(self):
self.service_connection.list_entities_for_policy(
'ListEntitiesForPolicy',
['policy_roles']),
['policy_groups']),
['policy_users']),
self.assertEqual({'user_name':
'Alice'}
response['list_entities_for_policy_response']
['policy_users'],
TestAttachGroupPolicy(AWSMockServiceTestCase):
test_attach_group_policy(self):
self.service_connection.attach_group_policy(
'AttachGroupPolicy',
response['attach_group_policy_response']
TestAttachRolePolicy(AWSMockServiceTestCase):
test_attach_role_policy(self):
self.service_connection.attach_role_policy(
'AttachRolePolicy',
response['attach_role_policy_response']
TestAttachUserPolicy(AWSMockServiceTestCase):
test_attach_user_policy(self):
self.service_connection.attach_user_policy(
'AttachUserPolicy',
response['attach_user_policy_response']
TestDetachGroupPolicy(AWSMockServiceTestCase):
test_detach_group_policy(self):
self.service_connection.detach_group_policy(
'DetachGroupPolicy',
response['detach_group_policy_response']
TestDetachRolePolicy(AWSMockServiceTestCase):
test_detach_role_policy(self):
self.service_connection.detach_role_policy(
'DetachRolePolicy',
response['detach_role_policy_response']
TestDetachUserPolicy(AWSMockServiceTestCase):
test_detach_user_policy(self):
self.service_connection.detach_user_policy(
'DetachUserPolicy',
response['detach_user_policy_response']
test_put_record_binary(self):
test_put_record_string(self):
test_put_records(self):
record_binary
record_str
self.service_connection.put_records(stream_name='stream-name',
records=[record_binary,
record_str])
self.assertEqual(body['Records'][0]['Data'],
self.assertEqual(body['Records'][1]['Data'],
test_binary_input(self):
self.assertEqual(body['Plaintext'],
test_non_binary_input_for_blobs_fails(self):
u'\u00e9'
test_binary_ouput(self):
{'Plaintext':
'AAECAwQF'}
self.service_connection.decrypt(b'some
arbitrary
self.assertEqual(response['Plaintext'],
b'\x00\x01\x02\x03\x04\x05')
TestDescribeLogs(AWSMockServiceTestCase):
b'{"logGroups":
[]}'
self.service_connection.describe_log_groups()
len(api_response['logGroups']))
self.assertTrue('DescribeLogGroups'
TestMachineLearning(AWSMockServiceTestCase):
test_predict(self):
predict_endpoint=ml_endpoint)
test_predict_with_scheme_in_endpoint(self):
predict_endpoint='https://'
TestSSHTimeout(unittest.TestCase):
paramiko,
'Paramiko
missing')
test_timeout(self):
client_tmp
client_mock():
client_tmp()
client.connect
mock.Mock(name='connect')
client_mock
paramiko.RSAKey.from_private_key_file
self.assertEqual(test._ssh_client.connect.call_args[1]['timeout'],
test2
timeout=30)
self.assertEqual(test2._ssh_client.connect.call_args[1]['timeout'],
GET_FILE_UPLOAD_URL
TestMTurkConnection(AWSMockServiceTestCase):
super(TestMTurkConnection,
test_get_file_upload_url_success(self):
body=GET_FILE_UPLOAD_URL)
self.service_connection.get_file_upload_url('aid',
'qid')
self.assertEquals(len(rset),
self.assertEquals(rset[0].FileUploadURL,
'http://s3.amazonaws.com/myawsbucket/puppy.jpg')
TestMTurkPostingWithQualificationsIn(AWSMockServiceTestCase):
super(TestMTurkPostingWithQualificationsIn,
test_locale_qualification_in(self):
test_locale_qualification_notin_in(self):
TestMTurkPostingWithQualificationsNotin(AWSMockServiceTestCase):
super(TestMTurkPostingWithQualificationsNotin,
test_locale_qualification_notin(self):
test_locale_qualification_in_notin(self):
'QualificationRequirement.2.LocaleValue.2.Country':
TestMTurkPostingWithQualificationsDoesnotexist(AWSMockServiceTestCase):
super(TestMTurkPostingWithQualificationsDoesnotexist,
test_qualification_doesnotexist(self):
comparator='DoesNotExist')
'DoesNotExist',
TestMTurkPostingWithQualificationsExists(AWSMockServiceTestCase):
super(TestMTurkPostingWithQualificationsExists,
test_qualification_exists(self):
comparator='Exists')
'Exists',
QUAL_WITH_SCORE_ID
TestMTurkPostingWithQualQualtypewithscoreIn(AWSMockServiceTestCase):
super(TestMTurkPostingWithQualQualtypewithscoreIn,
test_qualification_qualtypewithscore_in(self):
qualification_type_id=QUAL_WITH_SCORE_ID,
integer_value=[100,
80])
'333333333333333333333333333333',
'QualificationRequirement.1.IntegerValue.1':
'QualificationRequirement.1.IntegerValue.2':
'QualificationRequirement.1.IntegerValue.3':
80},
MWSConnection,
api_call_map,
destructure_object
(ResponseElement,
GetFeedSubmissionListResult,
ResponseFactory)
TestMWSConnection(AWSMockServiceTestCase):
default_body_error(self):
test_destructure_object(self):
ResponseElement()
response.C
'four'
response.D
'five'
('A',
'B'),
['B',
'A'],
set(['C']),
'C':
[{'D':
'E':
{'F':
'G':
'seven'}]},
'B'},
'String'},
{'Prefix.C':
'Prefix.D':
'Prefix.C.member.1.D':
'Prefix.C.member.1.E':
'Prefix.C.member.2.F':
'Prefix.C.member.2.G':
'seven'}
amazon
zip(inputs,
outputs):
inputs[-1]
destructure_object(user,
prefix='Prefix',
amazon)
test_decorator_order(self):
api_call_map.items():
getattr(self.service_connection,
func)
[func.__name__]
func:
'__closure__'):
len(func.__closure__):
func.__closure__[i].cell_contents
'requires'
value.__name__:
decs[-1]
'requires')
decs.append(value.__name__)
test_built_api_call_map(self):
self.assertTrue(len(api_call_map.keys())
test_method_for(self):
self.assertTrue('GetFeedSubmissionList'
api_call_map)
self.service_connection.method_for('GetFeedSubmissionList')
self.assertTrue(callable(func))
ideal
self.service_connection.get_feed_submission_list
ideal)
self.service_connection.method_for('NotHereNorThere')
test_response_factory(self):
'GetFeedSubmissionList'
GetFeedSubmissionListResult)
MyResult(GetFeedSubmissionListResult):
_hello
'_world'
{'GetFeedSubmissionListResult':
MyResult}
connection._setup_factories([scope])
MyResult)
self.assertEqual(response._result._hello,
'_world')
self.assertEqual(response._result.HasNext,
test_get_service_status(self):
self.service_connection.get_service_status()
self.assertTrue('products'
self.assertTrue('inventory'
self.assertTrue('feeds'
test_post_request(self):
MagicMock(
side_effect=
BotoServerError(500,
bee
throttled',
body=self.default_body_error()))
self.assertRaises(BotoServerError)
self.service_connection.get_lowest_offer_listings_for_asin(
MarketplaceId='12345',
ASINList='ASIN12345',
condition='Any',
SellerId='1234',
excludeme='True')
self.assertTrue('throttled'
str(err.reason))
self.assertEqual(int(err.status),
test_sandboxify(self):
MWSConnection(https_connection_factory=self.https_connection_factory,
sandbox=True)
self.assertEqual(conn._sandboxify('a/bogus/path'),
'a/bogus_Sandbox/path')
(ResponseFactory,
ResponseElement,
Element,
MemberList,
ElementList,
SimpleList)
TestMWSResponse(AWSMockServiceTestCase):
test_parsing_nested_elements(self):
Test9one(ResponseElement):
Nest
Zoom
Test9Result(ResponseElement):
Element(Test9one)
self.check_issue(Test9Result,
Item.Nest.__dict__.items()))
self.assertEqual(nest,
dict(Zip='Zap',
Zam='Zoo'))
x[0]
'Nest'
Item.__dict__.items()))
self.assertEqual(item,
dict(Foo='Bar',
Bif='Bam',
Zoom=None))
test_parsing_member_list_specification(self):
Test8extra(ResponseElement):
Test8Result(ResponseElement):
Extra
MemberList(Test8extra)
self.check_issue(Test8Result,
obj._result.Item)),
list(range(4)),
x.Foo)),
obj._result.Extra)),
[[4,
5],
[6,
7]],
test_parsing_nested_lists(self):
Test7Result(ResponseElement):
MemberList(Nest=MemberList(),
List=ElementList(Simple=SimpleList()))
self.check_issue(Test7Result,
self.assertEqual(len(item),
nests
[z.Nest
x.Nest,
item)]
[[y.Data
nest]
nests],
[[u'2',
[u'1',
u'5']],
[element.Simple
item[1].List],
[[u'4',
u'5',
[u'7',
u'8',
u'9']],
item[-1].List[0].Simple,
'3'],
self.assertEqual(item[-1].List[1].Simple,
test_parsing_member_list(self):
Test6Result(ResponseElement):
self.check_issue(Test6Result,
self.assertTrue(obj._result.Item[1].Error
'Four')
obj._result.Item[2].Error
test_parsing_empty_member_list(self):
Test5Result(ResponseElement):
MemberList(Nest=MemberList())
self.check_issue(Test5Result,
test_parsing_missing_member_list(self):
Test4Result(ResponseElement):
MemberList(NestedItem=MemberList())
self.check_issue(Test4Result,
test_parsing_element_lists(self):
Test1Result(ResponseElement):
self.check_issue(Test1Result,
self.assertTrue(len(obj._result.Item)
'Zip',
'?'))
list(map(elements,
obj._result.Item))
self.assertSequenceEqual(elements,
test_parsing_missing_lists(self):
Test2Result(ResponseElement):
self.check_issue(Test2Result,
self.assertEqual(obj._result.Item,
test_parsing_simple_lists(self):
Test3Result(ResponseElement):
self.check_issue(Test3Result,
check_issue(self,
klass,
klass.__name__[:-len('Result')]
ResponseFactory(scopes=[{klass.__name__:
klass}])
factory(action,
connection=self.service_connection)
self.service_connection._parse_response(parser,
u'iam_secret_key',
TestProvider(unittest.TestCase):
self.metadata_patch
mock.patch('boto.utils.get_instance_metadata')
self.config_object_patch
self.get_shared_config)
self.has_config_object_patch
'has_option',
self.has_shared_config)
self.get_instance_metadata
self.metadata_patch.start()
self.config_object_patch.start()
self.has_config_object_patch.start()
self.metadata_patch.stop()
self.config_object_patch.stop()
self.has_config_object_patch.stop()
has_shared_config(self,
get_shared_config(self,
test_passed_in_values_are_used(self):
'secret_key',
test_environment_variables_are_used(self):
test_environment_variable_aws_security_token(self):
test_no_credentials_provided(self):
provider.Provider(
'aws',
provider.NO_CREDENTIALS_PROVIDED
test_config_profile_values_are_used(self):
dev':
'dev_access_key',
'dev_secret_key',
prod':
prod_withtoken':
'prod_token',
profile_name='prod')
profile_name='prod_withtoken')
'prod_token')
profile_name='dev')
self.assertEqual(q.access_key,
'dev_access_key')
self.assertEqual(q.secret_key,
'dev_secret_key')
test_config_missing_profile(self):
self.assertRaises(provider.ProfileNotFoundError):
profile_name='doesntexist')
test_config_values_are_used(self):
test_config_value_security_token_is_used(self):
test_keyring_is_used(self):
'keyring':
type(mock)('keyring',
mock.patch('keyring.get_password',
keyring.get_password.side_effect
kr,
login:
kr+login+'pw')
'testcfg_access_keypw')
imported:
test_passed_in_values_beat_env_vars(self):
test_env_vars_beat_shared_creds_values(self):
test_shared_creds_beat_config_values(self):
test_shared_creds_profile_beats_defaults(self):
'foo_access_key',
'foo_secret_key',
profile_name='foo')
'foo_access_key')
'foo_secret_key')
test_env_profile_loads_profile(self):
self.environ['AWS_PROFILE']
'shared_access_key_foo',
'shared_secret_key_foo',
foo':
'cfg_access_key_foo',
'cfg_secret_key_foo',
'shared_access_key_foo')
'shared_secret_key_foo')
'cfg_access_key_foo')
'cfg_secret_key_foo')
test_env_vars_security_token_beats_config_values(self):
'shared_security_token',
'shared_security_token')
test_metadata_server_credentials(self):
self.get_instance_metadata.call_args[1]['data'],
'meta-data/iam/security-credentials/')
test_metadata_server_returns_bad_type(self):
test_metadata_server_returns_empty_string(self):
test_metadata_server_returns_missing_keys(self):
test_refresh_credentials(self):
first_expiration
timedelta(seconds=10)).strftime(
u'first_access_key',
first_expiration,
u'first_secret_key',
u'first_token',
{'allowall':
credentials}
'first_access_key')
'first_secret_key')
'first_token')
self.assertIsNotNone(p._credential_expiry_time)
timedelta(seconds=20)
p._credential_expiry_time
credentials['AccessKeyId']
'second_access_key'
credentials['SecretAccessKey']
'second_secret_key'
credentials['Token']
'second_token'
'second_access_key')
'second_secret_key')
'second_token')
@mock.patch('boto.provider.config.getint')
@mock.patch('boto.provider.config.getfloat')
test_metadata_config_params(self,
config_float,
config_int):
config_int.return_value
config_float.return_value
4.0
self.get_instance_metadata.assert_called_with(
timeout=4.0,
test_provider_google(self):
self.environ['GS_ACCESS_KEY_ID']
self.environ['GS_SECRET_ACCESS_KEY']
@mock.patch('os.path.isfile',
@mock.patch.object(provider.Config,
'load_from_path')
test_shared_config_loading(self,
load_from_path,
exists):
'.aws',
exists.reset_mock()
load_from_path.reset_mock()
'.google',
boto.pyami
TestCanLoadConfigFile(unittest.TestCase):
self.file_contents
file_contents
StringIO(
'[Boto]\n'
'https_validate_certificates
true\n'
'other
false\n'
'http_socket_timeout
1\n'
'[Credentials]\n'
'aws_access_key_id=foo\n'
'aws_secret_access_key=bar\n'
config.Config(fp=file_contents)
test_can_get_bool(self):
self.config.getbool('Boto',
'https_validate_certificates'))
'other'))
'does-not-exist'))
test_can_get_int(self):
'http_socket_timeout'),
'does-not-exist'),
self.config.getint('Boto',
'does-not-exist',
default=20),
test_can_get_strings(self):
'aws_access_key_id'),
self.assertIsNone(
'no-exist'))
'no-exist',
'default-value'),
'default-value')
saxutils
TestRDSConnection(AWSMockServiceTestCase):
super(TestRDSConnection,
test_get_all_db_instances(self):
self.service_connection.get_all_dbinstances('instance_id')
self.assertEqual(db.create_time,
db.endpoint,
(u'mydbinstance2.c0hjqouvn9mf.us-west-2.rds.amazonaws.com',
3306))
self.assertEqual(db.availability_zone,
self.assertEqual(db.preferred_backup_window,
self.assertEqual(db.preferred_maintenance_window,
self.assertEqual(db.latest_restorable_time,
self.assertEqual(db.iops,
'default.mysql5.5')
self.assertEqual(db.security_group.owner_id,
self.assertEqual(db.security_group.name,
self.assertEqual(db.security_group.description,
self.assertEqual(db.security_group.ec2_groups,
self.assertEqual(db.security_group.ip_ranges,
self.assertEqual(len(db.status_infos),
self.assertEqual(db.status_infos[0].message,
self.assertEqual(db.status_infos[0].normal,
self.assertEqual(db.status_infos[0].status,
'replicating')
self.assertEqual(db.status_infos[0].status_type,
replication')
self.assertEqual(db.vpc_security_groups[0].status,
self.assertEqual(db.vpc_security_groups[0].vpc_group,
'sg-1')
self.assertEqual(db.license_model,
self.assertEqual(db.engine_version,
self.assertEqual(db.auto_minor_version_upgrade,
self.assertEqual(db.subnet_group.name,
'mydbsubnetgroup')
TestRDSCCreateDBInstance(AWSMockServiceTestCase):
super(TestRDSCCreateDBInstance,
test_create_db_instance_param_group_name(self):
backup_retention_period=0)
3306
test_create_db_instance_param_group_instance(self):
param_group
ParameterGroup()
param_group.name
'default.mysql5.1'
param_group=param_group,
db_subnet_group_name='dbSubnetgroup01')
TestRDSConnectionRestoreDBInstanceFromPointInTime(AWSMockServiceTestCase):
super(TestRDSConnectionRestoreDBInstanceFromPointInTime,
test_restore_dbinstance_from_point_in_time(self):
'restored-db')
test_restore_dbinstance_from_point_in_time__db_subnet_group_name(self):
db_subnet_group_name='dbsubnetgroup')
'dbsubnetgroup',
test_create_db_instance_vpc_sg_str(self):
'sg-1'),
'sg-2')]
test_create_db_instance_vpc_sg_obj(self):
sg1
SecurityGroup(name='sg-1')
sg2
SecurityGroup(name='sg-2')
sg1.name),
sg2.name)]
TestRDSOptionGroups(AWSMockServiceTestCase):
super(TestRDSOptionGroups,
test_describe_option_groups(self):
self.service_connection.describe_option_groups()
'myoptiongroup')
response[1]
'default:oracle-se1-11-2')
'Default
Option
Group.')
TestRDSLogFile(AWSMockServiceTestCase):
super(TestRDSLogFile,
test_get_all_logs_simple(self):
self.service_connection.get_all_logs('db1')
test_get_all_logs_filtered(self):
self.service_connection.get_all_logs('db_instance_1',
marker='error/mysql-error.log',
file_size=2000000,
filename_contains='error',
file_last_written=12345678)
'db_instance_1',
'error/mysql-error.log',
'FileSize':
2000000,
'FilenameContains':
'FileLastWritten':
12345678,
TestRDSLogFileDownload(AWSMockServiceTestCase):
logfile_sample
super(TestRDSLogFileDownload,
self.logfile_sample
test_single_download(self):
self.assertEqual(response.marker,
'0:4485')
self.assertEqual(response.dbinstance_id,
'db1')
self.assertEqual(response.log_filename,
self.assertEqual(response.data,
saxutils.unescape(self.logfile_sample))
test_multi_args(self):
marker='0:4485',
number_of_lines=10)
'0:4485',
'NumberOfLines':
TestRDSOptionGroupOptions(AWSMockServiceTestCase):
super(TestRDSOptionGroupOptions,
test_describe_option_group_options(self):
self.service_connection.describe_option_group_options()
'OEM')
'Oracle
Enterprise
Manager')
self.assertEqual(options.min_minor_engine_version,
'0.2.v3')
self.assertEqual(options.port_required,
self.assertEqual(options.default_port,
1158)
self.assertEqual(options.permanent,
self.assertEqual(options.persistent,
self.assertEqual(options.depends_on,
TestDescribeDBSnapshots(AWSMockServiceTestCase):
test_describe_dbinstances_by_instance(self):
self.service_connection.get_all_dbsnapshots(instance_id='simcoprod01')
'DescribeDBSnapshots',
self.assertEqual(response[0].instance_id,
self.assertEqual(response[0].engine_version,
'5.1.50')
self.assertEqual(response[0].license_model,
self.assertEqual(response[0].iops,
self.assertEqual(response[0].option_group_name,
'myoptiongroupname')
self.assertEqual(response[0].percent_progress,
self.assertEqual(response[0].snapshot_type,
self.assertEqual(response[0].source_region,
'eu-west-1')
self.assertEqual(response[0].vpc_id,
'myvpc')
TestCreateDBSnapshot(AWSMockServiceTestCase):
test_create_dbinstance(self):
self.service_connection.create_dbsnapshot('mydbsnapshot',
'CreateDBSnapshot',
self.assertEqual(response.instance_id,
TestCopyDBSnapshot(AWSMockServiceTestCase):
test_copy_dbinstance(self):
self.service_connection.copy_dbsnapshot('myautomaticdbsnapshot',
'CopyDBSnapshot',
'myautomaticdbsnapshot',
'mycopieddbsnapshot'
TestDeleteDBSnapshot(AWSMockServiceTestCase):
test_delete_dbinstance(self):
self.service_connection.delete_dbsnapshot('mysnapshot2')
'DeleteDBSnapshot',
'mysnapshot2'
'mysnapshot2')
'deleted')class
TestRestoreDBInstanceFromDBSnapshot(AWSMockServiceTestCase):
test_restore_dbinstance_from_dbsnapshot(self):
self.service_connection.restore_dbinstance_from_dbsnapshot('mydbsnapshot',
'RestoreDBInstanceFromDBSnapshot',
'myrestoreddbinstance')
self.assertEqual(response.instance_class,
self.assertEqual(response.multi_az,
TestRDS2Connection(AWSMockServiceTestCase):
super(TestRDS2Connection,
test_describe_db_instances(self):
self.service_connection.describe_db_instances('instance_id')
response['DescribeDBInstancesResponse']\
['DescribeDBInstancesResult']['DBInstances'][0]\
['DBInstance']
self.assertEqual(db['DBInstanceIdentifier'],
self.assertEqual(db['InstanceCreateTime'],
self.assertEqual(db['Engine'],
self.assertEqual(db['DBInstanceStatus'],
self.assertEqual(db['AllocatedStorage'],
self.assertEqual(db['Endpoint']['Port'],
3306)
self.assertEqual(db['DBInstanceClass'],
self.assertEqual(db['MasterUsername'],
self.assertEqual(db['AvailabilityZone'],
self.assertEqual(db['BackupRetentionPeriod'],
self.assertEqual(db['PreferredBackupWindow'],
self.assertEqual(db['PreferredMaintenanceWindow'],
self.assertEqual(db['MultiAZ'],
self.assertEqual(db['Iops'],
self.assertEqual(db['PendingModifiedValues'],
db['DBParameterGroups'][0]['DBParameterGroup']\
['DBParameterGroupName'],
'default.mysql5.5'
db['DBSecurityGroups'][0]['DBSecurityGroup']['DBSecurityGroupName'],
db['DBSecurityGroups'][0]['DBSecurityGroup']['Status'],
self.assertEqual(len(db['StatusInfos']),
db['StatusInfos'][0]['DBInstanceStatusInfo']['Message'],
db['StatusInfos'][0]['DBInstanceStatusInfo']['Normal'],
db['StatusInfos'][0]['DBInstanceStatusInfo']['Status'],
'replicating'
db['StatusInfos'][0]['DBInstanceStatusInfo']['StatusType'],
replication'
db['VpcSecurityGroups'][0]['VpcSecurityGroupMembership']['Status'],
db['VpcSecurityGroups'][0]['VpcSecurityGroupMembership']\
['VpcSecurityGroupId'],
'sg-1'
self.assertEqual(db['LicenseModel'],
self.assertEqual(db['EngineVersion'],
self.assertEqual(db['AutoMinorVersionUpgrade'],
db['DBSubnetGroup']['DBSubnetGroupName'],
'mydbsubnetgroup'
ResourceRecordSets,
Record
TestRoute53Connection(AWSMockServiceTestCase):
super(TestRoute53Connection,
self.calls
test_typical_400(self):
'AccessDenied'],
test_retryable_400_prior_request_not_complete(self):
test_retryable_400_throttling(self):
do_retry_handler(self,
incr_retry_handler(func):
self.calls['count']
incr_retry_handler(
test_private_zone_invalid_vpc_400(self):
'InvalidVPCId'],
private_zone=True)
TestCreateZoneRoute53(AWSMockServiceTestCase):
super(TestCreateZoneRoute53,
test_create_zone(self):
self.service_connection.create_zone("example.com.")
"Z11111")
test_create_hosted_zone(self):
"my_ref",
comment")
['ns-100.awsdns-01.com',
'ns-1000.awsdns-01.co.uk',
'ns-1000.awsdns-01.org',
'ns-900.awsdns-01.net'])
['HostedZone']['Config']['PrivateZone'],
u'false')
TestCreatePrivateZoneRoute53(AWSMockServiceTestCase):
super(TestCreatePrivateZoneRoute53,
vpc_id='vpc-1a2b3c4d',
vpc_region='us-east-1'
['Config']['PrivateZone'],
u'true')
['VPC']['VPCId'],
u'vpc-1a2b3c4d')
['VPC']['VPCRegion'],
u'us-east-1')
TestGetZoneRoute53(AWSMockServiceTestCase):
super(TestGetZoneRoute53,
['example2.com.',
'example1.com.',
'example.com.']
print(response['ListHostedZonesResponse']['HostedZones'][0])
response['ListHostedZonesResponse']['HostedZones']:
print("Removing:
d['Name'])
domains.remove(d['Name'])
self.assertEqual(domains,
test_get_zone(self):
self.service_connection.get_zone('example.com.')
TestGetHostedZoneRoute53(AWSMockServiceTestCase):
super(TestGetHostedZoneRoute53,
self.service_connection.get_hosted_zone("Z1111")
['HostedZone']['Id'],
'/hostedzone/Z1111')
['HostedZone']['Name'],
'example.com.')
['ns-1000.awsdns-40.org',
'ns-200.awsdns-30.com',
'ns-900.awsdns-50.net',
'ns-1000.awsdns-00.co.uk'])
TestGetAllRRSetsRoute53(AWSMockServiceTestCase):
super(TestGetAllRRSetsRoute53,
self.assertIn(self.actual_request.path,
("/2013-04-01/hostedzone/Z1111/rrset?type=A&name=example.com.",
"/2013-04-01/hostedzone/Z1111/rrset?name=example.com.&type=A"))
ResourceRecordSets))
self.assertEqual(response.hosted_zone_id,
"Z1111")
Record))
self.assertTrue(response[0].name,
"test.example.com.")
self.assertTrue(response[0].ttl,
"60")
self.assertTrue(response[0].type,
evaluate_record
response[2]
self.assertEqual(evaluate_record.name,
'us-west-2-evaluate-health.example.com.')
self.assertEqual(evaluate_record.type,
self.assertEqual(evaluate_record.identifier,
'latency-example-us-west-2-evaluate-health')
self.assertEqual(evaluate_record.region,
self.assertEqual(evaluate_record.alias_hosted_zone_id,
'ABCDEFG123456')
self.assertTrue(evaluate_record.alias_evaluate_target_health)
self.assertEqual(evaluate_record.alias_dns_name,
'example-123456-evaluate-health.us-west-2.elb.amazonaws.com.')
evaluate_xml
evaluate_record.to_xml()
self.assertTrue(evaluate_record.health_check,
self.assertTrue('<EvaluateTargetHealth>true</EvaluateTargetHealth>'
evaluate_xml)
no_evaluate_record
response[3]
self.assertEqual(no_evaluate_record.name,
'us-west-2-no-evaluate-health.example.com.')
self.assertEqual(no_evaluate_record.type,
self.assertEqual(no_evaluate_record.identifier,
'latency-example-us-west-2-no-evaluate-health')
self.assertEqual(no_evaluate_record.region,
self.assertEqual(no_evaluate_record.alias_hosted_zone_id,
'ABCDEFG567890')
self.assertFalse(no_evaluate_record.alias_evaluate_target_health)
self.assertEqual(no_evaluate_record.alias_dns_name,
'example-123456-no-evaluate-health.us-west-2.elb.amazonaws.com.')
no_evaluate_xml
no_evaluate_record.to_xml()
self.assertTrue(no_evaluate_record.health_check,
self.assertTrue('<EvaluateTargetHealth>false</EvaluateTargetHealth>'
no_evaluate_xml)
failover_record
response[4]
self.assertEqual(failover_record.name,
'failover.example.com.')
self.assertEqual(failover_record.type,
self.assertEqual(failover_record.identifier,
'failover-primary')
self.assertEqual(failover_record.failover,
'PRIMARY')
self.assertEqual(failover_record.ttl,
'60')
healthcheck_record
response[5]
self.assertEqual(healthcheck_record.health_check,
'076a32f8-86f7-4c9e-9fa2-c163d5be67d9')
self.assertEqual(healthcheck_record.name,
'us-west-2-evaluate-health-healthcheck.example.com.')
self.assertEqual(healthcheck_record.identifier,
'latency-example-us-west-2-evaluate-health-healthcheck')
self.assertEqual(healthcheck_record.alias_dns_name,
'example-123456-evaluate-health-healthcheck.us-west-2.elb.amazonaws.com.')
TestTruncatedGetAllRRSetsRoute53(AWSMockServiceTestCase):
super(TestTruncatedGetAllRRSetsRoute53,
paged_body(self):
maxitems=3)
self.assertEqual(self.actual_request.path,
'/2013-04-01/hostedzone/Z1111/rrset?maxitems=3')
body=self.paged_body())
self.assertEqual(len(list(response)),
url_parts
urllib.parse.urlparse(self.actual_request.path)
self.assertEqual(url_parts.path,
'/2013-04-01/hostedzone/Z1111/rrset')
self.assertEqual(urllib.parse.parse_qs(url_parts.query),
dict(type=['A'],
name=['wrr.example.com.'],
identifier=['secondary']))
TestCreateHealthCheckRoute53IpAddress(AWSMockServiceTestCase):
super(TestCreateHealthCheckRoute53IpAddress,
test_create_health_check_ip_address(self):
HealthCheck(ip_addr='74.125.228.81',
hc_type='HTTPS_STR_MATCH',
string_match='OK')
self.assertFalse('<FullyQualifiedDomainName>'
self.assertTrue('<IPAddress>'
self.assertEqual(hc_resp['IPAddress'],
'74.125.228.81')
self.assertEqual(hc_resp['SearchString'],
'OK')
'34778cf8-e31e-4974-bad0-b108bd1623d3')
TestGetCheckerIpRanges(AWSMockServiceTestCase):
test_get_checker_ip_ranges(self):
self.service_connection.get_checker_ip_ranges()
ip_ranges
response['GetCheckerIpRangesResponse']['CheckerIpRanges']
self.assertEqual(len(ip_ranges),
self.assertIn('54.183.255.128/26',
self.assertIn('54.228.16.0/26',
self.assertIn('54.232.40.64/26',
self.assertIn('177.71.207.128/26',
self.assertIn('176.34.159.192/26',
TestCreateHealthCheckRoute53FQDN(AWSMockServiceTestCase):
super(TestCreateHealthCheckRoute53FQDN,
test_create_health_check_fqdn(self):
HealthCheck(ip_addr='',
hc_type='HTTPS',
fqdn='example.com')
self.assertTrue('<FullyQualifiedDomainName>'
self.assertFalse('<IPAddress>'
self.assertEqual(hc_resp['FullyQualifiedDomainName'],
'f9abfe10-8d2a-4bbd-8f35-796f0f8572f2')
TestChangeResourceRecordSetsRoute53(AWSMockServiceTestCase):
super(TestChangeResourceRecordSetsRoute53,
test_record_commit(self):
rrsets
ResourceRecordSets(self.service_connection)
Record('vanilla.example.com',
['1.2.3.4']))
Record('alias.example.com',
alias_hosted_zone_id='Z123OTHER',
alias_dns_name='target.other',
alias_evaluate_target_health=True))
Record('wrr.example.com',
['cname.target'],
weight=10,
identifier='weight-1'))
Record('lbr.example.com',
['text
record'],
region='us-west-2',
identifier='region-1'))
Record('failover.example.com',
['2.2.2.2'],
health_check='hc-1234',
failover='PRIMARY',
identifier='primary'))
changes_xml
rrsets.to_xml()
actual_xml
xml.dom.minidom.parseString(changes_xml).toprettyxml())
xml.dom.minidom.parseString(b).toprettyxml())
self.assertEqual(actual_xml,
expected_xml)
TestZone(unittest.TestCase):
test_find_records(self):
Zone(mock_connection,
zone.id
rr_names
['amazon.com',
'aws.amazon.com',
'aws.amazon.com']
rr_names:
mock_rr
mock_rr.name
mock_rr.type
'A'
mock_rr.weight
mock_rr.region
mock_rrs.append(mock_rr)
mock_rrs[3]
mock_connection.get_all_rrsets.return_value
mock_connection._make_qualified.return_value
'amazon.com'
result_rrs
zone.find_records('amazon.com',
self.fail("find_records()
iterated
far
resource"
list.")
self.assertEqual(result_rrs,
[mock_rrs[0],
mock_rrs[1]])
patch
Location,
TestS3Bucket(AWSMockServiceTestCase):
super(TestS3Bucket,
test_bucket_create_bucket(self):
self.service_connection.create_bucket('mybucket_create')
'mybucket_create')
test_bucket_create_eu_central_1_location(self):
self.service_connection.create_bucket(
'eu_central_1_bucket',
location=Location.EUCentral1
'eu_central_1_bucket')
test_bucket_constructor(self):
test_bucket_basics(self):
self.assertEqual(bucket.__repr__(),
mybucket>')
test_bucket_new_key(self):
self.assertEqual(key.bucket,
self.assertEqual(key.key,
test_bucket_new_key_missing_name(self):
bucket.new_key('')
test_bucket_delete_key_missing_name(self):
bucket.delete_key('')
test_bucket_kwargs_misspelling(self):
bucket.get_all_keys(delimeter='foo')
test__get_all_query_args(self):
bukket
Bucket()
bukket._get_all_query_args({})
bukket._get_all_query_args({},
'foo=true')
'initial=1&foo=true')
multiple_params
'bar':
'☃',
u'χ',
'some_other':
'thing',
'notthere':
'notpresenteither':
bukket._get_all_query_args(multiple_params)
'bar=%E2%98%83&baz=%CF%87&foo=true&max-keys=0&some-other=thing'
bukket._get_all_query_args(multiple_params,
'initial=1&bar=%E2%98%83&baz=%CF%87&foo=true&max-keys=0&some-other=thing'
@patch.object(S3Connection,
'head_bucket')
test_bucket_copy_key_no_validate(self,
mock_head_bucket):
self.service_connection.get_bucket('mybucket',
self.assertTrue(mock_head_bucket.called)
mock_head_bucket.reset_mock()
bucket.copy_key('newkey',
'srcbucket',
'srckey',
preserve_acl=True)
'_get_all')
test_bucket_encoding(self,
mock_get_all):
bucket.get_all_keys(encoding_type='url')
bucket.get_all_versions(encoding_type='url')
('Version',
DeleteMarker),
bucket.get_all_multipart_uploads(encoding_type='url')
('Upload',
'get_all_keys')
'_get_key_internal')
test_bucket_get_key_no_validate(self,
mock_gki,
mock_gak):
bucket.get_key('mykey',
self.assertEqual(len(mock_gki.mock_calls),
self.assertTrue(isinstance(key,
Key))
self.assertEqual(key.name,
bucket.get_key(
version_id='something',
validate=False
acl_policy(self):
test_bucket_acl_policy_namespace(self):
body=self.acl_policy())
xml_policy
policy.to_xml()
xml.dom.minidom.parseString(xml_policy)
document.documentElement.namespaceURI
self.assertEqual(namespace,
'http://s3.amazonaws.com/doc/2006-03-01/')
multipart_upload_lister
versioned_bucket_lister
S3BucketListResultSetTest
_test_patched_lister_encoding(self,
outer_method):
call_args
first.append('foo')
first.next_key_marker
'a+b'
first.is_truncated
second.append('bar')
second.is_truncated
[first,
second]
return_pages(**kwargs):
call_args.append(kwargs)
pages.pop(0)
setattr(bucket,
return_pages)
list(outer_method(bucket,
encoding_type='url'))
self.assertEqual(['foo',
'bar'],
self.assertEqual('a
b',
call_args[1]['key_marker'])
test_list_object_versions_with_url_encoding(self):
'get_all_versions',
versioned_bucket_lister)
test_list_multipart_upload_with_url_encoding(self):
'get_all_multipart_uploads',
multipart_upload_lister)
HostRequiredError
TestSignatureAlteration(AWSMockServiceTestCase):
TestPresigned(MockServiceWithConfigTestCase):
test_presign_respect_query_auth(self):
TestSigV4HostError(MockServiceWithConfigTestCase):
test_historical_behavior(self):
self.assertEqual(self.service_connection.host,
test_sigv4_opt_in(self):
self.assertRaises(HostRequiredError):
aws_secret_access_key='more'
conn.host,
's3.cn-north-1.amazonaws.com.cn'
TestSigV4Presigned(MockServiceWithConfigTestCase):
test_sigv4_presign(self):
iso_date='20140625T000000Z')
'a937f5fbc125d98ac8f04c49e0204ea1526a7b8ca058000a54c192457be05b7d',
test_sigv4_presign_respects_is_secure(self):
'https://examplebucket.s3.amazonaws.com/test.txt?'))
'http://examplebucket.s3.amazonaws.com/test.txt?'))
test_sigv4_presign_optional_params(self):
security_token='token',
version_id=2)
self.assertIn('VersionId=2',
self.assertIn('X-Amz-Security-Token=token',
test_sigv4_presign_respect_query_auth(self):
test_sigv4_presign_headers(self):
{'x-amz-meta-key':
self.assertIn('x-amz-meta-key',
test_sigv4_presign_response_headers(self):
filename="file.ext"'}
self.assertIn('response-content-disposition',
TestUnicodeCallingFormat(AWSMockServiceTestCase):
kwargs['calling_format']
u'boto.s3.connection.OrdinaryCallingFormat'
super(TestUnicodeCallingFormat,
test_unicode_calling_format(self):
self.service_connection.get_all_buckets()
TestHeadBucket(AWSMockServiceTestCase):
test_head_bucket_success(self):
buck
self.service_connection.head_bucket('my-test-bucket')
self.assertTrue(isinstance(buck,
Bucket))
self.assertEqual(buck.name,
'my-test-bucket')
test_head_bucket_forbidden(self):
self.set_http_response(status_code=403)
self.service_connection.head_bucket('cant-touch-this')
'AccessDenied')
Denied')
test_head_bucket_notfound(self):
self.set_http_response(status_code=404)
self.service_connection.head_bucket('totally-doesnt-exist')
'NoSuchBucket')
exist')
test_head_bucket_other(self):
self.set_http_response(status_code=405)
self.service_connection.head_bucket('you-broke-it')
405)
CORS_BODY_1
'<ID>foobar_rule</ID>'
CORS_BODY_2
CORS_BODY_3
TestCORSConfiguration(unittest.TestCase):
test_one_rule_with_id(self):
CORS_BODY_1)
test_two_rules(self):
expose_header='x-amz-server-side-encryption')
max_age_seconds=3000)
CORS_BODY_2)
'*')
CORS_BODY_3)
TestS3Key(AWSMockServiceTestCase):
super(TestS3Key,
"default
test_unicode_name(self):
u'Österreich'
print(repr(k))
test_when_no_restore_header_present(self):
self.assertIsNone(k.ongoing_restore)
test_restore_header_with_ongoing_restore(self):
'ongoing-request="true"')])
self.assertTrue(k.ongoing_restore)
test_restore_completed(self):
'ongoing-request="false",
'expiry-date="Fri,
GMT"')])
self.assertFalse(k.ongoing_restore)
self.assertEqual(k.expiry_date,
'Fri,
test_delete_key_return_key(self):
self.set_http_response(status_code=204,
b.delete_key('fookey')
self.assertIsNotNone(key)
test_storage_class(self):
sc_value
self.assertEqual(sc_value,
k.bucket.list.assert_called_with(k.name.encode('utf-8'))
k.bucket.list.reset_mock()
test_change_storage_class(self):
k.change_storage_class('REDUCED_REDUNDANCY')
test_change_storage_class_new_bucket(self):
k.copy.reset_mock()
k.change_storage_class('REDUCED_REDUNDANCY',
dst_bucket='yourbucket')
'yourbucket',
counter(fn):
fn(*args,
TestS3KeyRetries(AWSMockServiceTestCase):
test_500_retry(self,
self.set_http_response(status_code=500)
retry.')
test_400_timeout(self,
"<Error><Code>RequestTimeout</Code></Error>"
test_502_bad_gateway(self,
"<Error><Code>BadGateway</Code></Error>"
self.set_http_response(status_code=502,
test_504_gateway_timeout(self,
"<Error><Code>GatewayTimeout</Code></Error>"
self.set_http_response(status_code=504,
TestFileError(unittest.TestCase):
test_file_error(self):
key.get_contents_to_file
side_effect=CustomException('File
blew
up!'))
self.assertRaises(CustomException):
key.get_contents_to_filename('foo.txt')
KeyfileTest(unittest.TestCase):
service_connection
self.contents
'0123456789'
MockBucket(service_connection,
key.set_contents_from_string(self.contents)
self.keyfile
KeyFile(key)
testReadFull(self):
self.assertEqual(self.keyfile.read(len(self.contents)),
self.contents)
testReadPartial(self):
self.contents[:5])
testTell(self):
self.keyfile.read(4)
self.keyfile.read(6)
self.keyfile.tell()
'I/O
testSeek(self):
self.keyfile.seek(0)
self.keyfile.seek(5)
self.keyfile.seek(-5)
self.keyfile.read(10)
self.assertEqual(self.keyfile.read(20),
self.keyfile.seek(50)
testSeekEnd(self):
self.keyfile.seek(0,
self.keyfile.seek(-1,
'9')
self.keyfile.seek(-100,
testSeekCur(self):
self.contents[0])
self.keyfile.seek(1,
os.SEEK_CUR)
self.contents[2:6])
testSetEtag(self):
b'test'
self.keyfile.key.etag
Lifecycle,
TestS3LifeCycle(AWSMockServiceTestCase):
_get_bucket_lifecycle_config(self):
bucket.get_lifecycle_config()
test_lifecycle_response_contains_all_rules(self):
self.assertEqual(len(self._get_bucket_lifecycle_config()),
test_parse_lifecycle_id(self):
'rule-1')
test_parse_lifecycle_prefix(self):
'prefix/foo')
test_parse_lifecycle_no_prefix(self):
self.assertEquals(rule.prefix,
test_parse_lifecycle_enabled(self):
test_parse_lifecycle_disabled(self):
'Disabled')
test_parse_expiration_days(self):
365)
test_parse_expiration_date(self):
test_parse_expiration_not_required(self):
self.assertIsNone(rule.expiration)
test_parse_transition_days(self):
test_parse_transition_days_deprecated(self):
self._get_bucket_lifecycle_config()[0].transition
test_parse_transition_date(self):
test_parse_transition_date_deprecated(self):
test_parse_storage_class_standard_ia(self):
test_parse_storage_class_glacier(self):
test_parse_storage_class_deprecated(self):
test_parse_multiple_lifecycle_rules(self):
self._get_bucket_lifecycle_config()[2].transition
self.assertEqual(len(transition),
test_expiration_with_no_transition(self):
lifecycle.to_xml()
test_expiration_is_optional(self):
'<Transition><StorageClass>GLACIER</StorageClass><Days>30</Days>',
test_transition_is_optional(self):
'<Rule><ID>myid</ID><Prefix>prefix</Prefix><Status>Enabled</Status></Rule>',
test_expiration_and_transition(self):
Transition(date='2012-11-30T00:00:000Z',
expiration=30,
'<Transition><StorageClass>GLACIER</StorageClass>'
'<Date>2012-11-30T00:00:000Z</Date>',
TestS3Tagging(AWSMockServiceTestCase):
test_parse_tagging_response(self):
b.get_tags()
self.assertEqual(len(api_response[0]),
self.assertEqual(api_response[0][0].key,
'Project')
self.assertEqual(api_response[0][0].value,
'Project
One')
self.assertEqual(api_response[0][1].key,
'User')
self.assertEqual(api_response[0][1].value,
'jsmith')
test_tag_equality(self):
t2
t3
'baz')
t4
Tag('baz',
self.assertEqual(t1,
t2)
t3)
t4)
MockBucketStorageUri
UriTest(unittest.TestCase):
test_provider_uri(self):
test_bucket_uri_no_trailing_slash(self):
'%s://bucket'
self.assertEqual('%s/'
test_bucket_uri_with_trailing_slash(self):
'%s://bucket/'
test_non_versioned_object_uri(self):
'%s://bucket/obj/a/b'
test_versioned_gs_object_uri(self):
'gs://bucket/obj/a/b#1359908801674000'
self.assertEqual(1359908801674000,
test_versioned_gs_object_uri_with_legacy_generation_value(self):
'gs://bucket/obj/a/b#1'
test_roundtrip_versioned_gs_object_uri_parsed(self):
'gs://bucket/obj#1359908801674000'
roundtrip_uri
boto.storage_uri(uri.uri,
self.assertEqual(uri.uri,
roundtrip_uri.uri)
test_versioned_s3_object_uri(self):
's3://bucket/obj/a/b#eMuM0J15HkJ9QHlktfNP5MfA.oYR2q6S'
self.assertEqual('s3',
self.assertEqual('s3://bucket/obj/a/b',
self.assertEqual('eMuM0J15HkJ9QHlktfNP5MfA.oYR2q6S',
test_explicit_file_uri(self):
'file://%s'
test_implicit_file_uri(self):
tmp_dir,
test_gs_object_uri_contains_sharp_not_matching_version_syntax(self):
'gs://bucket/obj#13a990880167400'
self.assertEqual('gs://bucket/obj#13a990880167400',
self.assertEqual('obj#13a990880167400',
test_file_containing_colon(self):
'abc:def'
test_invalid_scheme(self):
'mars://bucket/object'
self.assertIn('Unrecognized
scheme',
WebsiteConfiguration
Condition
RoutingRule
Redirect
pretty_print_xml(text):
''.join(t.strip()
text.splitlines())
xml.dom.minidom.parseString(text)
x.toprettyxml()
TestS3WebsiteConfiguration(unittest.TestCase):
test_suffix_only(self):
WebsiteConfiguration(suffix='index.html')
'<IndexDocument><Suffix>index.html</Suffix></IndexDocument>',
test_suffix_and_error(self):
error_key='error.html')
'<ErrorDocument><Key>error.html</Key></ErrorDocument>',
test_redirect_all_request_to_with_just_host(self):
RedirectLocation(hostname='example.com')
'example.com</HostName></RedirectAllRequestsTo>'),
test_redirect_all_requests_with_protocol(self):
RedirectLocation(hostname='example.com',
protocol='https')
'example.com</HostName><Protocol>https</Protocol>'
'</RedirectAllRequestsTo>'),
test_routing_rules_key_prefix(self):
Condition(key_prefix='docs/')
Redirect(replace_key_prefix='documents/')
test_routing_rules_to_host_on_404(self):
test_key_prefix(self):
Condition(key_prefix="images/")
Redirect(replace_key='folderdeleted.html')
test_builders(self):
rules.to_xml()
rules2
RoutingRules().add_rule(
RoutingRule.when(http_error_code=404).then_redirect(
hostname='example.com',
replace_key_prefix='report-404/'))
xml2
rules2.to_xml()
self.assertEqual(x(xml),
x(xml2))
test_parse_xml(self):
xml_in
webconfig
WebsiteConfiguration()
handler.XmlHandler(webconfig,
xml.sax.parseString(xml_in.encode('utf-8'),
xml_out
webconfig.to_xml()
self.assertEqual(x(xml_in),
x(xml_out))
TestSESIdentity(AWSMockServiceTestCase):
super(TestSESIdentity,
test_ses_get_identity_dkim_list(self):
.get_identity_dkim_attributes(['test@amazon.com',
'secondtest@amazon.com'])
response['GetIdentityDkimAttributesResult']
first_entry
result['DkimAttributes'][0]
first_entry['key']
first_entry['value']
tokens
attributes['DkimTokens']
'test@amazon.com')
self.assertEqual(ListElement,
type(tokens))
len(tokens))
self.assertEqual('vvjuipp74whm76gqoni7qmwwn4w4qusjiainivf6f',
tokens[0])
self.assertEqual('3frqe7jn4obpuxjpwpolz6ipb3k5nvt2nhjpik2oy',
tokens[1])
self.assertEqual('wrqplteh7oodxnad7hsl4mixg2uavzneazxv5sxi2',
tokens[2])
second_entry
result['DkimAttributes'][1]
second_entry['key']
second_entry['value']
attributes['DkimEnabled']
dkim_verification_status
attributes['DkimVerificationStatus']
'secondtest@amazon.com')
self.assertEqual(dkim_enabled,
self.assertEqual(dkim_verification_status,
'NotStarted')
TestSESSetIdentityNotificationTopic(AWSMockServiceTestCase):
super(TestSESSetIdentityNotificationTopic,
test_ses_set_identity_notification_topic_bounce(self):
notification_type='Bounce',
test_ses_set_identity_notification_topic_complaint(self):
notification_type='Complaint',
TestSESSetIdentityFeedbackForwardingEnabled(AWSMockServiceTestCase):
super(TestSESSetIdentityFeedbackForwardingEnabled,
test_ses_set_identity_feedback_forwarding_enabled_true(self):
forwarding_enabled=True)
test_ses_set_identity_notification_topic_enabled_false(self):
forwarding_enabled=False)
u'Policy':
(u'{"Version":"2008-10-17","Id":"arn:aws:sqs:us-east-1:'
'idnum:testqueuepolicy/SQSDefaultPolicy","Statement":'
'[{"Sid":"sidnum","Effect":"Allow","Principal":{"AWS":"*"},'
'"Action":"SQS:GetQueueUrl","Resource":'
'"arn:aws:sqs:us-east-1:idnum:testqueuepolicy"}]}')}
TestSNSConnection(AWSMockServiceTestCase):
super(TestSNSConnection,
b"{}"
test_sqs_with_existing_policy(self):
self.assertEqual(actual_policy['Version'],
'2008-10-17')
self.assertEqual(actual_policy['Statement'][1]['Action'],
'SQS:SendMessage')
test_sqs_with_no_previous_policy(self):
test_publish_with_positional_args(self):
test_publish_with_kwargs(self):
test_publish_with_target_arn(self):
self.service_connection.publish(target_arn='target_arn',
test_create_platform_application(self):
self.service_connection.create_platform_application(
name='MyApp',
platform='APNS',
attributes={
'PlatformPrincipal':
'CreatePlatformApplication',
'MyApp',
'Platform':
'APNS',
test_set_platform_application_attributes(self):
self.service_connection.set_platform_application_attributes(
attributes={'PlatformPrincipal':
key'})
'SetPlatformApplicationAttributes',
test_create_platform_endpoint(self):
self.service_connection.create_platform_endpoint(
token='abcde12345',
custom_user_data='john',
attributes={'Enabled':
'CreatePlatformEndpoint',
'abcde12345',
'CustomUserData':
test_set_endpoint_attributes(self):
self.service_connection.set_endpoint_attributes(
endpoint_arn='arn:myendpoint',
attributes={'CustomUserData':
'SetEndpointAttributes',
'EndpointArn':
'arn:myendpoint',
'CustomUserData',
test_message_is_required(self):
test_publish_with_json(self):
target_arn='target_arn'
'Message'])
json.loads(self.actual_request.params["Message"]),
{"default":
"Ignored.",
"GCM":
here"}})
test_publish_with_utf8_message(self):
utf-8'.encode('utf-8')
test_publish_with_attributes(self):
sort_keys=True),
target_arn='target_arn',
'{"GCM":
here"},
"Ignored."}',
'MessageAttributes.entry.1.Name':
'MessageAttributes.entry.1.Value.DataType':
'MessageAttributes.entry.1.Value.StringValue':
'42',
'MessageAttributes.entry.2.Name':
'MessageAttributes.entry.2.Value.DataType':
'MessageAttributes.entry.2.Value.StringValue':
AWSMockServiceTestCase,
SQSAuthParams(AWSMockServiceTestCase):
super(SQSAuthParams,
test_auth_service_name_override(self):
'service_override'
self.assertIn('us-east-1/service_override/aws4_request',
test_class_attribute_can_set_service_name(self):
self.assertEqual(self.service_connection.AuthServiceName,
'sqs')
self.assertIn('us-east-1/sqs/aws4_request',
test_auth_region_name_is_automatically_updated(self):
self.assertIn('us-west-2/sqs/aws4_request',
test_set_get_auth_service_and_region_names(self):
'service_name'
self.service_connection.auth_region_name
'region_name'
self.assertEqual(self.service_connection.auth_service_name,
self.assertEqual(self.service_connection.auth_region_name,
test_get_queue_with_owner_account_id_returns_queue(self):
self.service_connection.get_queue('my_queue',
'QueueOwnerAWSAccountId'
self.actual_request.params.keys()
self.assertEquals(self.actual_request.params['QueueOwnerAWSAccountId'],
SQSProfileName(MockServiceWithConfigTestCase):
'prod'
super(SQSProfileName,
prod":
'secret_access',
test_profile_name_gets_passed(self):
profile_name=self.profile_name)
self.assertEquals(self.service_connection.profile_name,
self.profile_name)
SQSMessageAttributesParsing(AWSMockServiceTestCase):
test_message_attribute_response(self):
self.service_connection.receive_message(queue)[0]
self.assertEqual(message.id,
'7049431b-e5f6-430b-93c4-ded53864d02b')
self.assertEqual(message.md5,
'ce114e4501d2f4e2dcea3e17b546f339')
self.assertEqual(message.md5_message_attributes,
'324758f82d026ac6ec5b31a3b192d1e3')
mattributes
message.message_attributes
self.assertEqual(len(mattributes.keys()),
self.assertEqual(mattributes['Count']['data_type'],
'Number')
self.assertEqual(mattributes['Foo']['string_value'],
SQSSendMessageAttributes(AWSMockServiceTestCase):
self.service_connection.send_message(queue,
'SendMessage',
'MessageAttribute.1.Name':
'MessageAttribute.1.Value.DataType':
'MessageAttribute.1.Value.StringValue':
'MessageAttribute.2.Name':
'MessageAttribute.2.Value.DataType':
'MessageAttribute.2.Value.StringValue':
'MessageBody':
SQSSendBatchMessageAttributes(AWSMockServiceTestCase):
message1
'foo'}})
message2
{'name2':
'1'}})
self.service_connection.send_message_batch(queue,
(message1,
message2))
'SendMessageBatch',
'SendMessageBatchRequestEntry.1.DelaySeconds':
'SendMessageBatchRequestEntry.1.Id':
'SendMessageBatchRequestEntry.1.MessageAttribute.1.Name':
'SendMessageBatchRequestEntry.1.MessageAttribute.1.Value.DataType':
'SendMessageBatchRequestEntry.1.MessageAttribute.1.Value.StringValue':
'SendMessageBatchRequestEntry.1.MessageBody':
'SendMessageBatchRequestEntry.2.DelaySeconds':
'SendMessageBatchRequestEntry.2.Id':
'SendMessageBatchRequestEntry.2.MessageAttribute.1.Name':
'SendMessageBatchRequestEntry.2.MessageAttribute.1.Value.DataType':
'SendMessageBatchRequestEntry.2.MessageAttribute.1.Value.StringValue':
'SendMessageBatchRequestEntry.2.MessageBody':
TestMHMessage(unittest.TestCase):
MHMessage()
msg.update({'hello':
'world'})
self.assertTrue('hello'
DecodeExceptionRaisingMessage(RawMessage):
SQSDecodeError('Sample
TestEncodeMessage(unittest.TestCase):
test_message_id_available(self):
sample_value
'abcdef'
tuple([sample_value]
ResultSet([('Message',
DecodeExceptionRaisingMessage)])
XmlHandler(rs,
context.exception.message
self.assertEquals(message.id,
self.assertEquals(message.receipt_handle,
test_encode_bytes_message(self):
test_encode_string_message(self):
'aGVsbG8gd29ybGQ=')
test_s3url_parsing(self):
BigMessage()
msg._get_bucket_key('s3://foo')
msg._get_bucket_key('s3://foo/')
msg._get_bucket_key('s3://foo/bar')
msg._get_bucket_key('s3://foo/bar/fie/baz')
'bar/fie/baz')
msg._get_bucket_key('foo/bar')
TestQueue(unittest.TestCase):
test_queue_arn(self):
self.assertEqual(q.arn,
'arn:aws:sqs:us-east-1:id:queuename')
test_queue_name(self):
self.assertEqual(q.name,
'queuename')
TestSecurityToken(AWSMockServiceTestCase):
kwargs['security_token']
'token'
super(TestSecurityToken,
test_security_token(self):
self.assertEqual('token',
self.service_connection.provider.security_token)
TestSTSConnection(AWSMockServiceTestCase):
super(TestSTSConnection,
test_assume_role(self):
self.service_connection.assume_role('arn:role',
'mysession')
'mysession'},
test_assume_role_with_mfa(self):
self.service_connection.assume_role(
mfa_serial_number='GAHT12345678',
mfa_token='abc123'
'GAHT12345678',
'TokenCode':
'abc123'},
TestSTSWebIdentityConnection(AWSMockServiceTestCase):
super(TestSTSWebIdentityConnection,
self.service_connection.assume_role_with_web_identity(
'guestuser',
wit,
'www.amazon.com',
'AssumeRoleWithWebIdentity'
response.credentials.access_key.strip(),
'accesskey'
response.credentials.secret_key.strip(),
'secretkey'
response.credentials.session_token.strip(),
'AQoDYXdzEE0a8ANXXXXXXXXNO1ewxE5TijQyp+IPfnyowF'
response.user.arn.strip(),
'arn:aws:sts::000240903217:assumed-role/FederatedWebIdentityRole/app1'
response.user.assume_role_id.strip(),
'AROACLKWSDQRAOFQC3IDI:app1'
TestSTSSAMLConnection(AWSMockServiceTestCase):
super(TestSTSSAMLConnection,
test_assume_role_with_saml(self):
'arn:aws:iam::000240903217:role/Test'
principal
'arn:aws:iam::000240903217:role/Principal'
self.service_connection.assume_role_with_saml(
principal_arn=principal,
saml_assertion=assertion
principal,
assertion,
'AssumeRoleWithSAML'
STSCredentialsTest(unittest.TestCase):
super(STSCredentialsTest,
self.creds
self.assertEqual(self.creds.to_dict(),
creds.access_key
creds.secret_key
'crypto'
creds.session_token
creds.expiration
'way'
creds.request_id
'comes'
self.assertEqual(creds.to_dict(),
'something',
'way',
'comes',
'crypto',
TestDecisions(unittest.TestCase):
self.decisions
boto.swf.layer1_decisions.Layer1Decisions()
assert_data(self,
*data):
self.assertEquals(self.decisions._data,
list(data))
test_continue_as_new_workflow_execution(self):
self.decisions.continue_as_new_workflow_execution(
child_policy='TERMINATE',
execution_start_to_close_timeout='10',
input='input',
tag_list=['t1',
task_list='tasklist',
start_to_close_timeout='20',
workflow_type_version='v2'
self.assert_data({
'decisionType':
'ContinueAsNewWorkflowExecution',
'continueAsNewWorkflowExecutionDecisionAttributes':
'input',
['t1',
'tasklist'},
'20',
'workflowTypeVersion':
'v2',
Decider,
ActivityWorker
TestActors(unittest.TestCase):
self.worker
ActivityWorker(name='test-worker',
self.decider
Decider(name='test-worker',
self.worker._swf
self.decider._swf
test_decider_pass_tasktoken(self):
self.decider._swf.poll_for_decision_task.return_value
'events':
[{'eventId':
'WorkflowExecutionStarted',
'workflowExecutionStartedEventAttributes':
'parentInitiatedEventId':
'test_list'},
'v1'}}},
{'decisionTaskScheduledEventAttributes':
{'startToCloseTimeout':
'test_list'}},
'DecisionTaskScheduled'},
{'decisionTaskStartedEventAttributes':
{'scheduledEventId':
1379019495.585,
'DecisionTaskStarted'}],
'previousStartedEventId':
'my_specific_task_token',
'fwr243dsa324132jmflkfu0943tr09=',
'test_workflow_name-v1-1379019427'},
'v1'}}
self.decider.complete()
self.decider._swf.respond_decision_task_completed.assert_called_with('my_specific_task_token',
self.assertEqual('my_specific_task_token',
self.decider.last_tasktoken)
test_worker_pass_tasktoken(self):
'worker_task_token'
self.worker._swf.poll_for_activity_task.return_value
'activityId':
'SomeActivity-1379020713',
'SomeActivity',
'12T026NzGK5c4eMti06N9O3GHFuTDaNyA+8LFtoDkAwfE=',
'MyWorkflow-1.0-1379020705'}}
self.worker.cancel(details='Cancelling!')
self.worker.complete(result='Done!')
self.worker.fail(reason='Failure!')
self.worker.heartbeat()
self.worker._swf.respond_activity_task_canceled.assert_called_with(task_token,
'Cancelling!')
self.worker._swf.respond_activity_task_completed.assert_called_with(task_token,
'Done!')
self.worker._swf.respond_activity_task_failed.assert_called_with(task_token,
'Failure!')
self.worker._swf.record_activity_task_heartbeat.assert_called_with(task_token,
test_actor_poll_without_tasklist_override(self):
test_worker_override_tasklist(self):
self.worker.poll(task_list='some_other_tasklist')
test_decider_override_tasklist(self):
self.decider.poll(task_list='some_other_tasklist')
SWFBase
MOCK_DOMAIN
'Mock'
MOCK_ACCESS_KEY
MOCK_SECRET_KEY
MOCK_REGION
'Mock
Region'
TestBase(unittest.TestCase):
self.swf_base
SWFBase(
domain=MOCK_DOMAIN,
aws_access_key_id=MOCK_ACCESS_KEY,
aws_secret_access_key=MOCK_SECRET_KEY,
region=MOCK_REGION
test_instantiation(self):
self.assertEquals(MOCK_DOMAIN,
self.swf_base.domain)
self.assertEquals(MOCK_ACCESS_KEY,
self.swf_base.aws_access_key_id)
self.assertEquals(MOCK_SECRET_KEY,
self.swf_base.aws_secret_access_key)
self.assertEquals(MOCK_REGION,
self.swf_base.region)
boto.swf.layer2.Layer1.assert_called_with(
MOCK_ACCESS_KEY,
MOCK_SECRET_KEY,
region=MOCK_REGION)
TestDomain(unittest.TestCase):
Domain(name='test-domain',
description='My
domain')
self.domain.aws_access_key_id
self.domain.aws_secret_access_key
self.domain.region
'test-region'
test_domain_instantiation(self):
self.assertEquals('test-domain',
self.assertEquals('My
domain',
self.domain.description)
test_domain_list_activities(self):
self.domain._swf.list_activity_types.return_value
[{'activityType':
'DeleteLocalFile',
1332853651.235,
'DoUpdate',
'test'},
1333463734.528,
1332853651.18,
1332853651.264,
1332853651.314,
'1.1'},
1333373797.734,
'REGISTERED'}]}
('DeleteLocalFile',
'DoUpdate')
activity_types
self.domain.activities()
len(activity_types))
activity_type
activity_types:
self.assertIsInstance(activity_type,
ActivityType)
self.assertTrue(activity_type.name
activity_type.region)
test_domain_list_workflows(self):
self.domain._swf.list_workflow_types.return_value
[{'creationDate':
1332853651.136,
'Image
{'creationDate':
1333551719.89,
'v1'}}]}
('ProcessFile',
'test_workflow_name')
workflow_types
self.domain.workflows()
len(workflow_types))
workflow_type
workflow_types:
self.assertIsInstance(workflow_type,
WorkflowType)
self.assertTrue(workflow_type.name
workflow_type.aws_access_key_id)
workflow_type.aws_secret_access_key)
workflow_type.domain)
workflow_type.region)
test_domain_list_executions(self):
self.domain._swf.list_open_workflow_executions.return_value
'executionInfos':
[{'cancelRequested':
'12OeDTyoD27TDaafViz/QIlCHrYzspZmDgj0coIfjm868=',
'ProcessFile-1.0-1378933928'},
1378933928.676,
'12GwBkx4hH6t2yaIh8LYxy5HyCM6HcyhDKePJCg0/ciJk=',
'ProcessFile-1.0-1378933927'},
1378933927.919,
'12oRG3vEWrQ7oYBV+Bqi33Fht+ZRCYTt+tOdn5kLVcwKI=',
'ProcessFile-1.0-1378933926'},
1378933927.04,
'12qrdcpYmad2cjnqJcM4Njm3qrCGvmRFR1wwQEt+a2ako=',
'ProcessFile-1.0-1378933874'},
1378933874.956,
'1.0'}}]}
self.domain.executions()
self.assertEquals(4,
len(executions))
wf_execution
executions:
self.assertIsInstance(wf_execution,
wf_execution.aws_access_key_id)
wf_execution.aws_secret_access_key)
wf_execution.domain)
wf_execution.region)
TestTypes(unittest.TestCase):
test_workflow_type_register_defaults(self):
wf_type.register()
wf_type._swf.register_workflow_type.assert_called_with('test',
default_execution_start_to_close_timeout=ANY,
default_task_start_to_close_timeout=ANY,
default_child_policy=ANY
test_activity_type_register_defaults(self):
act_type
ActivityType(name='name',
act_type.register()
act_type._swf.register_activity_type.assert_called_with('test',
default_task_heartbeat_timeout=ANY,
default_task_schedule_to_close_timeout=ANY,
default_task_schedule_to_start_timeout=ANY,
default_task_start_to_close_timeout=ANY
test_workflow_type_start_execution(self):
'122aJcg6ic7MRAkjDRzLBsqU/R49qt5D0LPHycT/6ArN4='
wf_type._swf.start_workflow_execution.return_value
run_id}
execution
wf_type.start(task_list='hello_world')
self.assertIsInstance(execution,
self.assertEquals(wf_type.name,
execution.name)
self.assertEquals(wf_type.version,
execution.version)
self.assertEquals(run_id,
execution.runId)
_build_instance_metadata_url
retry_url
LazyLoadMetadata
_thread
@unittest.skip("http://bugs.python.org/issue7980")
TestThreadImport(unittest.TestCase):
test_strptime(self):
f():
13):
range(1,29):
boto.utils.parse_ts('2013-01-01T00:00:00Z')
_thread.start_new_thread(f,
TestPassword(unittest.TestCase):
clstest(self,
cls('foo')
self.assertNotEquals(password,
hashed
str(password)
cls(hashed)
self.assertNotEquals(password.str,
test_aaa_version_1_9_default_behavior(self):
self.clstest(Password)
test_custom_hashclass(self):
SHA224Password(Password):
hashlib.sha224
SHA224Password()
self.assertEquals(hashlib.sha224(b'foo').hexdigest(),
str(password))
test_hmac(self):
hmac_hashfunc(cls,
isinstance(msg,
msg.encode('utf-8')
HMACPassword(Password):
self.clstest(HMACPassword)
HMACPassword()
self.assertEquals(str(password),
Password(hashfunc=hmac_hashfunc)
TestPythonizeName(unittest.TestCase):
self.assertEqual(pythonize_name(''),
test_all_lower_case(self):
self.assertEqual(pythonize_name('lowercase'),
'lowercase')
test_all_upper_case(self):
self.assertEqual(pythonize_name('UPPERCASE'),
'uppercase')
test_camel_case(self):
self.assertEqual(pythonize_name('OriginallyCamelCased'),
'originally_camel_cased')
test_already_pythonized(self):
self.assertEqual(pythonize_name('already_pythonized'),
'already_pythonized')
test_multiple_upper_cased_letters(self):
self.assertEqual(pythonize_name('HTTPRequest'),
'http_request')
self.assertEqual(pythonize_name('RequestForHTTP'),
'request_for_http')
test_string_with_numbers(self):
self.assertEqual(pythonize_name('HTTPStatus200Ok'),
'http_status_200_ok')
TestBuildInstanceMetadataURL(unittest.TestCase):
test_normal(self):
'http://169.254.169.254/latest/meta-data/'
test_custom_path(self):
'dynamic/'
'http://169.254.169.254/latest/dynamic/'
test_custom_version(self):
'http://169.254.169.254/1.0/meta-data/'
test_custom_url(self):
'http://10.0.1.5/latest/meta-data/'
test_all_custom(self):
'2013-03-22',
'user-data'
'http://10.0.1.5/2013-03-22/user-data'
TestRetryURL(unittest.TestCase):
self.urlopen_patch
mock.patch('boto.compat.urllib.request.urlopen')
self.opener_patch
mock.patch('boto.compat.urllib.request.build_opener')
self.urlopen
self.urlopen_patch.start()
self.opener
self.opener_patch.start()
self.urlopen_patch.stop()
self.opener_patch.stop()
self.urlopen.return_value
set_no_proxy_allowed_response(self,
test_retry_url_uses_proxy(self):
self.set_normal_response('normal
self.set_no_proxy_allowed_response('no
test_retry_url_using_bytes_and_string_response(self):
test_value.encode('utf-8')
TestLazyLoadMetadata(unittest.TestCase):
self.retry_url_patch
mock.patch('boto.utils.retry_url')
self.retry_url_patch.start()
self.retry_url_patch.stop()
fake_response.side_effect
test_meta_data_with_invalid_json_format_happened_once(self):
valid_data])
self.assertEqual(list(response.values())[0],
json.loads(valid_data))
test_meta_data_with_invalid_json_format_happened_twice(self):
invalid_data])
response.values()[0]
test_user_data(self):
test_user_data_timeout(self):
get_instance_userdata(timeout=1,
num_retries=2)
timeout=1)
TestStringToDatetimeParsing(unittest.TestCase):
self._saved
'de_DE.UTF-8')
locale.Error:
self.skipTest('Unsupported
self._saved)
test_nonus_locale(self):
'Thu,
May
2014
09:06:03
datetime.datetime.strptime(test_string,
boto.utils.RFC1123)
boto.utils.parse_ts(test_string)
self.assertEqual(2014,
result.year)
result.month)
self.assertEqual(15,
result.day)
self.assertEqual(9,
result.hour)
self.assertEqual(6,
result.minute)
TestHostIsIPV6(unittest.TestCase):
test_is_ipv6_no_brackets(self):
test_is_ipv6_with_brackets(self):
test_is_ipv6_with_brackets_and_port(self):
test_is_ipv6_no_brackets_abbreviated(self):
'bf1d:cb48:4513::'
test_is_ipv6_with_brackets_abbreviated(self):
'[bf1d:cb48:4513::'
test_is_ipv6_with_brackets_and_port_abbreviated(self):
'[bf1d:cb48:4513::]:8080'
boto.utils.host_is_ipv6('')
test_not_of_string_type(self):
hostnames
{}]
hostnames:
boto.utils.host_is_ipv6(h)
test_ipv4_no_port(self):
boto.utils.host_is_ipv6('192.168.1.1')
test_ipv4_with_port(self):
boto.utils.host_is_ipv6('192.168.1.1:8080')
test_hostnames_are_not_ipv6_with_port(self):
boto.utils.host_is_ipv6('example.org:8080')
test_hostnames_are_not_ipv6_without_port(self):
boto.utils.host_is_ipv6('example.org')
TestParseHost(unittest.TestCase):
test_parses_ipv6_hosts_no_brackets(self):
test_parses_ipv6_hosts_with_brackets_stripping_them(self):
test_parses_ipv6_hosts_with_brackets_and_port(self):
test_parses_ipv4_hosts(self):
'10.0.1.1'
test_parses_ipv4_hosts_with_port(self):
'192.168.168.200:8080'
'192.168.168.200')
test_parses_hostnames_with_port(self):
'example.org:8080'
'example.org')
test_parses_hostnames_without_port(self):
'example.org'
TestDescribeCustomerGateways(AWSMockServiceTestCase):
test_get_all_customer_gateways(self):
self.service_connection.get_all_customer_gateways(
('ip-address',
'12.1.2.3')]))
'DescribeCustomerGateways',
'CustomerGatewayId.1':
'ip-address',
'12.1.2.3'},
TestCreateCustomerGateway(AWSMockServiceTestCase):
test_create_customer_gateway(self):
self.service_connection.create_customer_gateway(
'CreateCustomerGateway',
65534},
self.assertEquals(api_response.type,
self.assertEquals(api_response.ip_address,
'12.1.2.3')
self.assertEquals(api_response.bgp_asn,
TestDeleteCustomerGateway(AWSMockServiceTestCase):
test_delete_customer_gateway(self):
self.service_connection.delete_customer_gateway('cgw-b4dc3961')
'DeleteCustomerGateway',
'cgw-b4dc3961'},
TestDescribeDhcpOptions(AWSMockServiceTestCase):
test_get_all_dhcp_options(self):
self.service_connection.get_all_dhcp_options(['dopt-7a8b9c2d'],
[('key',
'domain-name')])
'DescribeDhcpOptions',
'DhcpOptionsId.1':
'domain-name'},
self.assertEquals(api_response[0].options['domain-name'],
self.assertEquals(api_response[0].options['domain-name-servers'],
TestCreateDhcpOptions(AWSMockServiceTestCase):
test_create_dhcp_options(self):
self.service_connection.create_dhcp_options(
domain_name='example.com',
domain_name_servers=['10.2.5.1',
'10.2.5.2'],
ntp_servers=('10.12.12.1',
'10.12.12.2'),
netbios_name_servers='10.20.20.1',
netbios_node_type='2')
'CreateDhcpOptions',
'DhcpConfiguration.1.Key':
'DhcpConfiguration.1.Value.1':
'DhcpConfiguration.2.Key':
'DhcpConfiguration.2.Value.1':
'10.2.5.1',
'DhcpConfiguration.2.Value.2':
'10.2.5.2',
'DhcpConfiguration.3.Key':
'DhcpConfiguration.3.Value.1':
'10.12.12.1',
'DhcpConfiguration.3.Value.2':
'10.12.12.2',
'DhcpConfiguration.4.Key':
'DhcpConfiguration.4.Value.1':
'10.20.20.1',
'DhcpConfiguration.5.Key':
'DhcpConfiguration.5.Value.1':
self.assertEquals(api_response.options['domain-name'],
self.assertEquals(api_response.options['domain-name-servers'],
self.assertEquals(api_response.options['ntp-servers'],
['10.12.12.1',
'10.12.12.2'])
self.assertEquals(api_response.options['netbios-name-servers'],
['10.20.20.1'])
self.assertEquals(api_response.options['netbios-node-type'],
['2'])
TestDeleteDhcpOptions(AWSMockServiceTestCase):
test_delete_dhcp_options(self):
self.service_connection.delete_dhcp_options('dopt-7a8b9c2d')
'DeleteDhcpOptions',
'dopt-7a8b9c2d'},
TestAssociateDhcpOptions(AWSMockServiceTestCase):
test_associate_dhcp_options(self):
self.service_connection.associate_dhcp_options(
'AssociateDhcpOptions',
TestDescribeInternetGateway(AWSMockServiceTestCase):
test_describe_internet_gateway(self):
self.service_connection.get_all_internet_gateways(
filters=[('attachment.state',
['available',
'pending'])])
'DescribeInternetGateways',
'InternetGatewayId.1':
'attachment.state',
'pending'},
'igw-eaad4883EXAMPLE')
TestCreateInternetGateway(AWSMockServiceTestCase):
test_create_internet_gateway(self):
self.service_connection.create_internet_gateway()
'CreateInternetGateway'},
self.assertEqual(api_response.id,
TestDeleteInternetGateway(AWSMockServiceTestCase):
test_delete_internet_gateway(self):
self.service_connection.delete_internet_gateway('igw-eaad4883')
'DeleteInternetGateway',
TestAttachInternetGateway(AWSMockServiceTestCase):
test_attach_internet_gateway(self):
self.service_connection.attach_internet_gateway(
'AttachInternetGateway',
TestDetachInternetGateway(AWSMockServiceTestCase):
test_detach_internet_gateway(self):
self.service_connection.detach_internet_gateway(
'DetachInternetGateway',
TestDescribeNetworkAcls(AWSMockServiceTestCase):
test_get_all_network_acls(self):
self.service_connection.get_all_network_acls(['acl-5566953c',
'acl-5d659634'],
[('vpc-id',
'vpc-5266953b')])
'DescribeNetworkAcls',
'NetworkAclId.1':
'NetworkAclId.2':
'acl-5d659634',
'vpc-5266953b'},
TestReplaceNetworkAclAssociation(AWSMockServiceTestCase):
get_all_network_acls_vpc_body
get_all_network_acls_subnet_body
test_associate_network_acl(self):
self.service_connection.associate_network_acl('acl-5fb85d36',
'subnet-ff669596')
'acl-5fb85d36',
test_disassociate_network_acl(self):
body=self.get_all_network_acls_vpc_body),
self.service_connection.disassociate_network_acl('subnet-ff669596',
'vpc-5266953b')
TestCreateNetworkAcl(AWSMockServiceTestCase):
self.service_connection.create_network_acl('vpc-11ad4878')
'CreateNetworkAcl',
'acl-5fb85d36')
DeleteCreateNetworkAcl(AWSMockServiceTestCase):
self.service_connection.delete_network_acl('acl-2cb85d45')
'DeleteNetworkAcl',
'acl-2cb85d45'},
TestCreateNetworkAclEntry(AWSMockServiceTestCase):
port_range_from=53,
port_range_to=53)
53,
53},
test_create_network_acl_icmp(self):
egress='true',
TestReplaceNetworkAclEntry(AWSMockServiceTestCase):
test_replace_network_acl(self):
port_range_from=139,
port_range_to=139)
139,
139},
test_replace_network_acl_icmp(self):
TestDeleteNetworkAclEntry(AWSMockServiceTestCase):
self.service_connection.delete_network_acl_entry('acl-2cb85d45',
egress=False)
'DeleteNetworkAclEntry',
TestGetNetworkAclAssociations(AWSMockServiceTestCase):
test_get_network_acl_associations(self):
self.service_connection.get_all_network_acls()
api_response[0].associations[0]
self.assertEqual(association.network_acl_id,
'acl-5d659634')
TestDescribeRouteTables(AWSMockServiceTestCase):
test_get_all_route_tables(self):
self.service_connection.get_all_route_tables(
['rtb-13ad487a',
'rtb-f9ad4890'],
filters=[('route.state',
'active')])
'DescribeRouteTables',
'RouteTableId.1':
'rtb-13ad487a',
'RouteTableId.2':
'rtb-f9ad4890',
'route.state',
'active'},
self.assertEquals(len(api_response[0].routes),
self.assertEquals(api_response[0].routes[0].destination_cidr_block,
self.assertEquals(api_response[0].routes[0].gateway_id,
self.assertEquals(api_response[0].routes[0].state,
self.assertEquals(api_response[0].routes[0].origin,
self.assertEquals(len(api_response[0].associations),
self.assertEquals(api_response[0].associations[0].id,
'rtbassoc-12ad487b')
self.assertEquals(api_response[0].associations[0].route_table_id,
self.assertIsNone(api_response[0].associations[0].subnet_id)
self.assertEquals(api_response[0].associations[0].main,
self.assertEquals(api_response[1].id,
self.assertEquals(len(api_response[1].routes),
self.assertEquals(api_response[1].routes[0].destination_cidr_block,
self.assertEquals(api_response[1].routes[0].gateway_id,
self.assertEquals(api_response[1].routes[0].state,
self.assertEquals(api_response[1].routes[0].origin,
self.assertEquals(api_response[1].routes[1].destination_cidr_block,
self.assertEquals(api_response[1].routes[1].gateway_id,
self.assertEquals(api_response[1].routes[1].state,
self.assertEquals(api_response[1].routes[1].origin,
self.assertEquals(api_response[1].routes[2].destination_cidr_block,
'10.0.0.0/21')
self.assertEquals(api_response[1].routes[2].interface_id,
'eni-884ec1d1')
self.assertEquals(api_response[1].routes[2].state,
self.assertEquals(api_response[1].routes[2].origin,
self.assertEquals(api_response[1].routes[3].destination_cidr_block,
'11.0.0.0/22')
self.assertEquals(api_response[1].routes[3].vpc_peering_connection_id,
'pcx-efc52b86')
self.assertEquals(api_response[1].routes[3].state,
self.assertEquals(api_response[1].routes[3].origin,
self.assertEquals(len(api_response[1].associations),
self.assertEquals(api_response[1].associations[0].id,
self.assertEquals(api_response[1].associations[0].route_table_id,
self.assertEquals(api_response[1].associations[0].subnet_id,
self.assertEquals(api_response[1].associations[0].main,
TestAssociateRouteTable(AWSMockServiceTestCase):
test_associate_route_table(self):
self.service_connection.associate_route_table(
'AssociateRouteTable',
'subnet-15ad487c'},
'rtbassoc-f8ad4891')
TestDisassociateRouteTable(AWSMockServiceTestCase):
test_disassociate_route_table(self):
self.service_connection.disassociate_route_table('rtbassoc-fdad4894')
'DisassociateRouteTable',
'rtbassoc-fdad4894'},
TestCreateRouteTable(AWSMockServiceTestCase):
test_create_route_table(self):
self.service_connection.create_route_table('vpc-11ad4878')
'CreateRouteTable',
self.assertEquals(len(api_response.routes),
self.assertEquals(api_response.routes[0].destination_cidr_block,
self.assertEquals(api_response.routes[0].gateway_id,
self.assertEquals(api_response.routes[0].state,
TestDeleteRouteTable(AWSMockServiceTestCase):
test_delete_route_table(self):
self.service_connection.delete_route_table('rtb-e4ad488d')
'DeleteRouteTable',
'rtb-e4ad488d'},
TestReplaceRouteTableAssociation(AWSMockServiceTestCase):
test_replace_route_table_assocation(self):
self.service_connection.replace_route_table_assocation(
test_replace_route_table_association_with_assoc(self):
self.service_connection.replace_route_table_association_with_assoc(
TestCreateRoute(AWSMockServiceTestCase):
test_create_route_gateway(self):
test_create_route_instance(self):
test_create_route_interface(self):
test_create_route_vpc_peering_connection(self):
TestReplaceRoute(AWSMockServiceTestCase):
test_replace_route_gateway(self):
test_replace_route_instance(self):
test_replace_route_interface(self):
test_replace_route_vpc_peering_connection(self):
TestDeleteRoute(AWSMockServiceTestCase):
test_delete_route(self):
self.service_connection.delete_route('rtb-e4ad488d',
'172.16.1.0/24')
'DeleteRoute',
'172.16.1.0/24'},
TestDescribeSubnets(AWSMockServiceTestCase):
test_get_all_subnets(self):
self.service_connection.get_all_subnets(
'subnet-6e7f829e'],
'available'),
('vpc-id',
'subnet-6e7f829e'])]))
'DescribeSubnets',
'SubnetId.1':
'SubnetId.2':
'subnet-6e7f829e',
'Filter.2.Value.2':
'subnet-6e7f829e'},
self.assertEqual(api_response[1].id,
'subnet-6e7f829e')
TestCreateSubnet(AWSMockServiceTestCase):
test_create_subnet(self):
self.service_connection.create_subnet(
'CreateSubnet',
'10.0.1.0/24')
self.assertEquals(api_response.available_ip_address_count,
251)
self.assertEquals(api_response.availability_zone,
TestDeleteSubnet(AWSMockServiceTestCase):
test_delete_subnet(self):
self.service_connection.delete_subnet('subnet-9d4a7b6c')
'DeleteSubnet',
'subnet-9d4a7b6c'},
<DescribeVpcsResponse
<requestId>623040d1-b51c-40bc-8080-93486f38d03d</requestId>
<vpcSet>
<vpcId>vpc-12345678</vpcId>
<cidrBlock>172.16.0.0/16</cidrBlock>
<dhcpOptionsId>dopt-12345678</dhcpOptionsId>
<instanceTenancy>default</instanceTenancy>
<isDefault>false</isDefault>
</vpcSet>
</DescribeVpcsResponse>'''
TestDescribeVPCs(AWSMockServiceTestCase):
self.service_connection.get_all_vpcs()
self.assertFalse(vpc.is_default)
self.assertEqual(vpc.instance_tenancy,
TestCreateVpc(AWSMockServiceTestCase):
test_create_vpc(self):
self.service_connection.create_vpc('10.0.0.0/16',
'CreateVpc',
'10.0.0.0/16'},
'10.0.0.0/16')
self.assertEquals(api_response.dhcp_options_id,
'dopt-1a2b3c4d2')
self.assertEquals(api_response.instance_tenancy,
TestDeleteVpc(AWSMockServiceTestCase):
test_delete_vpc(self):
self.service_connection.delete_vpc('vpc-1a2b3c4d')
'DeleteVpc',
TestModifyVpcAttribute(AWSMockServiceTestCase):
test_modify_vpc_attribute_dns_support(self):
enable_dns_support=True)
'EnableDnsSupport.Value':
test_modify_vpc_attribute_dns_hostnames(self):
enable_dns_hostnames=True)
'EnableDnsHostnames.Value':
TestGetAllClassicLinkVpc(AWSMockServiceTestCase):
test_get_all_classic_link_vpcs(self):
self.service_connection.get_all_classic_link_vpcs()
self.assertEqual(vpc.id,
'vpc-6226ab07')
self.assertEqual(vpc.classic_link_enabled,
self.assertEqual(vpc.tags,
test_get_all_classic_link_vpcs_params(self):
self.service_connection.get_all_classic_link_vpcs(
vpc_ids=['id1',
'VpcId.2':
TestVpcClassicLink(AWSMockServiceTestCase):
super(TestVpcClassicLink,
VPC(self.service_connection)
'myid'
self.vpc.id
TestAttachClassicLinkVpc(TestVpcClassicLink):
test_attach_classic_link_instance_string_groups(self):
['sg-foo',
'sg-bar']
test_attach_classic_link_instance_object_groups(self):
sec_group_1
sec_group_1.id
'sg-foo'
sec_group_2
sec_group_2.id
'sg-bar'
[sec_group_1,
sec_group_2]
TestDetachClassicLinkVpc(TestVpcClassicLink):
test_detach_classic_link_instance(self):
self.vpc.detach_classic_instance(
'DetachClassicLinkVpc',
TestEnableClassicLinkVpc(TestVpcClassicLink):
self.vpc.enable_classic_link(
'EnableVpcClassicLink',
TestDisableClassicLinkVpc(TestVpcClassicLink):
self.vpc.disable_classic_link(
'DisableVpcClassicLink',
TestUpdateClassicLinkVpc(TestVpcClassicLink):
test_vpc_update_classic_link_enabled(self):
self.vpc.classic_link_enabled
self.vpc.update_classic_link_enabled(
validate=True
self.assertEqual(self.vpc.classic_link_enabled,
VpcPeeringConnection,
TestDescribeVpcPeeringConnections(AWSMockServiceTestCase):
test_get_vpc_peering_connections(self):
self.service_connection.get_all_vpc_peering_connections(
['pcx-111aaa22',
'pcx-444bbb88'],
filters=[('status-code',
['pending-acceptance'])])
api_response:
vpc_peering_connection.id
'pcx-111aaa22':
'pcx-111aaa22')
111122223333')
'172.31.0.0/16')
'111122223333')
'vpc-aa22cc33')
'2014-02-17T16:00:50.000Z')
'pcx-444bbb88')
98654313')
'1237897234')
'vpc-2398abcd')
'172.30.0.0/16')
'98654313')
'vpc-0983bcda')
'2015-02-17T16:00:50.000Z')
TestCreateVpcPeeringConnection(AWSMockServiceTestCase):
CREATE_VPC_PEERING_CONNECTION=
self.CREATE_VPC_PEERING_CONNECTION
test_create_vpc_peering_connection(self):
self.service_connection.create_vpc_peering_connection('vpc-1a2b3c4d',
'vpc-a1b2c3d4',
'pcx-73a5401a')
'initiating-request')
'Initiating
123456789012')
'vpc-a1b2c3d4')
'2014-02-18T14:37:25.000Z')
TestDeleteVpcPeeringConnection(AWSMockServiceTestCase):
self.assertEquals(self.service_connection.delete_vpc_peering_connection('pcx-12345678'),
TestDeleteVpcPeeringConnectionShortForm(unittest.TestCase):
vpc_conn
VPCConnection(aws_access_key_id='aws_access_key_id',
vpc_peering_connections
vpc_conn.get_all_vpc_peering_connections()
len(vpc_peering_connections))
vpc_peering_connections[0]
self.assertEquals(True,
vpc_peering_connection.delete())
self.assertIn('DeleteVpcPeeringConnection',
self.assertNotIn('DeleteVpc',
TestRejectVpcPeeringConnection(AWSMockServiceTestCase):
REJECT_VPC_PEERING_CONNECTION=
self.REJECT_VPC_PEERING_CONNECTION
test_reject_vpc_peering_connection(self):
self.assertEquals(self.service_connection.reject_vpc_peering_connection('pcx-12345678'),
TestAcceptVpcPeeringConnection(AWSMockServiceTestCase):
ACCEPT_VPC_PEERING_CONNECTION=
self.ACCEPT_VPC_PEERING_CONNECTION
test_accept_vpc_peering_connection(self):
self.service_connection.accept_vpc_peering_connection('pcx-1a2b3c4d')
'pcx-1a2b3c4d')
'Active')
'vpc-111aaa22')
self.assertEqual(vpc_peering_connection.accepter_vpc_info.cidr_block,
'10.0.1.0/28')
<DescribeVpnConnectionsResponse
<requestId>12345678-asdf-ghjk-zxcv-0987654321nb</requestId>
<vpnConnectionSet>
<vpnConnectionId>vpn-12qw34er56ty</vpnConnectionId>
<customerGatewayId>cgw-1234qwe9</customerGatewayId>
<vpnGatewayId>vgw-lkjh1234</vpnGatewayId>
<tagSet>
<key>Name</key>
<value>VPN
1</value>
</tagSet>
<outsideIpAddress>123.45.67.89</outsideIpAddress>
<status>DOWN</status>
<lastStatusChange>2013-03-19T19:20:34.000Z</lastStatusChange>
<outsideIpAddress>123.45.67.90</outsideIpAddress>
<lastStatusChange>2013-03-20T08:00:14.000Z</lastStatusChange>
<destinationCidrBlock>192.168.0.0/24</destinationCidrBlock>
<vpnConnectionId>vpn-qwerty12</vpnConnectionId>
<customerGatewayId>cgw-01234567</customerGatewayId>
<vpnGatewayId>vgw-asdfghjk</vpnGatewayId>
<outsideIpAddress>134.56.78.78</outsideIpAddress>
<lastStatusChange>2013-03-20T01:46:30.000Z</lastStatusChange>
<outsideIpAddress>134.56.78.79</outsideIpAddress>
<lastStatusChange>2013-03-19T19:23:59.000Z</lastStatusChange>
<destinationCidrBlock>10.0.0.0/16</destinationCidrBlock>
</vpnConnectionSet>
</DescribeVpnConnectionsResponse>'''
TestDescribeVPNConnections(AWSMockServiceTestCase):
self.service_connection.get_all_vpn_connections(
['vpn-12qw34er56ty',
'vpn-qwerty12'],
filters=[('state',
'available'])])
'DescribeVpnConnections',
'VpnConnectionId.1':
'vpn-12qw34er56ty',
'VpnConnectionId.2':
'vpn-qwerty12',
'available'},
vpn0
self.assertEqual(vpn0.type,
self.assertEqual(vpn0.customer_gateway_id,
'cgw-1234qwe9')
self.assertEqual(vpn0.vpn_gateway_id,
'vgw-lkjh1234')
self.assertEqual(len(vpn0.tunnels),
self.assertDictEqual(vpn0.tags,
'VPN
1'})
vpn1
api_response[1]
self.assertEqual(vpn1.state,
self.assertEqual(len(vpn1.static_routes),
self.assertTrue(vpn1.options.static_routes_only)
self.assertEqual(vpn1.tunnels[0].status,
self.assertEqual(vpn1.tunnels[1].status,
self.assertDictEqual(vpn1.tags,
self.assertEqual(vpn1.static_routes[0].source,
'static')
self.assertEqual(vpn1.static_routes[0].state,
TestCreateVPNConnection(AWSMockServiceTestCase):
test_create_vpn_connection(self):
self.service_connection.create_vpn_connection(
static_routes_only=True)
'CreateVpnConnection',
'Options.StaticRoutesOnly':
self.assertEquals(api_response.customer_gateway_id,
self.assertEquals(api_response.options.static_routes_only,
TestDeleteVPNConnection(AWSMockServiceTestCase):
test_delete_vpn_connection(self):
self.service_connection.delete_vpn_connection('vpn-44a8938f')
'DeleteVpnConnection',
'vpn-44a8938f'},
TestCreateVPNConnectionRoute(AWSMockServiceTestCase):
test_create_vpn_connection_route(self):
self.service_connection.create_vpn_connection_route(
'CreateVpnConnectionRoute',
TestDeleteVPNConnectionRoute(AWSMockServiceTestCase):
test_delete_vpn_connection_route(self):
self.service_connection.delete_vpn_connection_route(
'DeleteVpnConnectionRoute',
TestDescribeVpnGateways(AWSMockServiceTestCase):
test_get_all_vpn_gateways(self):
self.service_connection.get_all_vpn_gateways(
('availability-zone',
'us-east-1a')]))
'DescribeVpnGateways',
'VpnGatewayId.1':
'availability-zone',
TestCreateVpnGateway(AWSMockServiceTestCase):
self.service_connection.create_vpn_gateway('ipsec.1',
'CreateVpnGateway',
'ipsec.1'},
TestDeleteVpnGateway(AWSMockServiceTestCase):
self.service_connection.delete_vpn_gateway('vgw-8db04f81')
'DeleteVpnGateway',
'vgw-8db04f81'},
TestAttachVpnGateway(AWSMockServiceTestCase):
test_attach_vpn_gateway(self):
self.service_connection.attach_vpn_gateway('vgw-8db04f81',
'AttachVpnGateway',
'attaching')
TestDetachVpnGateway(AWSMockServiceTestCase):
test_detach_vpn_gateway(self):
self.service_connection.detach_vpn_gateway('vgw-8db04f81',
'DetachVpnGateway',
TestDisableVgwRoutePropagation(AWSMockServiceTestCase):
test_disable_vgw_route_propagation(self):
self.service_connection.disable_vgw_route_propagation(
'DisableVgwRoutePropagation',
TestEnableVgwRoutePropagation(AWSMockServiceTestCase):
test_enable_vgw_route_propagation(self):
self.service_connection.enable_vgw_route_propagation(
'EnableVgwRoutePropagation',

